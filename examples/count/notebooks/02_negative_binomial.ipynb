{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Overdispersion and Negative Binomial Regression\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Recognize and diagnose** overdispersion in count data\n",
    "2. **Understand** the Negative Binomial distribution and NB2 parametrization\n",
    "3. **Estimate** Negative Binomial regression models using PanelBox\n",
    "4. **Perform** likelihood ratio tests comparing Poisson vs NB\n",
    "5. **Interpret** the dispersion parameter $\\alpha$\n",
    "6. **Choose** between Poisson and NB models appropriately\n",
    "\n",
    "### Prerequisites\n",
    "- Completed Notebook 01 (Poisson Introduction)\n",
    "- Understanding of variance-mean relationship\n",
    "- Familiarity with likelihood ratio tests\n",
    "\n",
    "### Duration\n",
    "- **Estimated time**: 60 minutes\n",
    "- **Sections**: 7 main sections\n",
    "\n",
    "### Dataset\n",
    "- **File**: `firm_patents.csv` — Patent counts for 1,500 manufacturing firms over 5 years (7,500 obs)\n",
    "- **Key feature**: Severe overdispersion (Var/Mean $\\approx$ 18)\n",
    "\n",
    "### References\n",
    "- Cameron, A. C., & Trivedi, P. K. (2013). *Regression Analysis of Count Data*. Cambridge University Press.\n",
    "- Hilbe, J. M. (2011). *Negative Binomial Regression*. Cambridge University Press."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from scipy.special import gammaln\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PanelBox imports\n",
    "import statsmodels.api as sm\n",
    "from panelbox.models.count import PooledPoisson, NegativeBinomial\n",
    "\n",
    "# Visualization configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures' / '02_negbin'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables' / '02_negbin'\n",
    "\n",
    "# Create output directories\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Setup complete!')\n",
    "print(f'Figures will be saved to: {FIGURES_DIR}')\n",
    "print(f'Tables will be saved to: {TABLES_DIR}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Data Loading and Exploration\n",
    "\n",
    "We use a dataset of patent counts for manufacturing firms. This data exhibits **severe overdispersion** — a violation of the Poisson equidispersion assumption that motivates the Negative Binomial model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(DATA_DIR / 'firm_patents.csv')\n",
    "\n",
    "print('Dataset shape:', df.shape)\n",
    "print(f'N firms: {df[\"firm_id\"].nunique()}')\n",
    "print(f'T years: {df[\"year\"].nunique()} ({df[\"year\"].min()}-{df[\"year\"].max()})')\n",
    "print()\n",
    "print('First rows:')\n",
    "display(df.head(10))\n",
    "\n",
    "print('\\nVariable types:')\n",
    "print(df.dtypes)\n",
    "\n",
    "print('\\nSummary statistics:')\n",
    "display(df.describe().round(2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: The Problem of Equidispersion\n",
    "\n",
    "### Poisson Equidispersion Assumption\n",
    "\n",
    "Recall from Notebook 01 that the Poisson model assumes:\n",
    "\n",
    "$$\\text{Var}[Y|\\mathbf{X}] = E[Y|\\mathbf{X}] = \\mu$$\n",
    "\n",
    "This is called **equidispersion**: the conditional variance equals the conditional mean.\n",
    "\n",
    "### Why Is This a Problem?\n",
    "\n",
    "In practice, count data often violates this assumption. **Overdispersion** occurs when:\n",
    "\n",
    "$$\\text{Var}[Y|\\mathbf{X}] > E[Y|\\mathbf{X}]$$\n",
    "\n",
    "#### Common causes of overdispersion:\n",
    "1. **Unobserved heterogeneity**: Firms differ in unmeasured ways that affect patenting\n",
    "2. **Clustering**: Patents may come in \"bunches\" (e.g., from a single research program)\n",
    "3. **Omitted variables**: Important predictors missing from the model\n",
    "4. **Contagion**: One patent may lead to others (cumulative innovation)\n",
    "\n",
    "#### Consequences of ignoring overdispersion:\n",
    "- Standard errors are **too small** (underestimated)\n",
    "- Test statistics are **inflated**\n",
    "- Confidence intervals are **too narrow**\n",
    "- Leads to **spurious significance** — finding effects that don't exist"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Explore the distribution of patents\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Histogram of patent counts\n",
    "ax = axes[0]\n",
    "max_display = int(min(df['patents'].quantile(0.99), 40))\n",
    "bins = np.arange(0, max_display + 2) - 0.5\n",
    "ax.hist(df['patents'].clip(upper=max_display), bins=bins,\n",
    "        color='steelblue', edgecolor='white', alpha=0.8)\n",
    "ax.set_xlabel('Number of Patents')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Patent Counts')\n",
    "\n",
    "# Add mean and variance annotations\n",
    "mean_pat = df['patents'].mean()\n",
    "var_pat = df['patents'].var()\n",
    "ax.axvline(mean_pat, color='red', linestyle='--', linewidth=2, label=f'Mean = {mean_pat:.1f}')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "textstr = f'Mean = {mean_pat:.2f}\\nVar = {var_pat:.2f}\\nVar/Mean = {var_pat/mean_pat:.1f}'\n",
    "ax.text(0.95, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', horizontalalignment='right',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# 2. Boxplot by year\n",
    "ax = axes[1]\n",
    "df.boxplot(column='patents', by='year', ax=ax,\n",
    "           flierprops=dict(markersize=2, alpha=0.3))\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Number of Patents')\n",
    "ax.set_title('Patents by Year')\n",
    "plt.sca(ax)\n",
    "plt.title('Patents by Year')\n",
    "\n",
    "# 3. Log-scale histogram\n",
    "ax = axes[2]\n",
    "ax.hist(np.log1p(df['patents']), bins=30, color='darkgreen',\n",
    "        edgecolor='white', alpha=0.8)\n",
    "ax.set_xlabel('log(1 + Patents)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Log-transformed Distribution')\n",
    "\n",
    "fig.suptitle('')  # Remove auto-generated suptitle from boxplot\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'patent_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f'Mean patents: {mean_pat:.2f}')\n",
    "print(f'Variance: {var_pat:.2f}')\n",
    "print(f'Overdispersion index (Var/Mean): {var_pat/mean_pat:.1f}')\n",
    "print(f'Zero count: {(df[\"patents\"] == 0).sum()} ({100*(df[\"patents\"]==0).mean():.1f}%)')\n",
    "print(f'Max patents: {df[\"patents\"].max()}')\n",
    "print(f'Median patents: {df[\"patents\"].median():.0f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Variance-Mean Relationship by Firm\n",
    "# ============================================================\n",
    "\n",
    "# Compute within-firm variance and mean\n",
    "firm_stats = df.groupby('firm_id')['patents'].agg(['mean', 'var']).dropna()\n",
    "firm_stats.columns = ['mean', 'variance']\n",
    "firm_stats = firm_stats[firm_stats['variance'] > 0]  # Need positive variance\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Scatter plot\n",
    "ax.scatter(firm_stats['mean'], firm_stats['variance'],\n",
    "           alpha=0.3, s=20, color='steelblue', label='Firm-level data')\n",
    "\n",
    "# 45-degree line (Poisson reference)\n",
    "ref_range = np.linspace(0, firm_stats['mean'].max() * 1.1, 100)\n",
    "ax.plot(ref_range, ref_range, 'r--', linewidth=2,\n",
    "        label='Poisson: Var = Mean (45-degree line)')\n",
    "\n",
    "# NB2 reference: Var = mu + alpha*mu^2\n",
    "alpha_approx = 2.0\n",
    "ax.plot(ref_range, ref_range + alpha_approx * ref_range**2, 'g-', linewidth=2,\n",
    "        label=f'NB2: Var = mu + {alpha_approx}*mu^2', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Within-firm Mean', fontsize=12)\n",
    "ax.set_ylabel('Within-firm Variance', fontsize=12)\n",
    "ax.set_title('Variance vs Mean by Firm\\n(Points above 45-degree line indicate overdispersion)', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Set sensible axis limits\n",
    "ax.set_xlim(0, firm_stats['mean'].quantile(0.98) * 1.1)\n",
    "ax.set_ylim(0, firm_stats['variance'].quantile(0.98) * 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'variance_mean_firms.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Percentage of firms with Var > Mean\n",
    "pct_over = (firm_stats['variance'] > firm_stats['mean']).mean() * 100\n",
    "print(f'\\nPercentage of firms with Var > Mean: {pct_over:.1f}%')\n",
    "print(f'Median within-firm Var/Mean ratio: {(firm_stats[\"variance\"]/firm_stats[\"mean\"]).median():.1f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Demonstrate Consequences: Fit Poisson Model\n",
    "# ============================================================\n",
    "\n",
    "# Prepare data\n",
    "df['log_rd'] = np.log(df['rd_spending'])\n",
    "df['log_emp'] = np.log(df['employees'])\n",
    "\n",
    "y = df['patents'].values\n",
    "X_vars = df[['log_rd', 'log_emp', 'firm_age', 'tech_sector',\n",
    "             'public_funding', 'international']].values\n",
    "X = sm.add_constant(X_vars)\n",
    "\n",
    "var_names = ['const', 'log_rd', 'log_emp', 'firm_age',\n",
    "             'tech_sector', 'public_funding', 'international']\n",
    "\n",
    "# Fit Poisson model\n",
    "print('Fitting Poisson model...')\n",
    "print('=' * 60)\n",
    "poisson_model = PooledPoisson(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    entity_id=df['firm_id'].values,\n",
    "    time_id=df['year'].values\n",
    ")\n",
    "poisson_result = poisson_model.fit(se_type='cluster')\n",
    "\n",
    "# Store log-likelihood for later\n",
    "poisson_llf = poisson_model.llf\n",
    "\n",
    "# Display basic results\n",
    "print(f'Log-likelihood: {poisson_llf:.2f}')\n",
    "print(f'Number of observations: {len(y)}')\n",
    "print()\n",
    "\n",
    "# Create coefficient table\n",
    "poisson_se = np.sqrt(np.diag(poisson_result.vcov))\n",
    "poisson_t = poisson_result.params / poisson_se\n",
    "poisson_p = 2 * (1 - stats.norm.cdf(np.abs(poisson_t)))\n",
    "\n",
    "poisson_table = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Coefficient': poisson_result.params,\n",
    "    'Std. Error': poisson_se,\n",
    "    'z-statistic': poisson_t,\n",
    "    'p-value': poisson_p\n",
    "})\n",
    "\n",
    "def add_stars(p):\n",
    "    if p < 0.001: return '***'\n",
    "    elif p < 0.01: return '**'\n",
    "    elif p < 0.05: return '*'\n",
    "    else: return ''\n",
    "\n",
    "poisson_table['Sig'] = poisson_table['p-value'].apply(add_stars)\n",
    "\n",
    "print('Poisson Regression Results')\n",
    "print('=' * 70)\n",
    "display(poisson_table.round(4))\n",
    "print('\\nSignificance: *** p<0.001, ** p<0.01, * p<0.05')\n",
    "\n",
    "# Compute overdispersion index\n",
    "fitted_poisson = np.exp(X @ poisson_result.params)\n",
    "pearson_resid = (y - fitted_poisson) / np.sqrt(fitted_poisson)\n",
    "disp_index = np.sum(pearson_resid**2) / (len(y) - len(var_names))\n",
    "print(f'\\nPearson dispersion statistic: {disp_index:.2f}')\n",
    "print(f'(Should be ~1 under Poisson; values >> 1 indicate overdispersion)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Detecting Overdispersion\n",
    "\n",
    "Before fitting a Negative Binomial model, we should formally test for overdispersion. Several diagnostic methods are available:\n",
    "\n",
    "1. **Overdispersion Index**: Simple ratio Var(y)/E(y)\n",
    "2. **Pearson Dispersion**: Sum of squared Pearson residuals / (n - k)\n",
    "3. **Cameron-Trivedi Test**: Regression-based auxiliary test\n",
    "4. **Visual Diagnostics**: Rootogram and residual plots"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Method 1: Cameron-Trivedi Test (manual implementation)\n",
    "# ============================================================\n",
    "\n",
    "# The Cameron-Trivedi test is based on an auxiliary regression:\n",
    "#   (y - mu)^2 - y = alpha * mu^2 + error\n",
    "# If alpha > 0, overdispersion is present.\n",
    "#\n",
    "# Alternatively (simpler form):\n",
    "#   [(y - mu)^2 - y] / mu = alpha * mu + error\n",
    "\n",
    "mu_hat = fitted_poisson\n",
    "\n",
    "# Auxiliary dependent variable\n",
    "aux_y = ((y - mu_hat)**2 - y) / mu_hat\n",
    "\n",
    "# Auxiliary regressor\n",
    "aux_x = mu_hat.reshape(-1, 1)\n",
    "\n",
    "# OLS regression (no intercept)\n",
    "from numpy.linalg import lstsq\n",
    "slope, _, _, _ = lstsq(aux_x, aux_y, rcond=None)\n",
    "alpha_ct = slope[0]\n",
    "\n",
    "# Standard error via OLS\n",
    "residuals_ct = aux_y - alpha_ct * mu_hat\n",
    "se_ct = np.sqrt(np.sum(residuals_ct**2) / (len(y) - 1)) / np.sqrt(np.sum(mu_hat**2))\n",
    "t_stat_ct = alpha_ct / se_ct\n",
    "p_value_ct = 2 * (1 - stats.norm.cdf(np.abs(t_stat_ct)))\n",
    "\n",
    "print('Cameron-Trivedi Test for Overdispersion')\n",
    "print('=' * 50)\n",
    "print(f'H0: Var(Y|X) = E(Y|X)  (equidispersion)')\n",
    "print(f'H1: Var(Y|X) = E(Y|X) + alpha * E(Y|X)^2')\n",
    "print()\n",
    "print(f'Estimated alpha: {alpha_ct:.4f}')\n",
    "print(f'Standard error:  {se_ct:.4f}')\n",
    "print(f't-statistic:     {t_stat_ct:.2f}')\n",
    "print(f'p-value:         {p_value_ct:.6f}')\n",
    "print()\n",
    "if p_value_ct < 0.001:\n",
    "    print('RESULT: Strong evidence of overdispersion (p < 0.001)')\n",
    "    print('=> Poisson model is inadequate. Consider Negative Binomial.')\n",
    "elif p_value_ct < 0.05:\n",
    "    print('RESULT: Evidence of overdispersion (p < 0.05)')\n",
    "else:\n",
    "    print('RESULT: No significant evidence of overdispersion')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Method 2: Multiple Overdispersion Diagnostics\n",
    "# ============================================================\n",
    "\n",
    "# 1. Unconditional overdispersion index\n",
    "unconditional_oi = df['patents'].var() / df['patents'].mean()\n",
    "\n",
    "# 2. Pearson dispersion (conditional)\n",
    "pearson_disp = np.sum(pearson_resid**2) / (len(y) - len(var_names))\n",
    "\n",
    "# 3. Deviance-based\n",
    "deviance_contribs = np.where(\n",
    "    y > 0,\n",
    "    2 * (y * np.log(y / mu_hat) - (y - mu_hat)),\n",
    "    2 * mu_hat\n",
    ")\n",
    "deviance = np.sum(deviance_contribs)\n",
    "deviance_disp = deviance / (len(y) - len(var_names))\n",
    "\n",
    "# Create diagnostic summary table\n",
    "diagnostics = pd.DataFrame({\n",
    "    'Diagnostic': [\n",
    "        'Unconditional Var/Mean',\n",
    "        'Pearson Dispersion',\n",
    "        'Deviance Dispersion',\n",
    "        'Cameron-Trivedi alpha',\n",
    "        'Cameron-Trivedi p-value'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f'{unconditional_oi:.2f}',\n",
    "        f'{pearson_disp:.2f}',\n",
    "        f'{deviance_disp:.2f}',\n",
    "        f'{alpha_ct:.4f}',\n",
    "        f'{p_value_ct:.2e}'\n",
    "    ],\n",
    "    'Expected (Poisson)': [\n",
    "        '~1.0',\n",
    "        '~1.0',\n",
    "        '~1.0',\n",
    "        '~0.0',\n",
    "        '> 0.05'\n",
    "    ],\n",
    "    'Conclusion': [\n",
    "        'Severe overdispersion' if unconditional_oi > 2 else 'Mild',\n",
    "        'Severe overdispersion' if pearson_disp > 2 else 'Mild',\n",
    "        'Severe overdispersion' if deviance_disp > 2 else 'Mild',\n",
    "        'Significant' if p_value_ct < 0.05 else 'Not significant',\n",
    "        'Reject Poisson' if p_value_ct < 0.05 else 'Fail to reject'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print('Overdispersion Diagnostic Summary')\n",
    "print('=' * 80)\n",
    "display(diagnostics)\n",
    "\n",
    "# Save table\n",
    "diagnostics.to_csv(TABLES_DIR / 'table_01_overdispersion_tests.csv', index=False)\n",
    "print(f'\\nSaved to {TABLES_DIR / \"table_01_overdispersion_tests.csv\"}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Visual Diagnostics for Overdispersion\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "# 1. Rootogram: Observed vs Poisson-predicted frequencies\n",
    "ax = axes[0]\n",
    "max_count = min(int(df['patents'].quantile(0.95)), 30)\n",
    "observed_freq = np.bincount(np.minimum(y, max_count))\n",
    "counts = np.arange(len(observed_freq))\n",
    "\n",
    "# Expected under Poisson\n",
    "expected_freq = np.zeros(len(observed_freq))\n",
    "for k in range(len(observed_freq)):\n",
    "    if k < max_count:\n",
    "        expected_freq[k] = len(y) * np.mean(stats.poisson.pmf(k, mu_hat))\n",
    "    else:\n",
    "        expected_freq[k] = len(y) * np.mean(1 - stats.poisson.cdf(k - 1, mu_hat))\n",
    "\n",
    "bar_width = 0.35\n",
    "ax.bar(counts - bar_width/2, np.sqrt(observed_freq), bar_width,\n",
    "       label='Observed', color='steelblue', alpha=0.8)\n",
    "ax.bar(counts + bar_width/2, np.sqrt(expected_freq), bar_width,\n",
    "       label='Poisson predicted', color='coral', alpha=0.8)\n",
    "ax.set_xlabel('Patent Count')\n",
    "ax.set_ylabel('sqrt(Frequency)')\n",
    "ax.set_title('Rootogram: Observed vs Poisson')\n",
    "ax.legend(fontsize=9)\n",
    "ax.set_xlim(-0.5, max_count + 0.5)\n",
    "\n",
    "# 2. Pearson residuals vs fitted\n",
    "ax = axes[1]\n",
    "ax.scatter(np.log(mu_hat + 0.5), pearson_resid, alpha=0.1, s=5, color='steelblue')\n",
    "ax.axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "ax.axhline(y=2, color='orange', linestyle=':', linewidth=1)\n",
    "ax.axhline(y=-2, color='orange', linestyle=':', linewidth=1)\n",
    "ax.set_xlabel('log(Fitted Values)')\n",
    "ax.set_ylabel('Pearson Residuals')\n",
    "ax.set_title('Pearson Residuals vs Fitted Values')\n",
    "pct_outside = (np.abs(pearson_resid) > 2).mean() * 100\n",
    "ax.text(0.05, 0.95, f'{pct_outside:.1f}% outside +/-2',\n",
    "        transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# 3. Binned variance-mean diagnostic\n",
    "ax = axes[2]\n",
    "n_bins = 15\n",
    "bin_edges = np.percentile(mu_hat, np.linspace(0, 100, n_bins + 1))\n",
    "bin_edges = np.unique(bin_edges)\n",
    "bin_idx = np.digitize(mu_hat, bin_edges) - 1\n",
    "bin_idx = np.clip(bin_idx, 0, len(bin_edges) - 2)\n",
    "\n",
    "bin_means = []\n",
    "bin_vars = []\n",
    "for b in range(len(bin_edges) - 1):\n",
    "    mask = bin_idx == b\n",
    "    if mask.sum() > 10:\n",
    "        bin_means.append(np.mean(y[mask]))\n",
    "        bin_vars.append(np.var(y[mask]))\n",
    "\n",
    "bin_means = np.array(bin_means)\n",
    "bin_vars = np.array(bin_vars)\n",
    "\n",
    "ax.scatter(bin_means, bin_vars, s=60, color='steelblue',\n",
    "           edgecolor='navy', zorder=5, label='Binned data')\n",
    "ref = np.linspace(0, max(bin_means) * 1.1, 100)\n",
    "ax.plot(ref, ref, 'r--', linewidth=2, label='Poisson: Var = Mean')\n",
    "ax.set_xlabel('Binned Mean')\n",
    "ax.set_ylabel('Binned Variance')\n",
    "ax.set_title('Binned Variance vs Mean')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'overdispersion_diagnostic.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('All diagnostics point to severe overdispersion.')\n",
    "print('The Poisson model is inadequate for this data.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: The Negative Binomial Model\n",
    "\n",
    "### NB2 Parametrization\n",
    "\n",
    "The **Negative Binomial (NB2)** model generalizes the Poisson by adding a dispersion parameter $\\alpha$:\n",
    "\n",
    "$$E[Y|\\mathbf{X}] = \\mu = \\exp(\\mathbf{X}'\\boldsymbol{\\beta})$$\n",
    "\n",
    "$$\\text{Var}[Y|\\mathbf{X}] = \\mu + \\alpha \\cdot \\mu^2$$\n",
    "\n",
    "### Key Properties:\n",
    "\n",
    "| Parameter | Interpretation |\n",
    "|-----------|---------------|\n",
    "| $\\alpha = 0$ | Reduces to Poisson (equidispersion) |\n",
    "| $\\alpha > 0$ | Overdispersion present |\n",
    "| Small $\\alpha$ (~0.1) | Mild overdispersion |\n",
    "| Large $\\alpha$ (~2+) | Severe overdispersion |\n",
    "\n",
    "### NB1 vs NB2\n",
    "\n",
    "- **NB1**: $\\text{Var} = \\mu(1 + \\alpha)$ — variance is linear in mean\n",
    "- **NB2**: $\\text{Var} = \\mu + \\alpha\\mu^2$ — variance is quadratic in mean (more common)\n",
    "\n",
    "PanelBox implements the **NB2** parametrization, which is the standard in econometrics.\n",
    "\n",
    "### Log-likelihood\n",
    "\n",
    "The NB2 log-likelihood for observation $i$ is:\n",
    "\n",
    "$$\\ell_i = \\ln\\Gamma(y_i + r) - \\ln\\Gamma(y_i + 1) - \\ln\\Gamma(r) + r\\ln\\left(\\frac{r}{r + \\mu_i}\\right) + y_i\\ln\\left(\\frac{\\mu_i}{r + \\mu_i}\\right)$$\n",
    "\n",
    "where $r = 1/\\alpha$ is the shape parameter and $\\Gamma(\\cdot)$ is the gamma function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Estimate Negative Binomial Model\n",
    "# ============================================================\n",
    "\n",
    "print('Fitting Negative Binomial (NB2) model...')\n",
    "print('=' * 60)\n",
    "\n",
    "nb_model = NegativeBinomial(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    entity_id=df['firm_id'].values,\n",
    "    time_id=df['year'].values\n",
    ")\n",
    "nb_result = nb_model.fit()\n",
    "\n",
    "print(f'Converged: {nb_result.converged}')\n",
    "print(f'Log-likelihood: {nb_result.llf:.2f}')\n",
    "print(f'Number of observations: {len(y)}')\n",
    "print()\n",
    "\n",
    "# Extract and display alpha\n",
    "print('Dispersion Parameter')\n",
    "print('-' * 40)\n",
    "print(f'Alpha (a):     {nb_result.alpha:.4f}')\n",
    "print(f'log(Alpha):    {np.log(nb_result.alpha):.4f}')\n",
    "\n",
    "# SE of alpha from last element of vcov\n",
    "alpha_se = np.sqrt(nb_result.vcov[-1, -1]) * nb_result.alpha  # Delta method\n",
    "alpha_ci_low = nb_result.alpha * np.exp(-1.96 * np.sqrt(nb_result.vcov[-1, -1]))\n",
    "alpha_ci_high = nb_result.alpha * np.exp(1.96 * np.sqrt(nb_result.vcov[-1, -1]))\n",
    "\n",
    "print(f'SE(Alpha):     {alpha_se:.4f}')\n",
    "print(f'95% CI:        [{alpha_ci_low:.4f}, {alpha_ci_high:.4f}]')\n",
    "print()\n",
    "\n",
    "# Interpretation\n",
    "if nb_result.alpha < 0.5:\n",
    "    severity = 'mild'\n",
    "elif nb_result.alpha < 2.0:\n",
    "    severity = 'moderate'\n",
    "else:\n",
    "    severity = 'severe'\n",
    "print(f'Interpretation: Alpha = {nb_result.alpha:.2f} indicates {severity} overdispersion.')\n",
    "print(f'At mean count = {mean_pat:.1f}: Var/Mean = 1 + alpha*mean = {1 + nb_result.alpha*mean_pat:.1f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# NB Results Table\n",
    "# ============================================================\n",
    "\n",
    "# Extract coefficients (excluding alpha)\n",
    "nb_params = nb_result.params_exog\n",
    "nb_se = np.sqrt(np.diag(nb_result.vcov))[:-1]  # Exclude alpha SE\n",
    "nb_t = nb_params / nb_se\n",
    "nb_p = 2 * (1 - stats.norm.cdf(np.abs(nb_t)))\n",
    "\n",
    "nb_table = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Coefficient': nb_params,\n",
    "    'Std. Error': nb_se,\n",
    "    'z-statistic': nb_t,\n",
    "    'p-value': nb_p\n",
    "})\n",
    "\n",
    "nb_table['Sig'] = nb_table['p-value'].apply(add_stars)\n",
    "\n",
    "print('Negative Binomial (NB2) Regression Results')\n",
    "print('=' * 70)\n",
    "display(nb_table.round(4))\n",
    "print(f'\\nAlpha (dispersion): {nb_result.alpha:.4f}')\n",
    "print(f'Log-likelihood: {nb_result.llf:.2f}')\n",
    "print('Significance: *** p<0.001, ** p<0.01, * p<0.05')\n",
    "\n",
    "# Save\n",
    "nb_table.to_csv(TABLES_DIR / 'table_02_nb_estimates.csv', index=False)\n",
    "print(f'\\nSaved to {TABLES_DIR / \"table_02_nb_estimates.csv\"}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Compare Poisson vs NB Coefficients\n",
    "# ============================================================\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Poisson_Coef': poisson_result.params,\n",
    "    'Poisson_SE': poisson_se,\n",
    "    'NB_Coef': nb_params,\n",
    "    'NB_SE': nb_se,\n",
    "    'SE_Ratio': nb_se / poisson_se\n",
    "})\n",
    "\n",
    "print('Coefficient Comparison: Poisson vs Negative Binomial')\n",
    "print('=' * 80)\n",
    "display(comparison.round(4))\n",
    "print()\n",
    "print('Key observations:')\n",
    "print(f'  - Average SE ratio (NB/Poisson): {comparison[\"SE_Ratio\"].mean():.2f}')\n",
    "print(f'  - NB standard errors are generally larger (corrected for overdispersion)')\n",
    "print(f'  - Coefficients may differ slightly due to different likelihood functions')\n",
    "\n",
    "comparison.to_csv(TABLES_DIR / 'table_03_poisson_vs_nb_coefs.csv', index=False)\n",
    "print(f'\\nSaved to {TABLES_DIR / \"table_03_poisson_vs_nb_coefs.csv\"}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Visualization: Fit Comparison and Coefficient Forest Plot\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Observed vs Fitted distributions\n",
    "ax = axes[0]\n",
    "max_count = min(int(df['patents'].quantile(0.95)), 25)\n",
    "observed_freq = np.bincount(np.minimum(y, max_count))\n",
    "counts_range = np.arange(len(observed_freq))\n",
    "\n",
    "# Poisson predicted frequencies\n",
    "mu_pois = np.exp(X @ poisson_result.params)\n",
    "pois_freq = np.array([\n",
    "    len(y) * np.mean(stats.poisson.pmf(k, mu_pois)) if k < max_count\n",
    "    else len(y) * np.mean(1 - stats.poisson.cdf(k - 1, mu_pois))\n",
    "    for k in range(len(observed_freq))\n",
    "])\n",
    "\n",
    "# NB predicted frequencies\n",
    "mu_nb = np.exp(X @ nb_result.params_exog)\n",
    "r_nb = 1.0 / nb_result.alpha\n",
    "nb_freq = np.array([\n",
    "    len(y) * np.mean(stats.nbinom.pmf(k, r_nb, r_nb / (r_nb + mu_nb))) if k < max_count\n",
    "    else len(y) * np.mean(1 - stats.nbinom.cdf(k - 1, r_nb, r_nb / (r_nb + mu_nb)))\n",
    "    for k in range(len(observed_freq))\n",
    "])\n",
    "\n",
    "width = 0.25\n",
    "ax.bar(counts_range - width, observed_freq, width, label='Observed',\n",
    "       color='steelblue', alpha=0.8)\n",
    "ax.bar(counts_range, pois_freq, width, label='Poisson',\n",
    "       color='coral', alpha=0.8)\n",
    "ax.bar(counts_range + width, nb_freq, width, label='Neg. Binomial',\n",
    "       color='seagreen', alpha=0.8)\n",
    "ax.set_xlabel('Patent Count')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Observed vs Predicted Distributions')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlim(-0.5, max_count + 0.5)\n",
    "\n",
    "# 2. Forest plot comparing coefficients\n",
    "ax = axes[1]\n",
    "n_vars = len(var_names) - 1  # Exclude constant\n",
    "y_pos = np.arange(n_vars)\n",
    "var_labels = var_names[1:]  # Exclude constant\n",
    "\n",
    "# Plot Poisson\n",
    "ax.errorbar(poisson_result.params[1:], y_pos + 0.1,\n",
    "            xerr=1.96 * poisson_se[1:],\n",
    "            fmt='o', color='coral', markersize=8, capsize=4,\n",
    "            label='Poisson', linewidth=2)\n",
    "\n",
    "# Plot NB\n",
    "ax.errorbar(nb_params[1:], y_pos - 0.1,\n",
    "            xerr=1.96 * nb_se[1:],\n",
    "            fmt='s', color='seagreen', markersize=8, capsize=4,\n",
    "            label='Neg. Binomial', linewidth=2)\n",
    "\n",
    "ax.axvline(x=0, color='gray', linestyle='--', linewidth=1)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(var_labels)\n",
    "ax.set_xlabel('Coefficient (95% CI)')\n",
    "ax.set_title('Coefficient Comparison: Poisson vs NB')\n",
    "ax.legend(fontsize=10, loc='lower right')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'nb_fit_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(FIGURES_DIR / 'coefficient_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('NB model captures the dispersion much better than Poisson.')\n",
    "print('Note: NB confidence intervals are wider (more honest about uncertainty).')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Likelihood Ratio Test — Poisson vs Negative Binomial\n",
    "\n",
    "### Nested Models\n",
    "\n",
    "The Poisson model is a **special case** of the NB model when $\\alpha = 0$. This nesting allows us to use a **likelihood ratio (LR) test**.\n",
    "\n",
    "### LR Test Statistic\n",
    "\n",
    "$$LR = 2 \\left( \\ell_{\\text{NB}} - \\ell_{\\text{Poisson}} \\right)$$\n",
    "\n",
    "Under $H_0: \\alpha = 0$:\n",
    "\n",
    "$$LR \\sim \\chi^2(1)$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Large LR** (small p-value): Reject Poisson in favor of NB\n",
    "- **Small LR** (large p-value): Poisson is adequate\n",
    "\n",
    "**Note**: Since $\\alpha \\geq 0$ (on the boundary of the parameter space), the test is conservative. The true distribution is a mixture of $\\chi^2(0)$ and $\\chi^2(1)$, so the reported p-value is an upper bound."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Likelihood Ratio Test: PanelBox Built-in\n",
    "# ============================================================\n",
    "\n",
    "print('Likelihood Ratio Test: Poisson vs Negative Binomial')\n",
    "print('=' * 60)\n",
    "\n",
    "# Use PanelBox's built-in LR test\n",
    "lr_test = nb_result.lr_test_poisson()\n",
    "\n",
    "print(f'H0: alpha = 0 (Poisson is adequate)')\n",
    "print(f'H1: alpha > 0 (NB is needed)')\n",
    "print()\n",
    "print(f'LR statistic:    {lr_test[\"statistic\"]:.2f}')\n",
    "print(f'Degrees of freedom: {lr_test[\"df\"]}')\n",
    "print(f'p-value:         {lr_test[\"pvalue\"]:.2e}')\n",
    "print(f'Conclusion:      {lr_test[\"conclusion\"]}')\n",
    "print()\n",
    "\n",
    "# Log-likelihoods\n",
    "print(f'Log-likelihood (Poisson): {lr_test[\"llf_restricted\"]:.2f}')\n",
    "print(f'Log-likelihood (NB):      {lr_test[\"llf_unrestricted\"]:.2f}')\n",
    "print(f'Difference:               {lr_test[\"llf_unrestricted\"] - lr_test[\"llf_restricted\"]:.2f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Manual LR Test Computation (Pedagogical)\n",
    "# ============================================================\n",
    "\n",
    "# For understanding, let's compute the LR test step by step\n",
    "\n",
    "# Step 1: Get log-likelihoods\n",
    "ll_poisson = lr_test['llf_restricted']\n",
    "ll_nb = lr_test['llf_unrestricted']\n",
    "\n",
    "print('Step-by-step LR Test Computation')\n",
    "print('=' * 50)\n",
    "print(f'Step 1: Log-likelihoods')\n",
    "print(f'  LL(Poisson) = {ll_poisson:.2f}')\n",
    "print(f'  LL(NB)      = {ll_nb:.2f}')\n",
    "\n",
    "# Step 2: Compute LR statistic\n",
    "LR = 2 * (ll_nb - ll_poisson)\n",
    "print(f'\\nStep 2: LR statistic = 2 * ({ll_nb:.2f} - ({ll_poisson:.2f}))')\n",
    "print(f'  LR = {LR:.2f}')\n",
    "\n",
    "# Step 3: Compare to chi-squared distribution\n",
    "p_value = 1 - stats.chi2.cdf(LR, df=1)\n",
    "print(f'\\nStep 3: Compare to chi-squared(1)')\n",
    "print(f'  chi-squared(1) critical value at 5%: {stats.chi2.ppf(0.95, 1):.2f}')\n",
    "print(f'  LR statistic: {LR:.2f}')\n",
    "print(f'  p-value: {p_value:.2e}')\n",
    "\n",
    "# Step 4: Decision\n",
    "print(f'\\nStep 4: Decision')\n",
    "if p_value < 0.001:\n",
    "    print(f'  LR = {LR:.2f} >> {stats.chi2.ppf(0.95, 1):.2f} (critical value)')\n",
    "    print(f'  p < 0.001: STRONGLY reject Poisson in favor of NB')\n",
    "elif p_value < 0.05:\n",
    "    print(f'  p < 0.05: Reject Poisson in favor of NB')\n",
    "else:\n",
    "    print(f'  p = {p_value:.4f}: Fail to reject Poisson')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# AIC/BIC Comparison\n",
    "# ============================================================\n",
    "\n",
    "# Poisson: k parameters (betas only)\n",
    "k_pois = len(var_names)\n",
    "aic_pois = -2 * ll_poisson + 2 * k_pois\n",
    "bic_pois = -2 * ll_poisson + np.log(len(y)) * k_pois\n",
    "\n",
    "# NB: k+1 parameters (betas + alpha)\n",
    "k_nb = len(var_names) + 1\n",
    "aic_nb = -2 * ll_nb + 2 * k_nb\n",
    "bic_nb = -2 * ll_nb + np.log(len(y)) * k_nb\n",
    "\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Metric': ['Log-Likelihood', 'Parameters', 'AIC', 'BIC', 'LR Statistic', 'LR p-value'],\n",
    "    'Poisson': [f'{ll_poisson:.2f}', k_pois, f'{aic_pois:.2f}', f'{bic_pois:.2f}', '--', '--'],\n",
    "    'Neg. Binomial': [f'{ll_nb:.2f}', k_nb, f'{aic_nb:.2f}', f'{bic_nb:.2f}',\n",
    "                      f'{LR:.2f}', f'{p_value:.2e}']\n",
    "})\n",
    "\n",
    "print('Model Comparison: Poisson vs Negative Binomial')\n",
    "print('=' * 70)\n",
    "display(model_comparison)\n",
    "\n",
    "print(f'\\nAIC difference: {aic_pois - aic_nb:.1f} (lower is better -> NB wins)')\n",
    "print(f'BIC difference: {bic_pois - bic_nb:.1f} (lower is better -> NB wins)')\n",
    "\n",
    "# Save table\n",
    "model_comparison.to_csv(TABLES_DIR / 'table_04_model_comparison.csv', index=False)\n",
    "print(f'\\nSaved to {TABLES_DIR / \"table_04_model_comparison.csv\"}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# AIC/BIC Visual Comparison\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# AIC comparison\n",
    "ax = axes[0]\n",
    "models = ['Poisson', 'Negative\\nBinomial']\n",
    "aic_values = [aic_pois, aic_nb]\n",
    "colors = ['coral', 'seagreen']\n",
    "bars = ax.bar(models, aic_values, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.set_ylabel('AIC')\n",
    "ax.set_title('AIC Comparison\\n(Lower is Better)')\n",
    "\n",
    "# Annotate values\n",
    "for bar, val in zip(bars, aic_values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(aic_values)*0.01,\n",
    "            f'{val:.0f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# BIC comparison\n",
    "ax = axes[1]\n",
    "bic_values = [bic_pois, bic_nb]\n",
    "bars = ax.bar(models, bic_values, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.set_ylabel('BIC')\n",
    "ax.set_title('BIC Comparison\\n(Lower is Better)')\n",
    "\n",
    "for bar, val in zip(bars, bic_values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(bic_values)*0.01,\n",
    "            f'{val:.0f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'aic_bic_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'NB model is strongly preferred on both AIC and BIC.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Model Interpretation and Application\n",
    "\n",
    "### Incidence Rate Ratios (IRRs)\n",
    "\n",
    "For the NB model, as with Poisson, we interpret coefficients through **Incidence Rate Ratios**:\n",
    "\n",
    "$$IRR_j = \\exp(\\beta_j)$$\n",
    "\n",
    "**Interpretation**:\n",
    "- $IRR = 1.0$: No effect\n",
    "- $IRR = 1.5$: 50% increase in expected count\n",
    "- $IRR = 0.8$: 20% decrease in expected count\n",
    "\n",
    "For **log-transformed** covariates (like log R&D), $\\beta$ is an **elasticity**: a 1% increase in R&D is associated with a $\\beta$% change in expected patents."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Incidence Rate Ratios for NB Model\n",
    "# ============================================================\n",
    "\n",
    "irr = np.exp(nb_params)\n",
    "irr_ci_low = np.exp(nb_params - 1.96 * nb_se)\n",
    "irr_ci_high = np.exp(nb_params + 1.96 * nb_se)\n",
    "pct_change = (irr - 1) * 100\n",
    "\n",
    "irr_table = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Coefficient': nb_params,\n",
    "    'IRR': irr,\n",
    "    'IRR_CI_Low': irr_ci_low,\n",
    "    'IRR_CI_High': irr_ci_high,\n",
    "    'Pct_Change': pct_change,\n",
    "    'p-value': nb_p,\n",
    "    'Sig': nb_table['Sig'].values\n",
    "})\n",
    "\n",
    "print('Incidence Rate Ratios (NB Model)')\n",
    "print('=' * 80)\n",
    "display(irr_table.round(4))\n",
    "print('\\nSignificance: *** p<0.001, ** p<0.01, * p<0.05')\n",
    "\n",
    "irr_table.to_csv(TABLES_DIR / 'table_05_irr_nb.csv', index=False)\n",
    "print(f'\\nSaved to {TABLES_DIR / \"table_05_irr_nb.csv\"}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Substantive Interpretation\n",
    "# ============================================================\n",
    "\n",
    "print('Substantive Interpretation of NB Results')\n",
    "print('=' * 60)\n",
    "print()\n",
    "\n",
    "# R&D spending (log)\n",
    "rd_coef = nb_params[var_names.index('log_rd')]\n",
    "rd_irr = np.exp(rd_coef)\n",
    "print(f'1. R&D Spending (elasticity = {rd_coef:.3f}):')\n",
    "print(f'   A 10% increase in R&D spending is associated with a')\n",
    "print(f'   {((1.10**rd_coef - 1) * 100):.1f}% increase in expected patents.')\n",
    "print(f'   Doubling R&D: +{((2**rd_coef - 1) * 100):.1f}% more patents.')\n",
    "print()\n",
    "\n",
    "# Employees (log)\n",
    "emp_coef = nb_params[var_names.index('log_emp')]\n",
    "print(f'2. Firm Size (elasticity = {emp_coef:.3f}):')\n",
    "print(f'   A 10% increase in employees is associated with a')\n",
    "print(f'   {((1.10**emp_coef - 1) * 100):.1f}% increase in expected patents.')\n",
    "print()\n",
    "\n",
    "# Tech sector\n",
    "tech_coef = nb_params[var_names.index('tech_sector')]\n",
    "tech_irr = np.exp(tech_coef)\n",
    "print(f'3. Tech Sector (IRR = {tech_irr:.3f}):')\n",
    "print(f'   High-tech firms produce {(tech_irr - 1) * 100:.1f}% more patents')\n",
    "print(f'   than traditional manufacturing firms, all else equal.')\n",
    "print()\n",
    "\n",
    "# Public funding\n",
    "fund_coef = nb_params[var_names.index('public_funding')]\n",
    "fund_irr = np.exp(fund_coef)\n",
    "print(f'4. Public Funding (IRR = {fund_irr:.3f}):')\n",
    "print(f'   Firms receiving public R&D funding produce {(fund_irr - 1) * 100:.1f}%')\n",
    "print(f'   more patents than those without.')\n",
    "print()\n",
    "\n",
    "# International\n",
    "intl_coef = nb_params[var_names.index('international')]\n",
    "intl_irr = np.exp(intl_coef)\n",
    "print(f'5. International Collaboration (IRR = {intl_irr:.3f}):')\n",
    "print(f'   Firms with international R&D collaborations produce')\n",
    "print(f'   {(intl_irr - 1) * 100:.1f}% more patents.')\n",
    "print()\n",
    "\n",
    "# Firm age\n",
    "age_coef = nb_params[var_names.index('firm_age')]\n",
    "print(f'6. Firm Age (coef = {age_coef:.4f}):')\n",
    "print(f'   Each additional year of firm age is associated with a')\n",
    "print(f'   {(np.exp(age_coef) - 1) * 100:.2f}% change in expected patents.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Predictions: Poisson vs NB\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Add small jitter for visualization\n",
    "jitter = np.random.uniform(-0.3, 0.3, len(y))\n",
    "\n",
    "# 1. Poisson predictions\n",
    "ax = axes[0]\n",
    "mu_pois = np.exp(X @ poisson_result.params)\n",
    "ax.scatter(mu_pois, y + jitter, alpha=0.05, s=5, color='coral')\n",
    "max_val = min(np.percentile(y, 99), np.percentile(mu_pois, 99))\n",
    "ax.plot([0, max_val], [0, max_val], 'k--', linewidth=2, label='Perfect fit')\n",
    "ax.set_xlabel('Poisson Predicted', fontsize=12)\n",
    "ax.set_ylabel('Observed Patents', fontsize=12)\n",
    "ax.set_title('Poisson: Predicted vs Observed')\n",
    "ax.set_xlim(0, max_val * 1.1)\n",
    "ax.set_ylim(-1, min(df['patents'].quantile(0.99), max_val * 2))\n",
    "ax.legend()\n",
    "corr_pois = np.corrcoef(mu_pois, y)[0, 1]\n",
    "ax.text(0.05, 0.95, f'Corr = {corr_pois:.3f}', transform=ax.transAxes,\n",
    "        fontsize=11, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# 2. NB predictions (same conditional mean)\n",
    "ax = axes[1]\n",
    "mu_nb = np.exp(X @ nb_result.params_exog)\n",
    "ax.scatter(mu_nb, y + jitter, alpha=0.05, s=5, color='seagreen')\n",
    "ax.plot([0, max_val], [0, max_val], 'k--', linewidth=2, label='Perfect fit')\n",
    "ax.set_xlabel('NB Predicted', fontsize=12)\n",
    "ax.set_ylabel('Observed Patents', fontsize=12)\n",
    "ax.set_title('Negative Binomial: Predicted vs Observed')\n",
    "ax.set_xlim(0, max_val * 1.1)\n",
    "ax.set_ylim(-1, min(df['patents'].quantile(0.99), max_val * 2))\n",
    "ax.legend()\n",
    "corr_nb = np.corrcoef(mu_nb, y)[0, 1]\n",
    "ax.text(0.05, 0.95, f'Corr = {corr_nb:.3f}', transform=ax.transAxes,\n",
    "        fontsize=11, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'predictions_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Correlation with observed:')\n",
    "print(f'  Poisson: {corr_pois:.4f}')\n",
    "print(f'  NB:      {corr_nb:.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: When to Use Each Model\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "| Criterion | Use Poisson | Use Negative Binomial |\n",
    "|-----------|------------|----------------------|\n",
    "| Overdispersion index | $\\approx$ 1 | >> 1 |\n",
    "| Cameron-Trivedi test | Not significant | Significant |\n",
    "| LR test | -- | Reject Poisson |\n",
    "| AIC/BIC | Lower for Poisson | Lower for NB |\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "1. **Always check overdispersion first** — compute Var/Mean ratio and run diagnostic tests\n",
    "2. **If in doubt, use NB** — it is more general and reduces to Poisson when appropriate\n",
    "3. **Use robust standard errors regardless** — provides additional protection against misspecification\n",
    "4. **Consider the data generating process** — think about why overdispersion might occur\n",
    "\n",
    "### When Poisson is Sufficient\n",
    "- Equidispersion approximately holds (index close to 1)\n",
    "- Diagnostic tests do not reject equidispersion\n",
    "- With robust/cluster standard errors for additional safety\n",
    "\n",
    "### When NB is Preferred\n",
    "- Clear overdispersion (index >> 1)\n",
    "- LR test rejects Poisson\n",
    "- Better AIC/BIC for NB\n",
    "- Theory suggests unobserved heterogeneity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Model Selection Flowchart\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(5, 9.5, 'Count Data Model Selection', fontsize=16, fontweight='bold',\n",
    "        ha='center', va='center')\n",
    "\n",
    "# Decision boxes\n",
    "box_props = dict(boxstyle='round,pad=0.5', facecolor='lightblue', edgecolor='navy', alpha=0.8)\n",
    "decision_props = dict(boxstyle='round,pad=0.4', facecolor='lightyellow', edgecolor='orange', alpha=0.8)\n",
    "result_props_green = dict(boxstyle='round,pad=0.4', facecolor='lightgreen', edgecolor='green', alpha=0.8)\n",
    "result_props_red = dict(boxstyle='round,pad=0.4', facecolor='mistyrose', edgecolor='red', alpha=0.8)\n",
    "\n",
    "# Start\n",
    "ax.text(5, 8.5, 'Count outcome variable Y >= 0', fontsize=11,\n",
    "        ha='center', va='center', bbox=box_props)\n",
    "\n",
    "# Arrow\n",
    "ax.annotate('', xy=(5, 7.6), xytext=(5, 8.1),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2))\n",
    "\n",
    "# Decision 1: Check overdispersion\n",
    "ax.text(5, 7.2, 'Compute Var(Y)/Mean(Y)\\nand Cameron-Trivedi test', fontsize=10,\n",
    "        ha='center', va='center', bbox=decision_props)\n",
    "\n",
    "# Branches\n",
    "ax.annotate('', xy=(2.5, 6.0), xytext=(4.0, 6.8),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2))\n",
    "ax.text(3.0, 6.5, 'Index ~ 1', fontsize=9, ha='center', color='green')\n",
    "\n",
    "ax.annotate('', xy=(7.5, 6.0), xytext=(6.0, 6.8),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2))\n",
    "ax.text(7.0, 6.5, 'Index >> 1', fontsize=9, ha='center', color='red')\n",
    "\n",
    "# Left branch: Poisson okay\n",
    "ax.text(2.5, 5.5, 'Poisson model\\n(with robust SEs)', fontsize=10,\n",
    "        ha='center', va='center', bbox=result_props_green)\n",
    "\n",
    "# Right branch: Fit NB\n",
    "ax.text(7.5, 5.5, 'Fit Negative Binomial\\n(NB2 model)', fontsize=10,\n",
    "        ha='center', va='center', bbox=box_props)\n",
    "\n",
    "# Arrow from NB\n",
    "ax.annotate('', xy=(7.5, 4.4), xytext=(7.5, 5.0),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2))\n",
    "\n",
    "# Decision 2: LR test\n",
    "ax.text(7.5, 4.0, 'LR test: Poisson vs NB\\nCompare AIC/BIC', fontsize=10,\n",
    "        ha='center', va='center', bbox=decision_props)\n",
    "\n",
    "# Branches from LR test\n",
    "ax.annotate('', xy=(5.5, 2.8), xytext=(6.5, 3.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2))\n",
    "ax.text(5.7, 3.3, 'Fail to\\nreject', fontsize=9, ha='center', color='green')\n",
    "\n",
    "ax.annotate('', xy=(9.0, 2.8), xytext=(8.5, 3.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2))\n",
    "ax.text(9.0, 3.3, 'Reject\\nPoisson', fontsize=9, ha='center', color='red')\n",
    "\n",
    "# Results\n",
    "ax.text(5.5, 2.3, 'Use Poisson\\n(with robust SEs)', fontsize=10,\n",
    "        ha='center', va='center', bbox=result_props_green)\n",
    "\n",
    "ax.text(9.0, 2.3, 'Use NB model', fontsize=10,\n",
    "        ha='center', va='center', bbox=result_props_red)\n",
    "\n",
    "# Additional note\n",
    "ax.text(5, 0.8, 'Note: Consider zero-inflated models if excess zeros are structural\\n'\n",
    "        'and panel FE/RE models for unobserved heterogeneity (see Notebooks 03, 05)',\n",
    "        fontsize=9, ha='center', va='center', style='italic',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'model_selection_flowchart.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Overdispersion is common** in real count data — always check before trusting Poisson results\n",
    "2. **NB2 relaxes equidispersion**: $\\text{Var}[Y|\\mathbf{X}] = \\mu + \\alpha\\mu^2$\n",
    "3. **$\\alpha$ measures overdispersion** — when $\\alpha = 0$, NB reduces to Poisson\n",
    "4. **LR test** provides a formal comparison: Poisson (restricted) vs NB (unrestricted)\n",
    "5. **NB is typically preferred** for highly dispersed count data like patent counts\n",
    "6. **Coefficients have the same interpretation** as Poisson (IRRs via $\\exp(\\beta)$), but SEs are corrected\n",
    "\n",
    "### PanelBox Workflow\n",
    "\n",
    "```python\n",
    "# Step 1: Check overdispersion\n",
    "poisson = PooledPoisson(y, X, entity_id=firms, time_id=years)\n",
    "pois_result = poisson.fit(se_type='cluster')\n",
    "# Check overdispersion index, run Cameron-Trivedi test\n",
    "\n",
    "# Step 2: Fit NB model\n",
    "nb = NegativeBinomial(y, X, entity_id=firms, time_id=years)\n",
    "nb_result = nb.fit()\n",
    "\n",
    "# Step 3: Compare models\n",
    "lr_test = nb_result.lr_test_poisson()\n",
    "# Compare AIC, BIC\n",
    "\n",
    "# Step 4: Interpret\n",
    "irr = np.exp(nb_result.params_exog)  # Incidence Rate Ratios\n",
    "```\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- **Notebook 03**: Fixed and Random Effects for panel count data — addressing unobserved firm heterogeneity\n",
    "- **Notebook 05**: Zero-inflated models — when excess zeros have a structural explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Sensitivity to Specification\n",
    "Re-estimate the NB model using only `log_rd` and `log_emp` as regressors. Does the dispersion parameter $\\alpha$ change? What does this tell you about omitted variable bias vs. inherent overdispersion?\n",
    "\n",
    "### Exercise 2: Predicted Probabilities\n",
    "Using the NB model, compute $P(Y = 0)$ for two types of firms: (a) a small non-tech firm with low R&D, and (b) a large tech firm with high R&D. Compare with the Poisson predictions. Which model gives more realistic zero probabilities?\n",
    "\n",
    "### Exercise 3: Subset Analysis\n",
    "Split the data into tech-sector and non-tech firms. Estimate separate NB models for each group. Do the overdispersion parameters differ? What does this suggest about the sources of overdispersion?\n",
    "\n",
    "### Exercise 4: Robust Standard Errors\n",
    "The NB model uses MLE standard errors by default. Compute cluster-robust standard errors (by firm) and compare. Are the conclusions affected?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Exercise Solutions (template)\n",
    "# ============================================================\n",
    "\n",
    "# Exercise 1: Sensitivity to specification\n",
    "# Hint:\n",
    "# X_reduced = sm.add_constant(df[['log_rd', 'log_emp']].values)\n",
    "# nb_reduced = NegativeBinomial(endog=y, exog=X_reduced)\n",
    "# nb_reduced_result = nb_reduced.fit()\n",
    "# print(f'Alpha (full model): {nb_result.alpha:.4f}')\n",
    "# print(f'Alpha (reduced model): {nb_reduced_result.alpha:.4f}')\n",
    "\n",
    "print('Complete the exercises above to deepen your understanding!')\n",
    "print('Solutions are available in the solutions notebook.')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}
