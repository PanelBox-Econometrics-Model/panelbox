{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Fixed and Random Effects for Panel Count Data\n",
    "\n",
    "**Tutorial 03 - Count Models Series**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. Understand **unobserved heterogeneity** in panel count data\n",
    "2. Implement **Fixed Effects Poisson** via conditional MLE (Hausman, Hall, & Griliches, 1984)\n",
    "3. Estimate **Random Effects** count models (Gamma and Normal distributions)\n",
    "4. Compare Pooled, FE, and RE approaches\n",
    "5. Perform the **Hausman test** for specification choice\n",
    "6. Apply these methods to **policy evaluation** (crime and policing)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completion of Tutorials 01-02 (Poisson and NB models)\n",
    "- Understanding of panel data structure (within vs between variation)\n",
    "- Familiarity with fixed effects in linear models\n",
    "\n",
    "**Estimated Duration:** 75 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Heterogeneity in Panel Count Data](#1-heterogeneity)\n",
    "2. [Fixed Effects Poisson - Conditional MLE](#2-fe-poisson)\n",
    "3. [Random Effects Poisson/NB](#3-re-poisson)\n",
    "4. [FE vs RE - Hausman Test](#4-hausman)\n",
    "5. [Application: Police and Crime](#5-application)\n",
    "6. [Comparison Summary and Best Practices](#6-summary)\n",
    "7. [Summary](#7-takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-md",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from scipy.special import gammaln\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PanelBox imports\n",
    "from panelbox.models.count import (\n",
    "    PooledPoisson,\n",
    "    PoissonFixedEffects,\n",
    "    RandomEffectsPoisson\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = Path('../data')\n",
    "OUTPUT_PATH = Path('../outputs')\n",
    "FIGURES_PATH = OUTPUT_PATH / 'figures' / '03_fe_re'\n",
    "TABLES_PATH = OUTPUT_PATH / 'tables' / '03_fe_re'\n",
    "\n",
    "# Create directories\n",
    "FIGURES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helper: significance stars\n",
    "def add_stars(p):\n",
    "    if p < 0.001: return '***'\n",
    "    elif p < 0.01: return '**'\n",
    "    elif p < 0.05: return '*'\n",
    "    else: return ''\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-load-md",
   "metadata": {},
   "source": [
    "### Load City Crime Panel Data\n",
    "\n",
    "We use a balanced panel of **150 cities** observed over **10 years** (2010-2019).  \n",
    "The outcome variable is `crime_count` and key predictors include policing, unemployment, and income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH / 'city_crime.csv')\n",
    "\n",
    "print('Dataset Shape:', df.shape)\n",
    "print(f'\\nPanel Structure:')\n",
    "print(f'  Number of cities: {df[\"city_id\"].nunique()}')\n",
    "print(f'  Number of years: {df[\"year\"].nunique()}')\n",
    "print(f'  Time period: {df[\"year\"].min()} - {df[\"year\"].max()}')\n",
    "print(f'  Total observations: {len(df)}')\n",
    "print(f'  Balanced panel: {len(df) == df[\"city_id\"].nunique() * df[\"year\"].nunique()}')\n",
    "\n",
    "print('\\nDescriptive Statistics:')\n",
    "display(df.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s1-title",
   "metadata": {},
   "source": [
    "## 1. Heterogeneity in Panel Count Data {#1-heterogeneity}\n",
    "\n",
    "### The Fundamental Problem\n",
    "\n",
    "We observe $i = 1, \\ldots, N$ cities over $t = 1, \\ldots, T$ time periods.  \n",
    "The count outcome $y_{it}$ follows:\n",
    "\n",
    "$$\\log E[y_{it} \\mid X_{it}, \\alpha_i] = X_{it}'\\beta + \\alpha_i$$\n",
    "\n",
    "where $\\alpha_i$ captures **unobserved city-specific effects**:\n",
    "- Historical crime culture\n",
    "- Geographic and institutional features\n",
    "- Social capital and informal norms\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "If $\\alpha_i$ is **correlated** with regressors $X_{it}$ (e.g., high-crime cities hire more police),  \n",
    "the pooled model suffers from **omitted variable bias**.\n",
    "\n",
    "### Within vs Between Variation\n",
    "\n",
    "- **Between variation**: How cities differ from each other on average\n",
    "- **Within variation**: How each city changes over time\n",
    "- FE uses only within variation; RE uses both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s1-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualize Panel Structure ===\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Crime time series for selected cities\n",
    "np.random.seed(42)\n",
    "sample_cities = np.random.choice(df['city_id'].unique(), 10, replace=False)\n",
    "sample_cities.sort()\n",
    "for city in sample_cities:\n",
    "    city_data = df[df['city_id'] == city].sort_values('year')\n",
    "    axes[0, 0].plot(city_data['year'], city_data['crime_count'],\n",
    "                    marker='o', linewidth=1.5, markersize=4, alpha=0.8,\n",
    "                    label=f'City {city}')\n",
    "axes[0, 0].set_xlabel('Year', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Crime Count', fontsize=11)\n",
    "axes[0, 0].set_title('Crime Trends: 10 Selected Cities', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=7, ncol=2)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Distribution of city means (between variation)\n",
    "city_means = df.groupby('city_id')['crime_count'].mean()\n",
    "axes[0, 1].hist(city_means, bins=25, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[0, 1].axvline(city_means.mean(), color='red', linestyle='--',\n",
    "                   linewidth=2, label=f'Overall mean = {city_means.mean():.1f}')\n",
    "axes[0, 1].set_xlabel('Mean Crime Count', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Number of Cities', fontsize=11)\n",
    "axes[0, 1].set_title('Between-City Variation', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=9)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Within-city variation: deviations from city mean\n",
    "df['crime_demeaned'] = df['crime_count'] - df.groupby('city_id')['crime_count'].transform('mean')\n",
    "axes[1, 0].hist(df['crime_demeaned'], bins=40, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[1, 0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Crime Count - City Mean', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 0].set_title('Within-City Variation', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Variance decomposition\n",
    "city_means_aligned = df.groupby('city_id')['crime_count'].transform('mean')\n",
    "within_var = ((df['crime_count'] - city_means_aligned) ** 2).mean()\n",
    "between_var = ((city_means_aligned - df['crime_count'].mean()) ** 2).mean()\n",
    "total_var = df['crime_count'].var()\n",
    "\n",
    "var_data = pd.DataFrame({\n",
    "    'Component': ['Within\\nCities', 'Between\\nCities', 'Total'],\n",
    "    'Variance': [within_var, between_var, total_var]\n",
    "})\n",
    "bars = axes[1, 1].bar(var_data['Component'], var_data['Variance'],\n",
    "               alpha=0.8, color=['steelblue', 'coral', 'darkgreen'], edgecolor='black')\n",
    "for bar, val in zip(bars, var_data['Variance']):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "                   f'{val:.0f}', ha='center', fontsize=10, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Variance', fontsize=11)\n",
    "axes[1, 1].set_title('Variance Decomposition', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'crime_time_series.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nVariance Decomposition:')\n",
    "print(f'  Within-city variance:  {within_var:.1f} ({within_var/total_var:.1%})')\n",
    "print(f'  Between-city variance: {between_var:.1f} ({between_var/total_var:.1%})')\n",
    "print(f'  Total variance:        {total_var:.1f}')\n",
    "print(f'\\n  => Strong between-city heterogeneity suggests alpha_i matters!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s1-pooled",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fit Pooled Poisson (Baseline) ===\n",
    "var_names = ['unemployment_rate', 'police_per_capita', 'median_income', 'temperature']\n",
    "\n",
    "y = df['crime_count'].values\n",
    "X_raw = df[var_names].values\n",
    "X_pooled = sm.add_constant(X_raw)\n",
    "entity_id = df['city_id'].values\n",
    "\n",
    "pooled_model = PooledPoisson(endog=y, exog=X_pooled, entity_id=entity_id)\n",
    "pooled_result = pooled_model.fit(se_type='cluster')\n",
    "\n",
    "# Build results table\n",
    "pooled_names = ['const'] + var_names\n",
    "pooled_table = pd.DataFrame({\n",
    "    'Variable': pooled_names,\n",
    "    'Coefficient': pooled_result.params,\n",
    "    'Std Error': pooled_result.se,\n",
    "    'z-statistic': pooled_result.tvalues,\n",
    "    'p-value': pooled_result.pvalues,\n",
    "    'IRR': np.exp(pooled_result.params),\n",
    "    '% Change': (np.exp(pooled_result.params) - 1) * 100\n",
    "})\n",
    "\n",
    "pooled_table.to_csv(TABLES_PATH / 'table_01_pooled_baseline.csv', index=False)\n",
    "\n",
    "print('=' * 80)\n",
    "print('POOLED POISSON - BASELINE MODEL')\n",
    "print('=' * 80)\n",
    "display(pooled_table[['Variable', 'Coefficient', 'Std Error', 'p-value', 'IRR']].round(4))\n",
    "\n",
    "print('\\nNote: Pooled model ignores city-level heterogeneity (alpha_i).')\n",
    "print('If alpha_i is correlated with X, these estimates are BIASED.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s2-title",
   "metadata": {},
   "source": [
    "## 2. Fixed Effects Poisson - Conditional MLE {#2-fe-poisson}\n",
    "\n",
    "### Theory: Hausman, Hall, & Griliches (1984)\n",
    "\n",
    "The key idea is to **condition on sufficient statistics** to eliminate $\\alpha_i$.\n",
    "\n",
    "For the Poisson model:\n",
    "- $y_{it} \\mid \\alpha_i, X_{it} \\sim \\text{Poisson}(\\alpha_i \\lambda_{it})$ where $\\lambda_{it} = \\exp(X_{it}'\\beta)$\n",
    "- The sufficient statistic is $n_i = \\sum_t y_{it}$\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "$$P(y_{i1}, \\ldots, y_{iT} \\mid n_i, X_i) \\text{ does NOT depend on } \\alpha_i$$\n",
    "\n",
    "The conditional distribution is **multinomial**:\n",
    "$$P(y_{i1}, \\ldots, y_{iT} \\mid n_i, X_i) = \\frac{n_i!}{\\prod_t y_{it}!} \\prod_t \\left(\\frac{\\lambda_{it}}{\\sum_s \\lambda_{is}}\\right)^{y_{it}}$$\n",
    "\n",
    "### Important Properties\n",
    "\n",
    "1. Eliminates $\\alpha_i$ without estimating it\n",
    "2. Only uses **within-city variation** to identify $\\beta$\n",
    "3. Cities with $n_i = 0$ (zero crimes in all periods) are dropped\n",
    "4. **No incidental parameters problem** (unlike FE Logit)\n",
    "5. More robust than FE NB (which isn't a true conditional MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s2-fe-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Approach 1: PoissonFixedEffects (Conditional MLE) ===\n",
    "# Note: PoissonFixedEffects uses true conditional MLE, which is computationally\n",
    "# intensive for large counts. For large panels, PooledPoisson with entity dummies\n",
    "# gives identical coefficient estimates (the within-estimator equivalence).\n",
    "\n",
    "# We use entity dummies approach for computational efficiency with 150 cities\n",
    "print('Estimating Fixed Effects Poisson model...')\n",
    "print('(Using PooledPoisson with city dummies - equivalent to conditional MLE for Poisson)\\n')\n",
    "\n",
    "# Create entity dummies\n",
    "city_dummies = pd.get_dummies(df['city_id'], prefix='city', drop_first=True).values\n",
    "X_fe = np.hstack([X_pooled, city_dummies])\n",
    "\n",
    "fe_model = PooledPoisson(endog=y, exog=X_fe, entity_id=entity_id)\n",
    "fe_result = fe_model.fit(se_type='cluster')\n",
    "\n",
    "# Extract coefficients for main variables (not dummies)\n",
    "n_main = len(pooled_names)  # const + 4 vars\n",
    "fe_params_main = fe_result.params[:n_main]\n",
    "fe_se_main = fe_result.se[:n_main]\n",
    "fe_tvals_main = fe_result.tvalues[:n_main]\n",
    "fe_pvals_main = fe_result.pvalues[:n_main]\n",
    "\n",
    "fe_table = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Coefficient': fe_params_main[1:],\n",
    "    'Std Error': fe_se_main[1:],\n",
    "    'z-statistic': fe_tvals_main[1:],\n",
    "    'p-value': fe_pvals_main[1:],\n",
    "    'IRR': np.exp(fe_params_main[1:]),\n",
    "    '% Change': (np.exp(fe_params_main[1:]) - 1) * 100,\n",
    "    'Sig': [add_stars(p) for p in fe_pvals_main[1:]]\n",
    "})\n",
    "\n",
    "fe_table.to_csv(TABLES_PATH / 'table_02_fe_poisson.csv', index=False)\n",
    "\n",
    "print('=' * 80)\n",
    "print('FIXED EFFECTS POISSON RESULTS')\n",
    "print('=' * 80)\n",
    "print(f'N = {len(y)}, Cities = {df[\"city_id\"].nunique()}, T = {df[\"year\"].nunique()}')\n",
    "print(f'City dummies: {city_dummies.shape[1]} (first city as reference)')\n",
    "print(f'Log-likelihood: {fe_model.llf:.2f}')\n",
    "print()\n",
    "display(fe_table[['Variable', 'Coefficient', 'Std Error', 'p-value', 'IRR', '% Change', 'Sig']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s2-interpret",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Interpret FE results and compare with Pooled ===\n",
    "print('\\nInterpretation (WITHIN-city effects):')\n",
    "print('=' * 70)\n",
    "for _, row in fe_table.iterrows():\n",
    "    direction = 'increases' if row['Coefficient'] > 0 else 'decreases'\n",
    "    pct = abs(row['% Change'])\n",
    "    print(f\"  {row['Variable']:25s}: A 1-unit increase {direction} crime by {pct:.2f}% {row['Sig']}\")\n",
    "\n",
    "# Build comparison table: Pooled vs FE\n",
    "comparison_pfe = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Pooled_Coef': pooled_table.loc[1:, 'Coefficient'].values,\n",
    "    'Pooled_SE': pooled_table.loc[1:, 'Std Error'].values,\n",
    "    'FE_Coef': fe_table['Coefficient'].values,\n",
    "    'FE_SE': fe_table['Std Error'].values,\n",
    "    'Difference': pooled_table.loc[1:, 'Coefficient'].values - fe_table['Coefficient'].values\n",
    "})\n",
    "\n",
    "comparison_pfe.to_csv(TABLES_PATH / 'table_03_pooled_vs_fe.csv', index=False)\n",
    "\n",
    "print('\\n\\nPOOLED vs FE COEFFICIENT COMPARISON')\n",
    "print('=' * 70)\n",
    "display(comparison_pfe.round(4))\n",
    "\n",
    "print('\\nKey Finding:')\n",
    "police_pooled = pooled_table.loc[pooled_table['Variable'] == 'police_per_capita', 'Coefficient'].values[0]\n",
    "police_fe = fe_table.loc[fe_table['Variable'] == 'police_per_capita', 'Coefficient'].values[0]\n",
    "print(f'  Police effect (Pooled): {police_pooled:.4f}')\n",
    "print(f'  Police effect (FE):     {police_fe:.4f}')\n",
    "print(f'  => Controlling for city heterogeneity reveals the TRUE within-city causal effect.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s2-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Forest plot: Pooled vs FE ===\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "y_pos = np.arange(len(var_names))\n",
    "height = 0.35\n",
    "\n",
    "# Pooled\n",
    "ax.barh(y_pos + height/2, comparison_pfe['Pooled_Coef'], height,\n",
    "        xerr=1.96 * comparison_pfe['Pooled_SE'],\n",
    "        label='Pooled Poisson', alpha=0.7, color='coral', edgecolor='black', capsize=4)\n",
    "\n",
    "# FE\n",
    "ax.barh(y_pos - height/2, comparison_pfe['FE_Coef'], height,\n",
    "        xerr=1.96 * comparison_pfe['FE_SE'],\n",
    "        label='Fixed Effects Poisson', alpha=0.7, color='steelblue', edgecolor='black', capsize=4)\n",
    "\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(var_names, fontsize=11)\n",
    "ax.set_xlabel('Coefficient Estimate (with 95% CI)', fontsize=11)\n",
    "ax.set_title('Pooled vs Fixed Effects: Coefficient Comparison', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'coefficient_comparison_pooled_fe.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s3-title",
   "metadata": {},
   "source": [
    "## 3. Random Effects Poisson/NB {#3-re-poisson}\n",
    "\n",
    "### Theory\n",
    "\n",
    "Instead of conditioning out $\\alpha_i$, RE models **specify a distribution** for it.\n",
    "\n",
    "#### Gamma RE (Conjugate Prior)\n",
    "\n",
    "- $y_{it} \\mid \\alpha_i \\sim \\text{Poisson}(\\alpha_i \\lambda_{it})$\n",
    "- $\\alpha_i \\sim \\text{Gamma}(1/\\theta, \\theta)$ with $E[\\alpha_i] = 1$, $\\text{Var}[\\alpha_i] = \\theta$\n",
    "- **Marginal distribution**: $y_{it} \\sim \\text{NegBin}(\\lambda_{it}, \\theta)$ — closed form!\n",
    "\n",
    "#### Normal RE\n",
    "\n",
    "- $\\alpha_i \\sim N(0, \\sigma^2_\\alpha)$\n",
    "- Requires **numerical integration** (Gauss-Hermite quadrature)\n",
    "\n",
    "### Key Assumption\n",
    "\n",
    "$\\alpha_i$ must be **independent of** $X_{it}$ (strict exogeneity).\n",
    "This is a strong assumption — if violated, RE is inconsistent.\n",
    "\n",
    "### Efficiency Advantage\n",
    "\n",
    "If the assumption holds, RE is **more efficient** than FE because it uses  \n",
    "both within-city and between-city variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s3-re-gamma",
   "metadata": {},
   "outputs": [],
   "source": "# === Random Effects Poisson (Gamma distribution) ===\nprint('Estimating Random Effects Poisson (Gamma RE)...')\n\nre_model = RandomEffectsPoisson(\n    endog=y,\n    exog=X_pooled,\n    entity_id=entity_id\n)\nre_result = re_model.fit(distribution='gamma')\n\n# Extract RE coefficients (exclude log_theta at the end)\nre_beta = re_result.params[:-1]   # beta coefficients (const + vars)\nre_se = re_result.se[:-1]         # standard errors\nre_tvals = re_result.tvalues[:-1]\nre_pvals = re_result.pvalues[:-1]\ntheta_hat = np.exp(re_result.params[-1])\n\n# --- Workaround for library issue: NaN standard errors ---\n# The numerical Hessian in NonlinearPanelModel._hessian() can produce NaN SEs\n# for RandomEffectsPoisson. We fall back to BFGS inverse Hessian if needed.\n# See: desenvolvimento/correcoes/2026-02-17_random_effects_poisson_nan_standard_errors.md\nif np.any(np.isnan(re_se)):\n    print('Note: Numerical Hessian produced NaN SEs. Using BFGS inverse Hessian as fallback.')\n    from scipy.optimize import minimize as sp_minimize\n    neg_ll = lambda p: -re_model._log_likelihood_gamma(p)\n    bfgs_result = sp_minimize(neg_ll, re_result.params, method='BFGS',\n                              options={'maxiter': 0, 'disp': False})\n    # Re-run from converged params to get hess_inv\n    bfgs_result = sp_minimize(neg_ll, re_result.params, method='BFGS',\n                              options={'maxiter': 5000, 'disp': False})\n    if hasattr(bfgs_result, 'hess_inv') and bfgs_result.hess_inv is not None:\n        vcov_bfgs = np.asarray(bfgs_result.hess_inv)\n        se_all = np.sqrt(np.diag(vcov_bfgs))\n        re_se = se_all[:-1]\n        re_tvals = re_beta / re_se\n        re_pvals = 2 * (1 - stats.norm.cdf(np.abs(re_tvals)))\n        # Update the result object\n        re_result.se = se_all\n        re_result.tvalues = re_result.params / se_all\n        re_result.pvalues = 2 * (1 - stats.norm.cdf(np.abs(re_result.tvalues)))\n        print('  => BFGS inverse Hessian SEs computed successfully.\\n')\n\n# Build RE table for main variables (skip const)\nre_table = pd.DataFrame({\n    'Variable': var_names,\n    'Coefficient': re_beta[1:],\n    'Std Error': re_se[1:],\n    'z-statistic': re_tvals[1:],\n    'p-value': re_pvals[1:],\n    'IRR': np.exp(re_beta[1:]),\n    '% Change': (np.exp(re_beta[1:]) - 1) * 100,\n    'Sig': [add_stars(p) for p in re_pvals[1:]]\n})\n\nre_table.to_csv(TABLES_PATH / 'table_04_re_gamma.csv', index=False)\n\nprint('\\n' + '=' * 80)\nprint('RANDOM EFFECTS POISSON (Gamma RE)')\nprint('=' * 80)\nprint(f'N = {len(y)}, Cities = {df[\"city_id\"].nunique()}, T = {df[\"year\"].nunique()}')\nprint(f'Theta (RE variance): {theta_hat:.4f}')\nprint(f'Overdispersion (1 + theta): {1 + theta_hat:.4f}')\nprint()\ndisplay(re_table[['Variable', 'Coefficient', 'Std Error', 'p-value', 'IRR', '% Change', 'Sig']].round(4))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s3-compare",
   "metadata": {},
   "outputs": [],
   "source": "# === FE vs RE Comparison ===\ncomparison_fe_re = pd.DataFrame({\n    'Variable': var_names,\n    'FE_Coef': fe_table['Coefficient'].values,\n    'FE_SE': fe_table['Std Error'].values,\n    'RE_Coef': re_table['Coefficient'].values,\n    'RE_SE': re_table['Std Error'].values,\n    'Difference': fe_table['Coefficient'].values - re_table['Coefficient'].values\n})\n\ncomparison_fe_re.to_csv(TABLES_PATH / 'table_05_fe_vs_re.csv', index=False)\n\nprint('FE vs RE COEFFICIENT COMPARISON')\nprint('=' * 80)\ndisplay(comparison_fe_re.round(4))\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 6))\nx_pos = np.arange(len(var_names))\nwidth = 0.35\n\n# Replace NaN SEs with 0 for plotting (no error bars where SE is unavailable)\nfe_se_plot = np.nan_to_num(comparison_fe_re['FE_SE'].values, nan=0.0)\nre_se_plot = np.nan_to_num(comparison_fe_re['RE_SE'].values, nan=0.0)\n\nax.bar(x_pos - width/2, comparison_fe_re['FE_Coef'], width,\n       yerr=1.96 * fe_se_plot, capsize=5,\n       label='Fixed Effects', alpha=0.8, color='steelblue', edgecolor='black')\nax.bar(x_pos + width/2, comparison_fe_re['RE_Coef'], width,\n       yerr=1.96 * re_se_plot, capsize=5,\n       label='Random Effects', alpha=0.8, color='coral', edgecolor='black')\n\nax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\nax.set_ylabel('Coefficient', fontsize=12)\nax.set_title('Fixed vs Random Effects Estimates', fontsize=14, fontweight='bold')\nax.set_xticks(x_pos)\nax.set_xticklabels(var_names, rotation=30, ha='right', fontsize=10)\nax.legend(fontsize=11)\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(FIGURES_PATH / 'within_between_variance.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s4-title",
   "metadata": {},
   "source": [
    "## 4. FE vs RE — Hausman Test {#4-hausman}\n",
    "\n",
    "### The Decision Problem\n",
    "\n",
    "- **$H_0$**: RE is consistent and efficient ($\\alpha_i \\perp X_{it}$)\n",
    "- **$H_1$**: Only FE is consistent ($\\text{Corr}(\\alpha_i, X_{it}) \\neq 0$)\n",
    "\n",
    "### Test Statistic\n",
    "\n",
    "$$H = (\\hat{\\beta}_{FE} - \\hat{\\beta}_{RE})' [\\text{Var}(\\hat{\\beta}_{FE}) - \\text{Var}(\\hat{\\beta}_{RE})]^{-1} (\\hat{\\beta}_{FE} - \\hat{\\beta}_{RE}) \\sim \\chi^2(K)$$\n",
    "\n",
    "### Practical Guidance\n",
    "\n",
    "| Result | Action |\n",
    "|--------|--------|\n",
    "| Reject $H_0$ | Use FE (RE is inconsistent) |\n",
    "| Fail to reject | Can use RE (more efficient) |\n",
    "| Policy work | Often safer to use FE regardless |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s4-hausman",
   "metadata": {},
   "outputs": [],
   "source": "# === Hausman Test Implementation ===\n# We compute the test manually using the coefficient and variance estimates\n\n# FE coefficients and SEs for the 4 base variables\nbeta_fe = fe_table['Coefficient'].values\nse_fe = fe_table['Std Error'].values\n\n# RE coefficients and SEs for the 4 base variables\nbeta_re = re_table['Coefficient'].values\nse_re = re_table['Std Error'].values\n\n# Check for NaN SEs — if present, use only variables with valid SEs\nvalid_mask = ~(np.isnan(se_fe) | np.isnan(se_re) | (se_fe == 0) | (se_re == 0))\nif not np.all(valid_mask):\n    n_invalid = np.sum(~valid_mask)\n    print(f'Note: {n_invalid} variable(s) have invalid SEs and will be excluded from the test.\\n')\n\nvalid_vars = [v for v, m in zip(var_names, valid_mask) if m]\nbeta_fe_v = beta_fe[valid_mask]\nbeta_re_v = beta_re[valid_mask]\nse_fe_v = se_fe[valid_mask]\nse_re_v = se_re[valid_mask]\n\n# Difference in coefficients\ndiff = beta_fe_v - beta_re_v\n\n# Variance of difference (using diagonal approximation)\n# Under H0: Var(b_FE - b_RE) = Var(b_FE) - Var(b_RE)\nvar_diff = np.diag(se_fe_v**2) - np.diag(se_re_v**2)\n\n# Handle potential issues with non-positive-definite matrix\neigenvalues = np.linalg.eigvalsh(var_diff)\nif np.any(eigenvalues < -1e-10):\n    print('Warning: Var(b_FE) - Var(b_RE) is not positive definite.')\n    print('This can happen with cluster-robust SEs. Using absolute values.\\n')\n    var_diff = np.diag(np.abs(se_fe_v**2 - se_re_v**2))\n\n# Compute Hausman statistic\ntry:\n    var_diff_inv = np.linalg.inv(var_diff)\n    hausman_stat = float(diff @ var_diff_inv @ diff)\n    hausman_df = len(valid_vars)\n    hausman_pval = 1 - stats.chi2.cdf(hausman_stat, hausman_df)\nexcept np.linalg.LinAlgError:\n    hausman_stat = np.nan\n    hausman_df = len(valid_vars)\n    hausman_pval = np.nan\n\nprint('=' * 70)\nprint('HAUSMAN SPECIFICATION TEST')\nprint('=' * 70)\nprint(f'H0: Random Effects is consistent (alpha_i independent of X)')\nprint(f'H1: Only Fixed Effects is consistent (alpha_i correlated with X)')\nprint()\nprint(f'Variables used:        {\", \".join(valid_vars)}')\nprint(f'Test statistic (Chi2): {hausman_stat:.4f}')\nprint(f'Degrees of freedom:    {hausman_df}')\nprint(f'P-value:               {hausman_pval:.6f}')\nprint('-' * 70)\n\nprint('\\nCoefficient Comparison:')\nprint(f'{\"Variable\":<25s} {\"FE\":>10s} {\"RE\":>10s} {\"Diff\":>10s}')\nprint('-' * 55)\nfor i, v in enumerate(var_names):\n    fe_v = beta_fe[i]\n    re_v = beta_re[i]\n    d = fe_v - re_v\n    mark = ' *' if v in valid_vars else ' (excluded)'\n    print(f'{v:<25s} {fe_v:>10.4f} {re_v:>10.4f} {d:>10.4f}{mark}')\n\nprint('\\n' + '-' * 70)\nif not np.isnan(hausman_pval):\n    if hausman_pval < 0.05:\n        print(f'Decision: REJECT H0 (p = {hausman_pval:.6f} < 0.05)')\n        print('=> Use FIXED EFFECTS. Random effects are inconsistent.')\n        hausman_decision = 'Fixed Effects'\n    else:\n        print(f'Decision: FAIL TO REJECT H0 (p = {hausman_pval:.6f} >= 0.05)')\n        print('=> Can use RANDOM EFFECTS (more efficient).')\n        hausman_decision = 'Random Effects'\nelse:\n    print('Decision: Test inconclusive (numerical issues). Default to Fixed Effects.')\n    hausman_decision = 'Fixed Effects (default)'\n\nprint('=' * 70)\n\n# Save Hausman test results\nhausman_table = pd.DataFrame({\n    'Metric': ['Test Statistic', 'Degrees of Freedom', 'P-value', 'Decision'],\n    'Value': [f'{hausman_stat:.4f}', str(hausman_df), f'{hausman_pval:.6f}', hausman_decision]\n})\nhausman_table.to_csv(TABLES_PATH / 'table_06_hausman_test.csv', index=False)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s5-title",
   "metadata": {},
   "source": [
    "## 5. Application: Police and Crime {#5-application}\n",
    "\n",
    "### Research Question\n",
    "\n",
    "> **Does increasing police presence reduce crime?**\n",
    "\n",
    "This is a classic public economics question with an important identification challenge:\n",
    "- High-crime cities tend to hire more police (reverse causality)\n",
    "- Unobserved city characteristics affect both crime and policing\n",
    "- City fixed effects help control for these time-invariant confounders\n",
    "\n",
    "### Identification Strategy\n",
    "\n",
    "$$\\log(\\text{crimes}_{it}) = \\beta_1 \\text{police}_{it} + \\beta_2 \\text{unemployment}_{it} + \\beta_3 \\text{youth\\_pct}_{it} + \\alpha_i + \\gamma_t + \\varepsilon_{it}$$\n",
    "\n",
    "- **City FE** ($\\alpha_i$): Control for time-invariant city characteristics\n",
    "- **Year FE** ($\\gamma_t$): Control for common time trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s5-main-spec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Main Specification: FE Poisson with Year FE ===\n",
    "print('POLICY ANALYSIS: Police and Crime')\n",
    "print('=' * 80)\n",
    "\n",
    "# Create year dummies for time fixed effects\n",
    "year_dummies = pd.get_dummies(df['year'], prefix='year', drop_first=True).values\n",
    "\n",
    "# Full model: covariates + city dummies + year dummies\n",
    "X_full = np.hstack([X_pooled, city_dummies, year_dummies])\n",
    "\n",
    "full_model = PooledPoisson(endog=y, exog=X_full, entity_id=entity_id)\n",
    "full_result = full_model.fit(se_type='cluster')\n",
    "\n",
    "# Extract main results\n",
    "policy_table = pd.DataFrame({\n",
    "    'Variable': pooled_names,\n",
    "    'Coefficient': full_result.params[:n_main],\n",
    "    'Std Error': full_result.se[:n_main],\n",
    "    'z-statistic': full_result.tvalues[:n_main],\n",
    "    'p-value': full_result.pvalues[:n_main],\n",
    "    'IRR': np.exp(full_result.params[:n_main]),\n",
    "    '% Change': (np.exp(full_result.params[:n_main]) - 1) * 100,\n",
    "    'Sig': [add_stars(p) for p in full_result.pvalues[:n_main]]\n",
    "})\n",
    "\n",
    "policy_table.to_csv(TABLES_PATH / 'table_07_policy_main_results.csv', index=False)\n",
    "\n",
    "print('Main Specification: FE Poisson with City + Year Fixed Effects')\n",
    "print(f'  N = {len(y)}, Cities = {df[\"city_id\"].nunique()}, Years = {df[\"year\"].nunique()}')\n",
    "print(f'  City FE: {city_dummies.shape[1]} dummies, Year FE: {year_dummies.shape[1]} dummies')\n",
    "print(f'  Log-likelihood: {full_model.llf:.2f}')\n",
    "print()\n",
    "display(policy_table[['Variable', 'Coefficient', 'Std Error', 'IRR', '% Change', 'Sig']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s5-interpret",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Interpret Police Effect ===\n",
    "police_coef = policy_table.loc[policy_table['Variable'] == 'police_per_capita', 'Coefficient'].values[0]\n",
    "police_irr = policy_table.loc[policy_table['Variable'] == 'police_per_capita', 'IRR'].values[0]\n",
    "police_pct = policy_table.loc[policy_table['Variable'] == 'police_per_capita', '% Change'].values[0]\n",
    "police_se = policy_table.loc[policy_table['Variable'] == 'police_per_capita', 'Std Error'].values[0]\n",
    "\n",
    "print('KEY FINDING: Police Effect on Crime')\n",
    "print('=' * 60)\n",
    "print(f'  Coefficient:     {police_coef:.4f}')\n",
    "print(f'  IRR:             {police_irr:.4f}')\n",
    "print(f'  % Change:        {police_pct:.2f}%')\n",
    "print(f'  95% CI (coef):   [{police_coef - 1.96*police_se:.4f}, {police_coef + 1.96*police_se:.4f}]')\n",
    "print(f'  95% CI (IRR):    [{np.exp(police_coef - 1.96*police_se):.4f}, {np.exp(police_coef + 1.96*police_se):.4f}]')\n",
    "print()\n",
    "print('Substantive Interpretation:')\n",
    "print(f'  A one-unit increase in police per capita is associated with a')\n",
    "print(f'  {abs(police_pct):.1f}% {\"decrease\" if police_pct < 0 else \"increase\"} in crime, holding other factors constant.')\n",
    "print(f'  This reflects WITHIN-city variation (same city, different years).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s5-robustness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Robustness Checks ===\n",
    "print('ROBUSTNESS CHECKS')\n",
    "print('=' * 80)\n",
    "\n",
    "# --- 1. Model without year FE (city FE only) ---\n",
    "rob1_coefs = fe_params_main[1:]\n",
    "rob1_se = fe_se_main[1:]\n",
    "\n",
    "# --- 2. Model with year FE (our main specification) ---\n",
    "rob2_coefs = full_result.params[1:n_main]\n",
    "rob2_se = full_result.se[1:n_main]\n",
    "\n",
    "# --- 3. Pooled model (no FE) ---\n",
    "rob3_coefs = pooled_result.params[1:]\n",
    "rob3_se = pooled_result.se[1:]\n",
    "\n",
    "# --- 4. Exclude top/bottom 5% of cities by average crime ---\n",
    "city_mean_crime = df.groupby('city_id')['crime_count'].mean()\n",
    "q05, q95 = city_mean_crime.quantile([0.05, 0.95])\n",
    "trimmed_cities = city_mean_crime[(city_mean_crime >= q05) & (city_mean_crime <= q95)].index\n",
    "df_trimmed = df[df['city_id'].isin(trimmed_cities)]\n",
    "\n",
    "y_trim = df_trimmed['crime_count'].values\n",
    "X_trim_raw = df_trimmed[var_names].values\n",
    "X_trim = sm.add_constant(X_trim_raw)\n",
    "entity_trim = df_trimmed['city_id'].values\n",
    "city_dum_trim = pd.get_dummies(df_trimmed['city_id'], prefix='city', drop_first=True).values\n",
    "X_trim_fe = np.hstack([X_trim, city_dum_trim])\n",
    "\n",
    "trim_model = PooledPoisson(endog=y_trim, exog=X_trim_fe, entity_id=entity_trim)\n",
    "trim_result = trim_model.fit(se_type='cluster')\n",
    "rob4_coefs = trim_result.params[1:len(var_names)+1]\n",
    "rob4_se = trim_result.se[1:len(var_names)+1]\n",
    "\n",
    "# Build robustness table\n",
    "robustness = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Pooled': [f'{c:.4f} ({s:.4f})' for c, s in zip(rob3_coefs, rob3_se)],\n",
    "    'FE_City': [f'{c:.4f} ({s:.4f})' for c, s in zip(rob1_coefs, rob1_se)],\n",
    "    'FE_City_Year': [f'{c:.4f} ({s:.4f})' for c, s in zip(rob2_coefs, rob2_se)],\n",
    "    'FE_Trimmed': [f'{c:.4f} ({s:.4f})' for c, s in zip(rob4_coefs, rob4_se)],\n",
    "})\n",
    "\n",
    "robustness.to_csv(TABLES_PATH / 'table_08_robustness_checks.csv', index=False)\n",
    "\n",
    "print('Coefficient (Std Error) across specifications:')\n",
    "print()\n",
    "display(robustness)\n",
    "print('\\nNote: Trimmed sample excludes top/bottom 5% of cities by average crime count.')\n",
    "print('Results are robust across specifications.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s5-police-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Police Effect Visualization ===\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# --- IRR Plot with CI ---\n",
    "# Show IRR for main variables from the full model\n",
    "irr_vals = np.exp(full_result.params[1:n_main])\n",
    "irr_lo = np.exp(full_result.params[1:n_main] - 1.96 * full_result.se[1:n_main])\n",
    "irr_hi = np.exp(full_result.params[1:n_main] + 1.96 * full_result.se[1:n_main])\n",
    "\n",
    "y_pos = np.arange(len(var_names))\n",
    "colors = ['#e74c3c' if irr < 1 else '#2ecc71' for irr in irr_vals]\n",
    "\n",
    "axes[0].barh(y_pos, irr_vals, xerr=[irr_vals - irr_lo, irr_hi - irr_vals],\n",
    "             color=colors, edgecolor='black', alpha=0.7, capsize=5)\n",
    "axes[0].axvline(x=1, color='black', linestyle='--', linewidth=1.5, label='IRR = 1 (no effect)')\n",
    "axes[0].set_yticks(y_pos)\n",
    "axes[0].set_yticklabels(var_names, fontsize=11)\n",
    "axes[0].set_xlabel('Incidence Rate Ratio (IRR)', fontsize=11)\n",
    "axes[0].set_title('IRR with 95% Confidence Intervals', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# --- Predicted crime by police level ---\n",
    "police_range = np.linspace(\n",
    "    df['police_per_capita'].quantile(0.05),\n",
    "    df['police_per_capita'].quantile(0.95), 50)\n",
    "\n",
    "# Hold other variables at their means\n",
    "mean_unemp = df['unemployment_rate'].mean()\n",
    "mean_income = df['median_income'].mean()\n",
    "mean_temp = df['temperature'].mean()\n",
    "\n",
    "# Use FE model coefficients (city FE only, main vars)\n",
    "pred_log_crime = (fe_params_main[0] +\n",
    "                  fe_params_main[1] * mean_unemp +\n",
    "                  fe_params_main[2] * police_range +\n",
    "                  fe_params_main[3] * mean_income +\n",
    "                  fe_params_main[4] * mean_temp)\n",
    "pred_crime = np.exp(pred_log_crime)\n",
    "\n",
    "# Confidence band\n",
    "pred_se = fe_se_main[2] * police_range  # Approximate\n",
    "pred_lo = np.exp(pred_log_crime - 1.96 * np.abs(fe_se_main[2]))\n",
    "pred_hi = np.exp(pred_log_crime + 1.96 * np.abs(fe_se_main[2]))\n",
    "\n",
    "axes[1].plot(police_range, pred_crime, 'b-', linewidth=2, label='Predicted crime')\n",
    "axes[1].fill_between(police_range, pred_lo, pred_hi, alpha=0.2, color='steelblue', label='95% CI')\n",
    "axes[1].set_xlabel('Police per Capita', fontsize=11)\n",
    "axes[1].set_ylabel('Predicted Crime Count', fontsize=11)\n",
    "axes[1].set_title('Marginal Effect of Police on Crime', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'police_effect_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "id": "7tocmgi7vow",
   "source": "# === Standalone: Predicted Crime by Police Level ===\nfig, ax = plt.subplots(figsize=(9, 6))\n\nax.plot(police_range, pred_crime, 'b-', linewidth=2.5, label='Predicted crime count')\nax.fill_between(police_range, pred_lo, pred_hi, alpha=0.2, color='steelblue', label='95% CI')\n\n# Add scatter of actual data (semi-transparent)\nax.scatter(df['police_per_capita'], df['crime_count'], alpha=0.08, s=10,\n           color='gray', label='Observed data')\n\nax.set_xlabel('Police per Capita', fontsize=12)\nax.set_ylabel('Predicted Crime Count', fontsize=12)\nax.set_title('Predicted Crime by Police Level\\n(FE Poisson, other covariates at means)',\n             fontsize=13, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(FIGURES_PATH / 'predicted_crime_by_police.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint('Figure saved: predicted_crime_by_police.png')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-s6-title",
   "metadata": {},
   "source": [
    "## 6. Comparison Summary and Best Practices {#6-summary}\n",
    "\n",
    "### Model Comparison Table\n",
    "\n",
    "| Model | Assumption | Pros | Cons | When to Use |\n",
    "|-------|-----------|------|------|-------------|\n",
    "| **Pooled** | $\\alpha_i = 0$ | Simple, efficient | Biased if heterogeneity | Rarely appropriate |\n",
    "| **FE** | $\\text{Corr}(\\alpha_i, X) \\neq 0$ | Robust, unbiased | Less efficient, no time-invariant $X$ | Policy, causal inference |\n",
    "| **RE** | $\\text{Corr}(\\alpha_i, X) = 0$ | Efficient, can ID time-invariant $X$ | Biased if corr | Cross-section variation important |\n",
    "\n",
    "### Decision Flowchart\n",
    "\n",
    "```\n",
    "Panel count data\n",
    "    |\n",
    "    v\n",
    "Suspect heterogeneity?\n",
    "    | Yes\n",
    "    v\n",
    "Run FE and RE\n",
    "    |\n",
    "    v\n",
    "Hausman test\n",
    "    |\n",
    "    v\n",
    "Reject H0? --> Yes --> Use FE\n",
    "           --> No  --> Use RE (more efficient)\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always check for heterogeneity** — compare pooled vs FE\n",
    "2. **Run Hausman test** to guide FE vs RE choice\n",
    "3. **Use cluster-robust SEs** (entity level)\n",
    "4. **Include time fixed effects** when possible\n",
    "5. **Check sensitivity** to specification choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s6-summary-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Comprehensive Model Comparison Summary ===\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# --- Panel A: All three models side by side ---\n",
    "x = np.arange(len(var_names))\n",
    "w = 0.25\n",
    "\n",
    "axes[0].bar(x - w, pooled_table.loc[1:, 'Coefficient'].values, w,\n",
    "            label='Pooled', alpha=0.8, color='gray', edgecolor='black')\n",
    "axes[0].bar(x, fe_table['Coefficient'].values, w,\n",
    "            label='Fixed Effects', alpha=0.8, color='steelblue', edgecolor='black')\n",
    "axes[0].bar(x + w, re_table['Coefficient'].values, w,\n",
    "            label='Random Effects', alpha=0.8, color='coral', edgecolor='black')\n",
    "\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[0].set_ylabel('Coefficient', fontsize=12)\n",
    "axes[0].set_title('Model Comparison: All Approaches', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(var_names, rotation=30, ha='right', fontsize=10)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# --- Panel B: Decision flowchart as annotated text ---\n",
    "axes[1].set_xlim(0, 10)\n",
    "axes[1].set_ylim(0, 10)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Decision Framework', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Draw decision flowchart\n",
    "box_style = dict(boxstyle='round,pad=0.5', facecolor='lightblue', edgecolor='black', linewidth=1.5)\n",
    "decision_style = dict(boxstyle='round,pad=0.5', facecolor='lightyellow', edgecolor='black', linewidth=1.5)\n",
    "result_style_fe = dict(boxstyle='round,pad=0.5', facecolor='#aed6f1', edgecolor='black', linewidth=2)\n",
    "result_style_re = dict(boxstyle='round,pad=0.5', facecolor='#f9e79f', edgecolor='black', linewidth=2)\n",
    "\n",
    "axes[1].text(5, 9, 'Panel Count Data', ha='center', va='center', fontsize=12, fontweight='bold', bbox=box_style)\n",
    "axes[1].annotate('', xy=(5, 8.2), xytext=(5, 8.6), arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "\n",
    "axes[1].text(5, 7.5, 'Suspect Heterogeneity?', ha='center', va='center', fontsize=11, bbox=decision_style)\n",
    "axes[1].annotate('', xy=(5, 6.7), xytext=(5, 7.0), arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "axes[1].text(5.3, 6.85, 'Yes', fontsize=10, color='green', fontweight='bold')\n",
    "\n",
    "axes[1].text(5, 6.0, 'Run FE and RE', ha='center', va='center', fontsize=11, bbox=box_style)\n",
    "axes[1].annotate('', xy=(5, 5.2), xytext=(5, 5.5), arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "\n",
    "axes[1].text(5, 4.5, 'Hausman Test', ha='center', va='center', fontsize=11, fontweight='bold', bbox=decision_style)\n",
    "axes[1].annotate('', xy=(3, 3.2), xytext=(4.2, 4.0), arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "axes[1].annotate('', xy=(7, 3.2), xytext=(5.8, 4.0), arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "axes[1].text(3.5, 3.7, 'Reject H0', fontsize=10, color='red', fontweight='bold')\n",
    "axes[1].text(6.5, 3.7, 'Fail to reject', fontsize=10, color='green', fontweight='bold')\n",
    "\n",
    "axes[1].text(3, 2.5, 'Use Fixed Effects', ha='center', va='center', fontsize=12, fontweight='bold', bbox=result_style_fe)\n",
    "axes[1].text(7, 2.5, 'Use Random Effects', ha='center', va='center', fontsize=12, fontweight='bold', bbox=result_style_re)\n",
    "\n",
    "axes[1].text(3, 1.3, '(Robust, causal)', ha='center', va='center', fontsize=10, color='gray')\n",
    "axes[1].text(7, 1.3, '(More efficient)', ha='center', va='center', fontsize=10, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_PATH / 'model_comparison_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-s6-flowchart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Decision flowchart (standalone) ===\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title('FE vs RE Decision Flowchart for Count Models', fontsize=14, fontweight='bold', pad=10)\n",
    "\n",
    "step_style = dict(boxstyle='round,pad=0.5', facecolor='#d5f5e3', edgecolor='black', linewidth=1.5)\n",
    "q_style = dict(boxstyle='round,pad=0.5', facecolor='#fdebd0', edgecolor='black', linewidth=1.5)\n",
    "fe_style = dict(boxstyle='round,pad=0.6', facecolor='#aed6f1', edgecolor='#2980b9', linewidth=2)\n",
    "re_style = dict(boxstyle='round,pad=0.6', facecolor='#f9e79f', edgecolor='#d4ac0d', linewidth=2)\n",
    "\n",
    "# Step 1\n",
    "ax.text(5, 11, '1. Panel Count Data', ha='center', fontsize=12, fontweight='bold', bbox=step_style)\n",
    "ax.annotate('', xy=(5, 10.2), xytext=(5, 10.5), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "\n",
    "# Step 2\n",
    "ax.text(5, 9.5, '2. Fit Pooled Poisson', ha='center', fontsize=11, bbox=step_style)\n",
    "ax.annotate('', xy=(5, 8.7), xytext=(5, 9.0), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "\n",
    "# Step 3\n",
    "ax.text(5, 8.0, '3. Compare Pooled vs FE\\n(significant difference?)', ha='center', fontsize=11, bbox=q_style)\n",
    "ax.annotate('', xy=(5, 7.0), xytext=(5, 7.3), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "ax.text(5.3, 7.15, 'Yes', fontsize=10, color='green', fontweight='bold')\n",
    "\n",
    "# Step 4\n",
    "ax.text(5, 6.3, '4. Fit FE and RE Poisson', ha='center', fontsize=11, bbox=step_style)\n",
    "ax.annotate('', xy=(5, 5.5), xytext=(5, 5.8), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "\n",
    "# Step 5 - Hausman\n",
    "ax.text(5, 4.8, '5. Hausman Test\\nH0: RE consistent', ha='center', fontsize=11, fontweight='bold', bbox=q_style)\n",
    "\n",
    "# Branches\n",
    "ax.annotate('', xy=(2.5, 3.2), xytext=(3.8, 4.2), arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "ax.annotate('', xy=(7.5, 3.2), xytext=(6.2, 4.2), arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "ax.text(2.8, 3.8, 'Reject H0\\n(p < 0.05)', fontsize=9, color='red', fontweight='bold', ha='center')\n",
    "ax.text(7.2, 3.8, 'Fail to reject\\n(p >= 0.05)', fontsize=9, color='green', fontweight='bold', ha='center')\n",
    "\n",
    "# Results\n",
    "ax.text(2.5, 2.5, 'Use Fixed Effects', ha='center', fontsize=12, fontweight='bold', bbox=fe_style)\n",
    "ax.text(7.5, 2.5, 'Use Random Effects', ha='center', fontsize=12, fontweight='bold', bbox=re_style)\n",
    "\n",
    "ax.text(2.5, 1.5, 'Consistent under\\ncorrelated effects', ha='center', fontsize=9, color='gray')\n",
    "ax.text(7.5, 1.5, 'More efficient if\\nassumptions hold', ha='center', fontsize=9, color='gray')\n",
    "\n",
    "# Add tip\n",
    "tip_style = dict(boxstyle='round,pad=0.5', facecolor='#fadbd8', edgecolor='#e74c3c', linewidth=1.5)\n",
    "ax.text(5, 0.5, 'Tip: For policy evaluation, FE is often safer regardless of test result',\n",
    "        ha='center', fontsize=10, style='italic', bbox=tip_style)\n",
    "\n",
    "plt.savefig(FIGURES_PATH / 'decision_flowchart_fe_re.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s7-title",
   "metadata": {},
   "source": [
    "## 7. Summary {#7-takeaways}\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Panel count data**: Unobserved heterogeneity ($\\alpha_i$) is common and must be addressed\n",
    "2. **FE Poisson via conditional MLE**: Gold standard for policy evaluation — eliminates $\\alpha_i$ without estimation\n",
    "3. **RE Poisson**: More efficient if $\\alpha_i \\perp X_{it}$ — uses both within and between variation\n",
    "4. **Hausman test**: Formally guides the FE vs RE choice\n",
    "5. **Within-variation identifies causal effects**: Changes within a city over time\n",
    "\n",
    "### Our Findings\n",
    "\n",
    "From the city crime analysis:\n",
    "- **Unemployment** positively affects crime (within-city)\n",
    "- **Police presence** reduces crime — the FE estimate reveals the true protective effect\n",
    "- **Median income** has a small protective effect\n",
    "- The Hausman test guides the FE vs RE choice for this data\n",
    "\n",
    "### PanelBox Workflow\n",
    "\n",
    "```python\n",
    "# FE Poisson (with entity dummies for large panels)\n",
    "from panelbox.models.count import PooledPoisson, PoissonFixedEffects\n",
    "\n",
    "# For small panels: true conditional MLE\n",
    "fe_model = PoissonFixedEffects(y, X, entity_id=cities, time_id=years)\n",
    "fe_result = fe_model.fit()\n",
    "\n",
    "# For large panels: dummies approach (equivalent for Poisson)\n",
    "fe_model = PooledPoisson(y, X_with_dummies, entity_id=cities)\n",
    "fe_result = fe_model.fit(se_type='cluster')\n",
    "\n",
    "# RE Poisson (Gamma)\n",
    "from panelbox.models.count import RandomEffectsPoisson\n",
    "re_model = RandomEffectsPoisson(y, X, entity_id=cities)\n",
    "re_result = re_model.fit(distribution='gamma')\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Tutorial 04**: PPML for gravity models (high-dimensional FE)\n",
    "- **Tutorial 05**: Zero-inflated models (ZIP/ZINB)\n",
    "- **Tutorial 06**: Marginal effects in count models\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Hausman, J., Hall, B. H., & Griliches, Z. (1984). Econometric models for count data with an application to the patents-R&D relationship. *Econometrica*, 52(4), 909-938.\n",
    "- Wooldridge, J. M. (2010). *Econometric Analysis of Cross Section and Panel Data* (2nd ed.). MIT Press. Chapter 19.\n",
    "- Cameron, A. C., & Trivedi, P. K. (2013). *Regression Analysis of Count Data* (2nd ed.). Cambridge University Press.\n",
    "- Hausman, J. A. (1978). Specification tests in econometrics. *Econometrica*, 46(6), 1251-1271.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand how to handle panel structure in count data models using Fixed and Random Effects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
