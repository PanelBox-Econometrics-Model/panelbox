{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Difference and Between Estimators\n",
    "\n",
    "**Level**: Advanced  \n",
    "**Duration**: 60-75 minutes  \n",
    "**Prerequisites**: Fixed Effects (Notebook 02), Time series concepts\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand** the First Difference (FD) transformation and how it eliminates \u03b1_i\n",
    "2. **Distinguish** between FD and FE (demeaning vs differencing)\n",
    "3. **Identify** when FD is preferable to FE (serial correlation, unit roots)\n",
    "4. **Recognize** the MA(1) structure induced by differencing\n",
    "5. **Estimate** Between Estimator and interpret cross-sectional relationships\n",
    "6. **Decompose** total variance into within and between components\n",
    "7. **Choose** appropriate estimator based on data characteristics\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "While Fixed Effects (FE) is the workhorse of panel data analysis, it's not always the best choice. In this notebook, we explore two important alternatives:\n",
    "\n",
    "- **First Difference (FD)**: Eliminates \u03b1_i by differencing, robust to serial correlation\n",
    "- **Between Estimator (BE)**: Uses cross-sectional variation (entity means)\n",
    "\n",
    "We'll learn when each estimator is appropriate and how to decompose variance into within and between components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import panelbox as pb\n",
    "from scipy import stats\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PanelBox version: {pb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: First Difference Transformation\n",
    "\n",
    "### 1.1 The Problem: Serial Correlation\n",
    "\n",
    "Fixed Effects assumes that idiosyncratic errors \u03b5_it are **uncorrelated over time**:\n",
    "\n",
    "$$\n",
    "\\text{Cov}(\\varepsilon_{it}, \\varepsilon_{is}) = 0 \\quad \\text{for } t \\neq s\n",
    "$$\n",
    "\n",
    "This assumption is violated if:\n",
    "- Errors follow AR(1): \u03b5_it = \u03c1\u03b5_{i,t-1} + \u03b7_it\n",
    "- Random walk: \u03b5_it = \u03b5_{i,t-1} + \u03b7_it (unit root)\n",
    "- Measurement error persists over time\n",
    "\n",
    "**Consequences**:\n",
    "- FE estimator **inconsistent** (biased even with large N, T)\n",
    "- Standard errors invalid\n",
    "\n",
    "**Solution**: Use **First Difference (FD)** estimator instead.\n",
    "\n",
    "### 1.2 FD Transformation\n",
    "\n",
    "The FD transformation differences the data:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_{it} &= \\beta x_{it} + \\alpha_i + \\varepsilon_{it} \\\\\n",
    "y_{i,t-1} &= \\beta x_{i,t-1} + \\alpha_i + \\varepsilon_{i,t-1} \\\\\n",
    "\\hline\n",
    "\\Delta y_{it} &= \\beta \\Delta x_{it} + \\Delta \\varepsilon_{it}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where \u0394y_it = y_it - y_{i,t-1} and **\u03b1_i cancels out** (\u0394\u03b1_i = 0).\n",
    "\n",
    "**Advantages**:\n",
    "1. Robust to arbitrary serial correlation in \u03b5_it\n",
    "2. Handles **unit roots** (non-stationarity)\n",
    "3. Better small-sample properties when \u03b5_it ~ AR(1)\n",
    "\n",
    "**Disadvantage**:\n",
    "- Loses one observation per entity (T-1 instead of T)\n",
    "\n",
    "Let's load data and estimate FD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Grunfeld investment data\n",
    "data = pb.datasets.load_grunfeld()\n",
    "\n",
    "print(\"Grunfeld Investment Data:\")\n",
    "print(data.head(10))\n",
    "print(f\"\\nShape: {data.shape}\")\n",
    "print(f\"Firms: {data['firm'].nunique()}, Years: {data['year'].nunique()}\")\n",
    "print(f\"\\nVariable descriptions:\")\n",
    "print(\"- invest: Gross investment\")\n",
    "print(\"- value: Market value (beginning of year)\")\n",
    "print(\"- capital: Capital stock (beginning of year)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate First Difference model\n",
    "print(\"=\"*70)\n",
    "print(\"FIRST DIFFERENCE ESTIMATOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fd_model = pb.FirstDifferenceEstimator(\n",
    "    \"invest ~ value + capital\", \n",
    "    data, \n",
    "    entity_col='firm', \n",
    "    time_col='year'\n",
    ")\n",
    "fd_results = fd_model.fit(cov_type='clustered')\n",
    "\n",
    "print(fd_results.summary())\n",
    "\n",
    "# Check observations dropped\n",
    "n_original = len(data)\n",
    "n_used = fd_results.nobs\n",
    "n_dropped = n_original - n_used\n",
    "n_firms = data['firm'].nunique()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"OBSERVATION COUNT:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Original observations: {n_original}\")\n",
    "print(f\"Used in estimation: {n_used}\")\n",
    "print(f\"Dropped: {n_dropped} (first observation for each of {n_firms} firms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key observations**:\n",
    "1. FD drops first period for each entity (loses N observations)\n",
    "2. Coefficients estimated on **changes** (\u0394y, \u0394x)\n",
    "3. Clustered SE account for within-entity correlation\n",
    "\n",
    "### 1.3 FD vs FE Comparison\n",
    "\n",
    "How do FD and FE compare?\n",
    "\n",
    "**Theoretical Results**:\n",
    "- **T=2**: FD \u2261 FE (numerically identical)\n",
    "- **T>2, \u03b5 ~ i.i.d.**: FE more efficient (uses all T observations)\n",
    "- **T>2, \u03b5 ~ AR(1)**: FD more efficient and consistent\n",
    "- **Unit roots**: FD consistent, FE inconsistent\n",
    "\n",
    "Let's compare numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Fixed Effects for comparison\n",
    "fe_model = pb.FixedEffects(\n",
    "    \"invest ~ value + capital\", \n",
    "    data, \n",
    "    entity_col='firm', \n",
    "    time_col='year'\n",
    ")\n",
    "fe_results = fe_model.fit(cov_type='clustered')\n",
    "\n",
    "# Compare coefficients and SEs\n",
    "comparison_df = pd.DataFrame({\n",
    "    'FD_coef': fd_results.params,\n",
    "    'FE_coef': fe_results.params,\n",
    "    'Diff_coef': fd_results.params - fe_results.params,\n",
    "    'FD_se': fd_results.std_errors,\n",
    "    'FE_se': fe_results.std_errors\n",
    "})\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FD vs FE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Coefficients are similar but not identical (T>2)\")\n",
    "print(\"2. FE has slightly smaller SEs (more efficient under i.i.d.)\")\n",
    "print(\"3. If serial correlation present, FD would be more reliable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use FD vs FE**:\n",
    "\n",
    "| Scenario | Preferred Estimator | Reason |\n",
    "|----------|-------------------|--------|\n",
    "| \u03b5 ~ i.i.d. | FE | More efficient |\n",
    "| \u03b5 ~ AR(1) | FD | Consistent |\n",
    "| Unit root (random walk) | FD | FE inconsistent |\n",
    "| T = 2 | Either | Identical |\n",
    "| Large T | FE | Efficiency matters |\n",
    "\n",
    "### 1.4 MA(1) Induced by Differencing\n",
    "\n",
    "**Important technical point**: Even if \u03b5_it are i.i.d., the differenced errors \u0394\u03b5_it have **MA(1) structure**:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta \\varepsilon_{it} &= \\varepsilon_{it} - \\varepsilon_{i,t-1} \\\\\n",
    "\\text{Cov}(\\Delta\\varepsilon_{it}, \\Delta\\varepsilon_{i,t-1}) &= \\text{Cov}(\\varepsilon_{it} - \\varepsilon_{i,t-1}, \\varepsilon_{i,t-1} - \\varepsilon_{i,t-2}) \\\\\n",
    "&= -\\sigma^2_{\\varepsilon}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Implication**: Must use **robust standard errors** (Driscoll-Kraay, Newey-West) to account for serial correlation in \u0394\u03b5_it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate FD with different SE types\n",
    "fd_clustered = fd_model.fit(cov_type='clustered')\n",
    "fd_dk = fd_model.fit(cov_type='driscoll_kraay', max_lags=1)\n",
    "\n",
    "# Compare standard errors\n",
    "se_comparison = pd.DataFrame({\n",
    "    'Clustered': fd_clustered.std_errors,\n",
    "    'Driscoll-Kraay': fd_dk.std_errors,\n",
    "    'Pct_Diff': (fd_dk.std_errors / fd_clustered.std_errors - 1) * 100\n",
    "})\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FD: STANDARD ERROR COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(se_comparison)\n",
    "print(\"\\nNote: Driscoll-Kraay SEs account for MA(1) in differenced errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Between Estimator\n",
    "\n",
    "### 2.1 Between Transformation\n",
    "\n",
    "The **Between Estimator (BE)** uses only **cross-sectional variation** by averaging over time:\n",
    "\n",
    "$$\n",
    "\\bar{y}_i = \\beta \\bar{x}_i + \\alpha + \\bar{u}_i\n",
    "$$\n",
    "\n",
    "where $\\bar{y}_i = \\frac{1}{T_i}\\sum_{t=1}^{T_i} y_{it}$ and $\\bar{u}_i = \\alpha_i + \\bar{\\varepsilon}_i$.\n",
    "\n",
    "**Key properties**:\n",
    "- Sample size: **N** (not NT)\n",
    "- Uses **between-entity** variation only\n",
    "- \u03b1_i absorbed into error (not eliminated)\n",
    "- Can include **time-invariant** regressors\n",
    "\n",
    "**When to use BE**:\n",
    "1. Interest in **long-run cross-sectional relationships**\n",
    "2. Time-invariant variables important (education, location)\n",
    "3. T small, N large\n",
    "4. Exploratory analysis\n",
    "\n",
    "Let's estimate the Between Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Between Estimator\n",
    "print(\"=\"*70)\n",
    "print(\"BETWEEN ESTIMATOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "be_model = pb.BetweenEstimator(\n",
    "    \"invest ~ value + capital\", \n",
    "    data, \n",
    "    entity_col='firm', \n",
    "    time_col='year'\n",
    ")\n",
    "be_results = be_model.fit(cov_type='robust')\n",
    "\n",
    "print(be_results.summary())\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"OBSERVATION COUNT:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Original observations (NT): {len(data)}\")\n",
    "print(f\"BE sample size (N): {be_results.nobs}\")\n",
    "print(f\"Average T per firm: {len(data) / data['firm'].nunique():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect entity means used in BE\n",
    "entity_means = data.groupby('firm')[['invest', 'value', 'capital']].mean()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTITY MEANS (used in Between Estimator)\")\n",
    "print(\"=\"*70)\n",
    "print(entity_means.head(10))\n",
    "print(f\"\\nShape: {entity_means.shape} (one row per firm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 When to Use Between Estimator\n",
    "\n",
    "**Use BE when**:\n",
    "1. **Long-run relationships**: \"Do high-value firms invest more?\"\n",
    "2. **Time-invariant variables**: Education, gender, location\n",
    "3. **T small**: Limited time variation\n",
    "4. **Exploratory**: Understand cross-sectional patterns\n",
    "\n",
    "**Don't use BE when**:\n",
    "1. **Unobserved heterogeneity**: \u03b1_i correlated with x_it (use FE/FD)\n",
    "2. **Reverse causality**: Need within variation for identification\n",
    "3. **T large**: FE/FD more efficient\n",
    "\n",
    "### 2.3 FE vs BE Interpretation\n",
    "\n",
    "**Critical distinction**:\n",
    "\n",
    "- **FE (within)**: \"When a firm increases value by 1, investment changes by \u03b2_FE\"\n",
    "  - Interpretation: **Within-firm changes**\n",
    "  - Controls for time-invariant \u03b1_i\n",
    "\n",
    "- **BE (between)**: \"Firms with 1 unit higher average value have \u03b2_BE higher investment\"\n",
    "  - Interpretation: **Cross-sectional differences**\n",
    "  - Confounded by \u03b1_i\n",
    "\n",
    "**Different questions, different answers!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare FE and BE coefficients\n",
    "fe_be_comp = pd.DataFrame({\n",
    "    'FE (within)': fe_results.params,\n",
    "    'BE (between)': be_results.params,\n",
    "    'Ratio (BE/FE)': be_results.params / fe_results.params\n",
    "})\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FE vs BE COEFFICIENT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(fe_be_comp)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*70)\n",
    "print(\"FE (value): When firm increases value by 1, invest increases by\", \n",
    "      f\"{fe_results.params['value']:.4f}\")\n",
    "print(\"BE (value): Firms with 1 unit higher avg value invest\",\n",
    "      f\"{be_results.params['value']:.4f} more\")\n",
    "print(\"\\nDifference suggests presence of \u03b1_i (unobserved heterogeneity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Variance Decomposition\n",
    "\n",
    "### 3.1 Within vs Between Variance\n",
    "\n",
    "Total variation in x_it can be decomposed:\n",
    "\n",
    "$$\n",
    "\\text{Var}(x_{it}) = \\underbrace{\\text{Var}(x_{it} - \\bar{x}_i)}_{\\text{Within}} + \\underbrace{\\text{Var}(\\bar{x}_i)}_{\\text{Between}}\n",
    "$$\n",
    "\n",
    "**Within variance**: Fluctuations around entity mean (used by FE, FD)  \n",
    "**Between variance**: Differences in entity means (used by BE)\n",
    "\n",
    "**Why it matters**:\n",
    "- FE/FD only work if **within variance > 0**\n",
    "- Time-invariant variables have **zero within variance**\n",
    "- High within variance \u2192 more precise FE estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute variance decomposition\n",
    "def variance_decomposition(data, entity_col, var):\n",
    "    \"\"\"\n",
    "    Decompose total variance into within and between components.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "    entity_col : str\n",
    "    var : str\n",
    "        Variable name\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    var_total = data[var].var()\n",
    "    \n",
    "    # Within variance: Var(x_it - x\u0304_i)\n",
    "    data_demeaned = data.groupby(entity_col)[var].transform(lambda x: x - x.mean())\n",
    "    var_within = data_demeaned.var()\n",
    "    \n",
    "    # Between variance: Var(x\u0304_i)\n",
    "    entity_means = data.groupby(entity_col)[var].mean()\n",
    "    var_between = entity_means.var()\n",
    "    \n",
    "    return {\n",
    "        'Variable': var,\n",
    "        'Total': var_total,\n",
    "        'Within': var_within,\n",
    "        'Between': var_between,\n",
    "        'Pct_Within': var_within / var_total * 100,\n",
    "        'Pct_Between': var_between / var_total * 100\n",
    "    }\n",
    "\n",
    "# Compute for all variables\n",
    "var_decomp_list = []\n",
    "for var in ['invest', 'value', 'capital']:\n",
    "    var_decomp_list.append(variance_decomposition(data, 'firm', var))\n",
    "\n",
    "var_decomp_df = pd.DataFrame(var_decomp_list)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VARIANCE DECOMPOSITION\")\n",
    "print(\"=\"*70)\n",
    "print(var_decomp_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"- 'invest' has {var_decomp_df.loc[0, 'Pct_Within']:.1f}% within variation\")\n",
    "print(f\"  \u2192 Substantial time variation within firms (good for FE/FD)\")\n",
    "print(f\"\\n- 'value' has {var_decomp_df.loc[1, 'Pct_Within']:.1f}% within variation\")\n",
    "print(f\"  \u2192 Most variation is between firms\")\n",
    "print(f\"\\n- 'capital' has {var_decomp_df.loc[2, 'Pct_Within']:.1f}% within variation\")\n",
    "print(f\"  \u2192 Changes slowly within firms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualization: Within vs Between Scatter\n",
    "\n",
    "Let's visualize the difference between within and between variation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create within vs between scatter plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel A: Between variation (entity means)\n",
    "firm_means = data.groupby('firm')[['invest', 'value']].mean()\n",
    "axes[0].scatter(firm_means['value'], firm_means['invest'], \n",
    "                s=100, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Add regression line\n",
    "z = np.polyfit(firm_means['value'], firm_means['invest'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(firm_means['value'].min(), firm_means['value'].max(), 100)\n",
    "axes[0].plot(x_line, p(x_line), 'r--', linewidth=2, \n",
    "             label=f'BE slope = {be_results.params[\"value\"]:.4f}')\n",
    "\n",
    "axes[0].set_xlabel('Average Value (x\u0304_i)', fontsize=11)\n",
    "axes[0].set_ylabel('Average Investment (\u0233_i)', fontsize=11)\n",
    "axes[0].set_title('Between-Firm Variation (BE uses this)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Panel B: Within variation (deviations from entity means)\n",
    "data_dm = data.copy()\n",
    "for col in ['invest', 'value']:\n",
    "    data_dm[col + '_dm'] = data.groupby('firm')[col].transform(lambda x: x - x.mean())\n",
    "\n",
    "axes[1].scatter(data_dm['value_dm'], data_dm['invest_dm'], \n",
    "                alpha=0.5, s=30, edgecolors='none')\n",
    "\n",
    "# Add regression line\n",
    "z = np.polyfit(data_dm['value_dm'], data_dm['invest_dm'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(data_dm['value_dm'].min(), data_dm['value_dm'].max(), 100)\n",
    "axes[1].plot(x_line, p(x_line), 'r--', linewidth=2, \n",
    "             label=f'FE slope = {fe_results.params[\"value\"]:.4f}')\n",
    "\n",
    "axes[1].axhline(0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[1].axvline(0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[1].set_xlabel('Value - Firm Mean (x_it - x\u0304_i)', fontsize=11)\n",
    "axes[1].set_ylabel('Investment - Firm Mean (y_it - \u0233_i)', fontsize=11)\n",
    "axes[1].set_title('Within-Firm Variation (FE/FD use this)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHT:\")\n",
    "print(\"- Left panel: Cross-sectional differences (between firms)\")\n",
    "print(\"- Right panel: Time variation within each firm (centered at 0)\")\n",
    "print(\"- Slopes differ \u2192 unobserved heterogeneity (\u03b1_i) important\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Comparison of All Estimators\n",
    "\n",
    "### 4.1 Comprehensive Comparison\n",
    "\n",
    "Let's compare **all four estimators**:\n",
    "1. **Pooled OLS**: Ignores panel structure\n",
    "2. **Fixed Effects (FE)**: Within transformation\n",
    "3. **First Difference (FD)**: Differencing transformation\n",
    "4. **Between Estimator (BE)**: Entity means\n",
    "\n",
    "This will show how different sources of variation lead to different estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate all four models\n",
    "pooled = pb.PooledOLS(\n",
    "    \"invest ~ value + capital\", \n",
    "    data, \n",
    "    entity_col='firm', \n",
    "    time_col='year'\n",
    ").fit(cov_type='clustered')\n",
    "\n",
    "fe = pb.FixedEffects(\n",
    "    \"invest ~ value + capital\", \n",
    "    data, \n",
    "    entity_col='firm', \n",
    "    time_col='year'\n",
    ").fit(cov_type='clustered')\n",
    "\n",
    "fd = pb.FirstDifferenceEstimator(\n",
    "    \"invest ~ value + capital\", \n",
    "    data, \n",
    "    entity_col='firm', \n",
    "    time_col='year'\n",
    ").fit(cov_type='clustered')\n",
    "\n",
    "be = pb.BetweenEstimator(\n",
    "    \"invest ~ value + capital\", \n",
    "    data, \n",
    "    entity_col='firm', \n",
    "    time_col='year'\n",
    ").fit(cov_type='robust')\n",
    "\n",
    "print(\"All models estimated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficients\n",
    "coef_comp = pd.DataFrame({\n",
    "    'Pooled': pooled.params,\n",
    "    'FE': fe.params,\n",
    "    'FD': fd.params,\n",
    "    'BE': be.params\n",
    "})\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COEFFICIENT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(coef_comp)\n",
    "\n",
    "# Compare standard errors\n",
    "se_comp = pd.DataFrame({\n",
    "    'Pooled': pooled.std_errors,\n",
    "    'FE': fe.std_errors,\n",
    "    'FD': fd.std_errors,\n",
    "    'BE': be.std_errors\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STANDARD ERROR COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(se_comp)\n",
    "\n",
    "# Compare R-squared\n",
    "r2_comp = pd.DataFrame({\n",
    "    'Estimator': ['Pooled', 'FE', 'FD', 'BE'],\n",
    "    'R\u00b2': [pooled.r2, fe.r2, fd.r2, be.r2],\n",
    "    'N': [pooled.nobs, fe.nobs, fd.nobs, be.nobs]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL FIT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(r2_comp.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key patterns**:\n",
    "1. **Pooled** has largest sample, but biased (ignores \u03b1_i)\n",
    "2. **FE** and **FD** similar (both use within variation)\n",
    "3. **BE** different (uses between variation, smaller N)\n",
    "4. BE > FE suggests **positive correlation** between x_it and \u03b1_i\n",
    "\n",
    "### 4.2 Coefficient Plot with Confidence Intervals\n",
    "\n",
    "Visualize coefficient estimates with 95% confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create coefficient plot with CIs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "vars_to_plot = ['value', 'capital']\n",
    "estimators = ['Pooled', 'FE', 'FD', 'BE']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for i, var in enumerate(vars_to_plot):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    x_pos = np.arange(len(estimators))\n",
    "    coefs = [coef_comp.loc[var, est] for est in estimators]\n",
    "    ses = [se_comp.loc[var, est] for est in estimators]\n",
    "    ci_lower = [c - 1.96*se for c, se in zip(coefs, ses)]\n",
    "    ci_upper = [c + 1.96*se for c, se in zip(coefs, ses)]\n",
    "    \n",
    "    # Plot coefficients\n",
    "    for j, (x, c, lower, upper, color) in enumerate(zip(x_pos, coefs, ci_lower, ci_upper, colors)):\n",
    "        ax.plot([x, x], [lower, upper], color=color, linewidth=2)\n",
    "        ax.scatter(x, c, s=100, color=color, zorder=3, edgecolors='black', linewidth=1)\n",
    "    \n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(estimators, fontsize=10)\n",
    "    ax.set_ylabel('Coefficient Estimate', fontsize=11)\n",
    "    ax.set_title(f'Variable: {var}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Comparison of Estimators (with 95% CI)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nINTERPRETATION:\")\n",
    "print(\"- Vertical lines represent 95% confidence intervals\")\n",
    "print(\"- BE estimates often differ from FE/FD (unobserved heterogeneity)\")\n",
    "print(\"- Pooled may be biased if \u03b1_i correlated with x_it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Summary Table\n",
    "\n",
    "Let's create a comprehensive summary table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_table = pd.DataFrame({\n",
    "    'Estimator': ['Pooled OLS', 'Fixed Effects', 'First Difference', 'Between'],\n",
    "    'Transformation': ['None', 'Demeaning', 'Differencing', 'Entity means'],\n",
    "    'Controls \u03b1_i': ['No', 'Yes', 'Yes', 'No'],\n",
    "    'Sample size': [pooled.nobs, fe.nobs, fd.nobs, be.nobs],\n",
    "    '\u03b2_value': [f\"{pooled.params['value']:.4f}\",\n",
    "                f\"{fe.params['value']:.4f}\",\n",
    "                f\"{fd.params['value']:.4f}\",\n",
    "                f\"{be.params['value']:.4f}\"],\n",
    "    'SE_value': [f\"{pooled.std_errors['value']:.4f}\",\n",
    "                 f\"{fe.std_errors['value']:.4f}\",\n",
    "                 f\"{fd.std_errors['value']:.4f}\",\n",
    "                 f\"{be.std_errors['value']:.4f}\"],\n",
    "    'R\u00b2': [f\"{pooled.r2:.4f}\", \n",
    "           f\"{fe.r2:.4f}\", \n",
    "           f\"{fd.r2:.4f}\", \n",
    "           f\"{be.r2:.4f}\"]\n",
    "})\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"COMPREHENSIVE ESTIMATOR COMPARISON\")\n",
    "print(\"=\"*90)\n",
    "print(summary_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"WHEN TO USE EACH ESTIMATOR:\")\n",
    "print(\"=\"*90)\n",
    "print(\"Pooled OLS:        No unobserved heterogeneity (rare in panel data)\")\n",
    "print(\"Fixed Effects:     Standard choice, \u03b5 ~ i.i.d., controls \u03b1_i\")\n",
    "print(\"First Difference:  Serial correlation, unit roots, T small\")\n",
    "print(\"Between:           Long-run cross-sectional, time-invariant vars, exploratory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Exercises\n",
    "\n",
    "### Exercise 5.1: FD vs FE with Serial Correlation\n",
    "\n",
    "**Objective**: Compare FD and FE performance when errors have AR(1) structure.\n",
    "\n",
    "**Instructions**:\n",
    "1. Simulate panel data with AR(1) errors: \u03b5_it = \u03c1 \u03b5_{i,t-1} + \u03b7_it\n",
    "2. Estimate both FE and FD\n",
    "3. Compare bias in coefficient estimates\n",
    "4. Which estimator is more robust to serial correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 5.1\n",
    "# Simulate panel data with AR(1) errors\n",
    "\n",
    "np.random.seed(42)\n",
    "N = 100  # firms\n",
    "T = 10   # time periods\n",
    "rho = 0.7  # AR(1) coefficient\n",
    "beta_true = 0.5\n",
    "\n",
    "# Your code here:\n",
    "# 1. Simulate x_it, \u03b1_i, \u03b5_it with AR(1), y_it\n",
    "# 2. Create DataFrame\n",
    "# 3. Estimate FE and FD\n",
    "# 4. Compare bias: |\u03b2\u0302 - \u03b2_true|\n",
    "\n",
    "# Starter code:\n",
    "# alpha_i = np.random.normal(0, 1, N)\n",
    "# x_it = np.random.normal(0, 1, (N, T))\n",
    "# epsilon_it = np.zeros((N, T))\n",
    "# for t in range(1, T):\n",
    "#     epsilon_it[:, t] = rho * epsilon_it[:, t-1] + np.random.normal(0, 0.5, N)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Variance Decomposition\n",
    "\n",
    "**Objective**: Analyze variance decomposition for wage panel data.\n",
    "\n",
    "**Instructions**:\n",
    "1. Load `wage_panel` dataset (if available, otherwise use another dataset)\n",
    "2. Compute variance decomposition for `experience` and `education`\n",
    "3. Which variable has more within variation?\n",
    "4. What implications does this have for FE vs BE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 5.2\n",
    "# Analyze variance decomposition\n",
    "\n",
    "# Your code here:\n",
    "# 1. Load wage_panel data (or use Grunfeld if unavailable)\n",
    "# 2. Apply variance_decomposition() to relevant variables\n",
    "# 3. Create bar chart comparing within vs between percentages\n",
    "# 4. Interpret results\n",
    "\n",
    "# Starter code:\n",
    "# data_wage = pb.load_dataset('wage_panel')  # or use another dataset\n",
    "# var_decomp_wage = [\n",
    "#     variance_decomposition(data_wage, 'person_id', 'experience'),\n",
    "#     variance_decomposition(data_wage, 'person_id', 'education')\n",
    "# ]\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.3: Hausman Test for FE vs FD\n",
    "\n",
    "**Objective**: Test whether FE and FD give statistically different results.\n",
    "\n",
    "**Instructions**:\n",
    "1. Compute Hausman test statistic: H = (\u03b2\u0302_FE - \u03b2\u0302_FD)' [Var(\u03b2\u0302_FE) - Var(\u03b2\u0302_FD)]^{-1} (\u03b2\u0302_FE - \u03b2\u0302_FD)\n",
    "2. Under H0: No serial correlation, H ~ \u03c7\u00b2(k)\n",
    "3. If reject, prefer FD (evidence of serial correlation)\n",
    "4. Apply to Grunfeld data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 5.3\n",
    "# Implement Hausman test for FE vs FD\n",
    "\n",
    "# Your code here:\n",
    "# 1. Extract \u03b2\u0302_FE, \u03b2\u0302_FD and covariance matrices\n",
    "# 2. Compute difference: d = \u03b2\u0302_FE - \u03b2\u0302_FD\n",
    "# 3. Compute variance: V = Var(\u03b2\u0302_FE) - Var(\u03b2\u0302_FD)\n",
    "# 4. Compute H = d' V^{-1} d\n",
    "# 5. Compare to \u03c7\u00b2(k) critical value\n",
    "\n",
    "# Starter code:\n",
    "# beta_diff = fe_results.params - fd_results.params\n",
    "# var_fe = fe_results.cov\n",
    "# var_fd = fd_results.cov\n",
    "# var_diff = var_fe - var_fd\n",
    "# H = beta_diff.T @ np.linalg.inv(var_diff) @ beta_diff\n",
    "# p_value = 1 - stats.chi2.cdf(H, df=len(beta_diff))\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **First Difference (FD)**:\n",
    "   - Eliminates \u03b1_i via differencing: \u0394y_it = \u03b2 \u0394x_it + \u0394\u03b5_it\n",
    "   - Robust to serial correlation and unit roots\n",
    "   - Induces MA(1) in errors \u2192 use robust SE\n",
    "   - Loses first observation per entity\n",
    "\n",
    "2. **FD vs FE**:\n",
    "   - T=2: Numerically equivalent\n",
    "   - \u03b5 ~ i.i.d.: FE more efficient\n",
    "   - \u03b5 ~ AR(1): FD consistent, FE inconsistent\n",
    "   - Unit roots: FD consistent, FE not\n",
    "\n",
    "3. **Between Estimator (BE)**:\n",
    "   - Uses entity means: \u0233_i = \u03b2 x\u0304_i + \u03b1 + \u016b_i\n",
    "   - Sample size N (not NT)\n",
    "   - Cross-sectional relationships\n",
    "   - Can include time-invariant variables\n",
    "\n",
    "4. **FE vs BE interpretation**:\n",
    "   - FE: \"Within-firm changes\"\n",
    "   - BE: \"Between-firm differences\"\n",
    "   - Different questions, different answers!\n",
    "\n",
    "5. **Variance decomposition**:\n",
    "   - Total = Within + Between\n",
    "   - FE/FD need within variation > 0\n",
    "   - Time-invariant vars have zero within variance\n",
    "\n",
    "6. **Estimator choice**:\n",
    "   - Serial correlation \u2192 FD\n",
    "   - Unit roots \u2192 FD\n",
    "   - Time-invariant vars \u2192 BE (or Random Effects)\n",
    "   - Standard case \u2192 FE\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "```\n",
    "Panel Data Model\n",
    "\u2502\n",
    "\u251c\u2500 Unobserved heterogeneity (\u03b1_i) present?\n",
    "\u2502  \u251c\u2500 No \u2192 Pooled OLS\n",
    "\u2502  \u2514\u2500 Yes\n",
    "\u2502     \u2502\n",
    "\u2502     \u251c\u2500 Serial correlation in \u03b5_it?\n",
    "\u2502     \u2502  \u251c\u2500 Yes \u2192 First Difference (FD)\n",
    "\u2502     \u2502  \u2514\u2500 No\n",
    "\u2502     \u2502     \u2502\n",
    "\u2502     \u2502     \u251c\u2500 T = 2? \u2192 FD or FE (equivalent)\n",
    "\u2502     \u2502     \u2514\u2500 T > 2 \u2192 Fixed Effects (FE)\n",
    "\u2502     \u2502\n",
    "\u2502     \u2514\u2500 Interest in cross-sectional or time-invariant vars?\n",
    "\u2502        \u2514\u2500 Yes \u2192 Between Estimator (BE) or Random Effects\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Notebook 05**: Panel IV (endogeneity treatment)\n",
    "- **Notebook 06**: Random Effects and Hausman test\n",
    "- **Advanced**: Dynamic panels with lagged dependent variables\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Wooldridge, J. M. (2010). *Econometric Analysis of Cross Section and Panel Data*. MIT Press. Chapter 10.\n",
    "2. Baltagi, B. H. (2021). *Econometric Analysis of Panel Data*. Springer. Chapter 3.\n",
    "3. Cameron, A. C., & Trivedi, P. K. (2005). *Microeconometrics: Methods and Applications*. Cambridge University Press. Chapter 21.\n",
    "4. Hsiao, C. (2014). *Analysis of Panel Data*. Cambridge University Press. Chapter 3.\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook complete!** You now understand when to use FD vs FE vs BE and how to decompose variance in panel data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
