{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced IV Diagnostics: Weak Instruments and Specification Testing\n",
    "\n",
    "**Level**: Advanced-Expert  \n",
    "**Estimated Duration**: 75-90 minutes  \n",
    "**Date**: 2026-02-16\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Diagnose** weak instruments using first-stage F-statistics and Stock-Yogo critical values\n",
    "2. **Understand** the consequences of weak instruments (bias, inconsistent inference)\n",
    "3. **Conduct** overidentification tests (Sargan/Hansen J-test)\n",
    "4. **Perform** endogeneity tests (Durbin-Wu-Hausman)\n",
    "5. **Interpret** advanced IV diagnostics in panel data contexts\n",
    "6. **Recognize** when IV estimation is unreliable\n",
    "7. **Apply** weak-instrument-robust inference methods\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Conceptual**:\n",
    "- Panel IV estimation (Notebook 05)\n",
    "- Advanced IV theory\n",
    "- Asymptotic theory basics\n",
    "\n",
    "**Technical**:\n",
    "- Hypothesis testing (Chi-squared, F-distribution)\n",
    "- Matrix algebra\n",
    "- Understanding of bias vs consistency\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import PanelBox\n",
    "import sys\n",
    "sys.path.insert(0, '/home/guhaase/projetos/panelbox')\n",
    "import panelbox as pb\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"PanelBox version:\", pb.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: The Problem of Weak Instruments\n",
    "\n",
    "### 1.1 What Are Weak Instruments?\n",
    "\n",
    "**Definition**: Instruments that have **low correlation** with the endogenous variable are called \"weak instruments.\"\n",
    "\n",
    "**Why does this matter?**\n",
    "\n",
    "Valid IV estimation requires two conditions:\n",
    "1. **Relevance**: Cov(Z, X) ‚â† 0 (instrument correlated with endogenous variable)\n",
    "2. **Exogeneity**: Cov(Z, u) = 0 (instrument uncorrelated with error)\n",
    "\n",
    "When instruments are **weak** (violate relevance strongly):\n",
    "\n",
    "- **Finite-sample bias** toward OLS (even if Z is valid!)\n",
    "- **Standard errors underestimated** ‚Üí tests over-reject\n",
    "- **Confidence intervals too narrow** ‚Üí misleading inference\n",
    "- **Asymptotic theory fails** in practice\n",
    "\n",
    "**Rule of Thumb**: First-stage F-statistic < 10 indicates weak instruments.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Simulating Weak vs Strong Instruments\n",
    "\n",
    "Let's simulate data to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate strong vs weak instruments\n",
    "np.random.seed(42)\n",
    "N = 500\n",
    "\n",
    "# Strong instrument (high correlation with X)\n",
    "z_strong = np.random.normal(0, 1, N)\n",
    "x_endo_strong = 0.7 * z_strong + np.random.normal(0, 1, N)  # Corr ‚âà 0.57\n",
    "\n",
    "# Weak instrument (low correlation with X)\n",
    "z_weak = np.random.normal(0, 1, N)\n",
    "x_endo_weak = 0.1 * z_weak + np.random.normal(0, 1, N)  # Corr ‚âà 0.10\n",
    "\n",
    "# True data-generating process\n",
    "beta_true = 2.0\n",
    "y_strong = beta_true * x_endo_strong + np.random.normal(0, 1, N)\n",
    "y_weak = beta_true * x_endo_weak + np.random.normal(0, 1, N)\n",
    "\n",
    "# Calculate correlations\n",
    "corr_strong = np.corrcoef(z_strong, x_endo_strong)[0, 1]\n",
    "corr_weak = np.corrcoef(z_weak, x_endo_weak)[0, 1]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INSTRUMENT STRENGTH COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Instrument Type':<20} {'Correlation(Z, X)':>20}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Strong Instrument':<20} {corr_strong:>20.3f}\")\n",
    "print(f\"{'Weak Instrument':<20} {corr_weak:>20.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTrue Œ≤: {beta_true}\")\n",
    "print(\"\\n‚ö† The weak instrument has correlation < 0.15 with X\")\n",
    "print(\"   This will likely result in:\")\n",
    "print(\"   - Biased estimates\")\n",
    "print(\"   - Unreliable standard errors\")\n",
    "print(\"   - Invalid hypothesis tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visualization: Weak vs Strong Instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize instrument strength\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Strong instrument\n",
    "axes[0].scatter(z_strong, x_endo_strong, alpha=0.5, s=20)\n",
    "axes[0].set_xlabel('Instrument (Z)', fontsize=11)\n",
    "axes[0].set_ylabel('Endogenous Variable (X)', fontsize=11)\n",
    "axes[0].set_title(f'Strong Instrument\\nCorr(Z, X) = {corr_strong:.3f}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Add regression line\n",
    "z_sorted_strong = np.sort(z_strong)\n",
    "fit_strong = np.polyfit(z_strong, x_endo_strong, 1)\n",
    "axes[0].plot(z_sorted_strong, fit_strong[0] * z_sorted_strong + fit_strong[1], \n",
    "             'r-', linewidth=2, label='First-stage fit')\n",
    "axes[0].legend()\n",
    "\n",
    "# Weak instrument\n",
    "axes[1].scatter(z_weak, x_endo_weak, alpha=0.5, s=20, color='coral')\n",
    "axes[1].set_xlabel('Instrument (Z)', fontsize=11)\n",
    "axes[1].set_ylabel('Endogenous Variable (X)', fontsize=11)\n",
    "axes[1].set_title(f'Weak Instrument\\nCorr(Z, X) = {corr_weak:.3f}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Add regression line\n",
    "z_sorted_weak = np.sort(z_weak)\n",
    "fit_weak = np.polyfit(z_weak, x_endo_weak, 1)\n",
    "axes[1].plot(z_sorted_weak, fit_weak[0] * z_sorted_weak + fit_weak[1], \n",
    "             'r-', linewidth=2, label='First-stage fit')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Notice:\")\n",
    "print(\"   - Strong instrument: Clear positive relationship\")\n",
    "print(\"   - Weak instrument: Nearly no visible relationship (almost flat line)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: First-Stage F-Statistic\n",
    "\n",
    "### 2.1 Understanding the First-Stage F-Statistic\n",
    "\n",
    "The **first-stage regression** is:\n",
    "\n",
    "$$\n",
    "X_{\\text{endo}} = \\pi_0 + \\pi_1 Z + v\n",
    "$$\n",
    "\n",
    "The **F-statistic** tests:\n",
    "- **H‚ÇÄ**: œÄ‚ÇÅ = 0 (instrument is irrelevant)\n",
    "- **H‚ÇÅ**: œÄ‚ÇÅ ‚â† 0 (instrument is relevant)\n",
    "\n",
    "**Interpretation**:\n",
    "- **F > 10**: Acceptable (rule of thumb)\n",
    "- **F > 16.38**: Strong (Stock-Yogo: < 10% relative bias)\n",
    "- **F > 19.93**: Very strong (Stock-Yogo: < 5% relative bias)\n",
    "\n",
    "**Stock-Yogo Critical Values** (for 1 instrument, 1 endogenous variable):\n",
    "\n",
    "| Max Relative Bias | Critical F |\n",
    "|-------------------|------------|\n",
    "| 5%               | 19.93      |\n",
    "| 10%              | 16.38      |\n",
    "| 20%              | 8.96       |\n",
    "| 30%              | 6.66       |\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Panel IV with First-Stage Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create panel data with strong instrument\n",
    "np.random.seed(123)\n",
    "data_panel = []\n",
    "\n",
    "for i in range(100):  # 100 entities\n",
    "    for t in range(5):  # 5 time periods\n",
    "        z = np.random.normal(0, 1)\n",
    "        x_endo = 0.6 * z + np.random.normal(0, 1)  # Strong instrument\n",
    "        y = 2 * x_endo + np.random.normal(0, 0.5)\n",
    "        data_panel.append({\n",
    "            'entity': i, \n",
    "            'time': t, \n",
    "            'y': y, \n",
    "            'x_endo': x_endo, \n",
    "            'z': z\n",
    "        })\n",
    "\n",
    "df_panel = pd.DataFrame(data_panel)\n",
    "\n",
    "print(\"Panel Data Shape:\", df_panel.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_panel.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute first-stage F-statistic\n",
    "def compute_first_stage_f(df, endog_col, instrument_col):\n",
    "    \"\"\"\n",
    "    Compute first-stage F-statistic.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    endog_col : str\n",
    "        Endogenous variable column\n",
    "    instrument_col : str or list\n",
    "        Instrument column(s)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : F-statistic and p-value\n",
    "    \"\"\"\n",
    "    X_endo = df[[endog_col]].values\n",
    "    \n",
    "    if isinstance(instrument_col, str):\n",
    "        Z = df[[instrument_col]].values\n",
    "    else:\n",
    "        Z = df[instrument_col].values\n",
    "    \n",
    "    # Add constant\n",
    "    Z_const = np.column_stack([np.ones(len(Z)), Z])\n",
    "    \n",
    "    # OLS\n",
    "    pi_hat = np.linalg.lstsq(Z_const, X_endo, rcond=None)[0]\n",
    "    X_pred = Z_const @ pi_hat\n",
    "    resid_fs = X_endo - X_pred\n",
    "    \n",
    "    # F-statistic\n",
    "    SSR_r = np.sum((X_endo - X_endo.mean())**2)\n",
    "    SSR_u = np.sum(resid_fs**2)\n",
    "    \n",
    "    num_instruments = Z.shape[1]\n",
    "    N = len(X_endo)\n",
    "    \n",
    "    f_stat = ((SSR_r - SSR_u) / num_instruments) / (SSR_u / (N - num_instruments - 1))\n",
    "    f_pval = 1 - stats.f.cdf(f_stat, num_instruments, N - num_instruments - 1)\n",
    "    \n",
    "    return {'f_statistic': f_stat, 'f_pvalue': f_pval}\n",
    "\n",
    "# Compute first-stage F\n",
    "fs_result = compute_first_stage_f(df_panel, 'x_endo', 'z')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FIRST-STAGE DIAGNOSTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFirst-Stage F-statistic: {fs_result['f_statistic']:.2f}\")\n",
    "print(f\"P-value:                 {fs_result['f_pvalue']:.6f}\")\n",
    "\n",
    "# Stock-Yogo evaluation\n",
    "f_stat = fs_result['f_statistic']\n",
    "print(\"\\nStock-Yogo Assessment:\")\n",
    "if f_stat > 19.93:\n",
    "    print(\"  ‚úì Very Strong Instrument (F > 19.93, < 5% bias)\")\n",
    "elif f_stat > 16.38:\n",
    "    print(\"  ‚úì Strong Instrument (F > 16.38, < 10% bias)\")\n",
    "elif f_stat > 10:\n",
    "    print(\"  ‚ö° Acceptable (F > 10, rule of thumb)\")\n",
    "else:\n",
    "    print(\"  ‚ö† WARNING: Weak Instrument (F < 10)\")\n",
    "    print(\"     ‚Üí IV estimates may be severely biased\")\n",
    "    print(\"     ‚Üí Standard errors unreliable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Comparing Strong vs Weak Instruments in Panel Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate weak instrument panel data\n",
    "np.random.seed(456)\n",
    "data_panel_weak = []\n",
    "\n",
    "for i in range(100):\n",
    "    for t in range(5):\n",
    "        z = np.random.normal(0, 1)\n",
    "        x_endo = 0.08 * z + np.random.normal(0, 1)  # WEAK instrument\n",
    "        y = 2 * x_endo + np.random.normal(0, 0.5)\n",
    "        data_panel_weak.append({\n",
    "            'entity': i,\n",
    "            'time': t,\n",
    "            'y': y,\n",
    "            'x_endo': x_endo,\n",
    "            'z': z\n",
    "        })\n",
    "\n",
    "df_panel_weak = pd.DataFrame(data_panel_weak)\n",
    "\n",
    "# Compare strong vs weak\n",
    "fs_strong = compute_first_stage_f(df_panel, 'x_endo', 'z')\n",
    "fs_weak = compute_first_stage_f(df_panel_weak, 'x_endo', 'z')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FIRST-STAGE F-STATISTIC COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Instrument Type':<20} {'F-Statistic':>15} {'Assessment':>25}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "f_strong = fs_strong['f_statistic']\n",
    "if f_strong > 19.93:\n",
    "    assess_strong = \"‚úì Very Strong\"\n",
    "elif f_strong > 16.38:\n",
    "    assess_strong = \"‚úì Strong\"\n",
    "elif f_strong > 10:\n",
    "    assess_strong = \"‚ö° Acceptable\"\n",
    "else:\n",
    "    assess_strong = \"‚ö† Weak\"\n",
    "\n",
    "print(f\"{'Strong Instrument':<20} {f_strong:>15.2f} {assess_strong:>25}\")\n",
    "\n",
    "f_weak = fs_weak['f_statistic']\n",
    "if f_weak > 19.93:\n",
    "    assess_weak = \"‚úì Very Strong\"\n",
    "elif f_weak > 16.38:\n",
    "    assess_weak = \"‚úì Strong\"\n",
    "elif f_weak > 10:\n",
    "    assess_weak = \"‚ö° Acceptable\"\n",
    "else:\n",
    "    assess_weak = \"‚ö† Weak\"\n",
    "\n",
    "print(f\"{'Weak Instrument':<20} {f_weak:>15.2f} {assess_weak:>25}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìå Key Insight:\")\n",
    "if f_weak < 10:\n",
    "    print(\"   The weak instrument has F < 10, indicating serious problems!\")\n",
    "    print(\"   IV estimates will be biased and unreliable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Overidentification Test (J-Test)\n",
    "\n",
    "### 3.1 The Sargan-Hansen J-Test\n",
    "\n",
    "**When applicable**: Number of instruments > Number of endogenous variables (\"overidentified\")\n",
    "\n",
    "**Purpose**: Test if **all** instruments are valid (uncorrelated with error term)\n",
    "\n",
    "**Hypotheses**:\n",
    "- **H‚ÇÄ**: All instruments are valid (E[Z_i ¬∑ u_i] = 0 for all instruments)\n",
    "- **H‚ÇÅ**: At least one instrument is invalid\n",
    "\n",
    "**Test Statistic**:\n",
    "$$\n",
    "J = N \\times R^2_{\\text{residuals on instruments}}\n",
    "$$\n",
    "\n",
    "**Distribution under H‚ÇÄ**:\n",
    "$$\n",
    "J \\sim \\chi^2(df = \\text{# instruments} - \\text{# endogenous})\n",
    "$$\n",
    "\n",
    "**Interpretation**:\n",
    "- **Reject H‚ÇÄ** (p < 0.05): At least one instrument is invalid\n",
    "- **Fail to reject**: Instruments appear valid (cannot reject orthogonality)\n",
    "\n",
    "**Limitations**:\n",
    "- Cannot detect if **all** instruments are invalid\n",
    "- Low power in some cases\n",
    "- Does NOT test relevance (only exogeneity)\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Implementing the J-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate overidentified model: 2 instruments, 1 endogenous variable\n",
    "np.random.seed(789)\n",
    "data_overid = []\n",
    "\n",
    "for i in range(100):\n",
    "    for t in range(5):\n",
    "        # Two valid instruments\n",
    "        z1 = np.random.normal(0, 1)\n",
    "        z2 = np.random.normal(0, 1)\n",
    "        \n",
    "        # Endogenous variable depends on both\n",
    "        x_endo = 0.5 * z1 + 0.4 * z2 + np.random.normal(0, 1)\n",
    "        \n",
    "        # Outcome (error uncorrelated with instruments)\n",
    "        u = np.random.normal(0, 0.5)\n",
    "        y = 2 * x_endo + u\n",
    "        \n",
    "        data_overid.append({\n",
    "            'entity': i,\n",
    "            'time': t,\n",
    "            'y': y,\n",
    "            'x_endo': x_endo,\n",
    "            'z1': z1,\n",
    "            'z2': z2\n",
    "        })\n",
    "\n",
    "df_overid = pd.DataFrame(data_overid)\n",
    "\n",
    "print(\"Overidentified Model Data:\")\n",
    "print(f\"  Observations: {len(df_overid)}\")\n",
    "print(f\"  # Instruments: 2\")\n",
    "print(f\"  # Endogenous: 1\")\n",
    "print(f\"  Degrees of overidentification: 2 - 1 = 1\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_overid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# J-test computation function\n",
    "def compute_j_test(y, X_endo, Z):\n",
    "    \"\"\"\n",
    "    Compute Sargan-Hansen J-test for overidentification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array\n",
    "        Dependent variable\n",
    "    X_endo : array\n",
    "        Endogenous variables\n",
    "    Z : array\n",
    "        Instruments\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : J-statistic, df, p-value\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    \n",
    "    # Add constants\n",
    "    X_endo_const = np.column_stack([np.ones(N), X_endo])\n",
    "    Z_const = np.column_stack([np.ones(N), Z])\n",
    "    \n",
    "    # 2SLS estimation\n",
    "    # First stage\n",
    "    pi_hat = np.linalg.lstsq(Z_const, X_endo_const[:, 1:], rcond=None)[0]\n",
    "    X_endo_pred = Z_const @ pi_hat\n",
    "    X_endo_pred_const = np.column_stack([np.ones(N), X_endo_pred])\n",
    "    \n",
    "    # Second stage\n",
    "    beta_2sls = np.linalg.lstsq(X_endo_pred_const, y, rcond=None)[0]\n",
    "    resid_2sls = y - X_endo_pred_const @ beta_2sls\n",
    "    \n",
    "    # Regress residuals on instruments\n",
    "    gamma_hat = np.linalg.lstsq(Z_const, resid_2sls, rcond=None)[0]\n",
    "    resid_on_z = Z_const @ gamma_hat\n",
    "    \n",
    "    # R-squared from residuals on instruments\n",
    "    ss_total = np.sum((resid_2sls - resid_2sls.mean())**2)\n",
    "    ss_resid = np.sum((resid_2sls - resid_on_z)**2)\n",
    "    r_squared = 1 - (ss_resid / ss_total)\n",
    "    \n",
    "    # J-statistic\n",
    "    j_stat = N * r_squared\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    num_instruments = Z.shape[1]\n",
    "    num_endog = X_endo.shape[1] if X_endo.ndim > 1 else 1\n",
    "    df_j = num_instruments - num_endog\n",
    "    \n",
    "    # P-value\n",
    "    p_value = 1 - stats.chi2.cdf(j_stat, df_j)\n",
    "    \n",
    "    return {\n",
    "        'j_statistic': j_stat,\n",
    "        'df': df_j,\n",
    "        'p_value': p_value\n",
    "    }\n",
    "\n",
    "# Apply to data\n",
    "y = df_overid['y'].values\n",
    "X_endo = df_overid[['x_endo']].values\n",
    "Z = df_overid[['z1', 'z2']].values\n",
    "\n",
    "j_test_result = compute_j_test(y, X_endo, Z)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OVERIDENTIFICATION TEST (J-TEST)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  J-statistic: {j_test_result['j_statistic']:.4f}\")\n",
    "print(f\"  df:          {j_test_result['df']}\")\n",
    "print(f\"  p-value:     {j_test_result['p_value']:.4f}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "if j_test_result['p_value'] > 0.05:\n",
    "    print(\"  ‚úì Fail to reject H‚ÇÄ: Instruments appear valid\")\n",
    "    print(\"    (Cannot reject orthogonality conditions)\")\n",
    "else:\n",
    "    print(\"  ‚úó Reject H‚ÇÄ: At least one instrument is invalid\")\n",
    "    print(\"    (Evidence against orthogonality conditions)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 J-Test with Invalid Instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate with one INVALID instrument\n",
    "np.random.seed(999)\n",
    "data_invalid = []\n",
    "\n",
    "for i in range(100):\n",
    "    for t in range(5):\n",
    "        u = np.random.normal(0, 0.5)  # Error term\n",
    "        \n",
    "        z1 = np.random.normal(0, 1)  # Valid instrument\n",
    "        z2 = np.random.normal(0, 1) + 0.5 * u  # INVALID: correlated with error!\n",
    "        \n",
    "        x_endo = 0.5 * z1 + 0.4 * z2 + np.random.normal(0, 1)\n",
    "        y = 2 * x_endo + u\n",
    "        \n",
    "        data_invalid.append({\n",
    "            'entity': i,\n",
    "            'time': t,\n",
    "            'y': y,\n",
    "            'x_endo': x_endo,\n",
    "            'z1': z1,\n",
    "            'z2': z2\n",
    "        })\n",
    "\n",
    "df_invalid = pd.DataFrame(data_invalid)\n",
    "\n",
    "# Compute J-test\n",
    "y_inv = df_invalid['y'].values\n",
    "X_endo_inv = df_invalid[['x_endo']].values\n",
    "Z_inv = df_invalid[['z1', 'z2']].values\n",
    "\n",
    "j_test_invalid = compute_j_test(y_inv, X_endo_inv, Z_inv)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"J-TEST WITH INVALID INSTRUMENT\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nData Setup:\")\n",
    "print(\"  z1: Valid instrument (uncorrelated with error)\")\n",
    "print(\"  z2: INVALID instrument (correlated with error)\")\n",
    "print(\"\\nJ-Test Results:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"  J-statistic: {j_test_invalid['j_statistic']:.4f}\")\n",
    "print(f\"  df:          {j_test_invalid['df']}\")\n",
    "print(f\"  p-value:     {j_test_invalid['p_value']:.4f}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "if j_test_invalid['p_value'] < 0.05:\n",
    "    print(\"  ‚úì Test CORRECTLY detects invalid instrument!\")\n",
    "    print(\"    (Reject H‚ÇÄ: evidence of instrument invalidity)\")\n",
    "else:\n",
    "    print(\"  ‚ö† Test failed to detect invalid instrument\")\n",
    "    print(\"    (Low power or insufficient violation)\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìå Key Lesson:\")\n",
    "print(\"   J-test can detect invalid instruments when they are\")\n",
    "print(\"   sufficiently correlated with the error term.\")\n",
    "print(\"   However, it has limitations (low power, cannot detect\")\n",
    "print(\"   if ALL instruments are invalid).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Endogeneity Test (Durbin-Wu-Hausman)\n",
    "\n",
    "### 4.1 Testing for Endogeneity\n",
    "\n",
    "**Question**: Is IV estimation even necessary? Or is X actually exogenous?\n",
    "\n",
    "**Durbin-Wu-Hausman (DWH) Test**:\n",
    "- **H‚ÇÄ**: X is exogenous (OLS is consistent and efficient)\n",
    "- **H‚ÇÅ**: X is endogenous (need IV)\n",
    "\n",
    "**Procedure**:\n",
    "1. Estimate **first stage**: X = Z œÄ + v, obtain residuals vÃÇ\n",
    "2. Estimate **augmented regression**: y = X Œ≤ + vÃÇ Œ¥ + u\n",
    "3. Test **H‚ÇÄ: Œ¥ = 0**\n",
    "   - If reject: X is endogenous ‚Üí use IV\n",
    "   - If fail to reject: X is exogenous ‚Üí use OLS\n",
    "\n",
    "**Intuition**: If vÃÇ (first-stage residuals) significantly predicts y, then X is endogenous.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Manual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endogeneity test implementation\n",
    "def durbin_wu_hausman_test(df, y_col, endog_col, instrument_cols):\n",
    "    \"\"\"\n",
    "    Perform Durbin-Wu-Hausman test for endogeneity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    y_col : str\n",
    "        Dependent variable\n",
    "    endog_col : str\n",
    "        Suspected endogenous variable\n",
    "    instrument_cols : list\n",
    "        List of instrument column names\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : Test results\n",
    "    \"\"\"\n",
    "    y = df[y_col].values\n",
    "    X_endo = df[[endog_col]].values\n",
    "    Z = df[instrument_cols].values\n",
    "    \n",
    "    N = len(y)\n",
    "    \n",
    "    # Add constants\n",
    "    Z_const = np.column_stack([np.ones(N), Z])\n",
    "    \n",
    "    # Step 1: First stage - regress X on Z\n",
    "    pi_hat = np.linalg.lstsq(Z_const, X_endo, rcond=None)[0]\n",
    "    X_pred = Z_const @ pi_hat\n",
    "    v_hat = X_endo - X_pred  # First-stage residuals\n",
    "    \n",
    "    # Step 2: Augmented regression - y on X and vÃÇ\n",
    "    X_augmented = np.column_stack([np.ones(N), X_endo, v_hat])\n",
    "    \n",
    "    beta_augmented = np.linalg.lstsq(X_augmented, y, rcond=None)[0]\n",
    "    delta_hat = beta_augmented[2]  # Coefficient on vÃÇ\n",
    "    \n",
    "    # Compute standard error of delta_hat\n",
    "    y_pred_aug = X_augmented @ beta_augmented\n",
    "    resid_aug = y - y_pred_aug\n",
    "    sigma2_aug = np.sum(resid_aug**2) / (N - X_augmented.shape[1])\n",
    "    \n",
    "    var_beta = sigma2_aug * np.linalg.inv(X_augmented.T @ X_augmented)\n",
    "    se_delta = np.sqrt(var_beta[2, 2])\n",
    "    \n",
    "    # t-statistic\n",
    "    t_stat = delta_hat / se_delta\n",
    "    p_value = 2 * (1 - stats.t.cdf(np.abs(t_stat), N - X_augmented.shape[1]))\n",
    "    \n",
    "    return {\n",
    "        'delta_hat': delta_hat,\n",
    "        'se_delta': se_delta,\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value\n",
    "    }\n",
    "\n",
    "# Test on original panel data (should be endogenous)\n",
    "dwh_result = durbin_wu_hausman_test(df_panel, 'y', 'x_endo', ['z'])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DURBIN-WU-HAUSMAN ENDOGENEITY TEST\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNull Hypothesis (H‚ÇÄ): X is exogenous\")\n",
    "print(\"Alternative (H‚ÇÅ):     X is endogenous\")\n",
    "print(\"-\"*60)\n",
    "print(f\"  Œ¥ÃÇ (coefficient on vÃÇ):  {dwh_result['delta_hat']:.4f}\")\n",
    "print(f\"  Standard error:        {dwh_result['se_delta']:.4f}\")\n",
    "print(f\"  t-statistic:           {dwh_result['t_statistic']:.4f}\")\n",
    "print(f\"  p-value:               {dwh_result['p_value']:.4f}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "if dwh_result['p_value'] < 0.05:\n",
    "    print(\"  ‚úó Reject H‚ÇÄ: X is ENDOGENOUS\")\n",
    "    print(\"    ‚Üí Use IV/2SLS estimation\")\n",
    "    print(\"    ‚Üí OLS would be biased and inconsistent\")\n",
    "else:\n",
    "    print(\"  ‚úì Fail to reject H‚ÇÄ: X is EXOGENOUS\")\n",
    "    print(\"    ‚Üí Use OLS (more efficient than IV)\")\n",
    "    print(\"    ‚Üí IV unnecessary\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Testing with Truly Exogenous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data where X is truly EXOGENOUS\n",
    "np.random.seed(2024)\n",
    "data_exog = []\n",
    "\n",
    "for i in range(100):\n",
    "    for t in range(5):\n",
    "        x = np.random.normal(0, 1)  # Exogenous X\n",
    "        z = np.random.normal(0, 1)  # Instrument (not needed)\n",
    "        y = 2 * x + np.random.normal(0, 0.5)  # No endogeneity!\n",
    "        \n",
    "        data_exog.append({\n",
    "            'entity': i,\n",
    "            'time': t,\n",
    "            'y': y,\n",
    "            'x': x,\n",
    "            'z': z\n",
    "        })\n",
    "\n",
    "df_exog = pd.DataFrame(data_exog)\n",
    "\n",
    "# Run DWH test\n",
    "dwh_exog = durbin_wu_hausman_test(df_exog, 'y', 'x', ['z'])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DWH TEST WITH TRULY EXOGENOUS VARIABLE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nData Setup: X is exogenous (no correlation with error)\")\n",
    "print(\"-\"*60)\n",
    "print(f\"  Œ¥ÃÇ:          {dwh_exog['delta_hat']:.4f}\")\n",
    "print(f\"  t-statistic: {dwh_exog['t_statistic']:.4f}\")\n",
    "print(f\"  p-value:     {dwh_exog['p_value']:.4f}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "if dwh_exog['p_value'] >= 0.05:\n",
    "    print(\"  ‚úì Test CORRECTLY fails to reject exogeneity\")\n",
    "    print(\"    ‚Üí OLS is appropriate\")\n",
    "else:\n",
    "    print(\"  ‚ö† Type I error: falsely rejected exogeneity\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìå Interpretation:\")\n",
    "print(\"   When X is truly exogenous, DWH test should fail to reject H‚ÇÄ.\")\n",
    "print(\"   This tells us OLS is more efficient than IV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Weak-Instrument-Robust Inference\n",
    "\n",
    "### 5.1 The Anderson-Rubin Test\n",
    "\n",
    "**Problem**: When instruments are weak (F < 10), standard 2SLS inference is invalid.\n",
    "\n",
    "**Solution**: **Anderson-Rubin (AR) Test** provides valid inference even with weak instruments.\n",
    "\n",
    "**Key Features**:\n",
    "- Tests hypotheses about Œ≤ (e.g., H‚ÇÄ: Œ≤ = Œ≤‚ÇÄ)\n",
    "- **Robust to weak instruments** (does not rely on first-stage strength)\n",
    "- Provides **confidence sets** instead of point estimates\n",
    "\n",
    "**Limitation**: Cannot estimate Œ≤, only test specific values.\n",
    "\n",
    "**Procedure** (simplified):\n",
    "1. For a given Œ≤‚ÇÄ, compute: y - Œ≤‚ÇÄ ¬∑ X\n",
    "2. Regress this on instruments Z\n",
    "3. Test if coefficients on Z are jointly zero\n",
    "4. If fail to reject ‚Üí Œ≤‚ÇÄ is in confidence set\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 When to Use Weak-Instrument-Robust Methods\n",
    "\n",
    "**Use when**:\n",
    "- First-stage F < 10 (weak instruments)\n",
    "- You need to test specific hypotheses about Œ≤\n",
    "- Standard 2SLS confidence intervals seem unreliable\n",
    "\n",
    "**Available methods**:\n",
    "- **Anderson-Rubin (AR) test**: Most common\n",
    "- **Conditional Likelihood Ratio (CLR) test**: More powerful\n",
    "- **Limited Information Maximum Likelihood (LIML)**: Alternative estimator\n",
    "\n",
    "**References**:\n",
    "- Stock & Yogo (2005): Testing for weak instruments\n",
    "- Andrews, Moreira, & Stock (2006): Optimal weak-instrument-robust tests\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Practical Advice\n",
    "\n",
    "**If F < 10**:\n",
    "1. **Do NOT trust** standard 2SLS inference\n",
    "2. **Report** first-stage F prominently\n",
    "3. **Consider**:\n",
    "   - Finding stronger instruments\n",
    "   - Using weak-instrument-robust methods (AR test)\n",
    "   - LIML estimator (less biased than 2SLS with weak instruments)\n",
    "4. **Be transparent** about instrument weakness in reporting\n",
    "\n",
    "**Note**: Anderson-Rubin and related tests are not typically implemented in standard packages. Advanced users can implement manually or use specialized software (e.g., Stata's `weakiv` package, R's `ivmodel`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual demonstration: AR test logic (simplified)\n",
    "def anderson_rubin_test_concept(y, X_endo, Z, beta_0):\n",
    "    \"\"\"\n",
    "    Simplified Anderson-Rubin test for H‚ÇÄ: Œ≤ = Œ≤‚ÇÄ.\n",
    "    \n",
    "    This is a CONCEPTUAL demonstration. Full implementation\n",
    "    requires additional corrections and is beyond scope.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array\n",
    "    X_endo : array\n",
    "    Z : array\n",
    "    beta_0 : float\n",
    "        Null hypothesis value\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : Test statistic and interpretation\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    \n",
    "    # Compute y - Œ≤‚ÇÄ ¬∑ X\n",
    "    y_transformed = y - beta_0 * X_endo.flatten()\n",
    "    \n",
    "    # Add constant to Z\n",
    "    Z_const = np.column_stack([np.ones(N), Z])\n",
    "    \n",
    "    # Regress transformed y on Z\n",
    "    gamma_hat = np.linalg.lstsq(Z_const, y_transformed, rcond=None)[0]\n",
    "    y_pred = Z_const @ gamma_hat\n",
    "    resid = y_transformed - y_pred\n",
    "    \n",
    "    # F-statistic for H‚ÇÄ: Œ≥ = 0 (all coefficients on Z are zero)\n",
    "    SSR_r = np.sum((y_transformed - y_transformed.mean())**2)\n",
    "    SSR_u = np.sum(resid**2)\n",
    "    \n",
    "    num_instruments = Z.shape[1]\n",
    "    \n",
    "    f_stat = ((SSR_r - SSR_u) / num_instruments) / (SSR_u / (N - num_instruments - 1))\n",
    "    p_value = 1 - stats.f.cdf(f_stat, num_instruments, N - num_instruments - 1)\n",
    "    \n",
    "    return {\n",
    "        'f_statistic': f_stat,\n",
    "        'p_value': p_value,\n",
    "        'beta_0': beta_0\n",
    "    }\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANDERSON-RUBIN TEST (CONCEPTUAL)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThis demonstrates the AR test logic for weak instruments.\")\n",
    "print(\"True Œ≤ = 2.0 in our simulated data.\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Test different values of Œ≤\n",
    "beta_values = [1.5, 2.0, 2.5, 3.0]\n",
    "\n",
    "y_test = df_panel['y'].values\n",
    "X_test = df_panel[['x_endo']].values\n",
    "Z_test = df_panel[['z']].values\n",
    "\n",
    "print(f\"\\n{'Œ≤‚ÇÄ':<10} {'F-statistic':<15} {'p-value':<15} {'In 95% CS?':>15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for beta_0 in beta_values:\n",
    "    ar_result = anderson_rubin_test_concept(y_test, X_test, Z_test, beta_0)\n",
    "    in_cs = \"Yes\" if ar_result['p_value'] > 0.05 else \"No\"\n",
    "    print(f\"{beta_0:<10.1f} {ar_result['f_statistic']:<15.2f} {ar_result['p_value']:<15.4f} {in_cs:>15}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìå Interpretation:\")\n",
    "print(\"   Values of Œ≤‚ÇÄ with p-value > 0.05 are in the 95% confidence set.\")\n",
    "print(\"   This method is valid even with weak instruments!\")\n",
    "print(\"\\n‚ö† Note: This is a simplified demonstration. Production use\")\n",
    "print(\"   requires proper implementation with finite-sample corrections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Complete Diagnostic Workflow\n",
    "\n",
    "### 6.1 Comprehensive IV Diagnostic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_iv_diagnostics(df, y_col, endog_col, instrument_cols, entity_col='entity', time_col='time'):\n",
    "    \"\"\"\n",
    "    Run complete IV diagnostic workflow.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    y_col : str\n",
    "    endog_col : str\n",
    "    instrument_cols : list\n",
    "    entity_col : str\n",
    "    time_col : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : All diagnostic results\n",
    "    \"\"\"\n",
    "    diagnostics = {}\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"COMPREHENSIVE IV DIAGNOSTICS WORKFLOW\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ===== STEP 1: First-Stage F-Statistic =====\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 1: FIRST-STAGE F-STATISTIC (Instrument Relevance)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    fs_result = compute_first_stage_f(df, endog_col, instrument_cols)\n",
    "    diagnostics['first_stage_f'] = fs_result\n",
    "    \n",
    "    f_stat = fs_result['f_statistic']\n",
    "    print(f\"\\n  F-statistic: {f_stat:.2f}\")\n",
    "    print(f\"  p-value:     {fs_result['f_pvalue']:.6f}\")\n",
    "    \n",
    "    print(\"\\n  Stock-Yogo Critical Values (1 instrument, 1 endogenous):\")\n",
    "    print(\"    F > 19.93: < 5% relative bias\")\n",
    "    print(\"    F > 16.38: < 10% relative bias\")\n",
    "    print(\"    F > 10:    Rule of thumb\")\n",
    "    \n",
    "    if f_stat > 19.93:\n",
    "        print(\"\\n  ‚úì VERDICT: Very strong instrument (F > 19.93)\")\n",
    "        diagnostics['instrument_strength'] = 'very_strong'\n",
    "    elif f_stat > 16.38:\n",
    "        print(\"\\n  ‚úì VERDICT: Strong instrument (F > 16.38)\")\n",
    "        diagnostics['instrument_strength'] = 'strong'\n",
    "    elif f_stat > 10:\n",
    "        print(\"\\n  ‚ö° VERDICT: Acceptable instrument (10 < F < 16.38)\")\n",
    "        diagnostics['instrument_strength'] = 'acceptable'\n",
    "    else:\n",
    "        print(\"\\n  ‚ö† WARNING: Weak instrument (F < 10)\")\n",
    "        print(\"     ‚Üí IV estimates will be biased\")\n",
    "        print(\"     ‚Üí Standard errors unreliable\")\n",
    "        print(\"     ‚Üí Consider weak-instrument-robust methods\")\n",
    "        diagnostics['instrument_strength'] = 'weak'\n",
    "    \n",
    "    # ===== STEP 2: Overidentification Test =====\n",
    "    num_instruments = len(instrument_cols) if isinstance(instrument_cols, list) else 1\n",
    "    num_endog = 1\n",
    "    \n",
    "    if num_instruments > num_endog:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 2: OVERIDENTIFICATION TEST (J-Test)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        y = df[y_col].values\n",
    "        X_endo = df[[endog_col]].values\n",
    "        Z = df[instrument_cols].values if isinstance(instrument_cols, list) else df[[instrument_cols]].values\n",
    "        \n",
    "        j_result = compute_j_test(y, X_endo, Z)\n",
    "        diagnostics['j_test'] = j_result\n",
    "        \n",
    "        print(f\"\\n  J-statistic: {j_result['j_statistic']:.4f}\")\n",
    "        print(f\"  df:          {j_result['df']}\")\n",
    "        print(f\"  p-value:     {j_result['p_value']:.4f}\")\n",
    "        \n",
    "        if j_result['p_value'] > 0.05:\n",
    "            print(\"\\n  ‚úì VERDICT: Cannot reject instrument validity\")\n",
    "            print(\"     (Orthogonality conditions appear satisfied)\")\n",
    "            diagnostics['instruments_valid'] = True\n",
    "        else:\n",
    "            print(\"\\n  ‚úó VERDICT: Reject instrument validity\")\n",
    "            print(\"     (At least one instrument appears invalid)\")\n",
    "            diagnostics['instruments_valid'] = False\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 2: OVERIDENTIFICATION TEST (J-Test) - SKIPPED\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\n  Model is exactly identified (# instruments = # endogenous)\")\n",
    "        print(\"  J-test requires overidentification.\")\n",
    "        diagnostics['j_test'] = None\n",
    "    \n",
    "    # ===== STEP 3: Endogeneity Test =====\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 3: ENDOGENEITY TEST (Durbin-Wu-Hausman)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    dwh_result = durbin_wu_hausman_test(df, y_col, endog_col, instrument_cols if isinstance(instrument_cols, list) else [instrument_cols])\n",
    "    diagnostics['dwh_test'] = dwh_result\n",
    "    \n",
    "    print(f\"\\n  Œ¥ÃÇ (coefficient on vÃÇ): {dwh_result['delta_hat']:.4f}\")\n",
    "    print(f\"  Standard error:        {dwh_result['se_delta']:.4f}\")\n",
    "    print(f\"  t-statistic:           {dwh_result['t_statistic']:.4f}\")\n",
    "    print(f\"  p-value:               {dwh_result['p_value']:.4f}\")\n",
    "    \n",
    "    if dwh_result['p_value'] < 0.05:\n",
    "        print(\"\\n  ‚úó VERDICT: Reject exogeneity (X is ENDOGENOUS)\")\n",
    "        print(\"     ‚Üí IV estimation is necessary\")\n",
    "        print(\"     ‚Üí OLS would be biased and inconsistent\")\n",
    "        diagnostics['is_endogenous'] = True\n",
    "    else:\n",
    "        print(\"\\n  ‚úì VERDICT: Cannot reject exogeneity (X may be EXOGENOUS)\")\n",
    "        print(\"     ‚Üí OLS is more efficient than IV\")\n",
    "        print(\"     ‚Üí IV may not be necessary\")\n",
    "        diagnostics['is_endogenous'] = False\n",
    "    \n",
    "    # ===== FINAL SUMMARY =====\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DIAGNOSTIC SUMMARY AND RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nüìä Results:\")\n",
    "    print(f\"   - Instrument Strength:  {diagnostics['instrument_strength']}\")\n",
    "    print(f\"   - Endogeneity Detected: {diagnostics['is_endogenous']}\")\n",
    "    if diagnostics['j_test'] is not None:\n",
    "        print(f\"   - Instruments Valid:    {diagnostics['instruments_valid']}\")\n",
    "    \n",
    "    print(\"\\nüéØ Recommendations:\")\n",
    "    \n",
    "    if diagnostics['instrument_strength'] == 'weak':\n",
    "        print(\"   ‚ö† CRITICAL: Weak instruments detected!\")\n",
    "        print(\"      ‚Üí Do NOT trust standard 2SLS inference\")\n",
    "        print(\"      ‚Üí Consider: finding stronger instruments, LIML, or AR test\")\n",
    "    elif not diagnostics['is_endogenous']:\n",
    "        print(\"   ‚úì No evidence of endogeneity - consider using OLS instead\")\n",
    "    elif diagnostics['j_test'] is not None and not diagnostics['instruments_valid']:\n",
    "        print(\"   ‚ö† Invalid instruments detected!\")\n",
    "        print(\"      ‚Üí Re-examine instrument validity\")\n",
    "        print(\"      ‚Üí Results may be unreliable\")\n",
    "    else:\n",
    "        print(\"   ‚úì IV estimation appears appropriate and reliable\")\n",
    "        print(\"      ‚Üí Instruments are strong and valid\")\n",
    "        print(\"      ‚Üí Endogeneity is present\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Running Complete Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive diagnostics on panel data\n",
    "diagnostics_panel = comprehensive_iv_diagnostics(\n",
    "    df=df_panel,\n",
    "    y_col='y',\n",
    "    endog_col='x_endo',\n",
    "    instrument_cols='z',\n",
    "    entity_col='entity',\n",
    "    time_col='time'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run diagnostics on overidentified model\n",
    "diagnostics_overid = comprehensive_iv_diagnostics(\n",
    "    df=df_overid,\n",
    "    y_col='y',\n",
    "    endog_col='x_endo',\n",
    "    instrument_cols=['z1', 'z2'],\n",
    "    entity_col='entity',\n",
    "    time_col='time'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Exercises\n",
    "\n",
    "### Exercise 7.1: Weak Instrument Simulation\n",
    "\n",
    "**Task**: Investigate the consequences of weak instruments.\n",
    "\n",
    "**Instructions**:\n",
    "1. Simulate panel data with a **weak instrument** (Corr(Z, X) ‚âà 0.05)\n",
    "2. Estimate IV model\n",
    "3. Compute first-stage F-statistic\n",
    "4. Compare IV estimate to true Œ≤ - observe bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 7.1\n",
    "# Hint: Modify the weak instrument simulation from Section 1\n",
    "# True Œ≤ = 2.0, use correlation ‚âà 0.05\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2: Overidentification Test with Invalid Instrument\n",
    "\n",
    "**Task**: Test if the J-test can detect invalid instruments.\n",
    "\n",
    "**Instructions**:\n",
    "1. Simulate with 3 instruments, 1 endogenous variable\n",
    "2. Make **one instrument invalid** (correlated with error term)\n",
    "3. Conduct J-test\n",
    "4. Interpret: Does the test detect the invalid instrument?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 7.2\n",
    "# Hint: Create z1, z2 (valid), z3 (invalid: correlated with u)\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3: Complete Diagnostic Workflow\n",
    "\n",
    "**Task**: Apply the comprehensive diagnostic workflow to new data.\n",
    "\n",
    "**Instructions**:\n",
    "1. Simulate panel data with your own specifications\n",
    "2. Run `comprehensive_iv_diagnostics()`\n",
    "3. Interpret all test results\n",
    "4. Make a recommendation: Is IV appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 7.3\n",
    "# Design your own simulation and test it!\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 7.1: Weak Instrument Simulation\n",
    "np.random.seed(1111)\n",
    "data_weak_ex = []\n",
    "\n",
    "for i in range(100):\n",
    "    for t in range(5):\n",
    "        z = np.random.normal(0, 1)\n",
    "        x_endo = 0.05 * z + np.random.normal(0, 1)  # Very weak!\n",
    "        y = 2 * x_endo + np.random.normal(0, 0.5)\n",
    "        data_weak_ex.append({\n",
    "            'entity': i, 'time': t, 'y': y, 'x_endo': x_endo, 'z': z\n",
    "        })\n",
    "\n",
    "df_weak_ex = pd.DataFrame(data_weak_ex)\n",
    "\n",
    "# First-stage F\n",
    "fs_weak_ex = compute_first_stage_f(df_weak_ex, 'x_endo', 'z')\n",
    "\n",
    "print(\"Solution 7.1: Weak Instrument Consequences\")\n",
    "print(\"=\"*60)\n",
    "print(f\"First-stage F: {fs_weak_ex['f_statistic']:.2f}\")\n",
    "print(f\"True Œ≤:        2.0\")\n",
    "print(\"\\nExpected outcome: F << 10, severe bias in IV estimates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 7.2: J-Test with Invalid Instrument\n",
    "np.random.seed(2222)\n",
    "data_3inst = []\n",
    "\n",
    "for i in range(100):\n",
    "    for t in range(5):\n",
    "        u = np.random.normal(0, 0.5)\n",
    "        \n",
    "        z1 = np.random.normal(0, 1)  # Valid\n",
    "        z2 = np.random.normal(0, 1)  # Valid\n",
    "        z3 = np.random.normal(0, 1) + 0.6 * u  # INVALID\n",
    "        \n",
    "        x_endo = 0.4 * z1 + 0.3 * z2 + 0.3 * z3 + np.random.normal(0, 1)\n",
    "        y = 2 * x_endo + u\n",
    "        \n",
    "        data_3inst.append({\n",
    "            'entity': i, 'time': t, 'y': y, 'x_endo': x_endo,\n",
    "            'z1': z1, 'z2': z2, 'z3': z3\n",
    "        })\n",
    "\n",
    "df_3inst = pd.DataFrame(data_3inst)\n",
    "\n",
    "# J-test\n",
    "y_3 = df_3inst['y'].values\n",
    "X_3 = df_3inst[['x_endo']].values\n",
    "Z_3 = df_3inst[['z1', 'z2', 'z3']].values\n",
    "\n",
    "j_3inst = compute_j_test(y_3, X_3, Z_3)\n",
    "\n",
    "print(\"Solution 7.2: J-Test with 3 Instruments (1 invalid)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"J-statistic: {j_3inst['j_statistic']:.4f}\")\n",
    "print(f\"df:          {j_3inst['df']}\")\n",
    "print(f\"p-value:     {j_3inst['p_value']:.4f}\")\n",
    "\n",
    "if j_3inst['p_value'] < 0.05:\n",
    "    print(\"\\n‚úì J-test successfully detected invalid instrument!\")\n",
    "else:\n",
    "    print(\"\\n‚ö† J-test did not detect invalidity (low power)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Weak Instruments (F < 10)**: Serious problem!\n",
    "   - Bias toward OLS (even with valid instruments)\n",
    "   - Unreliable standard errors\n",
    "   - Invalid hypothesis tests\n",
    "   - Solution: Find stronger instruments or use weak-instrument-robust methods\n",
    "\n",
    "2. **First-Stage F-Statistic**: Primary diagnostic for instrument relevance\n",
    "   - **F > 19.93**: Very strong (< 5% relative bias)\n",
    "   - **F > 16.38**: Strong (< 10% relative bias)\n",
    "   - **F > 10**: Acceptable (rule of thumb)\n",
    "   - **F < 10**: Weak (unreliable IV)\n",
    "\n",
    "3. **Overidentification Test (J-test)**: Tests instrument validity\n",
    "   - Only for overidentified models (# instruments > # endogenous)\n",
    "   - Tests if instruments are orthogonal to errors\n",
    "   - Cannot detect if ALL instruments are invalid\n",
    "   - Low power in some settings\n",
    "\n",
    "4. **Endogeneity Test (DWH)**: Tests if IV is necessary\n",
    "   - H‚ÇÄ: X is exogenous (use OLS)\n",
    "   - H‚ÇÅ: X is endogenous (use IV)\n",
    "   - Helps choose between OLS and IV\n",
    "\n",
    "5. **Complete Diagnostics**: Always run ALL tests before trusting IV results\n",
    "   - First-stage F (relevance)\n",
    "   - J-test (validity, if overidentified)\n",
    "   - DWH test (necessity)\n",
    "   - Report diagnostics transparently\n",
    "\n",
    "6. **Weak-Instrument-Robust Methods**: Advanced alternatives\n",
    "   - Anderson-Rubin test\n",
    "   - LIML estimator\n",
    "   - Conditional likelihood ratio test\n",
    "\n",
    "---\n",
    "\n",
    "### Diagnostic Checklist\n",
    "\n",
    "Before using IV estimation results:\n",
    "\n",
    "- [ ] **First-stage F > 10** (preferably > 16.38)\n",
    "- [ ] **J-test p-value > 0.05** (if overidentified)\n",
    "- [ ] **DWH test rejects exogeneity** (p < 0.05)\n",
    "- [ ] **Economic reasoning** supports instrument validity\n",
    "- [ ] **Diagnostics reported** in results (transparency)\n",
    "\n",
    "If ANY of these fail ‚Üí re-examine your instruments!\n",
    "\n",
    "---\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Congratulations!** You've completed the advanced IV diagnostics tutorial!\n",
    "\n",
    "After completing all 7 notebooks in the static panel models series (01-07), you now have:\n",
    "\n",
    "‚úÖ Comprehensive understanding of static panel estimators  \n",
    "‚úÖ Ability to choose appropriate models based on data/context  \n",
    "‚úÖ Skills to conduct rigorous specification tests  \n",
    "‚úÖ Expertise in diagnosing and addressing IV problems  \n",
    "‚úÖ Practical experience with real-world panel data analysis  \n",
    "\n",
    "**Next Steps**:\n",
    "- Explore **dynamic panel models** (GMM series)\n",
    "- Study **advanced topics**: quantile regression, nonlinear models\n",
    "- Apply to **research projects** with real data\n",
    "- Read **econometric literature** on weak instruments (Stock & Yogo, Andrews et al.)\n",
    "\n",
    "---\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "**Essential Papers**:\n",
    "1. Stock, J.H. & Yogo, M. (2005). \"Testing for Weak Instruments in Linear IV Regression.\" In *Identification and Inference for Econometric Models*.\n",
    "2. Andrews, D.W.K., Moreira, M.J., & Stock, J.H. (2006). \"Optimal Two-Sided Invariant Similar Tests for Instrumental Variables Regression.\" *Econometrica*, 74(3).\n",
    "3. Staiger, D. & Stock, J.H. (1997). \"Instrumental Variables Regression with Weak Instruments.\" *Econometrica*, 65(3).\n",
    "4. Hansen, L.P. (1982). \"Large Sample Properties of Generalized Method of Moments Estimators.\" *Econometrica*, 50(4).\n",
    "\n",
    "**Textbooks**:\n",
    "- Wooldridge, J.M. (2010). *Econometric Analysis of Cross Section and Panel Data*, 2nd ed. MIT Press.\n",
    "- Cameron, A.C. & Trivedi, P.K. (2005). *Microeconometrics: Methods and Applications*. Cambridge University Press.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
