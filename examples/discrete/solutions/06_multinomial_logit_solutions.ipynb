{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logit: Solutions\n",
    "\n",
    "**Tutorial Series**: Discrete Choice Econometrics with PanelBox\n",
    "\n",
    "**Notebook**: 06 - Multinomial Logit (Solutions)\n",
    "\n",
    "**Author**: PanelBox Contributors\n",
    "\n",
    "**Date**: 2026-02-17\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains complete solutions for the exercises in `06_multinomial_logit.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup (same as main notebook)\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm, chi2\n",
    "\n",
    "from panelbox.models.discrete.multinomial import MultinomialLogit\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"outputs\"\n",
    "FIG_DIR = OUTPUT_DIR / \"figures\"\n",
    "TABLE_DIR = OUTPUT_DIR / \"tables\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CAREER_LABELS = {0: 'Manual', 1: 'Technical', 2: 'Managerial'}\n",
    "CAREER_COLORS = {'Manual': '#e74c3c', 'Technical': '#3498db', 'Managerial': '#2ecc71'}\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(DATA_DIR / \"career_choice.csv\")\n",
    "full_vars = ['educ', 'exper', 'age', 'female', 'urban']\n",
    "X_full = data[full_vars].values\n",
    "y = data['career'].values\n",
    "\n",
    "# Estimate full model (needed for all exercises)\n",
    "model_full = MultinomialLogit(\n",
    "    endog=y, exog=X_full, n_alternatives=3, base_alternative=0\n",
    ")\n",
    "model_full.exog_names = full_vars\n",
    "results_full = model_full.fit()\n",
    "\n",
    "print(\"Setup complete. Full model estimated.\")\n",
    "print(f\"Log-L: {results_full.llf:.2f}, Accuracy: {results_full.accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Reference Category Invariance (Easy)\n",
    "\n",
    "**Task**: Estimate the model with `base_alternative=1` and verify predictions are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "\n",
    "# Estimate with base = 0 (Manual)\n",
    "model_base0 = MultinomialLogit(\n",
    "    endog=y, exog=X_full, n_alternatives=3, base_alternative=0\n",
    ")\n",
    "results_base0 = model_base0.fit()\n",
    "\n",
    "# Estimate with base = 1 (Technical)\n",
    "model_base1 = MultinomialLogit(\n",
    "    endog=y, exog=X_full, n_alternatives=3, base_alternative=1\n",
    ")\n",
    "results_base1 = model_base1.fit()\n",
    "\n",
    "# Estimate with base = 2 (Managerial)\n",
    "model_base2 = MultinomialLogit(\n",
    "    endog=y, exog=X_full, n_alternatives=3, base_alternative=2\n",
    ")\n",
    "results_base2 = model_base2.fit()\n",
    "\n",
    "print(\"=== Coefficients with Different Base Categories ===\")\n",
    "for base, res, mod in [(0, results_base0, model_base0),\n",
    "                        (1, results_base1, model_base1),\n",
    "                        (2, results_base2, model_base2)]:\n",
    "    print(f\"\\nBase = {base} ({CAREER_LABELS[base]}):\")\n",
    "    for idx, j in enumerate(mod.non_base_alts):\n",
    "        coefs = ', '.join([f'{v}={c:+.4f}' for v, c in zip(full_vars, res.params_matrix[idx])])\n",
    "        print(f\"  {CAREER_LABELS[j]:10s} vs {CAREER_LABELS[base]:10s}: {coefs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predicted probabilities\n",
    "probs_base0 = results_base0.predict_proba()\n",
    "probs_base1 = results_base1.predict_proba()\n",
    "probs_base2 = results_base2.predict_proba()\n",
    "\n",
    "print(\"=== First 5 Predicted Probability Vectors ===\")\n",
    "print(\"\\nBase = 0 (Manual):\")\n",
    "print(pd.DataFrame(probs_base0[:5], columns=['Manual', 'Technical', 'Managerial']).round(4))\n",
    "print(\"\\nBase = 1 (Technical):\")\n",
    "print(pd.DataFrame(probs_base1[:5], columns=['Manual', 'Technical', 'Managerial']).round(4))\n",
    "\n",
    "# Verify identity\n",
    "print(\"\\n=== Numerical Verification ===\")\n",
    "print(f\"Base 0 vs Base 1: max diff = {np.max(np.abs(probs_base0 - probs_base1)):.2e}\")\n",
    "print(f\"Base 0 vs Base 2: max diff = {np.max(np.abs(probs_base0 - probs_base2)):.2e}\")\n",
    "print(f\"Base 1 vs Base 2: max diff = {np.max(np.abs(probs_base1 - probs_base2)):.2e}\")\n",
    "print(f\"\\nAll identical: {np.allclose(probs_base0, probs_base1) and np.allclose(probs_base0, probs_base2)}\")\n",
    "\n",
    "print(\"\\n=== Interpretation ===\")\n",
    "print(\"Coefficients CHANGE with the base category (they are relative to the base).\")\n",
    "print(\"Predictions do NOT change. The base is a normalization, not a substantive choice.\")\n",
    "print(\"Choose the base that makes interpretation most natural.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: IIA Test (Medium)\n",
    "\n",
    "**Task**: Run the Hausman-McFadden test by omitting each non-base alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" \" * 15 + \"HAUSMAN-MCFADDEN IIA TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "iia_results = []\n",
    "\n",
    "for alt_to_remove in [1, 2]:  # Omit Technical, then Managerial\n",
    "    # Restrict data\n",
    "    mask = data['career'] != alt_to_remove\n",
    "    data_sub = data[mask].copy()\n",
    "    \n",
    "    # Remap to binary (0 = Manual, 1 = remaining non-base)\n",
    "    remaining_alts = sorted(set(range(3)) - {alt_to_remove})\n",
    "    remap = {old: new for new, old in enumerate(remaining_alts)}\n",
    "    y_sub = data_sub['career'].map(remap).values\n",
    "    X_sub = data_sub[full_vars].values\n",
    "    \n",
    "    base_in_sub = remap[0]  # Manual stays as base\n",
    "    \n",
    "    model_sub = MultinomialLogit(\n",
    "        endog=y_sub, exog=X_sub, n_alternatives=2, base_alternative=base_in_sub\n",
    "    )\n",
    "    results_sub = model_sub.fit()\n",
    "    \n",
    "    # Comparable parameters\n",
    "    remaining_nonbase = [a for a in remaining_alts if a != 0][0]\n",
    "    full_idx = list(model_full.non_base_alts).index(remaining_nonbase)\n",
    "    \n",
    "    beta_full = results_full.params_matrix[full_idx]\n",
    "    beta_sub = results_sub.params_matrix[0]\n",
    "    \n",
    "    K = model_full.K\n",
    "    start = full_idx * K\n",
    "    end = start + K\n",
    "    vcov_full = results_full.cov_params[start:end, start:end]\n",
    "    vcov_sub = results_sub.cov_params[:K, :K]\n",
    "    \n",
    "    diff = beta_sub - beta_full\n",
    "    vcov_diff = vcov_sub - vcov_full\n",
    "    \n",
    "    try:\n",
    "        H = float(diff @ np.linalg.inv(vcov_diff) @ diff)\n",
    "        df = K\n",
    "        p_value = 1 - chi2.cdf(abs(H), df)\n",
    "        conclusion = 'Fail to reject IIA' if p_value > 0.05 else 'Reject IIA'\n",
    "    except np.linalg.LinAlgError:\n",
    "        H, p_value = np.nan, np.nan\n",
    "        conclusion = 'Singular matrix'\n",
    "    \n",
    "    iia_results.append({\n",
    "        'Removed': CAREER_LABELS[alt_to_remove],\n",
    "        'N_obs': len(data_sub),\n",
    "        'H': H,\n",
    "        'df': K,\n",
    "        'p_value': p_value,\n",
    "        'Conclusion': conclusion\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nOmitting {CAREER_LABELS[alt_to_remove]}:\")\n",
    "    print(f\"  Coefficient comparison ({CAREER_LABELS[remaining_nonbase]} vs Manual):\")\n",
    "    for i, var in enumerate(full_vars):\n",
    "        print(f\"    {var:8s}: Full = {beta_full[i]:+.4f}, Restricted = {beta_sub[i]:+.4f}, Diff = {diff[i]:+.4f}\")\n",
    "    print(f\"  H = {H:.4f}, df = {K}, p = {p_value:.4f}\")\n",
    "    print(f\"  -> {conclusion}\")\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "iia_df = pd.DataFrame(iia_results)\n",
    "print(iia_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nInterpretation: If both tests fail to reject, the IIA assumption\")\n",
    "print(\"appears reasonable for this career choice model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Marginal Effects Interpretation (Medium)\n",
    "\n",
    "**Task**: Calculate AME for education and verify sum-to-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Solution\n",
    "\n",
    "# Step 1: AME for education\n",
    "me_educ_overall = results_full.marginal_effects(at='overall', variable=0)\n",
    "\n",
    "print(\"=== AME for Education (at='overall') ===\")\n",
    "for j in range(3):\n",
    "    print(f\"  P({CAREER_LABELS[j]:10s}): {me_educ_overall[j]:+.4f} ({me_educ_overall[j]*100:+.2f} pp per extra year)\")\n",
    "\n",
    "# Step 2: Verify sum-to-zero\n",
    "total = me_educ_overall.sum()\n",
    "print(f\"\\nSum of AME: {total:.8f}\")\n",
    "print(f\"Sum-to-zero check: {'PASS' if abs(total) < 1e-4 else 'CHECK'}\")\n",
    "\n",
    "# Step 3: Interpretation\n",
    "print(\"\\n=== Interpretation ===\")\n",
    "print(f\"One additional year of education:\")\n",
    "print(f\"  - INCREASES P(Managerial) by {me_educ_overall[2]*100:+.2f} pp\")\n",
    "print(f\"  - DECREASES P(Manual) by {me_educ_overall[0]*100:+.2f} pp\")\n",
    "print(f\"  - Changes P(Technical) by {me_educ_overall[1]*100:+.2f} pp\")\n",
    "print(f\"  - The gains in Managerial come mainly from losses in Manual and Technical.\")\n",
    "print(f\"  - This is a direct consequence of the sum-to-zero constraint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compare 'overall' vs 'mean'\n",
    "me_educ_mean = results_full.marginal_effects(at='mean', variable=0)\n",
    "\n",
    "print(\"=== AME: Overall vs At-Mean ===\")\n",
    "comparison = pd.DataFrame({\n",
    "    'AME (overall)': me_educ_overall,\n",
    "    'ME at mean': me_educ_mean,\n",
    "    'Difference': me_educ_overall - me_educ_mean\n",
    "}, index=[CAREER_LABELS[j] for j in range(3)])\n",
    "print(comparison.round(4))\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  'overall' = average of individual-level MEs (accounts for full distribution)\")\n",
    "print(\"  'mean'    = ME evaluated at the average individual\")\n",
    "print(\"  The difference arises because the MNL probability function is nonlinear.\")\n",
    "print(\"  AME (overall) is generally preferred for policy analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Multinomial vs Conditional Logit (Hard)\n",
    "\n",
    "**Task**: Compare MNL and Conditional Logit using transportation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 Solution\n",
    "\n",
    "from panelbox.models.discrete.multinomial import ConditionalLogit\n",
    "\n",
    "# Step 1: Load transportation data\n",
    "transport = pd.read_csv(DATA_DIR / \"transportation_choice.csv\")\n",
    "transport['choice_id'] = transport['id'].astype(str) + '_' + transport['year'].astype(str)\n",
    "\n",
    "print(f\"Transportation data: {transport.shape}\")\n",
    "print(f\"Modes: {sorted(transport['mode'].unique())}\")\n",
    "\n",
    "# Step 2: Conditional Logit (alternative-specific attributes)\n",
    "model_cl = ConditionalLogit(\n",
    "    data=transport,\n",
    "    choice_col='choice_id',\n",
    "    alt_col='mode',\n",
    "    chosen_col='choice',\n",
    "    alt_varying_vars=['cost', 'time', 'reliability', 'comfort']\n",
    ")\n",
    "results_cl = model_cl.fit()\n",
    "\n",
    "print(\"\\n=== Conditional Logit Results ===\")\n",
    "print(results_cl.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Multinomial Logit (individual-specific attributes)\n",
    "# First, reshape to wide format (one row per choice occasion)\n",
    "chosen = transport[transport['choice'] == 1][['choice_id', 'mode', 'income', 'distance']].copy()\n",
    "\n",
    "# Map mode to numeric\n",
    "mode_map = {m: i for i, m in enumerate(sorted(transport['mode'].unique()))}\n",
    "chosen['mode_num'] = chosen['mode'].map(mode_map)\n",
    "\n",
    "# Estimate MNL with individual-level variables\n",
    "X_transport = chosen[['income', 'distance']].values\n",
    "y_transport = chosen['mode_num'].values\n",
    "\n",
    "model_mnl = MultinomialLogit(\n",
    "    endog=y_transport,\n",
    "    exog=X_transport,\n",
    "    n_alternatives=len(mode_map),\n",
    "    base_alternative=0\n",
    ")\n",
    "model_mnl.exog_names = ['income', 'distance']\n",
    "results_mnl = model_mnl.fit()\n",
    "\n",
    "print(\"\\n=== Multinomial Logit Results ===\")\n",
    "print(results_mnl.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Comparison\n",
    "print(\"=\" * 70)\n",
    "print(\" \" * 10 + \"CONDITIONAL LOGIT vs MULTINOMIAL LOGIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'Conditional Logit':<20} {'Multinomial Logit':<20}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Log-likelihood':<25} {results_cl.llf:<20.2f} {results_mnl.llf:<20.2f}\")\n",
    "print(f\"{'Pseudo R-squared':<25} {results_cl.pseudo_r2:<20.4f} {results_mnl.pseudo_r2:<20.4f}\")\n",
    "print(f\"{'Accuracy':<25} {results_cl.accuracy:<20.4f} {results_mnl.accuracy:<20.4f}\")\n",
    "print(f\"{'N parameters':<25} {len(results_cl.params):<20d} {len(results_mnl.params):<20d}\")\n",
    "\n",
    "print(\"\\n=== When to Use Each ===\")\n",
    "print(\"\\nConditional Logit:\")\n",
    "print(\"  - When alternatives differ in measurable attributes (cost, time, quality)\")\n",
    "print(\"  - Common coefficients: effect of cost is the SAME for all alternatives\")\n",
    "print(\"  - Ideal for policy simulations (what if bus cost decreases?)\")\n",
    "\n",
    "print(\"\\nMultinomial Logit:\")\n",
    "print(\"  - When alternatives differ mainly in the eyes of the individual\")\n",
    "print(\"  - Alternative-specific coefficients: education affects each career differently\")\n",
    "print(\"  - Ideal for understanding WHO chooses WHAT\")\n",
    "\n",
    "print(\"\\nMixed (Generalized MNL):\")\n",
    "print(\"  - Use both individual-specific AND alternative-specific variables\")\n",
    "print(\"  - Most flexible but requires more data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Subgroup Analysis (Hard)\n",
    "\n",
    "**Task**: Estimate career choice models separately for men and women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 Solution\n",
    "\n",
    "# Step 1: Split by gender\n",
    "data_male = data[data['female'] == 0]\n",
    "data_female = data[data['female'] == 1]\n",
    "\n",
    "print(f\"Male observations: {len(data_male)}\")\n",
    "print(f\"Female observations: {len(data_female)}\")\n",
    "\n",
    "# Career distribution by gender\n",
    "print(\"\\nCareer distribution:\")\n",
    "for label, sub in [('Male', data_male), ('Female', data_female)]:\n",
    "    dist = sub['career'].value_counts(normalize=True).sort_index()\n",
    "    print(f\"  {label}: Manual={dist[0]:.1%}, Technical={dist[1]:.1%}, Managerial={dist[2]:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Estimate for each subgroup (drop 'female' from covariates)\n",
    "sub_vars = ['educ', 'exper', 'age', 'urban']\n",
    "\n",
    "# Male model\n",
    "model_male = MultinomialLogit(\n",
    "    endog=data_male['career'].values,\n",
    "    exog=data_male[sub_vars].values,\n",
    "    n_alternatives=3,\n",
    "    base_alternative=0\n",
    ")\n",
    "model_male.exog_names = sub_vars\n",
    "results_male = model_male.fit()\n",
    "\n",
    "# Female model\n",
    "model_female = MultinomialLogit(\n",
    "    endog=data_female['career'].values,\n",
    "    exog=data_female[sub_vars].values,\n",
    "    n_alternatives=3,\n",
    "    base_alternative=0\n",
    ")\n",
    "model_female.exog_names = sub_vars\n",
    "results_female = model_female.fit()\n",
    "\n",
    "print(\"=== Male Model ===\")\n",
    "print(results_male.summary())\n",
    "print(\"\\n=== Female Model ===\")\n",
    "print(results_female.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compare coefficients\n",
    "print(\"=== Coefficient Comparison: Male vs Female ===\")\n",
    "print(f\"\\n{'Variable':<10} {'Alt':<12} {'Male':>10} {'Female':>10} {'Diff':>10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for idx, j in enumerate(model_male.non_base_alts):\n",
    "    for k, var in enumerate(sub_vars):\n",
    "        coef_m = results_male.params_matrix[idx, k]\n",
    "        coef_f = results_female.params_matrix[idx, k]\n",
    "        print(f\"{var:<10} {CAREER_LABELS[j]:<12} {coef_m:+10.4f} {coef_f:+10.4f} {coef_f-coef_m:+10.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compare AME\n",
    "ame_male = results_male.marginal_effects(at='overall')\n",
    "ame_female = results_female.marginal_effects(at='overall')\n",
    "\n",
    "print(\"=== AME Comparison ===\")\n",
    "print(f\"\\n{'Variable':<10} {'Career':<12} {'Male AME':>10} {'Female AME':>10} {'Gap':>10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for k, var in enumerate(sub_vars):\n",
    "    for j in range(3):\n",
    "        me_m = ame_male[j, k]\n",
    "        me_f = ame_female[j, k]\n",
    "        print(f\"{var if j==0 else '':<10} {CAREER_LABELS[j]:<12} {me_m:+10.4f} {me_f:+10.4f} {me_f-me_m:+10.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize differences\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot AME of education by gender\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "labels_list = [CAREER_LABELS[j] for j in range(3)]\n",
    "\n",
    "# Education AME\n",
    "educ_idx = sub_vars.index('educ')\n",
    "bars1 = axes[0].bar(x - width/2, ame_male[:, educ_idx], width, label='Male',\n",
    "                    color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars2 = axes[0].bar(x + width/2, ame_female[:, educ_idx], width, label='Female',\n",
    "                    color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "axes[0].set_title('AME of Education by Gender', fontweight='bold')\n",
    "axes[0].set_xlabel('Career')\n",
    "axes[0].set_ylabel('AME')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(labels_list)\n",
    "axes[0].legend()\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Urban AME\n",
    "urban_idx = sub_vars.index('urban')\n",
    "bars1 = axes[1].bar(x - width/2, ame_male[:, urban_idx], width, label='Male',\n",
    "                    color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars2 = axes[1].bar(x + width/2, ame_female[:, urban_idx], width, label='Female',\n",
    "                    color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "axes[1].set_title('AME of Urban Location by Gender', fontweight='bold')\n",
    "axes[1].set_xlabel('Career')\n",
    "axes[1].set_ylabel('AME')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(labels_list)\n",
    "axes[1].legend()\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Subgroup Analysis: Career Choice Determinants by Gender',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Key Findings ===\")\n",
    "print(\"The subgroup analysis reveals whether career determinants\")\n",
    "print(\"operate differently for men and women. Differences in AME\")\n",
    "print(\"indicate that the same variable (e.g., education) may have\")\n",
    "print(\"different impacts on career selection for each gender.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Solutions**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
