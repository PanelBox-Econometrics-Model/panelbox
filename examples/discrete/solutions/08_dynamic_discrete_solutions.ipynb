{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Discrete Choice Models: Solutions\n",
    "\n",
    "**Tutorial Series**: Discrete Choice Econometrics with PanelBox\n",
    "\n",
    "**Notebook**: 08 - Dynamic Discrete Choice (Solutions)\n",
    "\n",
    "**Author**: PanelBox Contributors\n",
    "\n",
    "**Date**: 2026-02-17\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains complete solutions for the exercises in `08_dynamic_discrete.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup (same as main notebook)\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm, chi2\n",
    "from scipy.optimize import minimize\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from panelbox.models.discrete.dynamic import DynamicBinaryPanel\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"outputs\"\n",
    "FIG_DIR = OUTPUT_DIR / \"figures\"\n",
    "TABLE_DIR = OUTPUT_DIR / \"tables\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data and prepare dynamic dataset\n",
    "data = pd.read_csv(DATA_DIR / \"labor_dynamics.csv\")\n",
    "data = data.sort_values(['id', 'year'])\n",
    "\n",
    "exog_vars = ['age', 'educ', 'kids', 'married']\n",
    "mean_vars = ['age', 'kids', 'married']\n",
    "\n",
    "data['emp_lag'] = data.groupby('id')['employed'].shift(1)\n",
    "data['emp_init'] = data.groupby('id')['employed'].transform('first')\n",
    "for var in mean_vars:\n",
    "    data[f'{var}_mean'] = data.groupby('id')[var].transform('mean')\n",
    "\n",
    "data_dyn = data.dropna(subset=['emp_lag']).copy()\n",
    "\n",
    "# Estimate base models (needed for exercises)\n",
    "wooldridge_vars = exog_vars + ['emp_lag', 'emp_init'] + [f'{v}_mean' for v in mean_vars]\n",
    "X_wool = sm.add_constant(data_dyn[wooldridge_vars])\n",
    "wooldridge_pooled = sm.Probit(data_dyn['employed'], X_wool).fit(method='bfgs', disp=0)\n",
    "\n",
    "print(\"Setup complete.\")\n",
    "print(f\"Dynamic dataset: {len(data_dyn)} obs, {data_dyn['id'].nunique()} individuals\")\n",
    "print(f\"Wooldridge gamma: {wooldridge_pooled.params['emp_lag']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Data Preparation (Easy)\n",
    "\n",
    "**Task**: Starting from raw panel data, create the dynamic dataset: add lag of y, initial value $y_{i,0}$, and time means of X. Verify dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "\n",
    "# Step 1: Load fresh data and create lag\n",
    "ex1_data = pd.read_csv(DATA_DIR / \"labor_dynamics.csv\")\n",
    "ex1_data = ex1_data.sort_values(['id', 'year'])\n",
    "\n",
    "ex1_data['emp_lag'] = ex1_data.groupby('id')['employed'].shift(1)\n",
    "print(f\"Step 1: Created lagged employment\")\n",
    "print(f\"  NaN count: {ex1_data['emp_lag'].isna().sum()} (should be {ex1_data['id'].nunique()})\")\n",
    "\n",
    "# Step 2: Initial value\n",
    "ex1_data['emp_init'] = ex1_data.groupby('id')['employed'].transform('first')\n",
    "print(f\"\\nStep 2: Initial employment\")\n",
    "print(f\"  P(emp_init=1): {ex1_data['emp_init'].mean():.3f}\")\n",
    "\n",
    "# Step 3: Time means (for time-varying variables)\n",
    "time_varying = ['age', 'kids', 'married', 'husbinc']\n",
    "for var in time_varying:\n",
    "    ex1_data[f'{var}_mean'] = ex1_data.groupby('id')[var].transform('mean')\n",
    "print(f\"\\nStep 3: Time means computed for: {time_varying}\")\n",
    "\n",
    "# Step 4: Drop first period (no lag available)\n",
    "ex1_dyn = ex1_data.dropna(subset=['emp_lag']).copy()\n",
    "print(f\"\\nStep 4: Dropped first period\")\n",
    "print(f\"  Before: {len(ex1_data)} obs\")\n",
    "print(f\"  After:  {len(ex1_dyn)} obs\")\n",
    "\n",
    "# Step 5: Verify dimensions\n",
    "n_individuals = ex1_data['id'].nunique()\n",
    "n_periods = ex1_data['year'].nunique()\n",
    "expected_n = n_individuals * (n_periods - 1)\n",
    "print(f\"\\nStep 5: Verification\")\n",
    "print(f\"  Expected: {n_individuals} x {n_periods - 1} = {expected_n}\")\n",
    "print(f\"  Got: {len(ex1_dyn)}\")\n",
    "print(f\"  Match: {len(ex1_dyn) == expected_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer questions\n",
    "print(\"=== Answers ===\")\n",
    "\n",
    "# Q1: How many observations lost?\n",
    "lost = len(ex1_data) - len(ex1_dyn)\n",
    "print(f\"\\n1. Observations lost: {lost} (one per individual = {n_individuals})\")\n",
    "\n",
    "# Q2: Correlation between emp_init and mean employment\n",
    "emp_mean_by_id = ex1_data.groupby('id')['employed'].mean()\n",
    "emp_init_by_id = ex1_data.groupby('id')['emp_init'].first()\n",
    "corr = emp_mean_by_id.corr(emp_init_by_id)\n",
    "print(f\"\\n2. Corr(emp_init, mean_employed): {corr:.3f}\")\n",
    "print(f\"   This shows that initial employment is positively correlated\")\n",
    "print(f\"   with overall employment history, confirming the initial\")\n",
    "print(f\"   conditions problem: emp_init is not random.\")\n",
    "\n",
    "# Q3: Why only time-varying variables?\n",
    "print(f\"\\n3. We only include time-varying variables in Mundlak means because:\")\n",
    "print(f\"   - Time-invariant variables (e.g., educ) have X_mean = X for all t\")\n",
    "print(f\"   - Including their mean would create perfect collinearity\")\n",
    "print(f\"   - The Mundlak device aims to capture within-individual variation\")\n",
    "print(f\"     that correlates with the unobserved effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Naive vs Wooldridge (Medium)\n",
    "\n",
    "**Task**: Estimate dynamic Probit ignoring initial conditions (naive) and with Wooldridge approach. Compare $\\gamma$ estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution\n",
    "\n",
    "# Step 1: Naive dynamic probit (only X and emp_lag, no initial conditions)\n",
    "X_naive = sm.add_constant(data_dyn[exog_vars + ['emp_lag']])\n",
    "naive_probit = sm.Probit(data_dyn['employed'], X_naive).fit(method='bfgs', disp=0)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"   NAIVE DYNAMIC PROBIT\")\n",
    "print(\"=\" * 60)\n",
    "print(naive_probit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Wooldridge probit (already estimated in setup)\n",
    "print(\"=\" * 60)\n",
    "print(\"   WOOLDRIDGE DYNAMIC PROBIT\")\n",
    "print(\"=\" * 60)\n",
    "print(wooldridge_pooled.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compare gamma\n",
    "gamma_naive = naive_probit.params['emp_lag']\n",
    "gamma_wool = wooldridge_pooled.params['emp_lag']\n",
    "bias = gamma_naive - gamma_wool\n",
    "\n",
    "print(\"=== Comparison ===\")\n",
    "print(f\"\\n{'':20s} {'Naive':>12s} {'Wooldridge':>12s}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "for var in exog_vars + ['emp_lag']:\n",
    "    b_n = naive_probit.params[var]\n",
    "    b_w = wooldridge_pooled.params[var]\n",
    "    print(f\"{var:<20s} {b_n:>12.4f} {b_w:>12.4f}\")\n",
    "\n",
    "print(f\"\\n{'Log-likelihood':<20s} {naive_probit.llf:>12.2f} {wooldridge_pooled.llf:>12.2f}\")\n",
    "print(f\"{'AIC':<20s} {naive_probit.aic:>12.2f} {wooldridge_pooled.aic:>12.2f}\")\n",
    "\n",
    "# Step 4: Compute bias\n",
    "print(f\"\\n=== Bias Analysis ===\")\n",
    "print(f\"gamma_naive:      {gamma_naive:.4f}\")\n",
    "print(f\"gamma_Wooldridge: {gamma_wool:.4f}\")\n",
    "print(f\"Bias (naive - Wooldridge): {bias:+.4f}\")\n",
    "print(f\"Relative bias: {bias/gamma_wool:+.1%}\")\n",
    "\n",
    "print(f\"\\n=== Answers ===\")\n",
    "print(f\"1. Direction: The naive estimate is biased {'upward' if bias > 0 else 'downward'}.\")\n",
    "print(f\"   gamma_naive = {gamma_naive:.4f} vs gamma_Wooldridge = {gamma_wool:.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"2. Why upward bias: Ignoring initial conditions means the model\")\n",
    "print(f\"   attributes part of the unobserved heterogeneity (alpha_i) to the\")\n",
    "print(f\"   lagged dependent variable. Since Cov(y_{{i,t-1}}, alpha_i) > 0,\")\n",
    "print(f\"   the omitted variable bias pushes gamma upward.\")\n",
    "print(f\"\")\n",
    "print(f\"3. Is delta_1 (emp_init) significant?\")\n",
    "delta1 = wooldridge_pooled.params['emp_init']\n",
    "se_delta1 = wooldridge_pooled.bse['emp_init']\n",
    "z_delta1 = delta1 / se_delta1\n",
    "p_delta1 = 2 * (1 - norm.cdf(abs(z_delta1)))\n",
    "print(f\"   delta_1 = {delta1:.4f}, SE = {se_delta1:.4f}, z = {z_delta1:.2f}, p = {p_delta1:.6f}\")\n",
    "print(f\"   {'Yes, significant' if p_delta1 < 0.05 else 'Not significant'} at 5%.\")\n",
    "print(f\"   This confirms initial conditions are correlated with heterogeneity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Persistence Decomposition (Medium)\n",
    "\n",
    "**Task**: Using estimated parameters, decompose total persistence into state dependence and heterogeneity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Solution\n",
    "\n",
    "def simulate_persistence(n=1000, T=15, gamma=0.5, sigma_u=0.8, beta_x=0.3):\n",
    "    \"\"\"Simulate and compute serial correlation.\"\"\"\n",
    "    alpha = np.random.normal(0, sigma_u, n)\n",
    "    y_matrix = np.zeros((n, T))\n",
    "\n",
    "    for i in range(n):\n",
    "        x = np.random.normal(0, 1)\n",
    "        y_prev = int(np.random.normal(beta_x * x + alpha[i], 1) > 0)\n",
    "        for t in range(T):\n",
    "            xb = -0.3 + beta_x * x + gamma * y_prev + alpha[i]\n",
    "            y = int(np.random.normal(xb, 1) > 0)\n",
    "            y_matrix[i, t] = y\n",
    "            y_prev = y\n",
    "\n",
    "    y_flat = y_matrix[:, 1:].flatten()\n",
    "    y_lag_flat = y_matrix[:, :-1].flatten()\n",
    "    return np.corrcoef(y_flat, y_lag_flat)[0, 1]\n",
    "\n",
    "# Decomposition for different parameter combinations\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=== Persistence Decomposition Across Parameter Values ===\")\n",
    "print(f\"\\n{'gamma':>6s} {'sigma_u':>8s} {'Total':>8s} {'SD Only':>8s} {'Het Only':>8s} {'SD Share':>9s} {'Het Share':>10s}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "results_list = []\n",
    "for gamma_val in [0.2, 0.5, 0.8]:\n",
    "    for sigma_val in [0.3, 0.8, 1.5]:\n",
    "        total = simulate_persistence(gamma=gamma_val, sigma_u=sigma_val)\n",
    "        sd_only = simulate_persistence(gamma=gamma_val, sigma_u=0.0)\n",
    "        het_only = simulate_persistence(gamma=0.0, sigma_u=sigma_val)\n",
    "        sd_share = sd_only / total if total > 0 else 0\n",
    "        het_share = het_only / total if total > 0 else 0\n",
    "\n",
    "        print(f\"{gamma_val:>6.1f} {sigma_val:>8.1f} {total:>8.3f} {sd_only:>8.3f} {het_only:>8.3f} {sd_share:>8.0%} {het_share:>9.0%}\")\n",
    "\n",
    "        results_list.append({\n",
    "            'gamma': gamma_val, 'sigma_u': sigma_val,\n",
    "            'total': total, 'sd_only': sd_only, 'het_only': het_only,\n",
    "            'sd_share': sd_share, 'het_share': het_share\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: heatmap of SD share\n",
    "results_df = pd.DataFrame(results_list)\n",
    "pivot_sd = results_df.pivot(index='gamma', columns='sigma_u', values='sd_share')\n",
    "pivot_total = results_df.pivot(index='gamma', columns='sigma_u', values='total')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel A: State dependence share\n",
    "sns.heatmap(pivot_sd, annot=True, fmt='.0%', cmap='Blues', ax=axes[0],\n",
    "            linewidths=2, vmin=0, vmax=1)\n",
    "axes[0].set_title('State Dependence Share of Total Persistence', fontweight='bold')\n",
    "axes[0].set_xlabel('$\\\\sigma_u$ (heterogeneity)')\n",
    "axes[0].set_ylabel('$\\\\gamma$ (state dependence)')\n",
    "\n",
    "# Panel B: Total persistence\n",
    "sns.heatmap(pivot_total, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[1],\n",
    "            linewidths=2)\n",
    "axes[1].set_title('Total Persistence (Autocorrelation)', fontweight='bold')\n",
    "axes[1].set_xlabel('$\\\\sigma_u$ (heterogeneity)')\n",
    "axes[1].set_ylabel('$\\\\gamma$ (state dependence)')\n",
    "\n",
    "plt.suptitle('Persistence Decomposition Across Parameter Space',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== Policy Implications ===\")\n",
    "print(\"\\nWhen state dependence share is HIGH (top-left of heatmap):\")\n",
    "print(\"  -> Temporary employment programs have lasting effects\")\n",
    "print(\"  -> Getting someone into a job creates momentum\")\n",
    "print(\"\\nWhen heterogeneity share is HIGH (bottom-right of heatmap):\")\n",
    "print(\"  -> Need to target specific groups with persistent barriers\")\n",
    "print(\"  -> Temporary programs won't help in the long run\")\n",
    "print(\"  -> Focus on structural interventions (education, training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Counterfactual Simulation (Hard)\n",
    "\n",
    "**Task**: Simulate trajectories for job loss scenario. Compare recovery under different state dependence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 Solution\n",
    "\n",
    "np.random.seed(42)\n",
    "n_cf = 500\n",
    "n_periods_cf = 20\n",
    "\n",
    "gamma_est = wooldridge_pooled.params['emp_lag']\n",
    "intercept = wooldridge_pooled.params['const']\n",
    "beta_dict = {v: wooldridge_pooled.params[v] for v in exog_vars}\n",
    "X_mean = data_dyn[exog_vars].mean()\n",
    "\n",
    "def simulate_counterfactual(gamma, sigma_u=0.8, shock_period=5):\n",
    "    \"\"\"Simulate baseline and job-loss counterfactual.\"\"\"\n",
    "    alpha = np.random.normal(0, sigma_u, n_cf)\n",
    "    traj_base = np.zeros((n_cf, n_periods_cf))\n",
    "    traj_shock = np.zeros((n_cf, n_periods_cf))\n",
    "\n",
    "    for i in range(n_cf):\n",
    "        y_b, y_s = 1, 1  # Both start employed\n",
    "        for t in range(n_periods_cf):\n",
    "            xb = intercept + sum(beta_dict[v] * X_mean[v] for v in beta_dict) + alpha[i]\n",
    "\n",
    "            # Baseline\n",
    "            xb_b = xb + gamma * y_b\n",
    "            y_b = int(np.random.normal(xb_b, 1) > 0)\n",
    "            traj_base[i, t] = y_b\n",
    "\n",
    "            # Shock: force y=0 at shock_period\n",
    "            if t == shock_period - 1:\n",
    "                y_s = 0  # Forced job loss\n",
    "            else:\n",
    "                xb_s = xb + gamma * y_s\n",
    "                y_s = int(np.random.normal(xb_s, 1) > 0)\n",
    "            traj_shock[i, t] = y_s\n",
    "\n",
    "    return traj_base.mean(axis=0), traj_shock.mean(axis=0)\n",
    "\n",
    "# Compare different gamma values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "gamma_values = [0.3, 0.8, 1.5]\n",
    "periods = np.arange(1, n_periods_cf + 1)\n",
    "\n",
    "print(\"=== Counterfactual: Job Loss at t=5 ===\")\n",
    "print(f\"\\n{'gamma':>6s} {'Gap at t=6':>12s} {'Gap at t=10':>12s} {'Recovery t':>12s}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "for ax, gamma_cf in zip(axes, gamma_values):\n",
    "    np.random.seed(42)  # Same random draws for comparability\n",
    "    mean_base, mean_shock = simulate_counterfactual(gamma=gamma_cf)\n",
    "\n",
    "    ax.plot(periods, mean_base, 'b-o', linewidth=2, markersize=4, label='Baseline')\n",
    "    ax.plot(periods, mean_shock, 'r-s', linewidth=2, markersize=4, label='Job loss at t=5')\n",
    "    ax.axvline(x=5, color='gray', linestyle='--', alpha=0.7)\n",
    "    ax.fill_between(periods, mean_base, mean_shock, alpha=0.15, color='red')\n",
    "    ax.set_xlabel('Period')\n",
    "    ax.set_ylabel('Mean Employment Rate')\n",
    "    ax.set_title(f'$\\\\gamma = {gamma_cf}$', fontweight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Find recovery time\n",
    "    gap = mean_base - mean_shock\n",
    "    converge_idx = np.where(gap[5:] < 0.05)[0]\n",
    "    recovery_t = converge_idx[0] + 6 if len(converge_idx) > 0 else '>20'\n",
    "\n",
    "    print(f\"{gamma_cf:>6.1f} {gap[5]:>12.3f} {gap[9]:>12.3f} {str(recovery_t):>12s}\")\n",
    "\n",
    "plt.suptitle('Counterfactual Analysis: How Long Does Job Loss Effect Last?',\n",
    "             fontsize=15, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answers\n",
    "print(\"=== Answers ===\")\n",
    "print(\"\")\n",
    "print(\"1. Does the gap ever fully close?\")\n",
    "print(\"   For small gamma (0.3), the gap closes relatively quickly because\")\n",
    "print(\"   state dependence is weak. For large gamma (1.5), the gap persists\")\n",
    "print(\"   much longer because past employment strongly determines future.\")\n",
    "print(\"   With heterogeneity (sigma_u > 0), the gap may never fully close\")\n",
    "print(\"   because some individuals drawn with low alpha_i may never recover.\")\n",
    "print(\"\")\n",
    "print(\"2. Recovery time vs gamma:\")\n",
    "print(\"   Higher gamma -> longer recovery. With gamma=1.5, the effect of\")\n",
    "print(\"   job loss can persist for 10+ periods. This is because each period\")\n",
    "print(\"   of non-employment feeds back into the next period.\")\n",
    "print(\"\")\n",
    "print(\"3. Policy implications:\")\n",
    "print(\"   - If gamma is large, temporary job programs are very valuable\")\n",
    "print(\"     because getting someone back to work creates lasting momentum.\")\n",
    "print(\"   - Early intervention matters: the longer someone is unemployed,\")\n",
    "print(\"     the harder it is to recover (vicious cycle).\")\n",
    "print(\"   - Programs should focus on preventing job loss (retention support)\")\n",
    "print(\"     as much as re-employment assistance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Testing State Dependence (Hard)\n",
    "\n",
    "**Task**: Formally test $H_0: \\gamma = 0$ using both Wald and LR tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 Solution\n",
    "\n",
    "# Step 1: Wald test for gamma = 0\n",
    "gamma_hat = wooldridge_pooled.params['emp_lag']\n",
    "se_gamma = wooldridge_pooled.bse['emp_lag']\n",
    "z_stat = gamma_hat / se_gamma\n",
    "p_wald = 2 * (1 - norm.cdf(abs(z_stat)))\n",
    "\n",
    "print(\"=== Wald Test for State Dependence ===\")\n",
    "print(f\"\\nH0: gamma = 0 (no true state dependence)\")\n",
    "print(f\"H1: gamma != 0\")\n",
    "print(f\"\\ngamma_hat  = {gamma_hat:.4f}\")\n",
    "print(f\"SE(gamma)  = {se_gamma:.4f}\")\n",
    "print(f\"z-stat     = {z_stat:.4f}\")\n",
    "print(f\"p-value    = {p_wald:.10f}\")\n",
    "print(f\"\\nConclusion: {'Reject H0' if p_wald < 0.05 else 'Fail to reject H0'} at 5%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: LR test\n",
    "# Restricted model: Wooldridge without emp_lag (static CRE probit)\n",
    "restricted_vars = exog_vars + ['emp_init'] + [f'{v}_mean' for v in mean_vars]\n",
    "X_restricted = sm.add_constant(data_dyn[restricted_vars])\n",
    "restricted_probit = sm.Probit(data_dyn['employed'], X_restricted).fit(method='bfgs', disp=0)\n",
    "\n",
    "# LR statistic\n",
    "lr_stat = -2 * (restricted_probit.llf - wooldridge_pooled.llf)\n",
    "p_lr = 1 - chi2.cdf(lr_stat, 1)  # 1 restriction (gamma=0)\n",
    "\n",
    "print(\"=== Likelihood Ratio Test ===\")\n",
    "print(f\"\\nH0: gamma = 0\")\n",
    "print(f\"\\nLog-L (restricted, no emp_lag): {restricted_probit.llf:.4f}\")\n",
    "print(f\"Log-L (unrestricted, with emp_lag): {wooldridge_pooled.llf:.4f}\")\n",
    "print(f\"\\nLR statistic: {lr_stat:.4f}\")\n",
    "print(f\"df: 1\")\n",
    "print(f\"p-value: {p_lr:.10f}\")\n",
    "print(f\"\\nConclusion: {'Reject H0' if p_lr < 0.05 else 'Fail to reject H0'} at 5%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compute APE of emp_lag\n",
    "X_eval = sm.add_constant(data_dyn[wooldridge_vars])\n",
    "lp = X_eval.values @ wooldridge_pooled.params.values\n",
    "phi = norm.pdf(lp)\n",
    "mean_phi = np.mean(phi)\n",
    "\n",
    "ape_gamma = mean_phi * gamma_hat\n",
    "ape_se = mean_phi * se_gamma  # Delta method approximation\n",
    "\n",
    "print(\"=== Average Partial Effect of y_{t-1} ===\")\n",
    "print(f\"\\nAPE = mean(phi(X'beta)) * gamma\")\n",
    "print(f\"    = {mean_phi:.4f} * {gamma_hat:.4f}\")\n",
    "print(f\"    = {ape_gamma:.4f}\")\n",
    "print(f\"\\nSE(APE) = {ape_se:.4f}\")\n",
    "print(f\"95% CI: [{ape_gamma - 1.96*ape_se:.4f}, {ape_gamma + 1.96*ape_se:.4f}]\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  Having worked in period t-1 increases the probability of\")\n",
    "print(f\"  working in period t by {ape_gamma:.1%} percentage points,\")\n",
    "print(f\"  holding all other factors constant.\")\n",
    "\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Both Wald (p={p_wald:.6f}) and LR (p={p_lr:.6f}) tests reject H0.\")\n",
    "print(f\"There is strong evidence of true state dependence.\")\n",
    "print(f\"Past employment causally increases current employment probability\")\n",
    "print(f\"by approximately {ape_gamma:.1%} percentage points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Solutions**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
