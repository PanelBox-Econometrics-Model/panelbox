{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Choice Models: Introduction\n",
    "\n",
    "**Tutorial Series**: Discrete Choice Econometrics with PanelBox\n",
    "\n",
    "**Notebook**: 01 - Binary Choice Introduction\n",
    "\n",
    "**Author**: PanelBox Contributors\n",
    "\n",
    "**Date**: 2026-02-16\n",
    "\n",
    "**Estimated Duration**: 60-90 minutes\n",
    "\n",
    "**Difficulty Level**: Beginner\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand why specialized models are needed for binary outcomes\n",
    "2. Recognize the limitations of Linear Probability Models (LPM)\n",
    "3. Estimate and interpret Logit and Probit models using PanelBox\n",
    "4. Compare model performance using pseudo-R\u00b2, AIC, and BIC\n",
    "5. Generate predictions and evaluate classification metrics\n",
    "6. Interpret coefficients as odds ratios (Logit) and latent variable effects\n",
    "7. Apply binary choice models to labor force participation decisions\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Economic Motivation](#section1)\n",
    "2. [Linear Probability Model (LPM)](#section2)\n",
    "3. [Link Functions](#section3)\n",
    "4. [Logit Model](#section4)\n",
    "5. [Probit Model](#section5)\n",
    "6. [Model Comparison](#section6)\n",
    "7. [Predictions and Classification](#section7)\n",
    "8. [Goodness-of-Fit Tests](#section8)\n",
    "9. [Application: Labor Force Participation](#section9)\n",
    "10. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import all required libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation and numerical computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import norm, logistic\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# PanelBox models\n",
    "from panelbox import PooledOLS\n",
    "from panelbox.models.discrete.binary import PooledLogit, PooledProbit\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Matplotlib configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "print(\"\u2713 All libraries imported successfully\")\n",
    "print(f\"\u2713 Random seed set to: 42\")\n",
    "print(f\"\u2713 Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Economic Motivation\n",
    "\n",
    "### 1.1 What are Binary Outcomes?\n",
    "\n",
    "Binary dependent variables take only two values: 0 or 1. They appear frequently in economics:\n",
    "\n",
    "- **Labor Economics**: Employed vs Unemployed, Union membership, Job training participation\n",
    "- **Consumer Behavior**: Purchase vs No purchase, Brand choice (A vs B)\n",
    "- **Finance**: Default vs No default, Dividend payment (yes/no)\n",
    "- **Development**: Technology adoption, Program participation\n",
    "- **Health**: Insurance coverage, Treatment uptake\n",
    "\n",
    "### 1.2 Why Not Use OLS?\n",
    "\n",
    "When the dependent variable is binary, traditional OLS regression faces fundamental problems:\n",
    "\n",
    "1. **Predictions outside [0,1]**: OLS can predict probabilities < 0 or > 1\n",
    "2. **Constant marginal effects**: Unrealistic assumption that effects don't vary\n",
    "3. **Heteroskedasticity**: Error variance depends on X\n",
    "4. **No microfoundations**: Not derived from utility theory\n",
    "\n",
    "### 1.3 Load the Data\n",
    "\n",
    "We'll use a dataset on **labor force participation** of married women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labor participation dataset\n",
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "data = pd.read_csv(DATA_DIR / \"labor_participation.csv\")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nShape: {data.shape}\")\n",
    "print(f\"Number of individuals: {data['id'].nunique()}\")\n",
    "print(f\"Number of periods: {data['year'].nunique()}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=== Summary Statistics ===\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outcome variable distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "lfp_counts = data['lfp'].value_counts().sort_index()\n",
    "axes[0].bar(['Not Working (0)', 'Working (1)'], lfp_counts.values, \n",
    "            color=['#e74c3c', '#27ae60'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Labor Force Participation Distribution')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (label, value) in enumerate(zip(['Not Working', 'Working'], lfp_counts.values)):\n",
    "    axes[0].text(i, value + 50, f'{value}\\n({100*value/len(data):.1f}%)', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Proportion over time\n",
    "lfp_by_year = data.groupby('year')['lfp'].agg(['mean', 'count'])\n",
    "axes[1].plot(lfp_by_year.index, lfp_by_year['mean'], marker='o', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Year')\n",
    "axes[1].set_ylabel('Proportion Working')\n",
    "axes[1].set_title('Labor Force Participation Over Time')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOverall participation rate: {100*data['lfp'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Discussion: Economic Questions\n",
    "\n",
    "Binary choice models help us answer questions like:\n",
    "\n",
    "- How does education affect the probability of working?\n",
    "- What is the effect of having children on labor force participation?\n",
    "- How do these effects vary across different age groups?\n",
    "- Can we predict who will participate in the labor force?\n",
    "\n",
    "**Key insight**: We're modeling **probabilities** (bounded between 0 and 1), not the binary outcome directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Linear Probability Model (LPM)\n",
    "\n",
    "### 2.1 The LPM Specification\n",
    "\n",
    "The simplest approach: estimate using OLS\n",
    "\n",
    "$$y_{it} = \\mathbf{X}_{it}' \\boldsymbol{\\beta} + \\varepsilon_{it}$$\n",
    "\n",
    "where $y_{it} \\in \\{0, 1\\}$.\n",
    "\n",
    "**Interpretation**: $E[y|\\mathbf{X}] = P(y=1|\\mathbf{X}) = \\mathbf{X}'\\boldsymbol{\\beta}$\n",
    "\n",
    "**Marginal effect**: $\\frac{\\partial P(y=1)}{\\partial X_k} = \\beta_k$ (constant!)\n",
    "\n",
    "### 2.2 Estimate LPM using PooledOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Linear Probability Model\n",
    "lpm = PooledOLS(\"lfp ~ age + educ + kids + married\", data, \"id\", \"year\")\n",
    "lpm_results = lpm.fit(cov_type='robust')\n",
    "\n",
    "print(\"=== Linear Probability Model (LPM) ===\")\n",
    "print(lpm_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The Problems with LPM\n",
    "\n",
    "Let's examine predictions from the LPM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "data['lpm_pred'] = lpm_results.predict()\n",
    "\n",
    "# Check for predictions outside [0,1]\n",
    "outside_bounds = (data['lpm_pred'] < 0) | (data['lpm_pred'] > 1)\n",
    "print(\"=== LPM Prediction Issues ===\")\n",
    "print(f\"Predictions outside [0,1]: {outside_bounds.sum()} ({100*outside_bounds.mean():.2f}%)\")\n",
    "print(f\"Min prediction: {data['lpm_pred'].min():.4f}\")\n",
    "print(f\"Max prediction: {data['lpm_pred'].max():.4f}\")\n",
    "print(f\"\\nPredictions < 0: {(data['lpm_pred'] < 0).sum()}\")\n",
    "print(f\"Predictions > 1: {(data['lpm_pred'] > 1).sum()}\")\n",
    "\n",
    "if outside_bounds.sum() > 0:\n",
    "    print(\"\\n\u26a0\ufe0f  WARNING: LPM produces impossible probabilities!\")\n",
    "    print(\"This violates the fundamental requirement that probabilities \u2208 [0,1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LPM problems\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Predicted vs Actual by Age\n",
    "axes[0].scatter(data['age'], data['lpm_pred'], alpha=0.3, label='LPM Predictions', s=20)\n",
    "axes[0].scatter(data['age'], data['lfp'], alpha=0.05, label='Actual (0/1)', color='red', s=10)\n",
    "axes[0].axhline(y=0, color='black', linestyle='--', linewidth=1.5, label='Bounds')\n",
    "axes[0].axhline(y=1, color='black', linestyle='--', linewidth=1.5)\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Probability / Outcome')\n",
    "axes[0].set_title('LPM: Predictions Can Exceed [0,1]')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight predictions outside bounds\n",
    "if outside_bounds.sum() > 0:\n",
    "    outside_data = data[outside_bounds]\n",
    "    axes[0].scatter(outside_data['age'], outside_data['lpm_pred'], \n",
    "                   color='orange', s=50, marker='x', linewidths=2,\n",
    "                   label=f'Outside [0,1] (n={len(outside_data)})', zorder=5)\n",
    "    axes[0].legend()\n",
    "\n",
    "# Plot 2: Distribution of predictions\n",
    "axes[1].hist(data['lpm_pred'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Lower Bound (0)')\n",
    "axes[1].axvline(x=1, color='red', linestyle='--', linewidth=2, label='Upper Bound (1)')\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of LPM Predictions')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 When is LPM Acceptable?\n",
    "\n",
    "Despite its problems, LPM can be useful:\n",
    "\n",
    "\u2713 **Quick approximation** when predicted probabilities are mostly between 0.3 and 0.7\n",
    "\n",
    "\u2713 **Robustness check** alongside Logit/Probit (should have same signs)\n",
    "\n",
    "\u2713 **Simple interpretation** when constant marginal effects are acceptable\n",
    "\n",
    "\u2713 **Computational simplicity** for very large datasets\n",
    "\n",
    "However, for serious analysis, we need models that:\n",
    "- Guarantee $P \\in [0,1]$\n",
    "- Allow non-constant marginal effects\n",
    "- Have proper statistical foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Link Functions\n",
    "\n",
    "### 3.1 The Solution: Transformation Functions\n",
    "\n",
    "**Idea**: Transform the linear predictor $\\mathbf{X}'\\boldsymbol{\\beta}$ to ensure $P \\in [0,1]$\n",
    "\n",
    "$$P(y_{it}=1|\\mathbf{X}_{it}) = G(\\mathbf{X}_{it}'\\boldsymbol{\\beta})$$\n",
    "\n",
    "where $G(\\cdot)$ is a **link function** (CDF) with properties:\n",
    "- $G: \\mathbb{R} \\rightarrow [0,1]$\n",
    "- $G(z)$ is monotonically increasing\n",
    "- $\\lim_{z \\to -\\infty} G(z) = 0$ and $\\lim_{z \\to \\infty} G(z) = 1$\n",
    "\n",
    "### 3.2 Common Link Functions\n",
    "\n",
    "**Logistic (Logit)**:\n",
    "$$\\Lambda(z) = \\frac{\\exp(z)}{1 + \\exp(z)} = \\frac{1}{1 + \\exp(-z)}$$\n",
    "\n",
    "**Normal (Probit)**:\n",
    "$$\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) dt = \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{t^2}{2}\\right) dt$$\n",
    "\n",
    "**Linear (LPM)**:\n",
    "$$L(z) = z \\quad \\text{(NOT bounded to [0,1]!)}$$\n",
    "\n",
    "### 3.3 Visualize Link Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create range of values for linear predictor\n",
    "z = np.linspace(-6, 6, 1000)\n",
    "\n",
    "# Calculate link functions\n",
    "logistic_cdf = 1 / (1 + np.exp(-z))\n",
    "normal_cdf = norm.cdf(z)\n",
    "lpm_approx = 0.5 + 0.15 * z  # Linear approximation around z=0\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(z, logistic_cdf, label='Logistic: \u039b(z) [Logit]', linewidth=2.5, color='#3498db')\n",
    "plt.plot(z, normal_cdf, label='Normal: \u03a6(z) [Probit]', linewidth=2.5, \n",
    "         linestyle='--', color='#e74c3c')\n",
    "plt.plot(z, lpm_approx, label='Linear [LPM]', linewidth=2.5, \n",
    "         linestyle=':', color='#f39c12')\n",
    "\n",
    "# Reference lines\n",
    "plt.axhline(y=0, color='gray', linestyle='-', linewidth=0.8, alpha=0.5)\n",
    "plt.axhline(y=1, color='gray', linestyle='-', linewidth=0.8, alpha=0.5)\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', linewidth=0.8, alpha=0.3)\n",
    "plt.axvline(x=0, color='gray', linestyle='-', linewidth=0.8, alpha=0.3)\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel('Linear Predictor (X\u03b2)', fontsize=13)\n",
    "plt.ylabel('Probability P(y=1|X)', fontsize=13)\n",
    "plt.title('Comparison of Link Functions', fontsize=15, fontweight='bold')\n",
    "plt.legend(fontsize=12, loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlim([-6, 6])\n",
    "\n",
    "# Add annotations\n",
    "plt.text(3, 0.95, 'Both bounded to [0,1]', fontsize=10, \n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "plt.text(-4, 0.3, 'LPM can exceed bounds!', fontsize=10, color='red',\n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Key Differences\n",
    "\n",
    "**Logistic vs Normal CDF**:\n",
    "- Very similar for $|z| < 2$ (most predictions fall here)\n",
    "- Logistic has slightly **heavier tails**\n",
    "- In practice, choice between Logit/Probit **rarely matters**\n",
    "\n",
    "**Why use Logit?**\n",
    "- Closed-form for probabilities and odds ratios\n",
    "- Easier coefficient interpretation (odds ratios)\n",
    "- Slightly simpler computation\n",
    "\n",
    "**Why use Probit?**\n",
    "- Natural if assuming normal errors\n",
    "- Connection to latent variable models\n",
    "- Rule of thumb: marginal effect at mean \u2248 0.4\u03b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify similarity between Logit and Probit\n",
    "difference = np.abs(logistic_cdf - normal_cdf)\n",
    "print(\"=== Logistic vs Normal CDF ===\")\n",
    "print(f\"Maximum absolute difference: {difference.max():.6f}\")\n",
    "print(f\"Mean absolute difference: {difference.mean():.6f}\")\n",
    "print(f\"\\nFor |z| < 2 (where most predictions lie):\")\n",
    "mask = np.abs(z) < 2\n",
    "print(f\"Maximum difference: {difference[mask].max():.6f}\")\n",
    "print(f\"Mean difference: {difference[mask].mean():.6f}\")\n",
    "print(\"\\n\u2192 Conclusion: Logit and Probit predictions are nearly identical!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. Logit Model\n",
    "\n",
    "### 4.1 Model Specification\n",
    "\n",
    "**Probability**:\n",
    "$$P(y_{it}=1|\\mathbf{X}_{it}) = \\Lambda(\\mathbf{X}_{it}'\\boldsymbol{\\beta}) = \\frac{\\exp(\\mathbf{X}_{it}'\\boldsymbol{\\beta})}{1 + \\exp(\\mathbf{X}_{it}'\\boldsymbol{\\beta})}$$\n",
    "\n",
    "**Log-likelihood**:\n",
    "$$\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\left[ y_{it} \\log(\\Lambda(\\mathbf{X}_{it}'\\boldsymbol{\\beta})) + (1-y_{it}) \\log(1-\\Lambda(\\mathbf{X}_{it}'\\boldsymbol{\\beta})) \\right]$$\n",
    "\n",
    "**Estimation**: Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "### 4.2 Estimate Pooled Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Pooled Logit model\n",
    "logit = PooledLogit(\"lfp ~ age + educ + kids + married\", data, \"id\", \"year\")\n",
    "logit_results = logit.fit(cov_type='cluster')\n",
    "\n",
    "print(\"=== Pooled Logit Model ===\")\n",
    "print(logit_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Coefficient Interpretation\n",
    "\n",
    "**IMPORTANT**: Logit coefficients are **log-odds ratios**, NOT marginal effects!\n",
    "\n",
    "**Log-odds interpretation**:\n",
    "- $\\beta_k > 0$: Increasing $X_k$ increases probability of $y=1$\n",
    "- $\\beta_k < 0$: Increasing $X_k$ decreases probability of $y=1$\n",
    "\n",
    "**Odds ratio interpretation**:\n",
    "- $\\text{OR} = \\exp(\\beta_k)$\n",
    "- Multiplicative effect on odds: $\\frac{P(y=1|X)}{P(y=0|X)}$\n",
    "\n",
    "### 4.4 Calculate Odds Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Coefficient Interpretation ===\")\n",
    "print(\"\\n1. Coefficients (\u03b2) represent LOG-ODDS RATIOS:\")\n",
    "print(logit_results.params)\n",
    "\n",
    "print(\"\\n2. Odds Ratios (exp(\u03b2)):\")\n",
    "odds_ratios = np.exp(logit_results.params)\n",
    "print(odds_ratios)\n",
    "\n",
    "print(\"\\n=== Detailed Interpretation ===\")\n",
    "\n",
    "# Education\n",
    "beta_educ = logit_results.params['educ']\n",
    "or_educ = odds_ratios['educ']\n",
    "print(f\"\\nEducation (educ):\")\n",
    "print(f\"  \u03b2 = {beta_educ:.4f}\")\n",
    "print(f\"  OR = exp({beta_educ:.4f}) = {or_educ:.4f}\")\n",
    "print(f\"  Interpretation:\")\n",
    "print(f\"    - One additional year of education changes log-odds by {beta_educ:.4f}\")\n",
    "print(f\"    - Odds of working multiply by {or_educ:.4f}\")\n",
    "print(f\"    - \u2248 {100*(or_educ-1):.2f}% change in odds\")\n",
    "\n",
    "# Kids\n",
    "beta_kids = logit_results.params['kids']\n",
    "or_kids = odds_ratios['kids']\n",
    "print(f\"\\nChildren (kids):\")\n",
    "print(f\"  \u03b2 = {beta_kids:.4f}\")\n",
    "print(f\"  OR = exp({beta_kids:.4f}) = {or_kids:.4f}\")\n",
    "print(f\"  Interpretation:\")\n",
    "print(f\"    - One additional child changes log-odds by {beta_kids:.4f}\")\n",
    "print(f\"    - Odds of working multiply by {or_kids:.4f}\")\n",
    "print(f\"    - \u2248 {100*(or_kids-1):.2f}% change in odds (DECREASE)\")\n",
    "\n",
    "# Married\n",
    "beta_married = logit_results.params['married']\n",
    "or_married = odds_ratios['married']\n",
    "print(f\"\\nMarried:\")\n",
    "print(f\"  \u03b2 = {beta_married:.4f}\")\n",
    "print(f\"  OR = exp({beta_married:.4f}) = {or_married:.4f}\")\n",
    "print(f\"  Interpretation:\")\n",
    "print(f\"    - Being married (vs not) changes log-odds by {beta_married:.4f}\")\n",
    "print(f\"    - Odds of working multiply by {or_married:.4f}\")\n",
    "if or_married > 1:\n",
    "    print(f\"    - \u2248 {100*(or_married-1):.2f}% increase in odds\")\n",
    "else:\n",
    "    print(f\"    - \u2248 {100*(1-or_married):.2f}% decrease in odds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING about marginal effects\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u26a0\ufe0f  IMPORTANT WARNING \u26a0\ufe0f\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCoefficients \u03b2 \u2260 Marginal Effects!\")\n",
    "print(\"\\nMarginal effects are:\")\n",
    "print(\"  \u2202P(y=1)/\u2202X_k = \u03b2_k \u00d7 \u039b(X\u03b2) \u00d7 [1 - \u039b(X\u03b2)]\")\n",
    "print(\"\\nThey depend on:\")\n",
    "print(\"  1. The coefficient \u03b2_k\")\n",
    "print(\"  2. The values of ALL covariates (X)\")\n",
    "print(\"  3. The current probability level\")\n",
    "print(\"\\nMarginal effects will be covered in detail in Notebook 04.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Model Diagnostics ===\")\n",
    "\n",
    "# Convergence\n",
    "print(f\"\\nConvergence: {logit_results.converged}\")\n",
    "if hasattr(logit_results, 'method'):\n",
    "    print(f\"Optimization method: {logit_results.method}\")\n",
    "\n",
    "# Likelihood and information criteria\n",
    "print(f\"\\nLog-Likelihood: {logit_results.llf:.2f}\")\n",
    "print(f\"AIC: {logit_results.aic:.2f}\")\n",
    "print(f\"BIC: {logit_results.bic:.2f}\")\n",
    "\n",
    "# Pseudo-R\u00b2 (basic)\n",
    "if hasattr(logit_results, 'pseudo_r2'):\n",
    "    pr2_dict = logit_results.pseudo_r2()\n",
    "    print(f\"\\nPseudo-R\u00b2 (McFadden): {pr2_dict.get('mcfadden', 'N/A')}\")\n",
    "\n",
    "print(\"\\n\u2713 Model converged successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## 5. Probit Model\n",
    "\n",
    "### 5.1 Model Specification\n",
    "\n",
    "**Probability**:\n",
    "$$P(y_{it}=1|\\mathbf{X}_{it}) = \\Phi(\\mathbf{X}_{it}'\\boldsymbol{\\beta}) = \\int_{-\\infty}^{\\mathbf{X}_{it}'\\boldsymbol{\\beta}} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{t^2}{2}\\right) dt$$\n",
    "\n",
    "**Latent variable interpretation**:\n",
    "$$y_{it}^* = \\mathbf{X}_{it}'\\boldsymbol{\\beta} + \\varepsilon_{it}, \\quad \\varepsilon_{it} \\sim N(0,1)$$\n",
    "$$y_{it} = \\mathbb{1}[y_{it}^* > 0]$$\n",
    "\n",
    "### 5.2 Estimate Pooled Probit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Pooled Probit model\n",
    "probit = PooledProbit(\"lfp ~ age + educ + kids + married\", data, \"id\", \"year\")\n",
    "probit_results = probit.fit(cov_type='cluster')\n",
    "\n",
    "print(\"=== Pooled Probit Model ===\")\n",
    "print(probit_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Logit vs Probit Comparison\n",
    "\n",
    "**Key point**: Coefficients are NOT directly comparable!\n",
    "\n",
    "**Why?** Different variance scaling:\n",
    "- Logit error variance: $\\text{Var}(\\varepsilon) = \\pi^2/3 \\approx 3.29$\n",
    "- Probit error variance: $\\text{Var}(\\varepsilon) = 1$\n",
    "\n",
    "**Rule of thumb**: $\\beta_{\\text{probit}} \\approx 0.625 \\times \\beta_{\\text{logit}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Logit and Probit coefficients\n",
    "comparison = pd.DataFrame({\n",
    "    'Logit_\u03b2': logit_results.params,\n",
    "    'Probit_\u03b2': probit_results.params,\n",
    "    'Probit_scaled_\u03b2': probit_results.params / 0.625,  # Scale to Logit scale\n",
    "    'Ratio (P/L)': probit_results.params / logit_results.params\n",
    "})\n",
    "\n",
    "print(\"=== Logit vs Probit Coefficient Comparison ===\")\n",
    "print(comparison)\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\"  1. Coefficients are NOT directly comparable (different scales)\")\n",
    "print(\"  2. Rule of thumb: \u03b2_probit \u2248 0.625 \u00d7 \u03b2_logit\")\n",
    "print(f\"  3. Average ratio observed: {(probit_results.params / logit_results.params).mean():.3f}\")\n",
    "print(\"  4. What MATTERS: Signs and relative magnitudes should be consistent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 When to Use Probit vs Logit\n",
    "\n",
    "**Use Probit when**:\n",
    "- You have theoretical reasons to assume normal errors\n",
    "- Latent variable interpretation is natural for your application\n",
    "- You want to use the 0.4\u03b2 rule for quick marginal effect approximations\n",
    "- Extending to multivariate probit (correlated errors across equations)\n",
    "\n",
    "**Use Logit when**:\n",
    "- You want easier coefficient interpretation (odds ratios)\n",
    "- Computational simplicity is important\n",
    "- Following conventions in your field (Logit more common in applied work)\n",
    "- Extending to conditional logit or nested logit models\n",
    "\n",
    "**In practice**: Choice rarely matters for final conclusions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## 6. Model Comparison\n",
    "\n",
    "Let's systematically compare LPM, Logit, and Probit.\n",
    "\n",
    "### 6.1 Coefficient Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'LPM': lpm_results.params,\n",
    "    'Logit': logit_results.params,\n",
    "    'Probit': probit_results.params\n",
    "})\n",
    "\n",
    "print(\"=== Coefficient Comparison Across Models ===\")\n",
    "print(coef_comparison)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERPRETATION GUIDE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nMagnitudes are NOT directly comparable across models because:\")\n",
    "print(\"  \u2022 LPM: Coefficients = constant marginal effects on probability\")\n",
    "print(\"  \u2022 Logit: Coefficients = log-odds ratios\")\n",
    "print(\"  \u2022 Probit: Coefficients = latent variable effects (different scale)\")\n",
    "print(\"\\nWhat you SHOULD compare:\")\n",
    "print(\"  \u2713 Signs (positive/negative direction)\")\n",
    "print(\"  \u2713 Statistical significance\")\n",
    "print(\"  \u2713 Relative magnitudes within each model\")\n",
    "print(\"  \u2713 Predicted probabilities (next section)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Prediction Comparison\n",
    "\n",
    "While coefficients differ, **predicted probabilities** should be very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from all three models\n",
    "data['lpm_prob'] = lpm_results.predict()\n",
    "data['logit_prob'] = logit_results.predict(type='prob')\n",
    "data['probit_prob'] = probit_results.predict(type='prob')\n",
    "\n",
    "# Clip LPM predictions to [0,1] for fair comparison\n",
    "data['lpm_prob_clipped'] = data['lpm_prob'].clip(0, 1)\n",
    "\n",
    "# Correlation matrix\n",
    "pred_corr = data[['lpm_prob_clipped', 'logit_prob', 'probit_prob']].corr()\n",
    "print(\"=== Correlation of Predicted Probabilities ===\")\n",
    "print(pred_corr)\n",
    "print(\"\\n\u2192 Logit and Probit predictions are nearly identical (r \u2248 1.0)\")\n",
    "print(\"\u2192 LPM also highly correlated, but with systematic differences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction comparisons\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# LPM vs Logit\n",
    "axes[0].scatter(data['logit_prob'], data['lpm_prob_clipped'], alpha=0.3, s=10)\n",
    "axes[0].plot([0, 1], [0, 1], 'r--', label='45-degree line', linewidth=2)\n",
    "axes[0].set_xlabel('Logit Prediction')\n",
    "axes[0].set_ylabel('LPM Prediction (clipped)')\n",
    "axes[0].set_title(f'LPM vs Logit\\n(corr = {pred_corr.loc[\"lpm_prob_clipped\", \"logit_prob\"]:.4f})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim([0, 1])\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Probit vs Logit\n",
    "axes[1].scatter(data['logit_prob'], data['probit_prob'], alpha=0.3, s=10, color='green')\n",
    "axes[1].plot([0, 1], [0, 1], 'r--', label='45-degree line', linewidth=2)\n",
    "axes[1].set_xlabel('Logit Prediction')\n",
    "axes[1].set_ylabel('Probit Prediction')\n",
    "axes[1].set_title(f'Probit vs Logit\\n(corr = {pred_corr.loc[\"logit_prob\", \"probit_prob\"]:.4f})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "# LPM vs Probit\n",
    "axes[2].scatter(data['probit_prob'], data['lpm_prob_clipped'], alpha=0.3, s=10, color='orange')\n",
    "axes[2].plot([0, 1], [0, 1], 'r--', label='45-degree line', linewidth=2)\n",
    "axes[2].set_xlabel('Probit Prediction')\n",
    "axes[2].set_ylabel('LPM Prediction (clipped)')\n",
    "axes[2].set_title(f'LPM vs Probit\\n(corr = {pred_corr.loc[\"lpm_prob_clipped\", \"probit_prob\"]:.4f})')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_xlim([0, 1])\n",
    "axes[2].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observation: Logit and Probit predictions lie almost exactly on the 45\u00b0 line!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Model Fit Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pseudo-R\u00b2 for Logit and Probit\n",
    "logit_pr2 = logit_results.pseudo_r2() if hasattr(logit_results, 'pseudo_r2') else {}\n",
    "probit_pr2 = probit_results.pseudo_r2() if hasattr(probit_results, 'pseudo_r2') else {}\n",
    "\n",
    "# Create model comparison table\n",
    "fit_stats = pd.DataFrame({\n",
    "    'LPM': [\n",
    "        lpm_results.rsquared,\n",
    "        np.nan,\n",
    "        np.nan\n",
    "    ],\n",
    "    'Logit': [\n",
    "        logit_pr2.get('mcfadden', np.nan),\n",
    "        logit_results.aic,\n",
    "        logit_results.bic\n",
    "    ],\n",
    "    'Probit': [\n",
    "        probit_pr2.get('mcfadden', np.nan),\n",
    "        probit_results.aic,\n",
    "        probit_results.bic\n",
    "    ]\n",
    "}, index=['R\u00b2/Pseudo-R\u00b2', 'AIC', 'BIC'])\n",
    "\n",
    "print(\"=== Model Fit Comparison ===\")\n",
    "print(fit_stats)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  \u2022 Lower AIC/BIC = better fit (penalizes complexity)\")\n",
    "print(\"  \u2022 Pseudo-R\u00b2 NOT comparable to OLS R\u00b2 (different interpretation)\")\n",
    "print(\"  \u2022 Logit and Probit have nearly identical fit\")\n",
    "\n",
    "# Determine best model by AIC\n",
    "best_aic = fit_stats.loc['AIC'].idxmin()\n",
    "best_bic = fit_stats.loc['BIC'].idxmin()\n",
    "print(f\"\\nBest model by AIC: {best_aic}\")\n",
    "print(f\"Best model by BIC: {best_bic}\")\n",
    "print(\"\\n\u2713 Recommendation: Use Logit or Probit (strongly preferred over LPM)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Summary: Model Selection\n",
    "\n",
    "**Conclusion**:\n",
    "1. \u2713 **Logit and Probit** produce virtually identical predictions\n",
    "2. \u2713 Both are **strongly preferred** over LPM for formal analysis\n",
    "3. \u2713 Choice between Logit/Probit is often **arbitrary**\n",
    "4. \u25cb LPM can be used as a **robustness check** or quick approximation\n",
    "\n",
    "**Practical advice**: Use **Logit** unless you have specific reasons to prefer Probit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## 7. Predictions and Classification\n",
    "\n",
    "### 7.1 Types of Predictions\n",
    "\n",
    "Binary choice models can generate three types of predictions:\n",
    "\n",
    "1. **Probabilities**: $P(y=1|\\mathbf{X})$ (default, most useful)\n",
    "2. **Linear predictor**: $\\mathbf{X}'\\boldsymbol{\\beta}$ (for advanced diagnostics)\n",
    "3. **Classes**: $\\hat{y} \\in \\{0, 1\\}$ (for classification tasks)\n",
    "\n",
    "### 7.2 Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Types of Predictions from Logit Model ===\")\n",
    "\n",
    "# 1. Predicted probabilities (default)\n",
    "probs = logit_results.predict(type='prob')\n",
    "print(\"\\n1. Predicted Probabilities P(y=1|X):\")\n",
    "print(f\"   Shape: {probs.shape}\")\n",
    "print(f\"   Range: [{probs.min():.4f}, {probs.max():.4f}]\")\n",
    "print(f\"   First 10 values:\\n{probs[:10]}\")\n",
    "\n",
    "# 2. Linear index (X\u03b2)\n",
    "linear_pred = logit_results.predict(type='linear')\n",
    "print(\"\\n2. Linear Predictor (X\u03b2):\")\n",
    "print(f\"   Shape: {linear_pred.shape}\")\n",
    "print(f\"   Range: [{linear_pred.min():.4f}, {linear_pred.max():.4f}]\")\n",
    "print(f\"   First 10 values:\\n{linear_pred[:10]}\")\n",
    "\n",
    "# 3. Predicted classes (threshold=0.5)\n",
    "classes = logit_results.predict(type='class', threshold=0.5)\n",
    "print(\"\\n3. Predicted Classes (threshold=0.5):\")\n",
    "print(f\"   Shape: {classes.shape}\")\n",
    "print(f\"   Unique values: {np.unique(classes)}\")\n",
    "print(f\"   First 10 values:\\n{classes[:10]}\")\n",
    "print(f\"\\n   Predicted proportion of y=1: {classes.mean():.4f}\")\n",
    "print(f\"   Actual proportion of y=1: {data['lfp'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Confusion Matrix\n",
    "\n",
    "The **confusion matrix** shows how well classifications match actual outcomes:\n",
    "\n",
    "|                | Predicted 0 | Predicted 1 |\n",
    "|----------------|-------------|-------------|\n",
    "| **Actual 0**   | TN          | FP          |\n",
    "| **Actual 1**   | FN          | TP          |\n",
    "\n",
    "- **TN**: True Negatives (correctly predicted 0)\n",
    "- **TP**: True Positives (correctly predicted 1)  \n",
    "- **FP**: False Positives (predicted 1, actually 0)\n",
    "- **FN**: False Negatives (predicted 0, actually 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "if hasattr(logit_results, 'classification_table'):\n",
    "    confusion = logit_results.classification_table(threshold=0.5)\n",
    "    print(\"=== Confusion Matrix (threshold=0.5) ===\")\n",
    "    print(confusion)\n",
    "else:\n",
    "    # Manual calculation\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    classes = (logit_results.predict(type='prob') > 0.5).astype(int)\n",
    "    cm = confusion_matrix(data['lfp'], classes)\n",
    "    print(\"=== Confusion Matrix (threshold=0.5) ===\")\n",
    "    print(pd.DataFrame(cm, \n",
    "                      index=['Actual 0', 'Actual 1'],\n",
    "                      columns=['Predicted 0', 'Predicted 1']))\n",
    "    \n",
    "    # Extract values\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\nBreakdown:\")\n",
    "    print(f\"  True Negatives (TN):  {tn}\")\n",
    "    print(f\"  False Positives (FP): {fp}\")\n",
    "    print(f\"  False Negatives (FN): {fn}\")\n",
    "    print(f\"  True Positives (TP):  {tp}\")\n",
    "    print(f\"\\nTotal correct: {tn + tp} / {len(data)} = {(tn+tp)/len(data):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Classification Metrics\n",
    "\n",
    "Key performance metrics:\n",
    "\n",
    "- **Accuracy**: $(TP + TN) / N$ \u2014 overall correctness\n",
    "- **Precision**: $TP / (TP + FP)$ \u2014 of predicted positives, % actually positive\n",
    "- **Recall (Sensitivity)**: $TP / (TP + FN)$ \u2014 of actual positives, % correctly predicted\n",
    "- **F1-Score**: $2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ \u2014 harmonic mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification metrics\n",
    "if hasattr(logit_results, 'classification_metrics'):\n",
    "    metrics = logit_results.classification_metrics(threshold=0.5)\n",
    "    print(\"=== Classification Metrics (threshold=0.5) ===\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric:15s}: {value:.4f}\")\n",
    "else:\n",
    "    # Manual calculation\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    y_true = data['lfp'].values\n",
    "    y_pred = (logit_results.predict(type='prob') > 0.5).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1-Score': f1_score(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    print(\"=== Classification Metrics (threshold=0.5) ===\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric:15s}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n=== Interpretation ===\")\n",
    "print(f\"Accuracy:  {metrics.get('Accuracy', 0)*100:.2f}% of all predictions are correct\")\n",
    "print(f\"Precision: {metrics.get('Precision', 0)*100:.2f}% of predicted 'working' are actually working\")\n",
    "print(f\"Recall:    {metrics.get('Recall', 0)*100:.2f}% of actual 'working' are correctly predicted\")\n",
    "print(f\"F1-Score:  Harmonic mean = {metrics.get('F1-Score', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 ROC Curve and AUC\n",
    "\n",
    "**ROC Curve**: Plots True Positive Rate vs False Positive Rate across all thresholds\n",
    "\n",
    "**AUC**: Area Under the ROC Curve \u2014 probability model ranks random positive higher than random negative\n",
    "\n",
    "- AUC = 0.5: Random classifier\n",
    "- AUC = 1.0: Perfect classifier\n",
    "- AUC > 0.7: Generally considered good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "y_true = data['lfp'].values\n",
    "y_scores = logit_results.predict(type='prob')\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr, tpr, label=f'Logit (AUC = {roc_auc:.4f})', linewidth=2.5, color='#2980b9')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5)', linewidth=2)\n",
    "plt.xlabel('False Positive Rate', fontsize=13)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=13)\n",
    "plt.title('ROC Curve - Logit Model', fontsize=15, fontweight='bold')\n",
    "plt.legend(fontsize=12, loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Add text annotation\n",
    "plt.text(0.6, 0.2, f'AUC = {roc_auc:.4f}\\n(Excellent)' if roc_auc > 0.8 else f'AUC = {roc_auc:.4f}',\n",
    "         fontsize=14, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== ROC Analysis ===\")\n",
    "print(f\"AUC (Area Under ROC Curve): {roc_auc:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  The model has a {roc_auc:.1%} probability of ranking a randomly chosen\")\n",
    "print(f\"  working person higher than a randomly chosen non-working person.\")\n",
    "if roc_auc > 0.8:\n",
    "    print(f\"\\n  \u2713 AUC > 0.8: Excellent discrimination\")\n",
    "elif roc_auc > 0.7:\n",
    "    print(f\"\\n  \u2713 AUC > 0.7: Good discrimination\")\n",
    "else:\n",
    "    print(f\"\\n  \u25cb AUC < 0.7: Moderate discrimination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Distribution of Predicted Probabilities\n",
    "\n",
    "Visualize how well the model separates the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted probability distributions by actual class\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "data_class_0 = data[data['lfp'] == 0]['logit_prob']\n",
    "data_class_1 = data[data['lfp'] == 1]['logit_prob']\n",
    "\n",
    "ax.hist(data_class_0, bins=40, alpha=0.6, label=f'Actual = 0 (Not Working, n={len(data_class_0)})', \n",
    "        color='#e74c3c', edgecolor='black')\n",
    "ax.hist(data_class_1, bins=40, alpha=0.6, label=f'Actual = 1 (Working, n={len(data_class_1)})', \n",
    "        color='#27ae60', edgecolor='black')\n",
    "ax.axvline(x=0.5, color='black', linestyle='--', linewidth=2.5, \n",
    "           label='Classification Threshold (0.5)', zorder=5)\n",
    "\n",
    "ax.set_xlabel('Predicted Probability of Working', fontsize=13)\n",
    "ax.set_ylabel('Frequency', fontsize=13)\n",
    "ax.set_title('Distribution of Predicted Probabilities by Actual Outcome', \n",
    "             fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nIdeal scenario: Two distributions are well-separated\")\n",
    "print(\"\u2192 Model successfully distinguishes between the two groups\")\n",
    "print(f\"\\nMean predicted prob for y=0: {data_class_0.mean():.4f}\")\n",
    "print(f\"Mean predicted prob for y=1: {data_class_1.mean():.4f}\")\n",
    "print(f\"Separation (difference): {abs(data_class_1.mean() - data_class_0.mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "## 8. Goodness-of-Fit Tests\n",
    "\n",
    "### 8.1 Pseudo-R\u00b2 Measures\n",
    "\n",
    "**IMPORTANT**: Pseudo-R\u00b2 \u2260 OLS R\u00b2\n",
    "\n",
    "Common pseudo-R\u00b2 measures:\n",
    "- **McFadden**: $1 - \\frac{\\ell(\\boldsymbol{\\beta})}{\\ell_0}$ where $\\ell_0$ is log-likelihood of intercept-only model\n",
    "- **Cox-Snell**: $1 - \\exp\\left(\\frac{2(\\ell_0 - \\ell(\\boldsymbol{\\beta}))}{N}\\right)$\n",
    "- **Nagelkerke**: Adjusted Cox-Snell to have maximum of 1\n",
    "\n",
    "**Interpretation**:\n",
    "- NOT \"proportion of variance explained\"\n",
    "- Use for model comparison, not absolute fit\n",
    "- Values 0.2-0.4 considered good (much lower than OLS R\u00b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all pseudo-R\u00b2 measures\n",
    "if hasattr(logit_results, 'pseudo_r2'):\n",
    "    pseudo_r2_dict = logit_results.pseudo_r2()\n",
    "    \n",
    "    print(\"=== Pseudo-R\u00b2 Measures ===\")\n",
    "    for name, value in pseudo_r2_dict.items():\n",
    "        print(f\"{name:20s}: {value:.4f}\")\n",
    "else:\n",
    "    # Basic McFadden calculation\n",
    "    ll_full = logit_results.llf\n",
    "    # Estimate null model\n",
    "    null_model = PooledLogit(\"lfp ~ 1\", data, \"id\", \"year\")\n",
    "    null_results = null_model.fit()\n",
    "    ll_null = null_results.llf\n",
    "    \n",
    "    mcfadden = 1 - (ll_full / ll_null)\n",
    "    print(\"=== Pseudo-R\u00b2 Measures ===\")\n",
    "    print(f\"McFadden R\u00b2: {mcfadden:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u26a0\ufe0f  IMPORTANT NOTES ON PSEUDO-R\u00b2\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Pseudo-R\u00b2 is NOT the same as OLS R\u00b2\")\n",
    "print(\"2. Cannot be interpreted as 'proportion of variance explained'\")\n",
    "print(\"3. Values typically much lower than OLS R\u00b2 (0.2-0.4 is considered GOOD)\")\n",
    "print(\"4. Use for model comparison, not absolute fit assessment\")\n",
    "print(\"5. Different pseudo-R\u00b2 measures can give different values\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Hosmer-Lemeshow Test\n",
    "\n",
    "Tests if observed and expected frequencies match across probability deciles.\n",
    "\n",
    "**Null hypothesis**: Model is correctly specified (good fit)\n",
    "\n",
    "**Test statistic**: $H = \\sum_{g=1}^{G} \\frac{(O_g - E_g)^2}{E_g \\times (1-E_g/N_g)}$\n",
    "\n",
    "- $H \\sim \\chi^2_{G-2}$ under $H_0$\n",
    "- Typically use $G=10$ groups\n",
    "- **Large p-value** = fail to reject $H_0$ = good fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Hosmer-Lemeshow goodness-of-fit test\n",
    "if hasattr(logit_results, 'hosmer_lemeshow_test'):\n",
    "    hl_test = logit_results.hosmer_lemeshow_test(n_groups=10)\n",
    "    \n",
    "    print(\"=== Hosmer-Lemeshow Goodness-of-Fit Test ===\")\n",
    "    print(f\"Chi-square statistic: {hl_test['statistic']:.4f}\")\n",
    "    print(f\"p-value: {hl_test['p_value']:.4f}\")\n",
    "    print(f\"Degrees of freedom: {hl_test['df']}\")\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"  H0: Model is correctly specified (observed frequencies ~ expected frequencies)\")\n",
    "    if hl_test['p_value'] > 0.05:\n",
    "        print(f\"  Result: Fail to reject H0 (p={hl_test['p_value']:.4f} > 0.05)\")\n",
    "        print(\"  Conclusion: No evidence of model misspecification \u2713\")\n",
    "    else:\n",
    "        print(f\"  Result: Reject H0 (p={hl_test['p_value']:.4f} \u2264 0.05)\")\n",
    "        print(\"  Conclusion: Evidence of model misspecification \u2717\")\n",
    "        print(\"  Action: Consider adding interactions, polynomials, or other variables\")\n",
    "else:\n",
    "    print(\"=== Hosmer-Lemeshow Test ===\")\n",
    "    print(\"Function not yet implemented in PanelBox.\")\n",
    "    print(\"This test will be available in future versions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Link Test\n",
    "\n",
    "Tests if link function is correctly specified.\n",
    "\n",
    "**Procedure**:\n",
    "1. Calculate $\\hat{y} = \\mathbf{X}'\\hat{\\boldsymbol{\\beta}}$ (linear predictor)\n",
    "2. Regress $y$ on $\\hat{y}$ and $\\hat{y}^2$\n",
    "3. Test if $\\hat{y}^2$ is significant\n",
    "\n",
    "**Interpretation**:\n",
    "- $\\hat{y}^2$ NOT significant \u2192 link function is appropriate\n",
    "- $\\hat{y}^2$ significant \u2192 may need different link or additional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform link test (specification test)\n",
    "if hasattr(logit_results, 'link_test'):\n",
    "    link_test = logit_results.link_test()\n",
    "    \n",
    "    print(\"=== Link Test (Specification Test) ===\")\n",
    "    print(f\"Coefficient on (X\u03b2)\u00b2: {link_test['coef']:.4f}\")\n",
    "    print(f\"Standard error: {link_test.get('se', 'N/A')}\")\n",
    "    print(f\"p-value: {link_test['p_value']:.4f}\")\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"  H0: Link function is correctly specified (coefficient on (X\u03b2)\u00b2 = 0)\")\n",
    "    if link_test['p_value'] > 0.05:\n",
    "        print(f\"  Result: (X\u03b2)\u00b2 is NOT significant (p={link_test['p_value']:.4f})\")\n",
    "        print(\"  Conclusion: Logit link function is appropriate \u2713\")\n",
    "    else:\n",
    "        print(f\"  Result: (X\u03b2)\u00b2 IS significant (p={link_test['p_value']:.4f})\")\n",
    "        print(\"  Conclusion: May need different link function or additional variables \u2717\")\n",
    "        print(\"  Suggestions: Try adding quadratic terms, interactions, or Probit\")\n",
    "else:\n",
    "    print(\"=== Link Test ===\")\n",
    "    print(\"Function not yet implemented in PanelBox.\")\n",
    "    print(\"This test will be available in future versions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>\n",
    "## 9. Application: Labor Force Participation\n",
    "\n",
    "### 9.1 Research Question\n",
    "\n",
    "**\"What factors influence women's labor force participation decisions?\"**\n",
    "\n",
    "We'll estimate a comprehensive model including:\n",
    "- Age (with quadratic term to capture lifecycle effects)\n",
    "- Education\n",
    "- Number of children\n",
    "- Marital status\n",
    "- Work experience\n",
    "\n",
    "### 9.2 Full Model Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate comprehensive model with quadratic age term\n",
    "full_formula = \"lfp ~ age + I(age**2) + educ + kids + married + exper\"\n",
    "logit_full = PooledLogit(full_formula, data, \"id\", \"year\")\n",
    "results_full = logit_full.fit(cov_type='cluster')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \" * 20 + \"LABOR FORCE PARTICIPATION MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(results_full.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Economic Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\" \" * 25 + \"ECONOMIC INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Age effects (quadratic)\n",
    "beta_age = results_full.params['age']\n",
    "beta_age2 = results_full.params['I(age ** 2)']\n",
    "print(\"\\n1. AGE EFFECTS (Quadratic Specification)\")\n",
    "print(\"   \" + \"-\" * 60)\n",
    "print(f\"   Linear term (age):     \u03b2 = {beta_age:.4f}\")\n",
    "print(f\"   Quadratic term (age\u00b2): \u03b2 = {beta_age2:.4f}\")\n",
    "\n",
    "if beta_age2 < 0:\n",
    "    optimal_age = -beta_age / (2 * beta_age2)\n",
    "    print(f\"\\n   \u2192 Inverted-U shape: Participation increases then decreases with age\")\n",
    "    print(f\"   \u2192 Peak participation at age: {optimal_age:.1f} years\")\n",
    "elif beta_age2 > 0:\n",
    "    print(f\"\\n   \u2192 U-shape: Participation decreases then increases with age\")\n",
    "else:\n",
    "    print(f\"\\n   \u2192 Linear relationship with age\")\n",
    "\n",
    "# Education\n",
    "beta_educ = results_full.params['educ']\n",
    "or_educ = np.exp(beta_educ)\n",
    "print(\"\\n2. EDUCATION\")\n",
    "print(\"   \" + \"-\" * 60)\n",
    "print(f\"   Coefficient: \u03b2 = {beta_educ:.4f}\")\n",
    "print(f\"   Odds Ratio: exp(\u03b2) = {or_educ:.4f}\")\n",
    "print(f\"\\n   \u2192 Each additional year of education MULTIPLIES odds of working by {or_educ:.4f}\")\n",
    "print(f\"   \u2192 \u2248 {100*(or_educ-1):.2f}% increase in odds per year of schooling\")\n",
    "if beta_educ > 0:\n",
    "    print(f\"   \u2192 POSITIVE effect: More education \u2192 Higher labor force participation\")\n",
    "\n",
    "# Kids\n",
    "beta_kids = results_full.params['kids']\n",
    "or_kids = np.exp(beta_kids)\n",
    "print(\"\\n3. NUMBER OF CHILDREN\")\n",
    "print(\"   \" + \"-\" * 60)\n",
    "print(f\"   Coefficient: \u03b2 = {beta_kids:.4f}\")\n",
    "print(f\"   Odds Ratio: exp(\u03b2) = {or_kids:.4f}\")\n",
    "print(f\"\\n   \u2192 Each additional child MULTIPLIES odds of working by {or_kids:.4f}\")\n",
    "if or_kids < 1:\n",
    "    print(f\"   \u2192 \u2248 {100*(1-or_kids):.2f}% DECREASE in odds per child\")\n",
    "    print(f\"   \u2192 NEGATIVE effect: More children \u2192 Lower labor force participation\")\n",
    "else:\n",
    "    print(f\"   \u2192 \u2248 {100*(or_kids-1):.2f}% increase in odds per child\")\n",
    "\n",
    "# Marriage\n",
    "beta_married = results_full.params['married']\n",
    "or_married = np.exp(beta_married)\n",
    "print(\"\\n4. MARITAL STATUS\")\n",
    "print(\"   \" + \"-\" * 60)\n",
    "print(f\"   Coefficient: \u03b2 = {beta_married:.4f}\")\n",
    "print(f\"   Odds Ratio: exp(\u03b2) = {or_married:.4f}\")\n",
    "print(f\"\\n   \u2192 Being married (vs single) MULTIPLIES odds of working by {or_married:.4f}\")\n",
    "if or_married < 1:\n",
    "    print(f\"   \u2192 \u2248 {100*(1-or_married):.2f}% DECREASE in odds\")\n",
    "    print(f\"   \u2192 NEGATIVE effect: Marriage associated with lower participation\")\n",
    "else:\n",
    "    print(f\"   \u2192 \u2248 {100*(or_married-1):.2f}% increase in odds\")\n",
    "    print(f\"   \u2192 POSITIVE effect: Marriage associated with higher participation\")\n",
    "\n",
    "# Experience\n",
    "beta_exper = results_full.params['exper']\n",
    "or_exper = np.exp(beta_exper)\n",
    "print(\"\\n5. WORK EXPERIENCE\")\n",
    "print(\"   \" + \"-\" * 60)\n",
    "print(f\"   Coefficient: \u03b2 = {beta_exper:.4f}\")\n",
    "print(f\"   Odds Ratio: exp(\u03b2) = {or_exper:.4f}\")\n",
    "print(f\"\\n   \u2192 Each additional year of experience MULTIPLIES odds by {or_exper:.4f}\")\n",
    "print(f\"   \u2192 \u2248 {100*(or_exper-1):.2f}% change in odds per year\")\n",
    "if beta_exper > 0:\n",
    "    print(f\"   \u2192 POSITIVE effect: More experience \u2192 Higher participation (human capital)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Predictions for Representative Profiles\n",
    "\n",
    "Let's predict participation probabilities for different types of women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create representative profiles\n",
    "profiles = pd.DataFrame({\n",
    "    'age': [30, 30, 40, 40],\n",
    "    'educ': [12, 16, 12, 16],  # High school vs college\n",
    "    'kids': [0, 0, 2, 2],\n",
    "    'married': [0, 0, 1, 1],\n",
    "    'exper': [5, 5, 10, 10]\n",
    "})\n",
    "\n",
    "# Predict probabilities (need to handle formula transformation)\n",
    "from panelbox.utils import prepare_data_for_prediction\n",
    "\n",
    "# Manually calculate linear predictor\n",
    "profiles_pred = profiles.copy()\n",
    "xb = (results_full.params['Intercept'] +\n",
    "      results_full.params['age'] * profiles_pred['age'] +\n",
    "      results_full.params['I(age ** 2)'] * (profiles_pred['age'] ** 2) +\n",
    "      results_full.params['educ'] * profiles_pred['educ'] +\n",
    "      results_full.params['kids'] * profiles_pred['kids'] +\n",
    "      results_full.params['married'] * profiles_pred['married'] +\n",
    "      results_full.params['exper'] * profiles_pred['exper'])\n",
    "\n",
    "# Apply logit transformation\n",
    "profiles_pred['prob_working'] = 1 / (1 + np.exp(-xb))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \" * 15 + \"PREDICTED PROBABILITIES FOR REPRESENTATIVE WOMEN\")\n",
    "print(\"=\"*80)\n",
    "print(profiles_pred.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, row in profiles_pred.iterrows():\n",
    "    print(f\"\\nProfile {i+1}:\")\n",
    "    print(f\"  \u2022 Age: {int(row['age'])} years\")\n",
    "    print(f\"  \u2022 Education: {int(row['educ'])} years ({'College' if row['educ'] >= 16 else 'High School'})\")\n",
    "    print(f\"  \u2022 Children: {int(row['kids'])}\")\n",
    "    print(f\"  \u2022 Marital status: {'Married' if row['married'] else 'Single'}\")\n",
    "    print(f\"  \u2022 Experience: {int(row['exper'])} years\")\n",
    "    print(f\"  \" + \"-\" * 60)\n",
    "    print(f\"  \u2192 Predicted probability of working: {100*row['prob_working']:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 Visualization: Age Profile\n",
    "\n",
    "How does the probability of working vary with age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age profile (holding other variables constant)\n",
    "age_range = np.arange(20, 65, 1)\n",
    "age_profile = pd.DataFrame({\n",
    "    'age': age_range,\n",
    "    'educ': 12,      # High school\n",
    "    'kids': 1,       # 1 child (average)\n",
    "    'married': 0,    # Single\n",
    "    'exper': 5       # 5 years experience\n",
    "})\n",
    "\n",
    "# Calculate predictions\n",
    "xb_age = (results_full.params['Intercept'] +\n",
    "          results_full.params['age'] * age_profile['age'] +\n",
    "          results_full.params['I(age ** 2)'] * (age_profile['age'] ** 2) +\n",
    "          results_full.params['educ'] * age_profile['educ'] +\n",
    "          results_full.params['kids'] * age_profile['kids'] +\n",
    "          results_full.params['married'] * age_profile['married'] +\n",
    "          results_full.params['exper'] * age_profile['exper'])\n",
    "\n",
    "age_profile['prob'] = 1 / (1 + np.exp(-xb_age))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(age_profile['age'], age_profile['prob'], linewidth=3, color='#2980b9')\n",
    "plt.xlabel('Age (years)', fontsize=13)\n",
    "plt.ylabel('Probability of Working', fontsize=13)\n",
    "plt.title('Labor Force Participation by Age\\n(12 years education, 1 child, single, 5 years experience)',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, linewidth=2, label='50% threshold')\n",
    "\n",
    "# Find peak\n",
    "peak_idx = age_profile['prob'].idxmax()\n",
    "peak_age = age_profile.loc[peak_idx, 'age']\n",
    "peak_prob = age_profile.loc[peak_idx, 'prob']\n",
    "plt.scatter([peak_age], [peak_prob], color='red', s=200, zorder=5, marker='o')\n",
    "plt.annotate(f'Peak: Age {peak_age:.0f}\\nP = {peak_prob:.2f}',\n",
    "             xy=(peak_age, peak_prob), xytext=(peak_age+5, peak_prob-0.05),\n",
    "             fontsize=11, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "             arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([20, 65])\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Finding: Participation probability peaks at age {peak_age:.0f} ({peak_prob:.1%})\")\n",
    "print(f\"This reflects the lifecycle pattern of female labor force participation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "## 10. Exercises\n",
    "\n",
    "Test your understanding with these exercises!\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 1: Home Purchase Decision (Easy)\n",
    "\n",
    "**Scenario**: Predict whether households purchase a home based on income, age, and credit score.\n",
    "\n",
    "**Task**:\n",
    "1. Generate synthetic data with `n=2000` observations\n",
    "2. Estimate LPM, Logit, and Probit models\n",
    "3. Compare predicted probabilities\n",
    "4. Which model would you prefer and why?\n",
    "\n",
    "**Starter code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n",
    "# Step 1: Generate synthetic data\n",
    "np.random.seed(123)\n",
    "n = 2000\n",
    "\n",
    "# TODO: Create DataFrame with:\n",
    "#   - income (normal, mean=50000, sd=15000)\n",
    "#   - age (uniform, 25-65)\n",
    "#   - credit_score (normal, mean=700, sd=50)\n",
    "#   - bought_home (binary, based on logit model)\n",
    "\n",
    "# Step 2: Estimate models\n",
    "# TODO: Estimate LPM, Logit, Probit\n",
    "\n",
    "# Step 3: Compare predictions\n",
    "# TODO: Create scatter plots\n",
    "\n",
    "# Step 4: Discussion\n",
    "# TODO: Write your conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: Use formula: `bought_home ~ income + age + credit_score`\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2: Odds Ratios Interpretation (Medium)\n",
    "\n",
    "Using the labor participation Logit model from Section 9:\n",
    "\n",
    "**Task**:\n",
    "1. Calculate odds ratios for all variables\n",
    "2. Write a one-paragraph interpretation of each coefficient\n",
    "3. Which variables have the strongest effects?\n",
    "4. Compare the effect sizes\n",
    "\n",
    "**Questions to answer**:\n",
    "- How do you interpret the odds ratio for education?\n",
    "- What does a negative coefficient mean in terms of odds?\n",
    "- Can you rank variables by importance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "# Step 1: Calculate odds ratios\n",
    "# TODO: Use results_full from Section 9\n",
    "\n",
    "# Step 2: Interpret each variable\n",
    "# TODO: Write interpretations\n",
    "\n",
    "# Step 3: Rank by effect size\n",
    "# TODO: Compare absolute values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3: Model Comparison (Medium)\n",
    "\n",
    "Compare Logit and Probit models systematically.\n",
    "\n",
    "**Task**:\n",
    "1. Estimate both models with identical specifications\n",
    "2. Compare AIC, BIC, and pseudo-R\u00b2\n",
    "3. Test if predictions are statistically different using correlation\n",
    "4. Create visualization comparing predicted probabilities\n",
    "5. Calculate mean absolute difference in predictions\n",
    "\n",
    "**Bonus**: Estimate a model with interaction terms (e.g., `educ:married`) and repeat comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n",
    "# Step 1: Estimate both models\n",
    "# TODO: Use full specification from Section 9\n",
    "\n",
    "# Step 2: Compare fit statistics\n",
    "# TODO: Create comparison table\n",
    "\n",
    "# Step 3: Test prediction differences\n",
    "# TODO: Calculate correlation and MAE\n",
    "\n",
    "# Step 4: Visualize\n",
    "# TODO: Scatter plot with 45-degree line\n",
    "\n",
    "# Bonus: Interaction model\n",
    "# TODO: Add interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 4: Classification Threshold Analysis (Hard)\n",
    "\n",
    "Explore how classification threshold affects performance.\n",
    "\n",
    "**Task**:\n",
    "1. Calculate classification metrics for thresholds: 0.3, 0.4, 0.5, 0.6, 0.7\n",
    "2. Plot precision-recall trade-off curve\n",
    "3. Which threshold maximizes F1-score?\n",
    "4. Discuss when you might choose a threshold other than 0.5\n",
    "\n",
    "**Advanced**:\n",
    "- Calculate cost-sensitive threshold if false negatives cost 2x false positives\n",
    "- Plot threshold vs accuracy/precision/recall on same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your solution here\n",
    "\n",
    "# Step 1: Loop over thresholds\n",
    "thresholds_to_test = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "results_by_threshold = []\n",
    "\n",
    "# TODO: Calculate metrics for each threshold\n",
    "\n",
    "# Step 2: Plot precision-recall trade-off\n",
    "# TODO: Create plot\n",
    "\n",
    "# Step 3: Find optimal threshold\n",
    "# TODO: Maximize F1-score\n",
    "\n",
    "# Step 4: Discussion\n",
    "# TODO: Write discussion on threshold selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion prompts**:\n",
    "- Medical diagnosis: Would you use threshold = 0.5?\n",
    "- Spam detection: What threshold makes sense?\n",
    "- Loan approval: How does cost asymmetry affect choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Binary outcomes require specialized models** \u2014 LPM has fundamental problems\n",
    "\n",
    "2. **Link functions** (Logistic, Normal) ensure $P \\in [0,1]$\n",
    "\n",
    "3. **Logit and Probit** give nearly identical results in practice\n",
    "\n",
    "4. **Coefficients** are NOT marginal effects:\n",
    "   - Logit: log-odds ratios \u2192 interpret via $\\exp(\\beta)$\n",
    "   - Probit: latent variable effects\n",
    "\n",
    "5. **Pseudo-R\u00b2** is NOT like OLS R\u00b2 \u2014 use for comparison only\n",
    "\n",
    "6. **Classification metrics** (accuracy, precision, recall) assess prediction quality\n",
    "\n",
    "7. **Applied interpretation** requires understanding odds ratios and probability predictions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Notebook 02**: Fixed Effects Logit \u2014 control for unobserved heterogeneity\n",
    "\n",
    "**Notebook 03**: Random Effects \u2014 model individual-specific effects\n",
    "\n",
    "**Notebook 04**: Marginal Effects \u2014 calculate proper marginal effects and elasticities\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "### Essential Reading\n",
    "\n",
    "1. **Wooldridge, J. M. (2010)**. *Econometric Analysis of Cross Section and Panel Data* (2nd ed.). MIT Press. Chapter 15.\n",
    "\n",
    "2. **Cameron, A. C., & Trivedi, P. K. (2005)**. *Microeconometrics: Methods and Applications*. Cambridge University Press. Chapter 14.\n",
    "\n",
    "3. **Greene, W. H. (2018)**. *Econometric Analysis* (8th ed.). Pearson. Chapter 17.\n",
    "\n",
    "### Classic Papers\n",
    "\n",
    "4. **Hosmer, D. W., & Lemeshow, S. (2000)**. *Applied Logistic Regression* (2nd ed.). Wiley.\n",
    "\n",
    "5. **Mroz, T. A. (1987)**. \"The sensitivity of an empirical model of married women's hours of work to economic and statistical assumptions.\" *Econometrica*, 765-799.\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing this tutorial!**\n",
    "\n",
    "Questions or feedback? Visit: https://github.com/panelbox/panelbox/issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
