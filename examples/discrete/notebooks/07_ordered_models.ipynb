{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordered Logit/Probit: Ordinal Dependent Variables\n",
    "\n",
    "**Tutorial Series**: Discrete Choice Econometrics with PanelBox\n",
    "\n",
    "**Notebook**: 07 - Ordered Models\n",
    "\n",
    "**Author**: PanelBox Contributors\n",
    "\n",
    "**Date**: 2026-02-17\n",
    "\n",
    "**Estimated Duration**: 75 minutes\n",
    "\n",
    "**Difficulty Level**: Intermediate-Advanced\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Distinguish ordinal from nominal (multinomial) dependent variables\n",
    "2. Specify the latent variable framework with cutpoints (thresholds)\n",
    "3. Estimate Ordered Logit and Ordered Probit using PanelBox\n",
    "4. Interpret cutpoints and their relationship to category boundaries\n",
    "5. Test the parallel regression (proportional odds) assumption\n",
    "6. Calculate category-specific marginal effects\n",
    "7. Understand Random Effects Ordered models for panel data\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Ordinal vs Multinomial](#section1)\n",
    "2. [Latent Variable Framework](#section2)\n",
    "3. [Cutpoints Interpretation](#section3)\n",
    "4. [Estimation with PanelBox](#section4)\n",
    "5. [Parallel Regression Assumption](#section5)\n",
    "6. [Predicted Probabilities per Category](#section6)\n",
    "7. [Category-Specific Marginal Effects](#section7)\n",
    "8. [Random Effects Ordered Logit](#section8)\n",
    "9. [Application — Credit Rating Analysis](#section9)\n",
    "10. [Exercises](#exercises)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Required**: Notebook 01 (Binary Choice Introduction)\n",
    "- **Recommended**: Notebook 06 (Multinomial Logit) for comparison\n",
    "- **Conceptual**: Latent variables, cumulative distributions, ordered categories\n",
    "- **Technical**: Understanding of CDFs and thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import all required libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation and numerical computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import norm, chi2\n",
    "from scipy.special import expit\n",
    "\n",
    "# PanelBox models\n",
    "from panelbox.models.discrete.ordered import (\n",
    "    OrderedLogit, OrderedProbit, RandomEffectsOrderedLogit\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Matplotlib configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"outputs\"\n",
    "FIG_DIR = OUTPUT_DIR / \"figures\"\n",
    "TABLE_DIR = OUTPUT_DIR / \"tables\"\n",
    "REPORT_DIR = OUTPUT_DIR / \"reports\"\n",
    "\n",
    "# Create output directories if needed\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Rating labels for readability\n",
    "RATING_LABELS = {0: 'Poor', 1: 'Fair', 2: 'Good', 3: 'Excellent'}\n",
    "RATING_COLORS = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']\n",
    "\n",
    "print(\"All libraries imported successfully\")\n",
    "print(f\"Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Section 1: Ordinal vs Multinomial (20 min)\n",
    "\n",
    "## 1.1 What Are Ordinal Data?\n",
    "\n",
    "Ordinal data have categories with a **natural ordering** but **unknown distances** between categories:\n",
    "\n",
    "- Credit rating: poor < fair < good < excellent\n",
    "- Customer satisfaction: 1 < 2 < 3 < 4 < 5\n",
    "- Education level: primary < secondary < tertiary\n",
    "- Health status: poor < fair < good < very good < excellent\n",
    "\n",
    "## 1.2 Why Not Multinomial Logit?\n",
    "\n",
    "Multinomial Logit (Notebook 06) treats all categories as **unordered**:\n",
    "- **Ignores ordering information** inherent in the data\n",
    "- Requires $(J-1) \\times K$ parameters vs $K + (J-1)$ for ordered models\n",
    "- Less efficient: wastes degrees of freedom\n",
    "\n",
    "## 1.3 Why Not OLS?\n",
    "\n",
    "Treating ordinal categories as continuous (OLS regression) has problems:\n",
    "- Assumes **equal distances** between categories (is the gap from \"poor\" to \"fair\" the same as \"good\" to \"excellent\"?)\n",
    "- Predicted values can **fall outside the valid range** (e.g., predicted rating of -0.5 or 4.3)\n",
    "- Heteroskedastic errors by construction\n",
    "\n",
    "## 1.4 Ordered Models: Best of Both Worlds\n",
    "\n",
    "Ordered Logit/Probit models:\n",
    "- **Preserve the ordering** (poor < fair < good < excellent)\n",
    "- Allow **flexible category boundaries** (distances estimated from data)\n",
    "- Predictions are **proper probabilities** that sum to 1\n",
    "- Require only $K + (J-1)$ parameters\n",
    "\n",
    "## 1.5 Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credit rating panel data\n",
    "data = pd.read_csv(DATA_DIR / \"credit_rating.csv\")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nShape: {data.shape}\")\n",
    "print(f\"Number of firms: {data['id'].nunique()}\")\n",
    "print(f\"Number of periods: {data['year'].nunique()}\")\n",
    "print(f\"Years: {data['year'].min()} - {data['year'].max()}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating distribution\n",
    "print(\"=== Credit Rating Distribution ===\")\n",
    "rating_dist = data['rating'].value_counts().sort_index()\n",
    "rating_prop = data['rating'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "for code, label in RATING_LABELS.items():\n",
    "    print(f\"  {code} ({label:10s}): {rating_dist[code]:5d} obs  ({rating_prop[code]:.1%})\")\n",
    "\n",
    "print(f\"\\nTotal observations: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by rating category\n",
    "print(\"=== Firm Characteristics by Credit Rating ===\")\n",
    "summary = data.groupby('rating')[['income', 'debt_ratio', 'age', 'size', 'profitability']].mean()\n",
    "summary.index = [RATING_LABELS[r] for r in summary.index]\n",
    "print(summary.round(3))\n",
    "\n",
    "print(\"\\nKey patterns:\")\n",
    "print(\"  - Higher income -> better ratings\")\n",
    "print(\"  - Lower debt -> better ratings\")\n",
    "print(\"  - Larger and more profitable firms tend to have better ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rating distribution and characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Rating distribution\n",
    "labels = [RATING_LABELS[i] for i in range(4)]\n",
    "bars = axes[0, 0].bar(labels, rating_dist.values, color=RATING_COLORS, alpha=0.8, edgecolor='black')\n",
    "for bar, count in zip(bars, rating_dist.values):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
    "                    f'{count}', ha='center', va='bottom', fontsize=11)\n",
    "axes[0, 0].set_title('Credit Rating Distribution', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Income by rating\n",
    "for code, label in RATING_LABELS.items():\n",
    "    subset = data[data['rating'] == code]['income']\n",
    "    axes[0, 1].hist(subset, bins=25, alpha=0.5, label=label, color=RATING_COLORS[code])\n",
    "axes[0, 1].set_title('Income Distribution by Rating', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Log Firm Income')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Debt ratio by rating\n",
    "rating_data = [data[data['rating'] == c]['debt_ratio'] for c in range(4)]\n",
    "bp = axes[1, 0].boxplot(rating_data, labels=labels, patch_artist=True, notch=True)\n",
    "for patch, color in zip(bp['boxes'], RATING_COLORS):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "axes[1, 0].set_title('Debt Ratio by Rating', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Debt-to-Assets Ratio')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Rating transitions over time\n",
    "rating_by_year = data.groupby('year')['rating'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "rating_by_year.columns = [RATING_LABELS[c] for c in rating_by_year.columns]\n",
    "rating_by_year.plot(kind='bar', stacked=True, ax=axes[1, 1],\n",
    "                    color=RATING_COLORS, alpha=0.8, edgecolor='black')\n",
    "axes[1, 1].set_title('Rating Distribution Over Time', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Proportion')\n",
    "axes[1, 1].set_xlabel('Year')\n",
    "axes[1, 1].tick_params(axis='x', rotation=0)\n",
    "axes[1, 1].legend(title='Rating')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Credit Rating: Exploratory Analysis', fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '07_data_exploration.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/07_data_exploration.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Section 2: Latent Variable Framework (25 min)\n",
    "\n",
    "## 2.1 The Latent Variable\n",
    "\n",
    "Ordered models posit an **unobserved** continuous latent variable $y^*$:\n",
    "\n",
    "$$y^*_{it} = X_{it}'\\beta + \\varepsilon_{it}$$\n",
    "\n",
    "- $y^*$ represents the firm's underlying **credit quality** (continuous, unbounded)\n",
    "- We observe only the **discrete rating** category, not $y^*$ itself\n",
    "\n",
    "## 2.2 Cutpoints Map Latent Variable to Categories\n",
    "\n",
    "**Cutpoints** (thresholds) $\\kappa_0, \\kappa_1, \\kappa_2$ define the boundaries:\n",
    "\n",
    "$$\n",
    "y_{it} = \\begin{cases}\n",
    "0 \\text{ (poor)} & \\text{if } y^* \\leq \\kappa_0 \\\\\n",
    "1 \\text{ (fair)} & \\text{if } \\kappa_0 < y^* \\leq \\kappa_1 \\\\\n",
    "2 \\text{ (good)} & \\text{if } \\kappa_1 < y^* \\leq \\kappa_2 \\\\\n",
    "3 \\text{ (excellent)} & \\text{if } y^* > \\kappa_2\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Key**: $J$ categories require $J-1$ cutpoints (here: 4 categories, 3 cutpoints).\n",
    "\n",
    "## 2.3 Category Probabilities\n",
    "\n",
    "$$P(y=j \\mid X) = F(\\kappa_j - X'\\beta) - F(\\kappa_{j-1} - X'\\beta)$$\n",
    "\n",
    "where:\n",
    "- $F = \\Lambda$ (Logistic CDF) for **Ordered Logit**\n",
    "- $F = \\Phi$ (Normal CDF) for **Ordered Probit**\n",
    "\n",
    "## 2.4 Parameters to Estimate\n",
    "\n",
    "- $\\beta$: coefficient vector ($K$ parameters) — common across all thresholds\n",
    "- $\\kappa_0, \\kappa_1, \\ldots, \\kappa_{J-2}$: cutpoints ($J-1$ parameters)\n",
    "- Total: $K + (J-1)$ parameters\n",
    "\n",
    "## 2.5 Visualization: Latent Variable and Cutpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the latent variable framework\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# --- Panel A: Latent variable density with cutpoints ---\n",
    "ax = axes[0]\n",
    "x = np.linspace(-5, 7, 500)\n",
    "\n",
    "# Example: logistic density (standard)\n",
    "logistic_pdf = np.exp(-x) / (1 + np.exp(-x))**2\n",
    "\n",
    "# Example cutpoints\n",
    "kappa = [-0.8, 0.4, 1.6]\n",
    "\n",
    "# Fill regions\n",
    "regions = [\n",
    "    (-5, kappa[0], RATING_COLORS[0], 'Poor (y=0)'),\n",
    "    (kappa[0], kappa[1], RATING_COLORS[1], 'Fair (y=1)'),\n",
    "    (kappa[1], kappa[2], RATING_COLORS[2], 'Good (y=2)'),\n",
    "    (kappa[2], 7, RATING_COLORS[3], 'Excellent (y=3)'),\n",
    "]\n",
    "\n",
    "for lo, hi, color, label in regions:\n",
    "    mask = (x >= lo) & (x <= hi)\n",
    "    ax.fill_between(x[mask], logistic_pdf[mask], alpha=0.3, color=color, label=label)\n",
    "\n",
    "ax.plot(x, logistic_pdf, 'k-', linewidth=2, label='f(y*)')\n",
    "\n",
    "# Cutpoint lines\n",
    "for j, k in enumerate(kappa):\n",
    "    ax.axvline(x=k, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "    ax.text(k, ax.get_ylim()[1] * 0.95, f'$\\\\kappa_{j}$ = {k:.1f}',\n",
    "            ha='center', va='top', fontsize=11, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='black', alpha=0.8))\n",
    "\n",
    "ax.set_xlabel('Latent variable y* (credit quality)', fontsize=12)\n",
    "ax.set_ylabel('Density f(y*)', fontsize=12)\n",
    "ax.set_title('Panel A: Latent Variable Density with Cutpoints', fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Panel B: Cumulative probabilities ---\n",
    "ax = axes[1]\n",
    "\n",
    "# Show P(y <= j) = F(kappa_j - X'beta) for different X'beta values\n",
    "xb_values = np.linspace(-3, 5, 200)\n",
    "\n",
    "for j, k in enumerate(kappa):\n",
    "    cum_prob = expit(k - xb_values)  # F(kappa_j - X'beta)\n",
    "    ax.plot(xb_values, cum_prob, linewidth=2, color=RATING_COLORS[j],\n",
    "            label=f'P(y $\\\\leq$ {j}) = F($\\\\kappa_{j}$ - X\\'$\\\\beta$)')\n",
    "\n",
    "ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.set_xlabel(\"X'$\\\\beta$ (linear predictor)\", fontsize=12)\n",
    "ax.set_ylabel('Cumulative Probability', fontsize=12)\n",
    "ax.set_title('Panel B: Cumulative Probability Curves', fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "\n",
    "plt.suptitle('Ordered Choice: Latent Variable Framework', fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '07_latent_variable_framework.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/07_latent_variable_framework.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Section 3: Cutpoints Interpretation (20 min)\n",
    "\n",
    "## 3.1 Key Properties of Cutpoints\n",
    "\n",
    "1. **Ordering constraint**: $\\kappa_0 < \\kappa_1 < \\ldots < \\kappa_{J-2}$ (enforced via exponential reparameterization)\n",
    "2. **Estimated simultaneously** with $\\beta$ via maximum likelihood\n",
    "3. **Gaps between cutpoints** reflect category spacing:\n",
    "   - Large gap $\\kappa_j - \\kappa_{j-1}$ $\\rightarrow$ many observations in category $j$\n",
    "   - Small gap $\\rightarrow$ few observations in category $j$\n",
    "4. **Not interpretable in isolation** — they depend on the scale (no intercept in $X'\\beta$)\n",
    "\n",
    "## 3.2 Reparameterization\n",
    "\n",
    "PanelBox ensures ordering via:\n",
    "$$\\kappa_0 = \\gamma_0, \\quad \\kappa_j = \\kappa_{j-1} + \\exp(\\gamma_j) \\quad \\text{for } j > 0$$\n",
    "\n",
    "Since $\\exp(\\gamma_j) > 0$, this guarantees $\\kappa_j > \\kappa_{j-1}$.\n",
    "\n",
    "## 3.3 Example: What Cutpoints Tell Us\n",
    "\n",
    "If estimated cutpoints are $\\hat{\\kappa}_0 = -0.8$, $\\hat{\\kappa}_1 = 0.4$, $\\hat{\\kappa}_2 = 1.6$:\n",
    "- Gap between Poor/Fair boundary and Fair/Good boundary: $0.4 - (-0.8) = 1.2$\n",
    "- Gap between Fair/Good and Good/Excellent: $1.6 - 0.4 = 1.2$\n",
    "- Equal gaps suggest roughly equal difficulty in transitioning between adjacent categories\n",
    "\n",
    "We'll see the actual estimated cutpoints after fitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Section 4: Estimation with PanelBox (15 min)\n",
    "\n",
    "## 4.1 Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare variables\n",
    "exog_vars = ['income', 'debt_ratio', 'age', 'size', 'profitability']\n",
    "y = data['rating'].values\n",
    "X = data[exog_vars].values\n",
    "groups = data['id'].values\n",
    "time = data['year'].values\n",
    "\n",
    "print(f\"Dependent variable (rating): {np.unique(y)}\")\n",
    "print(f\"Number of covariates: {len(exog_vars)}\")\n",
    "print(f\"Total observations: {len(y)}\")\n",
    "print(f\"Number of categories: {len(np.unique(y))}\")\n",
    "print(f\"Parameters to estimate: {len(exog_vars)} betas + {len(np.unique(y)) - 1} cutpoints = {len(exog_vars) + len(np.unique(y)) - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Ordered Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Ordered Logit\n",
    "model_ologit = OrderedLogit(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    groups=groups,\n",
    "    time=time,\n",
    "    n_categories=4\n",
    ")\n",
    "\n",
    "# Store variable names for later use\n",
    "model_ologit.exog_names = exog_vars\n",
    "\n",
    "results_ologit = model_ologit.fit()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" \" * 15 + \"ORDERED LOGIT: CREDIT RATINGS\")\n",
    "print(\"=\" * 70)\n",
    "print(results_ologit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Coefficient Interpretation\n",
    "\n",
    "In ordered models, **$\\beta_k > 0$** means:\n",
    "- Higher $X_k$ $\\rightarrow$ higher latent $y^*$ $\\rightarrow$ higher probability of **better** rating\n",
    "- But $\\beta_k$ is **not** the marginal effect on any specific category probability\n",
    "\n",
    "The sign tells us the **direction**: which end of the scale benefits from an increase in $X_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed interpretation\n",
    "print(\"=== Coefficient Interpretation ===\")\n",
    "print(\"\\nSign interpretation (effect on latent credit quality y*):\")\n",
    "\n",
    "K = len(exog_vars)\n",
    "for k, var in enumerate(exog_vars):\n",
    "    coef = results_ologit.beta[k]\n",
    "    se = results_ologit.bse[k]\n",
    "    z = coef / se if se > 0 else np.nan\n",
    "    p = 2 * (1 - norm.cdf(abs(z))) if not np.isnan(z) else np.nan\n",
    "    sig = '***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.1 else ''\n",
    "\n",
    "    direction = 'better rating' if coef > 0 else 'worse rating'\n",
    "    print(f\"  {var:15s}: beta = {coef:+.4f} {sig:3s}  -> Higher {var} -> {direction}\")\n",
    "\n",
    "print(f\"\\nCutpoints (ordered):\")\n",
    "for j, kappa in enumerate(results_ologit.cutpoints):\n",
    "    print(f\"  kappa_{j}: {kappa:.4f}\")\n",
    "\n",
    "print(f\"\\nCutpoint gaps:\")\n",
    "for j in range(len(results_ologit.cutpoints) - 1):\n",
    "    gap = results_ologit.cutpoints[j+1] - results_ologit.cutpoints[j]\n",
    "    print(f\"  kappa_{j+1} - kappa_{j} = {gap:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Ordered Probit (Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Ordered Probit\n",
    "model_oprobit = OrderedProbit(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    groups=groups,\n",
    "    time=time,\n",
    "    n_categories=4\n",
    ")\n",
    "model_oprobit.exog_names = exog_vars\n",
    "\n",
    "results_oprobit = model_oprobit.fit()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" \" * 15 + \"ORDERED PROBIT: CREDIT RATINGS\")\n",
    "print(\"=\" * 70)\n",
    "print(results_oprobit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Ordered Logit vs Ordered Probit\n",
    "print(\"=== Ordered Logit vs Ordered Probit ===\")\n",
    "print(f\"\\n{'Variable':<15} {'Logit':>10} {'Probit':>10} {'Ratio':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for k, var in enumerate(exog_vars):\n",
    "    b_logit = results_ologit.beta[k]\n",
    "    b_probit = results_oprobit.beta[k]\n",
    "    ratio = b_logit / b_probit if abs(b_probit) > 1e-6 else np.nan\n",
    "    print(f\"{var:<15} {b_logit:>10.4f} {b_probit:>10.4f} {ratio:>10.2f}\")\n",
    "\n",
    "print(f\"\\nLog-likelihood:\")\n",
    "print(f\"  Logit:  {results_ologit.llf:.2f}\")\n",
    "print(f\"  Probit: {results_oprobit.llf:.2f}\")\n",
    "\n",
    "# Compute AIC/BIC\n",
    "n_params = K + 3  # beta + cutpoints\n",
    "n_obs = len(y)\n",
    "aic_logit = -2 * results_ologit.llf + 2 * n_params\n",
    "aic_probit = -2 * results_oprobit.llf + 2 * n_params\n",
    "bic_logit = -2 * results_ologit.llf + np.log(n_obs) * n_params\n",
    "bic_probit = -2 * results_oprobit.llf + np.log(n_obs) * n_params\n",
    "\n",
    "print(f\"\\nAIC:  Logit = {aic_logit:.2f}, Probit = {aic_probit:.2f} -> {'Logit' if aic_logit < aic_probit else 'Probit'} preferred\")\n",
    "print(f\"BIC:  Logit = {bic_logit:.2f}, Probit = {bic_probit:.2f} -> {'Logit' if bic_logit < bic_probit else 'Probit'} preferred\")\n",
    "print(f\"\\nNote: Logit/Probit coefficients differ by a scale factor of ~1.6-1.8.\")\n",
    "print(f\"This is expected: logistic variance = pi^2/3, normal variance = 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cutpoints on number line\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "cutpoints = results_ologit.cutpoints\n",
    "\n",
    "# Number line\n",
    "x_range = [cutpoints[0] - 2, cutpoints[-1] + 2]\n",
    "ax.axhline(y=0, color='black', linewidth=2, zorder=1)\n",
    "\n",
    "# Shade regions\n",
    "boundaries = [x_range[0]] + list(cutpoints) + [x_range[1]]\n",
    "for j in range(4):\n",
    "    ax.axvspan(boundaries[j], boundaries[j+1], alpha=0.2, color=RATING_COLORS[j])\n",
    "    mid = (boundaries[j] + boundaries[j+1]) / 2\n",
    "    ax.text(mid, 0.3, f'{RATING_LABELS[j]}\\n(y={j})', ha='center', va='bottom',\n",
    "            fontsize=12, fontweight='bold', color=RATING_COLORS[j])\n",
    "\n",
    "# Cutpoint markers\n",
    "for j, k in enumerate(cutpoints):\n",
    "    ax.plot(k, 0, 'ko', markersize=12, zorder=5)\n",
    "    ax.annotate(f'$\\\\hat{{\\\\kappa}}_{j}$ = {k:.2f}', xy=(k, 0), xytext=(k, -0.5),\n",
    "                ha='center', fontsize=11, fontweight='bold',\n",
    "                arrowprops=dict(arrowstyle='->', color='black'),\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', edgecolor='black'))\n",
    "\n",
    "ax.set_xlim(x_range)\n",
    "ax.set_ylim(-1.0, 0.8)\n",
    "ax.set_xlabel('Latent variable y* (credit quality)', fontsize=12)\n",
    "ax.set_title('Estimated Cutpoints from Ordered Logit', fontweight='bold', fontsize=14)\n",
    "ax.set_yticks([])\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '07_cutpoints_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/07_cutpoints_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Section 5: Parallel Regression Assumption (20 min)\n",
    "\n",
    "## 5.1 The Proportional Odds Assumption\n",
    "\n",
    "The Ordered Logit assumes that $\\beta$ is **common across all thresholds**:\n",
    "\n",
    "$$\\log \\frac{P(y \\leq j)}{P(y > j)} = \\kappa_j - X'\\beta \\quad \\text{for all } j$$\n",
    "\n",
    "The effect of $X_k$ on the log-odds of $y \\leq j$ vs $y > j$ is **constant** for all $j$.\n",
    "\n",
    "## 5.2 Violation\n",
    "\n",
    "If income matters more for the \"good $\\rightarrow$ excellent\" transition than for \"poor $\\rightarrow$ fair\", the parallel regression assumption is violated.\n",
    "\n",
    "## 5.3 Brant Test\n",
    "\n",
    "**Procedure**:\n",
    "1. Estimate $J-1$ separate binary logits (dichotomize at each threshold)\n",
    "2. Compare $\\beta$ across these models\n",
    "3. $H_0$: $\\beta$ are equal (parallel regression holds)\n",
    "4. Test statistic: $\\chi^2$ with $(J-2) \\times K$ degrees of freedom\n",
    "\n",
    "## 5.4 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Brant test for proportional odds\n# Estimate J-1 separate binary logits using statsmodels\nimport statsmodels.api as sm\n\nn_categories = 4\nn_thresholds = n_categories - 1  # 3 binary models\n\nbinary_betas = []\nbinary_vcovs = []\n\nprint(\"=== Separate Binary Logit Models ===\")\nprint(f\"\\nDichotomizing at each threshold:\\n\")\n\n# Add constant to X for binary logit models\nX_with_const = sm.add_constant(X)\n\nfor j in range(n_thresholds):\n    # Create binary outcome: y <= j vs y > j\n    y_binary = (y > j).astype(int)\n\n    # Estimate binary logit via statsmodels\n    logit_model = sm.Logit(y_binary, X_with_const)\n    res_binary = logit_model.fit(disp=False)\n\n    # Extract coefficients (excluding intercept)\n    beta_j = res_binary.params[1:]  # Skip intercept\n    binary_betas.append(beta_j)\n\n    # Extract covariance (excluding intercept rows/cols)\n    vcov_j = res_binary.cov_params()[1:, 1:]\n    binary_vcovs.append(vcov_j)\n\n    print(f\"  Binary Logit: P(rating > {j}) vs P(rating <= {j})\")\n    print(f\"  N(y=1) = {y_binary.sum()}, N(y=0) = {(1-y_binary).sum()}\")\n    coefs_str = ', '.join(f\"{var}={beta_j[k]:+.3f}\" for k, var in enumerate(exog_vars))\n    print(f\"  Coefficients: {coefs_str}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Brant test statistic\n",
    "print(\"=\" * 70)\n",
    "print(\" \" * 20 + \"BRANT TEST\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nH0: Parallel regression (proportional odds) assumption holds\")\n",
    "print(\"H1: Coefficients differ across thresholds\\n\")\n",
    "\n",
    "# Per-variable test: compare beta across J-1 binary models\n",
    "brant_results = []\n",
    "beta_matrix = np.array(binary_betas)  # (J-1) x K\n",
    "\n",
    "for k, var in enumerate(exog_vars):\n",
    "    # Get betas for variable k across all thresholds\n",
    "    betas_k = beta_matrix[:, k]\n",
    "\n",
    "    # Pairwise differences from the first binary model\n",
    "    diffs = betas_k[1:] - betas_k[0]\n",
    "\n",
    "    # Variance of differences (simplified: use diagonal elements)\n",
    "    var_diffs = np.array([\n",
    "        binary_vcovs[j+1][k, k] + binary_vcovs[0][k, k]\n",
    "        for j in range(len(diffs))\n",
    "    ])\n",
    "\n",
    "    # Chi-squared statistic for this variable\n",
    "    chi2_k = np.sum(diffs**2 / var_diffs)\n",
    "    df_k = len(diffs)\n",
    "    p_k = 1 - chi2.cdf(chi2_k, df_k)\n",
    "\n",
    "    brant_results.append({\n",
    "        'Variable': var,\n",
    "        'chi2': chi2_k,\n",
    "        'df': df_k,\n",
    "        'p_value': p_k,\n",
    "        'Conclusion': 'Reject' if p_k < 0.05 else 'Fail to reject'\n",
    "    })\n",
    "\n",
    "# Overall test\n",
    "all_diffs = beta_matrix[1:] - beta_matrix[0]  # (J-2) x K\n",
    "chi2_overall = 0\n",
    "for j in range(all_diffs.shape[0]):\n",
    "    vcov_diff = binary_vcovs[j+1] + binary_vcovs[0]\n",
    "    try:\n",
    "        chi2_overall += float(all_diffs[j] @ np.linalg.inv(vcov_diff) @ all_diffs[j])\n",
    "    except np.linalg.LinAlgError:\n",
    "        chi2_overall += float(np.sum(all_diffs[j]**2 / np.diag(vcov_diff)))\n",
    "\n",
    "df_overall = (n_thresholds - 1) * K\n",
    "p_overall = 1 - chi2.cdf(chi2_overall, df_overall)\n",
    "\n",
    "# Display results\n",
    "print(f\"{'Variable':<15} {'chi2':>10} {'df':>5} {'p-value':>10} {'Conclusion':>15}\")\n",
    "print(\"-\" * 60)\n",
    "for res in brant_results:\n",
    "    print(f\"{res['Variable']:<15} {res['chi2']:>10.3f} {res['df']:>5d} {res['p_value']:>10.4f} {res['Conclusion']:>15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Overall':<15} {chi2_overall:>10.3f} {df_overall:>5d} {p_overall:>10.4f} {'Reject' if p_overall < 0.05 else 'Fail to reject':>15}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "if p_overall >= 0.05:\n",
    "    print(f\"  The overall Brant test does not reject H0 (p = {p_overall:.4f}).\")\n",
    "    print(f\"  The proportional odds assumption appears reasonable.\")\n",
    "else:\n",
    "    print(f\"  The overall Brant test rejects H0 (p = {p_overall:.4f}).\")\n",
    "    print(f\"  Consider Generalized Ordered Logit or inspect per-variable results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: coefficient estimates from separate binary logits\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x_pos = np.arange(len(exog_vars))\n",
    "width = 0.25\n",
    "threshold_colors = ['#e74c3c', '#f39c12', '#3498db']\n",
    "\n",
    "for j in range(n_thresholds):\n",
    "    offset = (j - 1) * width\n",
    "    bars = ax.bar(x_pos + offset, binary_betas[j], width,\n",
    "                  label=f'P(rating > {j})', color=threshold_colors[j],\n",
    "                  alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Add ordered logit coefficients as reference line markers\n",
    "for k in range(len(exog_vars)):\n",
    "    ax.plot([x_pos[k] - 0.4, x_pos[k] + 0.4],\n",
    "            [results_ologit.beta[k], results_ologit.beta[k]],\n",
    "            'k--', linewidth=2, alpha=0.5)\n",
    "\n",
    "ax.axhline(y=0, color='black', linewidth=0.8, alpha=0.5)\n",
    "ax.set_xlabel('Variable')\n",
    "ax.set_ylabel('Coefficient Estimate')\n",
    "ax.set_title('Parallel Regression Test: Binary Logit Coefficients by Threshold\\n'\n",
    "             '(dashed line = Ordered Logit common coefficient)', fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(exog_vars)\n",
    "ax.legend(title='Binary Model')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add custom legend entry for ordered logit reference\n",
    "handles, labels_leg = ax.get_legend_handles_labels()\n",
    "handles.append(plt.Line2D([0], [0], color='black', linestyle='--', linewidth=2, alpha=0.5))\n",
    "labels_leg.append('Ordered Logit (pooled)')\n",
    "ax.legend(handles, labels_leg, title='Model')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '07_parallel_regression_test.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/07_parallel_regression_test.png\")\n",
    "print(\"\\nIf bars within each variable are approximately equal,\")\n",
    "print(\"the parallel regression assumption holds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Section 6: Predicted Probabilities per Category (15 min)\n",
    "\n",
    "## 6.1 Category Probabilities\n",
    "\n",
    "For each observation, we compute:\n",
    "\n",
    "$$P(y = j \\mid X) = F(\\kappa_j - X'\\beta) - F(\\kappa_{j-1} - X'\\beta)$$\n",
    "\n",
    "Key properties:\n",
    "- $\\sum_{j=0}^{J-1} P(y = j \\mid X) = 1$\n",
    "- Predicted class: $\\hat{y} = \\arg\\max_j P(y=j \\mid X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities for all observations\n",
    "probs = results_ologit.predict_proba()\n",
    "\n",
    "print(f\"Predicted probabilities shape: {probs.shape}\")\n",
    "print(f\"  {probs.shape[0]} observations x {probs.shape[1]} categories\\n\")\n",
    "\n",
    "# First 10 observations\n",
    "prob_df = pd.DataFrame(probs, columns=[RATING_LABELS[j] for j in range(4)])\n",
    "print(\"First 10 predicted probability vectors:\")\n",
    "print(prob_df.head(10).round(4))\n",
    "\n",
    "# Verify rows sum to 1\n",
    "row_sums = probs.sum(axis=1)\n",
    "print(f\"\\nRow sums: min = {row_sums.min():.10f}, max = {row_sums.max():.10f}\")\n",
    "print(f\"All sum to 1: {np.allclose(row_sums, 1.0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for representative firm profiles\n",
    "profiles = pd.DataFrame({\n",
    "    'Profile': [\n",
    "        'Healthy firm',\n",
    "        'Average firm',\n",
    "        'Distressed firm',\n",
    "        'High growth'\n",
    "    ],\n",
    "    'income':        [12.0,  10.5,  9.0,   11.5],\n",
    "    'debt_ratio':    [0.15,  0.40,  0.70,  0.30],\n",
    "    'age':           [30,    20,    10,    5],\n",
    "    'size':          [10.0,  8.0,   6.5,   9.0],\n",
    "    'profitability': [0.15,  0.07,  0.01,  0.12]\n",
    "})\n",
    "\n",
    "X_profiles = profiles[exog_vars].values\n",
    "probs_profiles = results_ologit.predict_proba(X_profiles)\n",
    "\n",
    "print(\"=== Predicted Probabilities for Representative Firms ===\")\n",
    "for i, row in profiles.iterrows():\n",
    "    print(f\"\\n{row['Profile']}:\")\n",
    "    print(f\"  (income={row['income']}, debt={row['debt_ratio']}, age={row['age']}, \"\n",
    "          f\"size={row['size']}, profit={row['profitability']})\")\n",
    "    for j in range(4):\n",
    "        bar_len = int(probs_profiles[i, j] * 40)\n",
    "        bar = '#' * bar_len + ' ' * (40 - bar_len)\n",
    "        print(f\"  P({RATING_LABELS[j]:10s}) = {probs_profiles[i, j]:.4f}  |{bar}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked bar chart of predicted probabilities for representative profiles\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(profiles))\n",
    "bottom = np.zeros(len(profiles))\n",
    "\n",
    "for j in range(4):\n",
    "    label = RATING_LABELS[j]\n",
    "    values = probs_profiles[:, j]\n",
    "    ax.bar(x, values, bottom=bottom, label=label,\n",
    "           color=RATING_COLORS[j], alpha=0.8, edgecolor='black')\n",
    "\n",
    "    # Add percentage labels\n",
    "    for i, (v, b) in enumerate(zip(values, bottom)):\n",
    "        if v > 0.04:\n",
    "            ax.text(i, b + v/2, f'{v:.0%}', ha='center', va='center',\n",
    "                    fontsize=9, fontweight='bold')\n",
    "    bottom += values\n",
    "\n",
    "ax.set_xlabel('Firm Profile')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title('Predicted Rating Probabilities for Representative Firms', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(profiles['Profile'], rotation=0)\n",
    "ax.legend(title='Rating')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '07_predicted_probabilities.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/07_predicted_probabilities.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Section 7: Category-Specific Marginal Effects (25 min)\n",
    "\n",
    "## 7.1 Why Marginal Effects Differ by Category\n",
    "\n",
    "In ordered models, marginal effects are **category-specific**:\n",
    "\n",
    "$$\\frac{\\partial P(y=j)}{\\partial x_k} = \\beta_k \\times \\left[ f(\\kappa_{j-1} - X'\\beta) - f(\\kappa_j - X'\\beta) \\right]$$\n",
    "\n",
    "## 7.2 Key Properties\n",
    "\n",
    "1. **Sum to zero**: $\\sum_j \\frac{\\partial P(y=j)}{\\partial x_k} = 0$\n",
    "2. **Extreme categories**: sign follows $\\beta$ (if $\\beta > 0$, P(highest) increases, P(lowest) decreases)\n",
    "3. **Intermediate categories**: sign is **ambiguous** — can increase or decrease!\n",
    "\n",
    "## 7.3 Average Marginal Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute category-specific Average Marginal Effects manually\n",
    "# (Direct computation to ensure compatibility with the model's API)\n",
    "\n",
    "beta = results_ologit.beta\n",
    "cutpoints = results_ologit.cutpoints\n",
    "cutpoints_ext = np.concatenate([[-np.inf], cutpoints, [np.inf]])\n",
    "\n",
    "# Logistic PDF\n",
    "def logistic_pdf(z):\n",
    "    F = expit(z)\n",
    "    return F * (1 - F)\n",
    "\n",
    "n_obs = len(X)\n",
    "n_cats = 4\n",
    "\n",
    "# AME: average over all observations\n",
    "ame_matrix = np.zeros((len(exog_vars), n_cats))\n",
    "\n",
    "for i in range(n_obs):\n",
    "    linear_pred = X[i] @ beta\n",
    "\n",
    "    for j in range(n_cats):\n",
    "        z_lower = cutpoints_ext[j] - linear_pred\n",
    "        z_upper = cutpoints_ext[j + 1] - linear_pred\n",
    "\n",
    "        pdf_lower = logistic_pdf(z_lower) if np.isfinite(z_lower) else 0\n",
    "        pdf_upper = logistic_pdf(z_upper) if np.isfinite(z_upper) else 0\n",
    "\n",
    "        for k in range(len(exog_vars)):\n",
    "            ame_matrix[k, j] += beta[k] * (pdf_lower - pdf_upper)\n",
    "\n",
    "ame_matrix /= n_obs\n",
    "\n",
    "# Create DataFrame\n",
    "ame_df = pd.DataFrame(\n",
    "    ame_matrix,\n",
    "    index=exog_vars,\n",
    "    columns=[RATING_LABELS[j] for j in range(n_cats)]\n",
    ")\n",
    "\n",
    "print(\"=== Average Marginal Effects (AME) by Category ===\")\n",
    "print(\"\\nEffect of a one-unit increase in each variable on P(rating=j)\\n\")\n",
    "print(ame_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify sum-to-zero property\n",
    "print(\"=== Sum-to-Zero Verification ===\")\n",
    "print(\"\\nSum of AME across categories for each variable:\")\n",
    "for k, var in enumerate(exog_vars):\n",
    "    row_sum = ame_matrix[k].sum()\n",
    "    status = 'PASS' if abs(row_sum) < 1e-8 else 'CHECK'\n",
    "    print(f\"  {var:15s}: {row_sum:+.10f}  [{status}]\")\n",
    "\n",
    "print(\"\\nThis confirms: probability mass shifts between categories, total remains 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of AME (categories x variables)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sns.heatmap(ame_df.T, annot=True, fmt='.4f', cmap='RdBu_r', center=0,\n",
    "            ax=ax, linewidths=1,\n",
    "            cbar_kws={'label': 'Average Marginal Effect'})\n",
    "ax.set_title('Average Marginal Effects by Category\\n'\n",
    "             '(effect on P(rating=j) per unit change in variable)',\n",
    "             fontweight='bold')\n",
    "ax.set_ylabel('Rating Category')\n",
    "ax.set_xlabel('Variable')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '07_marginal_effects_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/07_marginal_effects_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: intermediate category ambiguity\n",
    "print(\"=== Key Insight: Intermediate Category Ambiguity ===\")\n",
    "print(\"\\nFor income (beta > 0):\")\n",
    "for j in range(n_cats):\n",
    "    me = ame_matrix[0, j]  # income is first variable\n",
    "    print(f\"  AME on P({RATING_LABELS[j]:10s}): {me:+.4f}  \"\n",
    "          f\"({'increases' if me > 0 else 'decreases'})\")\n",
    "\n",
    "print(\"\\nFor debt_ratio (beta < 0):\")\n",
    "for j in range(n_cats):\n",
    "    me = ame_matrix[1, j]  # debt_ratio is second variable\n",
    "    print(f\"  AME on P({RATING_LABELS[j]:10s}): {me:+.4f}  \"\n",
    "          f\"({'increases' if me > 0 else 'decreases'})\")\n",
    "\n",
    "print(\"\\nNote: For extreme categories, the sign matches beta.\")\n",
    "print(\"For intermediate categories, the sign may differ from beta!\")\n",
    "print(\"This is the hallmark of ordered models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Section 8: Random Effects Ordered Logit (15 min)\n",
    "\n",
    "## 8.1 Motivation\n",
    "\n",
    "In panel data, firms have persistent unobserved characteristics (management quality, brand reputation) that affect ratings:\n",
    "\n",
    "$$y^*_{it} = X_{it}'\\beta + \\alpha_i + \\varepsilon_{it}, \\quad \\alpha_i \\sim N(0, \\sigma^2_\\alpha)$$\n",
    "\n",
    "## 8.2 Estimation\n",
    "\n",
    "The marginal likelihood integrates out $\\alpha_i$ using **Gauss-Hermite quadrature**:\n",
    "\n",
    "$$\\mathcal{L}_i = \\int \\prod_{t=1}^{T} P(y_{it} \\mid X_{it}, \\alpha_i) \\cdot \\phi(\\alpha_i) \\, d\\alpha_i$$\n",
    "\n",
    "## 8.3 Key Output\n",
    "\n",
    "- $\\hat{\\sigma}_\\alpha$: standard deviation of the random effect\n",
    "- $\\hat{\\rho} = \\frac{\\sigma^2_\\alpha}{\\sigma^2_\\alpha + \\pi^2/3}$: intraclass correlation for the latent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Effects Ordered Logit\n",
    "print(\"Fitting Random Effects Ordered Logit (this may take a moment)...\")\n",
    "print(\"Using Gauss-Hermite quadrature with 12 points.\\n\")\n",
    "\n",
    "model_re = RandomEffectsOrderedLogit(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    groups=groups,\n",
    "    time=time,\n",
    "    n_categories=4,\n",
    "    quadrature_points=12\n",
    ")\n",
    "model_re.exog_names = exog_vars\n",
    "\n",
    "results_re = model_re.fit(maxiter=500)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" \" * 10 + \"RANDOM EFFECTS ORDERED LOGIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nNumber of obs:        {model_re.n_obs:>8d}\")\n",
    "print(f\"Number of firms:      {model_re.n_entities:>8d}\")\n",
    "print(f\"Number of categories: {model_re.n_categories:>8d}\")\n",
    "print(f\"Log-likelihood:       {results_re.llf:>8.3f}\")\n",
    "print(f\"Converged:            {results_re.converged}\")\n",
    "\n",
    "print(f\"\\nCoefficients:\")\n",
    "print(f\"{'Variable':<15} {'Pooled':>10} {'RE':>10} {'Change':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for k, var in enumerate(exog_vars):\n",
    "    b_pooled = results_ologit.beta[k]\n",
    "    b_re = results_re.beta[k]\n",
    "    change = ((b_re - b_pooled) / abs(b_pooled)) * 100 if abs(b_pooled) > 1e-6 else np.nan\n",
    "    print(f\"{var:<15} {b_pooled:>10.4f} {b_re:>10.4f} {change:>+9.1f}%\")\n",
    "\n",
    "print(f\"\\nRandom Effects Parameters:\")\n",
    "sigma_alpha = results_re.sigma_alpha\n",
    "rho = sigma_alpha**2 / (sigma_alpha**2 + np.pi**2 / 3)\n",
    "print(f\"  sigma_alpha = {sigma_alpha:.4f}\")\n",
    "print(f\"  rho (ICC)   = {rho:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  {rho:.1%} of the latent variable variance is due to firm-level heterogeneity.\")\n",
    "print(f\"  Firms have persistent unobserved characteristics affecting their ratings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Section 9: Application — Credit Rating Analysis (40 min)\n",
    "\n",
    "**Research Question**: What financial characteristics drive credit rating categories? How does the parallel regression assumption hold?\n",
    "\n",
    "## 9.1 Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison table\n",
    "print(\"=\" * 70)\n",
    "print(\" \" * 15 + \"MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_params_logit = K + 3\n",
    "n_params_probit = K + 3\n",
    "n_params_re = K + 3 + 1  # + sigma_alpha\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Ordered Logit': {\n",
    "        'Log-likelihood': results_ologit.llf,\n",
    "        'N parameters': n_params_logit,\n",
    "        'AIC': -2 * results_ologit.llf + 2 * n_params_logit,\n",
    "        'BIC': -2 * results_ologit.llf + np.log(n_obs) * n_params_logit,\n",
    "    },\n",
    "    'Ordered Probit': {\n",
    "        'Log-likelihood': results_oprobit.llf,\n",
    "        'N parameters': n_params_probit,\n",
    "        'AIC': -2 * results_oprobit.llf + 2 * n_params_probit,\n",
    "        'BIC': -2 * results_oprobit.llf + np.log(n_obs) * n_params_probit,\n",
    "    },\n",
    "    'RE Ordered Logit': {\n",
    "        'Log-likelihood': results_re.llf,\n",
    "        'N parameters': n_params_re,\n",
    "        'AIC': -2 * results_re.llf + 2 * n_params_re,\n",
    "        'BIC': -2 * results_re.llf + np.log(n_obs) * n_params_re,\n",
    "    }\n",
    "})\n",
    "\n",
    "print(comparison.round(2))\n",
    "\n",
    "# Save comparison table\n",
    "comparison.to_csv(TABLE_DIR / '07_model_comparison.csv')\n",
    "print(f\"\\nTable saved to outputs/tables/07_model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient comparison visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x = np.arange(len(exog_vars))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, results_ologit.beta, width,\n",
    "               label='Ordered Logit', color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax.bar(x, results_oprobit.beta, width,\n",
    "               label='Ordered Probit', color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "bars3 = ax.bar(x + width, results_re.beta, width,\n",
    "               label='RE Ordered Logit', color='#2ecc71', alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax.axhline(y=0, color='black', linewidth=0.8, alpha=0.5)\n",
    "ax.set_xlabel('Variable')\n",
    "ax.set_ylabel('Coefficient')\n",
    "ax.set_title('Coefficient Comparison Across Ordered Models', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(exog_vars)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '07_credit_coefficient_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/07_credit_coefficient_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Policy Analysis: Income and Rating Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much income improvement moves a firm from \"fair\" to \"good\"?\n",
    "print(\"=== Policy Analysis: Income and Rating Transitions ===\")\n",
    "\n",
    "# Start with an \"average\" firm in the \"Fair\" range\n",
    "base_profile = data[exog_vars].mean().values\n",
    "\n",
    "# Vary income while holding other variables at their means\n",
    "income_range = np.linspace(8, 13, 100)\n",
    "probs_income = np.zeros((100, 4))\n",
    "\n",
    "for i, inc in enumerate(income_range):\n",
    "    profile = base_profile.copy()\n",
    "    profile[0] = inc  # Set income\n",
    "    probs_income[i] = results_ologit.predict_proba(profile.reshape(1, -1))\n",
    "\n",
    "# Find income level where P(Good) + P(Excellent) exceeds 50%\n",
    "prob_good_or_better = probs_income[:, 2] + probs_income[:, 3]\n",
    "threshold_idx = np.argmax(prob_good_or_better > 0.5)\n",
    "income_threshold = income_range[threshold_idx]\n",
    "\n",
    "print(f\"\\nAt average characteristics:\")\n",
    "for j in range(4):\n",
    "    base_prob = results_ologit.predict_proba(base_profile.reshape(1, -1))[0, j]\n",
    "    print(f\"  P({RATING_LABELS[j]}) = {base_prob:.4f}\")\n",
    "\n",
    "print(f\"\\nIncome threshold for P(Good or better) > 50%: {income_threshold:.2f}\")\n",
    "print(f\"Current average income: {base_profile[0]:.2f}\")\n",
    "print(f\"Required increase: {income_threshold - base_profile[0]:+.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probability curves as income varies\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Panel A: Individual category probabilities\n",
    "ax = axes[0]\n",
    "for j in range(4):\n",
    "    ax.plot(income_range, probs_income[:, j], linewidth=2,\n",
    "            color=RATING_COLORS[j], label=RATING_LABELS[j])\n",
    "\n",
    "ax.axvline(x=base_profile[0], color='gray', linestyle=':', alpha=0.7, label='Mean income')\n",
    "ax.set_xlabel('Log Firm Income')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title('Category Probabilities vs Income\\n(other variables at means)', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel B: Cumulative probabilities\n",
    "ax = axes[1]\n",
    "cum_probs = np.cumsum(probs_income, axis=1)\n",
    "for j in range(3):\n",
    "    ax.plot(income_range, cum_probs[:, j], linewidth=2,\n",
    "            color=RATING_COLORS[j], label=f'P(rating <= {j})')\n",
    "\n",
    "ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axvline(x=base_profile[0], color='gray', linestyle=':', alpha=0.7)\n",
    "ax.set_xlabel('Log Firm Income')\n",
    "ax.set_ylabel('Cumulative Probability')\n",
    "ax.set_title('Cumulative Probability Curves\\nP(rating <= j) vs Income', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "\n",
    "plt.suptitle('Credit Rating Response to Income Changes', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '07_credit_income_response.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/07_credit_income_response.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Healthy vs Distressed Firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare healthy vs distressed firm profiles\n",
    "print(\"=== Healthy vs Distressed Firm Comparison ===\")\n",
    "\n",
    "healthy = np.array([12.0, 0.15, 30, 10.0, 0.15])   # high income, low debt, large, profitable\n",
    "distressed = np.array([9.0, 0.70, 10, 6.5, 0.01])   # low income, high debt, small, low profit\n",
    "\n",
    "prob_healthy = results_ologit.predict_proba(healthy.reshape(1, -1))[0]\n",
    "prob_distressed = results_ologit.predict_proba(distressed.reshape(1, -1))[0]\n",
    "\n",
    "print(f\"\\n{'Category':<12} {'Healthy':>10} {'Distressed':>12} {'Difference':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for j in range(4):\n",
    "    diff = prob_healthy[j] - prob_distressed[j]\n",
    "    print(f\"{RATING_LABELS[j]:<12} {prob_healthy[j]:>10.4f} {prob_distressed[j]:>12.4f} {diff:>+12.4f}\")\n",
    "\n",
    "print(f\"\\nPredicted rating:\")\n",
    "print(f\"  Healthy firm:    {RATING_LABELS[np.argmax(prob_healthy)]}\")\n",
    "print(f\"  Distressed firm: {RATING_LABELS[np.argmax(prob_distressed)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 AME Table and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save AME table\n",
    "ame_df.to_csv(TABLE_DIR / '07_marginal_effects.csv')\n",
    "print(\"AME table saved to outputs/tables/07_marginal_effects.csv\")\n",
    "\n",
    "# Generate HTML report\n",
    "report_html = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head><title>Credit Rating Analysis Report</title>\n",
    "<style>body {{font-family: Arial; margin: 40px;}}\n",
    "table {{border-collapse: collapse; margin: 20px 0;}}\n",
    "th, td {{border: 1px solid #ddd; padding: 8px; text-align: right;}}\n",
    "th {{background-color: #3498db; color: white;}}\n",
    "h1 {{color: #2c3e50;}} h2 {{color: #3498db;}}</style></head>\n",
    "<body>\n",
    "<h1>Credit Rating Analysis Report</h1>\n",
    "<p>Generated: 2026-02-17 | PanelBox Ordered Models Tutorial</p>\n",
    "\n",
    "<h2>Dataset</h2>\n",
    "<p>Panel of {data['id'].nunique()} firms over {data['year'].nunique()} years ({len(data)} observations).</p>\n",
    "<p>Rating categories: Poor (0), Fair (1), Good (2), Excellent (3).</p>\n",
    "\n",
    "<h2>Model Comparison</h2>\n",
    "{comparison.to_html()}\n",
    "\n",
    "<h2>Average Marginal Effects (Ordered Logit)</h2>\n",
    "{ame_df.round(4).to_html()}\n",
    "\n",
    "<h2>Key Findings</h2>\n",
    "<ul>\n",
    "<li>Income and profitability are the strongest positive determinants of better ratings.</li>\n",
    "<li>Higher debt ratio significantly reduces rating quality.</li>\n",
    "<li>Random effects capture significant firm-level heterogeneity (rho = {rho:.2%}).</li>\n",
    "</ul>\n",
    "</body></html>\"\"\"\n",
    "\n",
    "with open(REPORT_DIR / '07_credit_rating_analysis.html', 'w') as f:\n",
    "    f.write(report_html)\n",
    "print(\"Report saved to outputs/reports/07_credit_rating_analysis.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Exercises\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 1: Ordered Logit vs Multinomial Logit (Medium)\n",
    "\n",
    "**Objective**: Compare Ordered Logit with Multinomial Logit on ordinal data.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Estimate a Multinomial Logit on the credit rating data (using `MultinomialLogit`)\n",
    "2. Compare AIC/BIC with the Ordered Logit\n",
    "3. Count the number of parameters in each model\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. Which model is more parsimonious?\n",
    "2. Does the Ordered Logit sacrifice fit for parsimony?\n",
    "3. When would you prefer MNL over Ordered Logit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n",
    "# Step 1: Estimate Multinomial Logit\n",
    "# from panelbox.models.discrete.multinomial import MultinomialLogit\n",
    "# model_mnl = MultinomialLogit(endog=y, exog=X, n_alternatives=4, base_alternative=0)\n",
    "# results_mnl = model_mnl.fit()\n",
    "\n",
    "# Step 2: Compare AIC/BIC\n",
    "# n_params_mnl = (4 - 1) * len(exog_vars)  # (J-1) x K\n",
    "# n_params_ologit = len(exog_vars) + 3      # K + (J-1)\n",
    "# print(f\"MNL parameters: {n_params_mnl}\")\n",
    "# print(f\"Ordered Logit parameters: {n_params_ologit}\")\n",
    "\n",
    "# Step 3: Compare fit\n",
    "# aic_mnl = -2 * results_mnl.llf + 2 * n_params_mnl\n",
    "# print(f\"AIC: MNL = {aic_mnl:.2f}, OLogit = {aic_logit:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Cutpoints Exploration (Easy)\n",
    "\n",
    "**Objective**: Understand how cutpoints change with category structure.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Estimate Ordered Logit on the full 4-category data\n",
    "2. Merge categories 0 and 1 (Poor + Fair = \"Low\") to create 3 categories\n",
    "3. Re-estimate Ordered Logit on the 3-category data\n",
    "4. Compare cutpoints and coefficients\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. How do the cutpoints change?\n",
    "2. Do the $\\beta$ coefficients change significantly?\n",
    "3. What happens to the log-likelihood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "# Step 1: Full model (already estimated as results_ologit)\n",
    "\n",
    "# Step 2: Merge categories 0 and 1\n",
    "# y_merged = y.copy()\n",
    "# y_merged[y_merged == 1] = 0  # Merge Fair into Poor -> \"Low\"\n",
    "# y_merged[y_merged == 2] = 1  # Good -> 1\n",
    "# y_merged[y_merged == 3] = 2  # Excellent -> 2\n",
    "\n",
    "# Step 3: Re-estimate\n",
    "# model_3cat = OrderedLogit(endog=y_merged, exog=X, groups=groups, time=time, n_categories=3)\n",
    "# results_3cat = model_3cat.fit()\n",
    "# print(results_3cat.summary())\n",
    "\n",
    "# Step 4: Compare\n",
    "# print(f\"4-cat cutpoints: {results_ologit.cutpoints}\")\n",
    "# print(f\"3-cat cutpoints: {results_3cat.cutpoints}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Brant Test Interpretation (Medium)\n",
    "\n",
    "**Objective**: Interpret the Brant test results and understand their implications.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Review the Brant test results from Section 5\n",
    "2. Which variables (if any) violate proportional odds?\n",
    "3. For violating variables, inspect the binary logit coefficients across thresholds\n",
    "4. Explain what the violation means economically\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. If `debt_ratio` violates proportional odds, what does it mean?\n",
    "2. What model would you use if proportional odds is rejected?\n",
    "3. Is it possible for proportional odds to hold for some variables but not others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n",
    "# Step 1: Review Brant test results (from Section 5)\n",
    "# Look at the per-variable p-values\n",
    "\n",
    "# Step 2: Identify violating variables\n",
    "# rejected_vars = [r['Variable'] for r in brant_results if r['p_value'] < 0.05]\n",
    "# print(f\"Variables violating proportional odds: {rejected_vars}\")\n",
    "\n",
    "# Step 3: Compare binary logit coefficients for these variables\n",
    "# For each rejected variable, plot the coefficient across thresholds\n",
    "\n",
    "# Step 4: Economic interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Marginal Effects Ambiguity (Medium)\n",
    "\n",
    "**Objective**: Demonstrate that intermediate category marginal effects can change sign.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Calculate the marginal effect of income on P(Fair) at different income levels\n",
    "2. Show that the sign can change depending on the evaluation point\n",
    "3. Plot the marginal effect of income on each category as income varies\n",
    "\n",
    "### Hint\n",
    "\n",
    "For observation with covariates $X$:\n",
    "$$\\frac{\\partial P(y=j)}{\\partial \\text{income}} = \\beta_{\\text{income}} \\times [f(\\kappa_{j-1} - X'\\beta) - f(\\kappa_j - X'\\beta)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your solution here\n",
    "\n",
    "# Step 1: Varying income, compute ME on each category\n",
    "# base = data[exog_vars].mean().values\n",
    "# income_range = np.linspace(8, 13, 100)\n",
    "# me_by_income = np.zeros((100, 4))\n",
    "# for i, inc in enumerate(income_range):\n",
    "#     profile = base.copy()\n",
    "#     profile[0] = inc\n",
    "#     lp = profile @ beta\n",
    "#     for j in range(4):\n",
    "#         z_lo = cutpoints_ext[j] - lp\n",
    "#         z_hi = cutpoints_ext[j+1] - lp\n",
    "#         pdf_lo = logistic_pdf(z_lo) if np.isfinite(z_lo) else 0\n",
    "#         pdf_hi = logistic_pdf(z_hi) if np.isfinite(z_hi) else 0\n",
    "#         me_by_income[i, j] = beta[0] * (pdf_lo - pdf_hi)\n",
    "\n",
    "# Step 2: Plot\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# for j in range(4):\n",
    "#     ax.plot(income_range, me_by_income[:, j], label=RATING_LABELS[j], color=RATING_COLORS[j])\n",
    "# ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "# ax.set_title('Marginal Effect of Income on P(rating=j)')\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: RE vs Pooled Ordered Logit (Hard)\n",
    "\n",
    "**Objective**: Evaluate the importance of unobserved heterogeneity.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Compare the pooled and RE Ordered Logit estimates\n",
    "2. Test whether $\\sigma_\\alpha$ is significant (LR test: compare log-likelihoods)\n",
    "3. Calculate the intraclass correlation $\\rho$\n",
    "4. Interpret: what does $\\rho$ tell us about firm-level heterogeneity?\n",
    "\n",
    "### Hint\n",
    "\n",
    "LR test: $\\chi^2 = -2 \\times (\\ell_{\\text{pooled}} - \\ell_{\\text{RE}})$, df=1, but uses a mixture distribution (boundary problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Your solution here\n",
    "\n",
    "# Step 1: Compare estimates (use results_ologit and results_re from above)\n",
    "# for k, var in enumerate(exog_vars):\n",
    "#     print(f\"{var}: pooled={results_ologit.beta[k]:.4f}, RE={results_re.beta[k]:.4f}\")\n",
    "\n",
    "# Step 2: LR test\n",
    "# lr_stat = -2 * (results_ologit.llf - results_re.llf)\n",
    "# p_lr = 0.5 * (1 - chi2.cdf(lr_stat, 1))  # mixture chi2 for boundary\n",
    "# print(f\"LR statistic: {lr_stat:.4f}\")\n",
    "# print(f\"p-value (mixture): {p_lr:.4f}\")\n",
    "\n",
    "# Step 3: Interpret rho\n",
    "# sigma = results_re.sigma_alpha\n",
    "# rho = sigma**2 / (sigma**2 + np.pi**2/3)\n",
    "# print(f\"rho = {rho:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary and Key Takeaways\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "1. **Ordered models** are appropriate when the dependent variable has a natural ordering (poor < fair < good < excellent), unlike MNL which ignores order\n",
    "\n",
    "2. **Latent variable framework**: An unobserved continuous $y^*$ is mapped to observed categories via cutpoints $\\kappa_j$\n",
    "\n",
    "3. **Coefficient interpretation**: $\\beta_k > 0$ means higher $X_k$ shifts probability toward higher categories, but this is NOT the marginal effect\n",
    "\n",
    "4. **Cutpoints** are estimated alongside $\\beta$ and define flexible category boundaries\n",
    "\n",
    "5. **Parallel regression assumption**: $\\beta$ is common across all thresholds. Always test with Brant test\n",
    "\n",
    "6. **Category-specific marginal effects**: Sum to zero across categories. Extreme categories follow the sign of $\\beta$; intermediate categories are ambiguous\n",
    "\n",
    "7. **Random Effects**: Account for firm-level persistent heterogeneity; $\\rho$ measures its importance\n",
    "\n",
    "## Key Formulas\n",
    "\n",
    "| Concept | Formula |\n",
    "|---------|--------|\n",
    "| Latent variable | $y^* = X'\\beta + \\varepsilon$ |\n",
    "| Category probability | $P(y=j) = F(\\kappa_j - X'\\beta) - F(\\kappa_{j-1} - X'\\beta)$ |\n",
    "| Marginal effect | $\\frac{\\partial P(y=j)}{\\partial x_k} = \\beta_k [f(\\kappa_{j-1} - X'\\beta) - f(\\kappa_j - X'\\beta)]$ |\n",
    "| Logit link | $F = \\Lambda$ (logistic CDF) |\n",
    "| Probit link | $F = \\Phi$ (normal CDF) |\n",
    "| ICC | $\\rho = \\sigma^2_\\alpha / (\\sigma^2_\\alpha + \\pi^2/3)$ |\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "1. Interpreting $\\beta$ as marginal effects (they are not — always compute AME)\n",
    "2. Ignoring intermediate category ambiguity (AME sign can differ from $\\beta$)\n",
    "3. Skipping the Brant test (proportional odds violation leads to inconsistency)\n",
    "4. Using MNL on ordinal data (wastes information) or ordered models on nominal data (imposes wrong structure)\n",
    "5. Interpreting cutpoints as \"distances\" between categories without considering scale\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Generalized Ordered Logit**: Relaxes proportional odds for specific variables\n",
    "- **Dynamic Ordered Models**: State dependence in ratings\n",
    "- **Fixed Effects Ordered Logit**: Bias-corrected estimation (Baetschmann et al., 2015)\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "### Essential Reading\n",
    "\n",
    "1. McKelvey, R. D., & Zavoina, W. (1975). A statistical model for the analysis of ordinal level dependent variables. *Journal of Mathematical Sociology*.\n",
    "\n",
    "2. Brant, R. (1990). Assessing proportionality in the proportional odds model. *Biometrics*.\n",
    "\n",
    "### Textbooks\n",
    "\n",
    "3. Long, J. S., & Freese, J. (2014). *Regression Models for Categorical Dependent Variables Using Stata*. Ch. 7.\n",
    "\n",
    "4. Wooldridge, J. M. (2010). *Econometric Analysis of Cross Section and Panel Data*. Ch. 15.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook 07: Ordered Logit/Probit Models**\n",
    "\n",
    "You're now ready to explore advanced topics in ordered choice models or revisit earlier notebooks for comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
