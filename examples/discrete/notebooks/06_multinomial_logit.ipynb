{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logit: Multiple Unordered Alternatives\n",
    "\n",
    "**Tutorial Series**: Discrete Choice Econometrics with PanelBox\n",
    "\n",
    "**Notebook**: 06 - Multinomial Logit\n",
    "\n",
    "**Author**: PanelBox Contributors\n",
    "\n",
    "**Date**: 2026-02-17\n",
    "\n",
    "**Estimated Duration**: 75 minutes\n",
    "\n",
    "**Difficulty Level**: Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Extend binary logit to J > 2 unordered alternatives\n",
    "2. Understand and choose the reference category for identification\n",
    "3. Estimate Multinomial Logit models using PanelBox\n",
    "4. Interpret coefficients as log-odds ratios relative to the base category\n",
    "5. Perform and interpret the Hausman-McFadden IIA test\n",
    "6. Calculate multi-category marginal effects (AME)\n",
    "7. Distinguish Multinomial Logit from Conditional Logit\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [From Binary to Multinomial](#section1)\n",
    "2. [Identification — Reference Category](#section2)\n",
    "3. [Estimation with PanelBox](#section3)\n",
    "4. [IIA Test — Hausman-McFadden](#section4)\n",
    "5. [Predicted Probabilities and Classification](#section5)\n",
    "6. [Marginal Effects in Multinomial Logit](#section6)\n",
    "7. [Application — Career Choice](#section7)\n",
    "8. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import all required libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation and numerical computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import norm, chi2\n",
    "\n",
    "# PanelBox models\n",
    "from panelbox.models.discrete.multinomial import MultinomialLogit\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Matplotlib configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"outputs\"\n",
    "FIG_DIR = OUTPUT_DIR / \"figures\"\n",
    "TABLE_DIR = OUTPUT_DIR / \"tables\"\n",
    "\n",
    "# Create output directories if needed\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Career labels for readability\n",
    "CAREER_LABELS = {0: 'Manual', 1: 'Technical', 2: 'Managerial'}\n",
    "CAREER_COLORS = {'Manual': '#e74c3c', 'Technical': '#3498db', 'Managerial': '#2ecc71'}\n",
    "\n",
    "print(\"All libraries imported successfully\")\n",
    "print(f\"Random seed set to: 42\")\n",
    "print(f\"Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. From Binary to Multinomial\n",
    "\n",
    "### 1.1 Review: Binary Logit\n",
    "\n",
    "In **Binary Logit** (Notebook 01), we modeled a binary outcome $y \\in \\{0, 1\\}$:\n",
    "\n",
    "$$P(y_i = 1 | X_i) = \\frac{\\exp(X_i' \\beta)}{1 + \\exp(X_i' \\beta)}$$\n",
    "\n",
    "This gives the log-odds:\n",
    "\n",
    "$$\\log \\frac{P(y=1)}{P(y=0)} = X' \\beta$$\n",
    "\n",
    "### 1.2 The Multinomial Extension\n",
    "\n",
    "When the outcome has **J > 2 unordered categories** (e.g., career choice: manual, technical, managerial), we extend to the **Multinomial Logit**.\n",
    "\n",
    "$$P(y_i = j | X_i) = \\frac{\\exp(X_i' \\beta_j)}{\\sum_{k=0}^{J-1} \\exp(X_i' \\beta_k)}$$\n",
    "\n",
    "Key features:\n",
    "- Covariates $X_i$ are **individual-specific** (same person, same $X$)\n",
    "- Each alternative $j$ has its **own coefficient vector** $\\beta_j$\n",
    "- One alternative is normalized ($\\beta_{\\text{base}} = 0$) for identification\n",
    "- Number of parameters: $(J-1) \\times K$\n",
    "\n",
    "### 1.3 MNL vs Conditional Logit\n",
    "\n",
    "| Feature | Multinomial Logit (MNL) | Conditional Logit (CL) |\n",
    "|---------|------------------------|------------------------|\n",
    "| Covariates | Individual-specific ($X_i$) | Alternative-specific ($Z_{ij}$) |\n",
    "| Coefficients | Alternative-specific ($\\beta_j$) | Common ($\\gamma$) |\n",
    "| Parameters | $(J-1) \\times K$ | $K$ |\n",
    "| Example | Age, education affect career | Cost, time of each mode affect transport choice |\n",
    "| \"Who chooses\" | Characteristics of the person | Attributes of the alternatives |\n",
    "\n",
    "### 1.4 Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load career choice panel data\n",
    "data = pd.read_csv(DATA_DIR / \"career_choice.csv\")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nShape: {data.shape}\")\n",
    "print(f\"Number of individuals: {data['id'].nunique()}\")\n",
    "print(f\"Number of periods: {data['year'].nunique()}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Career choice distribution\n",
    "print(\"=== Career Choice Distribution ===\")\n",
    "career_dist = data['career'].value_counts().sort_index()\n",
    "career_prop = data['career'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "for code, label in CAREER_LABELS.items():\n",
    "    print(f\"  {code} ({label:10s}): {career_dist[code]:5d} obs  ({career_prop[code]:.1%})\")\n",
    "\n",
    "print(f\"\\nTotal observations: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by career choice\n",
    "print(\"=== Characteristics by Career Choice ===\")\n",
    "summary = data.groupby('career')[['educ', 'exper', 'age', 'female', 'income', 'urban']].mean()\n",
    "summary.index = [CAREER_LABELS[c] for c in summary.index]\n",
    "print(summary.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize career distribution and characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Career distribution\n",
    "labels = [CAREER_LABELS[i] for i in range(3)]\n",
    "colors = [CAREER_COLORS[l] for l in labels]\n",
    "bars = axes[0, 0].bar(labels, career_dist.values, color=colors, alpha=0.8, edgecolor='black')\n",
    "for bar, count in zip(bars, career_dist.values):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20,\n",
    "                    f'{count}', ha='center', va='bottom', fontsize=11)\n",
    "axes[0, 0].set_title('Career Choice Distribution', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Education by career\n",
    "for code, label in CAREER_LABELS.items():\n",
    "    subset = data[data['career'] == code]['educ']\n",
    "    axes[0, 1].hist(subset, bins=20, alpha=0.5, label=label, color=CAREER_COLORS[label])\n",
    "axes[0, 1].set_title('Education Distribution by Career', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Years of Education')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Career by gender\n",
    "gender_career = data.groupby(['female', 'career']).size().unstack(fill_value=0)\n",
    "gender_career.columns = [CAREER_LABELS[c] for c in gender_career.columns]\n",
    "gender_career.index = ['Male', 'Female']\n",
    "gender_career_pct = gender_career.div(gender_career.sum(axis=1), axis=0)\n",
    "gender_career_pct.plot(kind='bar', stacked=True, ax=axes[1, 0],\n",
    "                       color=[CAREER_COLORS[c] for c in gender_career_pct.columns],\n",
    "                       alpha=0.8, edgecolor='black')\n",
    "axes[1, 0].set_title('Career Distribution by Gender', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Proportion')\n",
    "axes[1, 0].set_xlabel('')\n",
    "axes[1, 0].tick_params(axis='x', rotation=0)\n",
    "axes[1, 0].legend(title='Career')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Experience by career\n",
    "career_data = [data[data['career'] == c]['exper'] for c in range(3)]\n",
    "bp = axes[1, 1].boxplot(career_data, labels=labels, patch_artist=True, notch=True)\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "axes[1, 1].set_title('Experience Distribution by Career', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Years of Experience')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Career Choice: Exploratory Analysis', fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '06_career_exploration.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/06_career_exploration.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Identification — Reference Category\n",
    "\n",
    "### 2.1 The Over-Parameterization Problem\n",
    "\n",
    "If we estimate $\\beta_j$ for **all** J alternatives, the model is **not identified**. Adding a constant $c$ to all utilities doesn't change the probabilities:\n",
    "\n",
    "$$\\frac{\\exp(X'\\beta_j + c)}{\\sum_k \\exp(X'\\beta_k + c)} = \\frac{\\exp(c) \\cdot \\exp(X'\\beta_j)}{\\exp(c) \\cdot \\sum_k \\exp(X'\\beta_k)} = \\frac{\\exp(X'\\beta_j)}{\\sum_k \\exp(X'\\beta_k)}$$\n",
    "\n",
    "### 2.2 Normalization: Set $\\beta_{\\text{base}} = 0$\n",
    "\n",
    "To identify the model, we **normalize** one alternative's coefficients to zero. This gives:\n",
    "\n",
    "$$\\log \\frac{P(y=j)}{P(y=\\text{base})} = X' \\beta_j$$\n",
    "\n",
    "**Interpretation**: $\\beta_j$ captures the **log-odds ratio** of choosing alternative $j$ vs the base, for a unit change in $X$.\n",
    "\n",
    "### 2.3 Choice of Base Category\n",
    "\n",
    "Common strategies:\n",
    "- **Most common category**: Maximizes precision\n",
    "- **Theoretically relevant reference**: E.g., \"manual\" as the default career path\n",
    "\n",
    "**Key insight**: Changing the base changes the $\\beta$ values but **not** the predicted probabilities.\n",
    "\n",
    "### 2.4 Demonstration: Base Category Invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate reference category invariance\n",
    "# Estimate with each base alternative and compare predictions\n",
    "\n",
    "exog_vars = ['educ', 'exper', 'age', 'female']\n",
    "X = data[exog_vars].values\n",
    "y = data['career'].values\n",
    "\n",
    "predictions = {}\n",
    "coefficients = {}\n",
    "\n",
    "for base in range(3):\n",
    "    model = MultinomialLogit(\n",
    "        endog=y,\n",
    "        exog=X,\n",
    "        n_alternatives=3,\n",
    "        base_alternative=base\n",
    "    )\n",
    "    results = model.fit()\n",
    "    \n",
    "    # Store predictions and coefficients\n",
    "    predictions[base] = results.predict_proba()\n",
    "    coefficients[base] = results.params_matrix\n",
    "    \n",
    "    print(f\"\\n=== Base alternative = {base} ({CAREER_LABELS[base]}) ===\")\n",
    "    print(f\"Parameters shape: {results.params_matrix.shape}\")\n",
    "    for idx, j in enumerate(model.non_base_alts):\n",
    "        print(f\"  {CAREER_LABELS[j]} vs {CAREER_LABELS[base]}: \"\n",
    "              f\"educ={results.params_matrix[idx, 0]:+.4f}, \"\n",
    "              f\"exper={results.params_matrix[idx, 1]:+.4f}, \"\n",
    "              f\"age={results.params_matrix[idx, 2]:+.4f}, \"\n",
    "              f\"female={results.params_matrix[idx, 3]:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that predicted probabilities are identical regardless of base\n",
    "print(\"=== Prediction Invariance Check ===\")\n",
    "print(\"\\nMax absolute difference in predicted probabilities:\")\n",
    "for base_a in range(3):\n",
    "    for base_b in range(base_a + 1, 3):\n",
    "        diff = np.max(np.abs(predictions[base_a] - predictions[base_b]))\n",
    "        print(f\"  Base {base_a} vs Base {base_b}: {diff:.2e}\")\n",
    "\n",
    "print(\"\\nConclusion: Predictions are IDENTICAL regardless of base category.\")\n",
    "print(\"The base category is a normalization choice, not a substantive one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Estimation with PanelBox\n",
    "\n",
    "### 3.1 Model Specification\n",
    "\n",
    "We use career = 0 (Manual) as the base category. The model estimates:\n",
    "- $\\beta_1$: coefficients for **Technical vs Manual**\n",
    "- $\\beta_2$: coefficients for **Managerial vs Manual**\n",
    "\n",
    "Each coefficient set has K parameters (one per covariate).\n",
    "\n",
    "### 3.2 Estimate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Estimate Multinomial Logit with base = Manual (0)\nexog_vars = ['educ', 'exper', 'age', 'female']\nX = data[exog_vars].values\ny = data['career'].values\n\nmodel = MultinomialLogit(\n    endog=y,\n    exog=X,\n    n_alternatives=3,\n    base_alternative=0,  # Manual = reference\n)\n\n# Store variable names for later use\nmodel.exog_names = exog_vars\n\nresults = model.fit()\n\nprint(\"=\" * 70)\nprint(\" \" * 15 + \"MULTINOMIAL LOGIT: CAREER CHOICE\")\nprint(\"=\" * 70)\nprint(results.summary())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Coefficient Interpretation\n",
    "\n",
    "Each coefficient $\\beta_{j,k}$ tells us how a one-unit increase in variable $k$ changes the **log-odds** of choosing alternative $j$ vs the base (Manual):\n",
    "\n",
    "$$\\log \\frac{P(\\text{career} = j)}{P(\\text{career} = 0)} = X' \\beta_j$$\n",
    "\n",
    "- $\\beta_{1, \\text{educ}} > 0$: More education increases log-odds of Technical vs Manual\n",
    "- $\\beta_{2, \\text{educ}} > \\beta_{1, \\text{educ}}$: Education is **more important** for Managerial than Technical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed coefficient interpretation\n",
    "print(\"=== Coefficient Interpretation ===\")\n",
    "print(\"\\nAll coefficients are relative to the base category: Manual (career=0)\\n\")\n",
    "\n",
    "for idx, j in enumerate(model.non_base_alts):\n",
    "    print(f\"--- {CAREER_LABELS[j]} vs {CAREER_LABELS[model.base_alternative]} ---\")\n",
    "    for k, var in enumerate(exog_vars):\n",
    "        coef = results.params_matrix[idx, k]\n",
    "        se = results.bse_matrix[idx, k]\n",
    "        z = coef / se if not np.isnan(se) else np.nan\n",
    "        p = 2 * (1 - norm.cdf(abs(z))) if not np.isnan(z) else np.nan\n",
    "        sig = '***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.1 else ''\n",
    "        \n",
    "        # Odds ratio\n",
    "        odds_ratio = np.exp(coef)\n",
    "        \n",
    "        print(f\"  {var:8s}: beta = {coef:+.4f} {sig:3s}  (OR = {odds_ratio:.4f})\")\n",
    "        if var == 'educ':\n",
    "            print(f\"            1 extra year of education multiplies odds of {CAREER_LABELS[j]} by {odds_ratio:.2f}\")\n",
    "        elif var == 'female':\n",
    "            if coef < 0:\n",
    "                print(f\"            Women have {(1-odds_ratio)*100:.1f}% lower odds of {CAREER_LABELS[j]} vs Manual\")\n",
    "            else:\n",
    "                print(f\"            Women have {(odds_ratio-1)*100:.1f}% higher odds of {CAREER_LABELS[j]} vs Manual\")\n",
    "    print()\n",
    "\n",
    "# Compare education effects across alternatives\n",
    "educ_tech = results.params_matrix[0, 0]  # Technical vs Manual\n",
    "educ_mgr = results.params_matrix[1, 0]   # Managerial vs Manual\n",
    "print(f\"Education comparison:\")\n",
    "print(f\"  Technical vs Manual: {educ_tech:+.4f}\")\n",
    "print(f\"  Managerial vs Manual: {educ_mgr:+.4f}\")\n",
    "if abs(educ_mgr) > abs(educ_tech):\n",
    "    print(f\"  -> Education matters MORE for managerial career selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficients: grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(exog_vars))\n",
    "width = 0.35\n",
    "\n",
    "# Technical vs Manual\n",
    "bars1 = ax.bar(x - width/2, results.params_matrix[0], width,\n",
    "               label=f'{CAREER_LABELS[model.non_base_alts[0]]} vs {CAREER_LABELS[model.base_alternative]}',\n",
    "               color=CAREER_COLORS['Technical'], alpha=0.8, edgecolor='black')\n",
    "# Managerial vs Manual\n",
    "bars2 = ax.bar(x + width/2, results.params_matrix[1], width,\n",
    "               label=f'{CAREER_LABELS[model.non_base_alts[1]]} vs {CAREER_LABELS[model.base_alternative]}',\n",
    "               color=CAREER_COLORS['Managerial'], alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Add value labels\n",
    "for bar in list(bars1) + list(bars2):\n",
    "    h = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, h + (0.005 if h >= 0 else -0.015),\n",
    "            f'{h:.3f}', ha='center', va='bottom' if h >= 0 else 'top', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Variable')\n",
    "ax.set_ylabel('Coefficient (log-odds ratio)')\n",
    "ax.set_title('Multinomial Logit Coefficients by Alternative\\n(relative to Manual)', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(exog_vars)\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='k', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '06_coefficients_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/06_coefficients_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. IIA Test — Hausman-McFadden\n",
    "\n",
    "### 4.1 The IIA Property\n",
    "\n",
    "The **Independence of Irrelevant Alternatives** (IIA) property states:\n",
    "\n",
    "$$\\frac{P(y = j)}{P(y = k)} = \\exp\\left(X' (\\beta_j - \\beta_k)\\right)$$\n",
    "\n",
    "This ratio is **independent of other alternatives** in the choice set.\n",
    "\n",
    "### 4.2 Why It Matters\n",
    "\n",
    "If IIA is violated (e.g., Technical and Managerial are close substitutes), then:\n",
    "- Removing an alternative should change estimated coefficients\n",
    "- Adding a similar alternative may bias substitution patterns\n",
    "\n",
    "### 4.3 Hausman-McFadden Test\n",
    "\n",
    "**Procedure**:\n",
    "1. Estimate full model with all alternatives\n",
    "2. Omit one alternative and re-estimate\n",
    "3. Compare coefficients:\n",
    "\n",
    "$$H = (\\hat{\\beta}_R - \\hat{\\beta}_F)' [\\hat{V}_R - \\hat{V}_F]^{-1} (\\hat{\\beta}_R - \\hat{\\beta}_F) \\sim \\chi^2(K)$$\n",
    "\n",
    "- $H_0$: IIA holds (coefficients are stable)\n",
    "- $H_1$: IIA is violated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hausman-McFadden IIA Test\n",
    "print(\"=\" * 70)\n",
    "print(\" \" * 15 + \"HAUSMAN-MCFADDEN IIA TEST\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nH0: IIA holds (coefficients stable when alternative removed)\")\n",
    "print(\"H1: IIA is violated\\n\")\n",
    "\n",
    "# Full model\n",
    "results_full = results  # Already estimated above\n",
    "\n",
    "iia_results = []\n",
    "\n",
    "for alt_to_remove in range(3):\n",
    "    # Skip if removing the base alternative (requires re-specifying model)\n",
    "    if alt_to_remove == model.base_alternative:\n",
    "        continue\n",
    "    \n",
    "    # Restrict data: remove observations where career == alt_to_remove\n",
    "    mask = data['career'] != alt_to_remove\n",
    "    data_sub = data[mask].copy()\n",
    "    \n",
    "    # Remap career values to 0, 1 for the restricted model\n",
    "    remaining_alts = sorted(set(range(3)) - {alt_to_remove})\n",
    "    remap = {old: new for new, old in enumerate(remaining_alts)}\n",
    "    y_sub = data_sub['career'].map(remap).values\n",
    "    X_sub = data_sub[exog_vars].values\n",
    "    \n",
    "    # The base alternative index in the restricted model\n",
    "    base_in_sub = remap[model.base_alternative]\n",
    "    \n",
    "    # Estimate restricted model\n",
    "    model_sub = MultinomialLogit(\n",
    "        endog=y_sub,\n",
    "        exog=X_sub,\n",
    "        n_alternatives=2,\n",
    "        base_alternative=base_in_sub\n",
    "    )\n",
    "    results_sub = model_sub.fit()\n",
    "    \n",
    "    # Get comparable parameters\n",
    "    # The restricted model estimates beta for the remaining non-base alt\n",
    "    # This should correspond to one row of the full model's params_matrix\n",
    "    remaining_nonbase = [a for a in remaining_alts if a != model.base_alternative][0]\n",
    "    full_idx = list(model.non_base_alts).index(remaining_nonbase)\n",
    "    \n",
    "    beta_full = results_full.params_matrix[full_idx]\n",
    "    beta_sub = results_sub.params_matrix[0]\n",
    "    \n",
    "    # Covariance matrices for these parameters\n",
    "    K = model.K\n",
    "    start = full_idx * K\n",
    "    end = start + K\n",
    "    vcov_full = results_full.cov_params[start:end, start:end]\n",
    "    vcov_sub = results_sub.cov_params[:K, :K]\n",
    "    \n",
    "    # Hausman statistic\n",
    "    diff = beta_sub - beta_full\n",
    "    vcov_diff = vcov_sub - vcov_full\n",
    "    \n",
    "    try:\n",
    "        H = float(diff @ np.linalg.inv(vcov_diff) @ diff)\n",
    "        df = K\n",
    "        p_value = 1 - chi2.cdf(abs(H), df)\n",
    "        conclusion = 'Fail to reject IIA' if p_value > 0.05 else 'Reject IIA'\n",
    "    except np.linalg.LinAlgError:\n",
    "        H = np.nan\n",
    "        df = K\n",
    "        p_value = np.nan\n",
    "        conclusion = 'Singular matrix'\n",
    "    \n",
    "    iia_results.append({\n",
    "        'Removed': f\"{alt_to_remove} ({CAREER_LABELS[alt_to_remove]})\",\n",
    "        'N_obs': len(data_sub),\n",
    "        'H_statistic': H,\n",
    "        'df': df,\n",
    "        'p_value': p_value,\n",
    "        'Conclusion': conclusion\n",
    "    })\n",
    "    \n",
    "    print(f\"Omitting {CAREER_LABELS[alt_to_remove]}:\")\n",
    "    print(f\"  N = {len(data_sub)}, H = {H:.4f}, df = {df}, p = {p_value:.4f}\")\n",
    "    print(f\"  -> {conclusion}\")\n",
    "    print()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"  p > 0.05: No evidence against IIA (model assumption appears valid)\")\n",
    "print(\"  p < 0.05: Evidence against IIA (consider Nested Logit or Mixed Logit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## 5. Predicted Probabilities and Classification\n",
    "\n",
    "### 5.1 Probability Matrix\n",
    "\n",
    "The MNL produces a probability matrix of size $N \\times J$, where each row sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities\n",
    "probs = results.predict_proba()\n",
    "\n",
    "print(f\"Predicted probabilities shape: {probs.shape}\")\n",
    "print(f\"  {probs.shape[0]} observations x {probs.shape[1]} alternatives\\n\")\n",
    "\n",
    "# Create DataFrame for readability\n",
    "prob_df = pd.DataFrame(probs, columns=[CAREER_LABELS[j] for j in range(3)])\n",
    "print(\"First 10 predicted probability vectors:\")\n",
    "print(prob_df.head(10).round(4))\n",
    "\n",
    "# Verify rows sum to 1\n",
    "row_sums = probs.sum(axis=1)\n",
    "print(f\"\\nRow sums:\")\n",
    "print(f\"  Min: {row_sums.min():.10f}\")\n",
    "print(f\"  Max: {row_sums.max():.10f}\")\n",
    "print(f\"  All equal to 1: {np.allclose(row_sums, 1.0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed vs predicted choice shares\n",
    "observed_shares = data['career'].value_counts(normalize=True).sort_index()\n",
    "predicted_shares = prob_df.mean()\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Observed': [observed_shares[i] for i in range(3)],\n",
    "    'Predicted': predicted_shares.values,\n",
    "}, index=[CAREER_LABELS[i] for i in range(3)])\n",
    "comparison['Difference'] = comparison['Predicted'] - comparison['Observed']\n",
    "\n",
    "print(\"=== Observed vs Predicted Choice Shares ===\")\n",
    "print(comparison.round(4))\n",
    "print(f\"\\nMean absolute difference: {comparison['Difference'].abs().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification: predict most likely career\n",
    "y_pred = results.predict()\n",
    "y_true = data['career'].values\n",
    "\n",
    "accuracy = np.mean(y_pred == y_true)\n",
    "print(f\"=== Classification Performance ===\")\n",
    "print(f\"Overall accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"Random baseline (1/J): {1/3:.4f} ({100/3:.1f}%)\")\n",
    "print(f\"Improvement over random: {accuracy - 1/3:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = results.confusion_matrix\n",
    "print(f\"\\nConfusion Matrix (rows=actual, columns=predicted):\")\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=[f'Actual: {CAREER_LABELS[i]}' for i in range(3)],\n",
    "                     columns=[f'Pred: {CAREER_LABELS[i]}' for i in range(3)])\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap of confusion matrix\n",
    "labels_list = [CAREER_LABELS[i] for i in range(3)]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=labels_list, yticklabels=labels_list,\n",
    "            linewidths=1, square=True)\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Career')\n",
    "axes[0].set_ylabel('Actual Career')\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=labels_list, yticklabels=labels_list,\n",
    "            linewidths=1, square=True, vmin=0, vmax=1)\n",
    "axes[1].set_title('Confusion Matrix (Row-Normalized)', fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Career')\n",
    "axes[1].set_ylabel('Actual Career')\n",
    "\n",
    "plt.suptitle(f'Classification Performance (Accuracy: {accuracy:.1%})',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '06_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/06_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## 6. Marginal Effects in Multinomial Logit\n",
    "\n",
    "### 6.1 Why Coefficients Are Not Marginal Effects\n",
    "\n",
    "In MNL, $\\beta_{j,k}$ is **not** the marginal effect of $x_k$ on $P(y=j)$. The nonlinear probability structure means:\n",
    "\n",
    "$$\\frac{\\partial P_j}{\\partial x_k} = P_j \\left[ \\beta_{j,k} - \\sum_{m=0}^{J-1} P_m \\cdot \\beta_{m,k} \\right]$$\n",
    "\n",
    "This means changing $x_k$ affects **all J probabilities** simultaneously.\n",
    "\n",
    "### 6.2 Key Property: Sum to Zero\n",
    "\n",
    "Since probabilities must sum to 1:\n",
    "\n",
    "$$\\sum_{j=0}^{J-1} \\frac{\\partial P_j}{\\partial x_k} = 0$$\n",
    "\n",
    "An increase in probability for one alternative must come at the expense of others.\n",
    "\n",
    "### 6.3 Average Marginal Effects (AME)\n",
    "\n",
    "We compute AME by averaging marginal effects across all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Average Marginal Effects (AME)\n",
    "ame = results.marginal_effects(at='overall')\n",
    "\n",
    "print(\"=== Average Marginal Effects (AME) ===\")\n",
    "print(\"\\nEffect of a one-unit increase in each variable on P(career=j)\\n\")\n",
    "\n",
    "ame_df = pd.DataFrame(ame,\n",
    "                      index=[CAREER_LABELS[j] for j in range(3)],\n",
    "                      columns=exog_vars)\n",
    "print(ame_df.round(4))\n",
    "\n",
    "# Verify sum-to-zero property\n",
    "print(\"\\n=== Sum-to-Zero Verification ===\")\n",
    "print(\"Sum of AME across alternatives for each variable:\")\n",
    "for k, var in enumerate(exog_vars):\n",
    "    col_sum = ame[:, k].sum()\n",
    "    print(f\"  {var:8s}: {col_sum:+.6f}  {'PASS' if abs(col_sum) < 1e-4 else 'CHECK'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AME with standard errors\n",
    "ame_se = results.marginal_effects_se(at='overall')\n",
    "\n",
    "print(\"=== AME with Standard Errors ===\")\n",
    "print(f\"{'Variable':<10} {'Career':<12} {'AME':>10} {'SE':>10} {'z':>8} {'p':>8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for k, var in enumerate(exog_vars):\n",
    "    for j in range(3):\n",
    "        me = ame[j, k]\n",
    "        se = ame_se[j, k]\n",
    "        z = me / se if se > 0 else np.nan\n",
    "        p = 2 * (1 - norm.cdf(abs(z))) if not np.isnan(z) else np.nan\n",
    "        sig = '***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.1 else ''\n",
    "        print(f\"{var if j == 0 else '':8s}   {CAREER_LABELS[j]:<12} {me:+10.4f} {se:10.4f} {z:8.2f} {p:8.4f} {sig}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save AME table\n",
    "ame_table = pd.DataFrame(ame,\n",
    "                         index=[CAREER_LABELS[j] for j in range(3)],\n",
    "                         columns=exog_vars)\n",
    "ame_table.to_csv(TABLE_DIR / '06_marginal_effects.csv')\n",
    "print(\"AME table saved to outputs/tables/06_marginal_effects.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AME: heatmap (alternatives x variables)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sns.heatmap(ame_df, annot=True, fmt='.4f', cmap='RdBu_r', center=0,\n",
    "            ax=ax, linewidths=1, square=False,\n",
    "            cbar_kws={'label': 'Average Marginal Effect'})\n",
    "ax.set_title('Average Marginal Effects\\n(effect on P(career=j) per unit change in variable)',\n",
    "             fontweight='bold')\n",
    "ax.set_ylabel('Career Alternative')\n",
    "ax.set_xlabel('Variable')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '06_ame_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/06_ame_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot marginal effects using the built-in method\n",
    "fig = results.plot_marginal_effects(at='overall', figsize=(14, 8))\n",
    "plt.savefig(FIG_DIR / '06_marginal_effects_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/06_marginal_effects_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## 7. Application — Career Choice\n",
    "\n",
    "### Research Question\n",
    "\n",
    "**What factors determine career path selection (manual, technical, managerial)?**\n",
    "\n",
    "We investigate the roles of education, experience, age, and gender in career choice.\n",
    "\n",
    "### 7.1 Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation: Career by education level\n",
    "data['educ_level'] = pd.cut(data['educ'], bins=[0, 9, 12, 16, 25],\n",
    "                            labels=['< 9 yrs', '9-12 yrs', '12-16 yrs', '16+ yrs'])\n",
    "\n",
    "print(\"=== Career Distribution by Education Level ===\")\n",
    "cross_educ = pd.crosstab(data['educ_level'], data['career'].map(CAREER_LABELS),\n",
    "                         normalize='index').round(3)\n",
    "print(cross_educ)\n",
    "\n",
    "print(\"\\n=== Career Distribution by Gender ===\")\n",
    "cross_gender = pd.crosstab(\n",
    "    data['female'].map({0: 'Male', 1: 'Female'}),\n",
    "    data['career'].map(CAREER_LABELS),\n",
    "    normalize='index'\n",
    ").round(3)\n",
    "print(cross_gender)\n",
    "\n",
    "print(\"\\n=== Career Distribution by Location ===\")\n",
    "cross_urban = pd.crosstab(\n",
    "    data['urban'].map({0: 'Rural', 1: 'Urban'}),\n",
    "    data['career'].map(CAREER_LABELS),\n",
    "    normalize='index'\n",
    ").round(3)\n",
    "print(cross_urban)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize career by education level\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. By education level\n",
    "cross_educ.plot(kind='bar', stacked=True, ax=axes[0],\n",
    "                color=[CAREER_COLORS[c] for c in cross_educ.columns],\n",
    "                alpha=0.8, edgecolor='black')\n",
    "axes[0].set_title('Career by Education Level', fontweight='bold')\n",
    "axes[0].set_ylabel('Proportion')\n",
    "axes[0].set_xlabel('Education Level')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].legend(title='Career', loc='upper left')\n",
    "\n",
    "# 2. By gender\n",
    "cross_gender.plot(kind='bar', stacked=True, ax=axes[1],\n",
    "                  color=[CAREER_COLORS[c] for c in cross_gender.columns],\n",
    "                  alpha=0.8, edgecolor='black')\n",
    "axes[1].set_title('Career by Gender', fontweight='bold')\n",
    "axes[1].set_ylabel('Proportion')\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "axes[1].legend(title='Career')\n",
    "\n",
    "# 3. By location\n",
    "cross_urban.plot(kind='bar', stacked=True, ax=axes[2],\n",
    "                 color=[CAREER_COLORS[c] for c in cross_urban.columns],\n",
    "                 alpha=0.8, edgecolor='black')\n",
    "axes[2].set_title('Career by Location', fontweight='bold')\n",
    "axes[2].set_ylabel('Proportion')\n",
    "axes[2].set_xlabel('')\n",
    "axes[2].tick_params(axis='x', rotation=0)\n",
    "axes[2].legend(title='Career')\n",
    "\n",
    "plt.suptitle('Career Choice Patterns by Subgroup', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '06_career_by_subgroup.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/06_career_by_subgroup.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Full Model with All Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full model with all covariates\n",
    "full_vars = ['educ', 'exper', 'age', 'female', 'urban']\n",
    "X_full = data[full_vars].values\n",
    "\n",
    "model_full = MultinomialLogit(\n",
    "    endog=y,\n",
    "    exog=X_full,\n",
    "    n_alternatives=3,\n",
    "    base_alternative=0\n",
    ")\n",
    "model_full.exog_names = full_vars\n",
    "results_full = model_full.fit()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" \" * 10 + \"FULL MODEL: CAREER CHOICE DETERMINANTS\")\n",
    "print(\"=\" * 70)\n",
    "print(results_full.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Marginal Effects: Gender Gaps in Career Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AME from full model\n",
    "ame_full = results_full.marginal_effects(at='overall')\n",
    "\n",
    "print(\"=== Average Marginal Effects (Full Model) ===\")\n",
    "ame_full_df = pd.DataFrame(\n",
    "    ame_full,\n",
    "    index=[CAREER_LABELS[j] for j in range(3)],\n",
    "    columns=full_vars\n",
    ")\n",
    "print(ame_full_df.round(4))\n",
    "\n",
    "# Focus on education and gender\n",
    "print(\"\\n=== Key Findings ===\")\n",
    "\n",
    "print(\"\\nEducation effects (AME):\")\n",
    "for j in range(3):\n",
    "    me = ame_full[j, 0]  # educ is first variable\n",
    "    print(f\"  1 extra year of education: P({CAREER_LABELS[j]}) changes by {me:+.4f} ({me*100:+.2f} pp)\")\n",
    "\n",
    "print(\"\\nGender effects (AME of female):\")\n",
    "for j in range(3):\n",
    "    me = ame_full[j, 3]  # female is 4th variable\n",
    "    print(f\"  Being female: P({CAREER_LABELS[j]}) changes by {me:+.4f} ({me*100:+.2f} pp)\")\n",
    "\n",
    "print(\"\\nUrban effects (AME):\")\n",
    "for j in range(3):\n",
    "    me = ame_full[j, 4]  # urban is 5th variable\n",
    "    print(f\"  Urban location: P({CAREER_LABELS[j]}) changes by {me:+.4f} ({me*100:+.2f} pp)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Predictions for Representative Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for representative individuals\n",
    "profiles = pd.DataFrame({\n",
    "    'Profile': [\n",
    "        'Young woman, college degree',\n",
    "        'Older man, no college',\n",
    "        'Young urban male, graduate',\n",
    "        'Older rural female, basic ed'\n",
    "    ],\n",
    "    'educ':   [16,  9,  18,  8],\n",
    "    'exper':  [ 3, 25,   2, 20],\n",
    "    'age':    [25, 50,  26, 48],\n",
    "    'female': [ 1,  0,   0,  1],\n",
    "    'urban':  [ 1,  0,   1,  0]\n",
    "})\n",
    "\n",
    "X_profiles = profiles[full_vars].values\n",
    "prob_profiles = results_full.predict_proba(exog=X_profiles)\n",
    "\n",
    "print(\"=== Predicted Probabilities for Representative Profiles ===\")\n",
    "for i, row in profiles.iterrows():\n",
    "    print(f\"\\n{row['Profile']}:\")\n",
    "    print(f\"  (educ={row['educ']}, exper={row['exper']}, age={row['age']}, \"\n",
    "          f\"female={row['female']}, urban={row['urban']})\")\n",
    "    for j in range(3):\n",
    "        bar = '|' + '#' * int(prob_profiles[i, j] * 40) + ' ' * (40 - int(prob_profiles[i, j] * 40)) + '|'\n",
    "        print(f\"  P({CAREER_LABELS[j]:10s}) = {prob_profiles[i, j]:.4f}  {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize representative profiles\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(profiles))\n",
    "bottom = np.zeros(len(profiles))\n",
    "\n",
    "for j in range(3):\n",
    "    label = CAREER_LABELS[j]\n",
    "    values = prob_profiles[:, j]\n",
    "    ax.bar(x, values, bottom=bottom, label=label,\n",
    "           color=CAREER_COLORS[label], alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (v, b) in enumerate(zip(values, bottom)):\n",
    "        if v > 0.05:  # Only label if visible\n",
    "            ax.text(i, b + v/2, f'{v:.0%}', ha='center', va='center',\n",
    "                    fontsize=9, fontweight='bold')\n",
    "    bottom += values\n",
    "\n",
    "ax.set_xlabel('Profile')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title('Predicted Career Probabilities for Representative Profiles',\n",
    "             fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(profiles['Profile'], rotation=15, ha='right')\n",
    "ax.legend(title='Career')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '06_career_profiles.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/06_career_profiles.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Observed vs Predicted Choice Shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare observed vs predicted choice shares\n",
    "pred_probs_full = results_full.predict_proba()\n",
    "obs_shares = data['career'].value_counts(normalize=True).sort_index().values\n",
    "pred_shares = pred_probs_full.mean(axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, obs_shares * 100, width, label='Observed',\n",
    "               color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, pred_shares * 100, width, label='Predicted',\n",
    "               color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "\n",
    "for bar in list(bars1) + list(bars2):\n",
    "    h = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, h + 0.3, f'{h:.1f}%',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Career')\n",
    "ax.set_ylabel('Share (%)')\n",
    "ax.set_title('Observed vs Predicted Career Choice Shares', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([CAREER_LABELS[i] for i in range(3)])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / '06_career_shares.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to outputs/figures/06_career_shares.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.7 Summary of Findings"
  },
  {
   "cell_type": "markdown",
   "source": "### 7.6 Wald Test: Cross-Alternative Coefficient Equality\n\nWe can test whether the effect of a variable (e.g., education) is the **same** across alternatives. This is a **linear restriction** on the parameters:\n\n$$H_0: \\beta_{1,\\text{educ}} = \\beta_{2,\\text{educ}} \\quad \\text{(education equally important for Technical and Managerial)}$$\n\nThe Wald statistic is:\n\n$$W = (R\\hat{\\beta} - q)' [R \\hat{V} R']^{-1} (R\\hat{\\beta} - q) \\sim \\chi^2(m)$$\n\nwhere $R$ is the restriction matrix and $q = 0$.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Wald test: Is education equally important for Technical and Managerial?\n# H0: β_1,educ = β_2,educ  (same effect on Technical and Managerial vs Manual)\n\nfrom panelbox.utils.statistical import wald_test\n\n# Parameter vector layout: [β_1,educ, β_1,exper, β_1,age, β_1,female, β_1,urban,\n#                            β_2,educ, β_2,exper, β_2,age, β_2,female, β_2,urban]\n# We want to test β_1,educ - β_2,educ = 0\n# R = [1, 0, 0, 0, 0, -1, 0, 0, 0, 0], q = 0\n\nK = len(full_vars)\nn_params = results_full.params.shape[0]\n\n# Restriction matrix: β_1,educ - β_2,educ = 0\nR_educ = np.zeros((1, n_params))\nR_educ[0, 0] = 1     # β_1,educ (position 0)\nR_educ[0, K] = -1    # β_2,educ (position K)\n\nstat_educ, pval_educ, df_educ = wald_test(R_educ, results_full.params, results_full.cov_params)\n\nprint(\"=\" * 70)\nprint(\" \" * 10 + \"WALD TESTS: CROSS-ALTERNATIVE RESTRICTIONS\")\nprint(\"=\" * 70)\n\nprint(f\"\\nTest 1: β_educ(Technical) = β_educ(Managerial)\")\nprint(f\"  β_educ(Technical vs Manual):   {results_full.params_matrix[0, 0]:+.4f}\")\nprint(f\"  β_educ(Managerial vs Manual):  {results_full.params_matrix[1, 0]:+.4f}\")\nprint(f\"  Wald statistic: {stat_educ:.4f}\")\nprint(f\"  p-value: {pval_educ:.4f}\")\nprint(f\"  Conclusion: {'Reject H0 — education effects differ' if pval_educ < 0.05 else 'Fail to reject — no evidence of difference'}\")\n\n# Test 2: All coefficients equal across alternatives\n# H0: β_1 = β_2 (all K restrictions simultaneously)\nR_all = np.zeros((K, n_params))\nfor k in range(K):\n    R_all[k, k] = 1       # β_1,k\n    R_all[k, K + k] = -1  # β_2,k\n\nstat_all, pval_all, df_all = wald_test(R_all, results_full.params, results_full.cov_params)\n\nprint(f\"\\nTest 2: β(Technical) = β(Managerial) (joint test, all {K} coefficients)\")\nprint(f\"  Wald statistic: {stat_all:.4f}\")\nprint(f\"  df: {df_all}\")\nprint(f\"  p-value: {pval_all:.4f}\")\nprint(f\"  Conclusion: {'Reject H0 — alternatives have different determinants' if pval_all < 0.05 else 'Fail to reject — similar determinants'}\")\n\n# Save Wald test results\nwald_df = pd.DataFrame({\n    'Test': ['educ equality', 'all coefficients equality'],\n    'H0': ['β_1,educ = β_2,educ', 'β_1 = β_2 (all vars)'],\n    'Wald_stat': [stat_educ, stat_all],\n    'df': [df_educ, df_all],\n    'p_value': [pval_educ, pval_all]\n})\nwald_df.to_csv(TABLE_DIR / '06_wald_tests.csv', index=False)\nprint(f\"\\nWald test results saved to outputs/tables/06_wald_tests.csv\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\" \" * 15 + \"SUMMARY OF FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. MODEL FIT\")\n",
    "print(f\"   Log-likelihood: {results_full.llf:.2f}\")\n",
    "print(f\"   AIC: {results_full.aic:.2f}\")\n",
    "print(f\"   BIC: {results_full.bic:.2f}\")\n",
    "print(f\"   Pseudo R-squared: {results_full.pseudo_r2:.4f}\")\n",
    "print(f\"   Prediction accuracy: {results_full.accuracy:.1%}\")\n",
    "\n",
    "print(\"\\n2. EDUCATION\")\n",
    "print(f\"   Education is the strongest determinant of career choice.\")\n",
    "print(f\"   AME on P(Managerial): {ame_full[2, 0]:+.4f} per extra year\")\n",
    "print(f\"   AME on P(Manual): {ame_full[0, 0]:+.4f} per extra year\")\n",
    "\n",
    "print(\"\\n3. GENDER\")\n",
    "print(f\"   Women face lower probability of technical and managerial careers.\")\n",
    "print(f\"   AME on P(Managerial): {ame_full[2, 3]:+.4f}\")\n",
    "print(f\"   AME on P(Manual): {ame_full[0, 3]:+.4f}\")\n",
    "\n",
    "print(\"\\n4. LOCATION\")\n",
    "print(f\"   Urban location increases access to non-manual careers.\")\n",
    "print(f\"   AME on P(Managerial): {ame_full[2, 4]:+.4f}\")\n",
    "print(f\"   AME on P(Manual): {ame_full[0, 4]:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "## 8. Exercises\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 1: Reference Category Invariance (Easy)\n",
    "\n",
    "Estimate the model using `base_alternative=1` (Technical as reference). Compare predicted probabilities with the base=0 model.\n",
    "\n",
    "**Task**:\n",
    "1. Estimate the model with `base_alternative=1`\n",
    "2. Compare the first 5 predicted probability vectors\n",
    "3. Verify that they are numerically identical\n",
    "\n",
    "**Questions**:\n",
    "- Do the coefficients change? How?\n",
    "- Do the predictions change?\n",
    "- What does this tell us about the role of the base category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n",
    "# Step 1: Estimate model with base_alternative=1\n",
    "# TODO: model_base1 = MultinomialLogit(..., base_alternative=1)\n",
    "# TODO: results_base1 = model_base1.fit()\n",
    "\n",
    "# Step 2: Compare predicted probabilities\n",
    "# TODO: probs_base0 = results_full.predict_proba()\n",
    "# TODO: probs_base1 = results_base1.predict_proba()\n",
    "# TODO: Compare first 5 rows\n",
    "\n",
    "# Step 3: Verify numerical identity\n",
    "# TODO: np.allclose(probs_base0, probs_base1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 2: IIA Test (Medium)\n",
    "\n",
    "Run the Hausman-McFadden test by omitting each non-base alternative. Are the results consistent?\n",
    "\n",
    "**Task**:\n",
    "1. Estimate full model\n",
    "2. Omit Technical (career=1), re-estimate, compute Hausman statistic\n",
    "3. Omit Managerial (career=2), re-estimate, compute Hausman statistic\n",
    "4. Compare conclusions\n",
    "\n",
    "**Hint**: Use the code from Section 4 as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "# Step 1: Full model (already estimated as results_full)\n",
    "\n",
    "# Step 2: Omit Technical\n",
    "# TODO: Filter data, remap categories, estimate restricted model\n",
    "\n",
    "# Step 3: Omit Managerial\n",
    "# TODO: Same procedure\n",
    "\n",
    "# Step 4: Compare Hausman statistics and p-values\n",
    "# TODO: Create comparison table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3: Marginal Effects Interpretation (Medium)\n",
    "\n",
    "Calculate AME for education and verify the sum-to-zero property.\n",
    "\n",
    "**Task**:\n",
    "1. Compute AME for education using `results_full.marginal_effects(at='overall', variable=0)`\n",
    "2. Verify that effects sum to zero\n",
    "3. Explain: If education increases P(Managerial), which alternatives lose probability?\n",
    "4. Compare AME at mean vs AME overall — are they similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n",
    "# Step 1: AME for education\n",
    "# TODO: me_educ = results_full.marginal_effects(at='overall', variable=0)\n",
    "\n",
    "# Step 2: Verify sum-to-zero\n",
    "# TODO: print(f\"Sum: {me_educ.sum():.6f}\")\n",
    "\n",
    "# Step 3: Interpret which alternatives lose\n",
    "# TODO: Print and discuss\n",
    "\n",
    "# Step 4: Compare 'overall' vs 'mean'\n",
    "# TODO: me_educ_mean = results_full.marginal_effects(at='mean', variable=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 4: Multinomial vs Conditional Logit (Hard)\n",
    "\n",
    "Using the transportation dataset from Notebook 05, compare the approaches.\n",
    "\n",
    "**Task**:\n",
    "1. Load the transportation data\n",
    "2. Estimate a Conditional Logit (alternative-specific attributes: cost, time)\n",
    "3. Estimate a Multinomial Logit (individual-specific: income, distance)\n",
    "4. Discuss: When is each model appropriate? What are the tradeoffs?\n",
    "\n",
    "**Hint**: For MNL, you need to reshape the data to wide format (one row per individual-year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your solution here\n",
    "\n",
    "# Step 1: Load transportation data\n",
    "# TODO: transport = pd.read_csv(DATA_DIR / 'transportation_choice.csv')\n",
    "\n",
    "# Step 2: Conditional Logit (from Notebook 05)\n",
    "# TODO: Estimate with cost, time as alt_varying_vars\n",
    "\n",
    "# Step 3: Multinomial Logit\n",
    "# TODO: Reshape to wide format, extract individual-level variables\n",
    "# TODO: Estimate MultinomialLogit with income, distance as covariates\n",
    "\n",
    "# Step 4: Discussion\n",
    "# TODO: When to use MNL vs CL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 5: Subgroup Analysis (Hard)\n",
    "\n",
    "Estimate the career choice model separately for men and women.\n",
    "\n",
    "**Task**:\n",
    "1. Split data by gender\n",
    "2. Estimate MNL for each subgroup (using educ, exper, age, urban)\n",
    "3. Compare coefficients: Are determinants of career choice different by gender?\n",
    "4. Compare AME across genders\n",
    "5. Create a visualization showing the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Your solution here\n",
    "\n",
    "# Step 1: Split by gender\n",
    "# TODO: data_male = data[data['female'] == 0]\n",
    "# TODO: data_female = data[data['female'] == 1]\n",
    "\n",
    "# Step 2: Estimate for each subgroup\n",
    "# TODO: model_male = MultinomialLogit(...)\n",
    "# TODO: model_female = MultinomialLogit(...)\n",
    "\n",
    "# Step 3: Compare coefficients\n",
    "# TODO: Create comparison table\n",
    "\n",
    "# Step 4: Compare AME\n",
    "# TODO: ame_male = results_male.marginal_effects(at='overall')\n",
    "# TODO: ame_female = results_female.marginal_effects(at='overall')\n",
    "\n",
    "# Step 5: Visualize differences\n",
    "# TODO: Create grouped bar chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Multinomial Logit** extends binary logit to J > 2 **unordered** categories\n",
    "\n",
    "2. **Reference category normalization**: Set $\\beta_{\\text{base}} = 0$ for identification. This is a normalization, not a substantive choice — predictions are invariant\n",
    "\n",
    "3. **Coefficients** are **log-odds ratios** relative to the base category:\n",
    "   - $\\beta_{j,k} > 0$: higher $x_k$ increases odds of $j$ vs base\n",
    "   - $\\beta_{j,k} \\neq \\partial P_j / \\partial x_k$ (must compute AME)\n",
    "\n",
    "4. **IIA assumption**: $P(j)/P(k)$ independent of other alternatives. Test with Hausman-McFadden\n",
    "\n",
    "5. **Marginal effects** in MNL are more complex than binary logit:\n",
    "   - Changing $x_k$ affects **all J probabilities** simultaneously\n",
    "   - AME **sum to zero** across alternatives\n",
    "\n",
    "6. **MNL vs Conditional Logit**: MNL uses individual-specific $X_i$ with alternative-specific $\\beta_j$; CL uses alternative-specific $Z_{ij}$ with common $\\gamma$\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. Interpreting $\\beta_j$ as marginal effects (they are log-odds ratios)\n",
    "2. Comparing coefficients across models with different base categories\n",
    "3. Ignoring IIA when alternatives are similar\n",
    "4. Treating $\\beta_{\\text{base}} = 0$ as a finding rather than a normalization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Ordered Logit/Probit**: When alternatives have a natural ordering\n",
    "- **Nested Logit**: Relaxes IIA by grouping similar alternatives\n",
    "- **Mixed Logit**: Random coefficients for heterogeneous preferences\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "### Essential Reading\n",
    "\n",
    "1. Cameron, A. C., & Trivedi, P. K. (2005). *Microeconometrics: Methods and Applications*, Ch. 15. Cambridge University Press.\n",
    "\n",
    "2. Train, K. (2009). *Discrete Choice Methods with Simulation*. Cambridge University Press.\n",
    "\n",
    "### Classic Papers\n",
    "\n",
    "3. McFadden, D. (1973). \"Conditional logit analysis of qualitative choice behavior.\" In P. Zarembka (Ed.), *Frontiers in Econometrics*.\n",
    "\n",
    "4. Hausman, J., & McFadden, D. (1984). \"Specification tests for the multinomial logit model.\" *Econometrica*, 52(5), 1219-1240.\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing this tutorial!**\n",
    "\n",
    "Questions or feedback? Visit: https://github.com/panelbox/panelbox/issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
