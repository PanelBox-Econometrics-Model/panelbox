{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8db434da",
   "metadata": {},
   "source": [
    "# Tutorial 01: Robust Standard Errors - Fundamentals\n",
    "\n",
    "**Author**: PanelBox Development Team\n",
    "**Date**: 2026-02-16\n",
    "**Estimated Duration**: 45-60 minutes\n",
    "**Prerequisites**: Basic econometrics, Python, pandas\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. Diagnose heteroskedasticity in panel data using residual plots and formal tests\n",
    "2. Understand the difference between HC0, HC1, HC2, and HC3 robust standard errors\n",
    "3. Apply robust standard errors to linear panel models (Pooled OLS and Fixed Effects)\n",
    "4. Interpret the impact of heteroskedasticity on statistical inference\n",
    "5. Choose appropriate robust standard error corrections for different data structures\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Data Loading](#setup)\n",
    "2. [The Heteroskedasticity Problem](#problem)\n",
    "3. [Robust Standard Error Variants (HC0-HC3)](#variants)\n",
    "4. [Application to Panel Data](#application)\n",
    "5. [Comparison and Interpretation](#comparison)\n",
    "6. [Exercises](#exercises)\n",
    "7. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad39d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='setup'></a>\n",
    "## 1. Setup and Data Loading\n",
    "\n",
    "We'll start by importing the necessary libraries and loading the Grunfeld dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693da76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:56:59.100793Z",
     "iopub.status.busy": "2026-02-16T22:56:59.100668Z",
     "iopub.status.idle": "2026-02-16T22:57:01.065110Z",
     "shell.execute_reply": "2026-02-16T22:57:01.063209Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PanelBox imports\n",
    "import panelbox as pb\n",
    "from panelbox.models.static import PooledOLS, FixedEffects\n",
    "\n",
    "# Local utilities\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from plotting import plot_residuals, plot_se_comparison, plot_heteroskedasticity_test\n",
    "from diagnostics import test_heteroskedasticity\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Define paths\n",
    "DATA_PATH = '../data/'\n",
    "FIG_PATH = '../outputs/figures/01_robust/'\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c409b3",
   "metadata": {},
   "source": [
    "### Load Grunfeld Dataset\n",
    "\n",
    "The Grunfeld dataset contains investment data for 10 firms over 20 years (1935-1954)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a1f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:01.068955Z",
     "iopub.status.busy": "2026-02-16T22:57:01.068473Z",
     "iopub.status.idle": "2026-02-16T22:57:01.086173Z",
     "shell.execute_reply": "2026-02-16T22:57:01.085218Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(DATA_PATH + 'grunfeld.csv')\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(f\"\\nColumns: {list(data.columns)}\")\n",
    "print(f\"\\nEntities (firms): {data['firm_id'].nunique()}\")\n",
    "print(f\"Time periods (years): {data['year'].nunique()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fce665",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='problem'></a>\n",
    "## 2. The Heteroskedasticity Problem\n",
    "\n",
    "### What is Heteroskedasticity?\n",
    "\n",
    "**Heteroskedasticity** occurs when the variance of the error term is not constant across observations:\n",
    "\n",
    "$$\\text{Var}(u_i | X_i) = \\sigma_i^2 \\neq \\sigma^2$$\n",
    "\n",
    "In the presence of heteroskedasticity:\n",
    "- OLS coefficients remain **unbiased** and **consistent**\n",
    "- Standard errors are **biased** ‚Üí invalid t-tests and confidence intervals\n",
    "- Efficiency is lost (OLS is no longer BLUE)\n",
    "\n",
    "### Diagnosing Heteroskedasticity\n",
    "\n",
    "Let's estimate a simple investment model and check for heteroskedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c5f22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:01.088427Z",
     "iopub.status.busy": "2026-02-16T22:57:01.088206Z",
     "iopub.status.idle": "2026-02-16T22:57:01.114079Z",
     "shell.execute_reply": "2026-02-16T22:57:01.112974Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estimate pooled OLS model\n",
    "model_pooled = PooledOLS(\"invest ~ value + capital\", data, \"firm_id\", \"year\")\n",
    "\n",
    "result_pooled = model_pooled.fit()\n",
    "print(result_pooled.summary())\n",
    "\n",
    "# Extract fitted values and residuals\n",
    "fitted = result_pooled.fittedvalues\n",
    "residuals = result_pooled.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e401521a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:01.115944Z",
     "iopub.status.busy": "2026-02-16T22:57:01.115731Z",
     "iopub.status.idle": "2026-02-16T22:57:01.568442Z",
     "shell.execute_reply": "2026-02-16T22:57:01.566785Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot residuals vs fitted values\n",
    "fig = plot_residuals(fitted, residuals,\n",
    "                     title=\"Residuals vs Fitted Values - Grunfeld Data\")\n",
    "plt.savefig(FIG_PATH + 'residuals_vs_fitted.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nüìä Visual Inspection:\")\n",
    "print(\"Look for:\")\n",
    "print(\"  - Fan-shaped pattern ‚Üí increasing variance\")\n",
    "print(\"  - Funnel pattern ‚Üí decreasing variance\")\n",
    "print(\"  - Horizontal band ‚Üí homoskedasticity (ideal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0148a431",
   "metadata": {},
   "source": [
    "### Formal Tests for Heteroskedasticity\n",
    "\n",
    "We'll use two standard tests:\n",
    "\n",
    "1. **White Test**: General test with no specific functional form assumption\n",
    "2. **Breusch-Pagan Test**: Assumes variance is linear function of regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbec7f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:01.571527Z",
     "iopub.status.busy": "2026-02-16T22:57:01.571257Z",
     "iopub.status.idle": "2026-02-16T22:57:01.577996Z",
     "shell.execute_reply": "2026-02-16T22:57:01.576864Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare regressor matrix (without constant)\n",
    "X = data[['value', 'capital']].values\n",
    "\n",
    "# White test\n",
    "white_result = test_heteroskedasticity(residuals, X, test_type='white')\n",
    "print(\"=\" * 60)\n",
    "print(\"WHITE TEST FOR HETEROSKEDASTICITY\")\n",
    "print(\"=\" * 60)\n",
    "print(white_result)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Breusch-Pagan test\n",
    "bp_result = test_heteroskedasticity(residuals, X, test_type='breusch_pagan')\n",
    "print(\"=\" * 60)\n",
    "print(\"BREUSCH-PAGAN TEST FOR HETEROSKEDASTICITY\")\n",
    "print(\"=\" * 60)\n",
    "print(bp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rz94ksxkki",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='simulation'></a>\n",
    "## 2.5 Monte Carlo Simulation: Understanding SE Bias\n",
    "\n",
    "**Objective**: Demonstrate empirically that classical SEs are biased under heteroskedasticity, while robust SEs maintain valid inference.\n",
    "\n",
    "### Simulation Design\n",
    "\n",
    "**Data Generating Process (DGP)**:\n",
    "- Sample size: N = 500\n",
    "- True model: y = 2 + 0.5¬∑x + Œµ  \n",
    "- **Heteroskedastic errors**: Œµ ~ N(0, œÉ¬≤¬∑x¬≤) [variance proportional to x¬≤]\n",
    "- Replications: 1000\n",
    "\n",
    "**Test**: H‚ÇÄ: Œ≤ = 0.5 (the true value)\n",
    "**Expected rejection rate**: 5% (if inference is valid)\n",
    "\n",
    "> **Key Insight**: Under heteroskedasticity, classical SEs are systematically too small, leading to over-rejection (liberal tests). Robust SEs correct this bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafmf58p1mu",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:01.580543Z",
     "iopub.status.busy": "2026-02-16T22:57:01.580388Z",
     "iopub.status.idle": "2026-02-16T22:57:20.329012Z",
     "shell.execute_reply": "2026-02-16T22:57:20.327744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Monte Carlo Simulation\n",
    "n_simulations = 1000\n",
    "n_obs = 500\n",
    "true_beta = 0.5\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MONTE CARLO SIMULATION: CLASSICAL vs ROBUST SE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Replications: {n_simulations}\")\n",
    "print(f\"Sample size: {n_obs}\")\n",
    "print(f\"True Œ≤‚ÇÅ: {true_beta}\")\n",
    "print(\"\\nRunning simulation...\")\n",
    "\n",
    "# Storage for results\n",
    "reject_classical = []\n",
    "reject_robust = []\n",
    "se_classical_list = []\n",
    "se_robust_list = []\n",
    "\n",
    "for sim in range(n_simulations):\n",
    "    # Generate heteroskedastic data\n",
    "    x = np.random.uniform(1, 10, n_obs)\n",
    "    epsilon = np.random.normal(0, x**2, n_obs)  # Variance ‚àù x¬≤\n",
    "    y = 2 + true_beta * x + epsilon\n",
    "    \n",
    "    # Create DataFrame with required entity and time columns\n",
    "    sim_data = pd.DataFrame({\n",
    "        'y': y,\n",
    "        'x': x,\n",
    "        'entity': 1,  # Single entity for cross-sectional data\n",
    "        'time': range(n_obs)  # Time index\n",
    "    })\n",
    "    \n",
    "    # Estimate with classical SEs\n",
    "    model_sim = PooledOLS(\"y ~ x\", sim_data, \"entity\", \"time\")\n",
    "    res_classical = model_sim.fit(cov_type='nonrobust')\n",
    "    \n",
    "    # Estimate with robust SEs (HC1)\n",
    "    res_robust = model_sim.fit(cov_type='hc1')\n",
    "    \n",
    "    # Test H0: beta = true_beta\n",
    "    beta_hat = res_classical.params['x']\n",
    "    t_classical = (beta_hat - true_beta) / res_classical.std_errors['x']\n",
    "    t_robust = (beta_hat - true_beta) / res_robust.std_errors['x']\n",
    "    \n",
    "    # Record rejection (|t| > 1.96)\n",
    "    reject_classical.append(abs(t_classical) > 1.96)\n",
    "    reject_robust.append(abs(t_robust) > 1.96)\n",
    "    \n",
    "    # Store SEs\n",
    "    se_classical_list.append(res_classical.std_errors['x'])\n",
    "    se_robust_list.append(res_robust.std_errors['x'])\n",
    "    \n",
    "    if (sim + 1) % 250 == 0:\n",
    "        print(f\"  {sim + 1}/{n_simulations} completed...\")\n",
    "\n",
    "print(\"\\nSimulation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csr8wx6kqih",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:20.331377Z",
     "iopub.status.busy": "2026-02-16T22:57:20.331158Z",
     "iopub.status.idle": "2026-02-16T22:57:20.337971Z",
     "shell.execute_reply": "2026-02-16T22:57:20.336601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate empirical rejection rates\n",
    "rejection_rate_classical = np.mean(reject_classical)\n",
    "rejection_rate_robust = np.mean(reject_robust)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SIMULATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nEmpirical Rejection Rates (H‚ÇÄ is TRUE ‚Üí should reject ~5%):\")\n",
    "print(f\"  Classical SE:    {rejection_rate_classical:.3f} ({rejection_rate_classical*100:.1f}%)\")\n",
    "print(f\"  Robust SE (HC1): {rejection_rate_robust:.3f} ({rejection_rate_robust*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "if rejection_rate_classical > 0.07:\n",
    "    print(f\"  ‚úó Classical: TOO LIBERAL (rejects {rejection_rate_classical*100:.1f}% > 5%)\")\n",
    "    print(\"     ‚Üí Classical SEs are biased downward under heteroskedasticity\")\n",
    "    print(\"     ‚Üí Leads to spurious significant findings (Type I error inflation)\")\n",
    "else:\n",
    "    print(\"  ‚úì Classical: Acceptable rejection rate\")\n",
    "\n",
    "if 0.04 <= rejection_rate_robust <= 0.06:\n",
    "    print(f\"  ‚úì Robust: CORRECT SIZE (rejects {rejection_rate_robust*100:.1f}% ‚âà 5%)\")\n",
    "    print(\"     ‚Üí Robust SEs provide valid inference under heteroskedasticity\")\n",
    "else:\n",
    "    print(f\"  ~ Robust: {rejection_rate_robust*100:.1f}% (close to nominal 5%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY TAKEAWAY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Under heteroskedasticity:\")\n",
    "print(\"  ‚Ä¢ Classical SEs ‚Üí Invalid inference (over-rejection)\")\n",
    "print(\"  ‚Ä¢ Robust SEs ‚Üí Valid inference (correct test size)\")\n",
    "print(\"  ‚Ä¢ Always use robust SEs when heteroskedasticity is suspected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788nbcchzs",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:20.340350Z",
     "iopub.status.busy": "2026-02-16T22:57:20.340061Z",
     "iopub.status.idle": "2026-02-16T22:57:21.195070Z",
     "shell.execute_reply": "2026-02-16T22:57:21.193096Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize simulation results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Distribution of Standard Errors\n",
    "ax = axes[0]\n",
    "ax.hist(se_classical_list, bins=50, alpha=0.6, label='Classical',  \n",
    "        color='steelblue', edgecolor='black', linewidth=0.5)\n",
    "ax.hist(se_robust_list, bins=50, alpha=0.6, label='Robust (HC1)', \n",
    "        color='darkorange', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "mean_classical = np.mean(se_classical_list)\n",
    "mean_robust = np.mean(se_robust_list)\n",
    "\n",
    "ax.axvline(mean_classical, color='blue', linestyle='--', linewidth=2.5,\n",
    "           label=f'Mean Classical: {mean_classical:.4f}')\n",
    "ax.axvline(mean_robust, color='red', linestyle='--', linewidth=2.5,\n",
    "           label=f'Mean Robust: {mean_robust:.4f}')\n",
    "\n",
    "ax.set_xlabel('Standard Error', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribution of Standard Errors\\n(1000 Monte Carlo replications)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Rejection Rates\n",
    "ax = axes[1]\n",
    "methods = ['Classical\\nSE', 'Robust SE\\n(HC1)']\n",
    "rates = [rejection_rate_classical, rejection_rate_robust]\n",
    "colors = ['#d62728' if r > 0.07 else '#2ca02c' for r in rates]\n",
    "\n",
    "bars = ax.bar(methods, rates, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.axhline(0.05, color='black', linestyle='--', linewidth=2.5, \n",
    "           label='Nominal Size (5%)', zorder=10)\n",
    "\n",
    "ax.set_ylabel('Rejection Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Empirical Rejection Rates\\n(H‚ÇÄ is TRUE)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(0, max(rates) * 1.3)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, rate in zip(bars, rates):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "            f'{rate:.3f}\\n({rate*100:.1f}%)',\n",
    "            ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'monte_carlo_simulation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Simulation plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8443d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='variants'></a>\n",
    "## 3. Robust Standard Error Variants (HC0-HC3)\n",
    "\n",
    "When heteroskedasticity is detected, we need to correct the standard errors. The **robust covariance matrix** (also called sandwich estimator or Huber-White estimator) is:\n",
    "\n",
    "$$\\hat{V}_{\\text{robust}} = (X'X)^{-1} \\left(\\sum_{i=1}^n \\hat{u}_i^2 x_i x_i'\\right) (X'X)^{-1}$$\n",
    "\n",
    "Several variants exist that differ in how they weight the residuals:\n",
    "\n",
    "### HC0 (Original White)\n",
    "$$\\hat{V}_{\\text{HC0}} = (X'X)^{-1} \\left(\\sum_{i=1}^n \\hat{u}_i^2 x_i x_i'\\right) (X'X)^{-1}$$\n",
    "\n",
    "### HC1 (Degrees of Freedom Correction)\n",
    "$$\\hat{V}_{\\text{HC1}} = \\frac{n}{n-k} \\hat{V}_{\\text{HC0}}$$\n",
    "\n",
    "### HC2 (Leverage Correction)\n",
    "$$\\hat{u}_i^{(2)} = \\frac{\\hat{u}_i}{\\sqrt{1 - h_i}}$$\n",
    "\n",
    "where $h_i$ is the leverage of observation $i$.\n",
    "\n",
    "### HC3 (Davidson-MacKinnon)\n",
    "$$\\hat{u}_i^{(3)} = \\frac{\\hat{u}_i}{1 - h_i}$$\n",
    "\n",
    "**Recommendation**: HC3 is generally preferred in small samples as it provides better finite-sample properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d51e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='application'></a>\n",
    "## 4. Application to Panel Data\n",
    "\n",
    "Let's estimate the model with different robust SE variants and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c102a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:21.197510Z",
     "iopub.status.busy": "2026-02-16T22:57:21.197261Z",
     "iopub.status.idle": "2026-02-16T22:57:21.259435Z",
     "shell.execute_reply": "2026-02-16T22:57:21.258589Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-estimate with different robust SE methods\n",
    "se_types = ['classical', 'HC0', 'HC1', 'HC2', 'HC3']\n",
    "results_dict = {}\n",
    "\n",
    "for se_type in se_types:\n",
    "    if se_type == 'classical':\n",
    "        cov_type = 'nonrobust'\n",
    "    else:\n",
    "        cov_type = se_type.lower()\n",
    "\n",
    "    result = model_pooled.fit(cov_type=cov_type)\n",
    "    results_dict[se_type] = result\n",
    "\n",
    "print(\"‚úì Estimated models with all SE variants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9cabd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:21.261635Z",
     "iopub.status.busy": "2026-02-16T22:57:21.261368Z",
     "iopub.status.idle": "2026-02-16T22:57:21.270004Z",
     "shell.execute_reply": "2026-02-16T22:57:21.268759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for var in ['value', 'capital']:\n",
    "    for se_type in se_types:\n",
    "        res = results_dict[se_type]\n",
    "        comparison_data.append({\n",
    "            'Variable': var,\n",
    "            'SE Type': se_type,\n",
    "            'Coefficient': res.params[var],\n",
    "            'Std Error': res.std_errors[var],\n",
    "            't-statistic': res.tvalues[var],\n",
    "            'p-value': res.pvalues[var]\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON OF STANDARD ERROR METHODS\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9f5a0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:21.271672Z",
     "iopub.status.busy": "2026-02-16T22:57:21.271530Z",
     "iopub.status.idle": "2026-02-16T22:57:21.839140Z",
     "shell.execute_reply": "2026-02-16T22:57:21.837314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot SE comparison for 'value' variable\n",
    "estimates = {se: results_dict[se].params['value'] for se in se_types}\n",
    "std_errors = {se: results_dict[se].std_errors['value'] for se in se_types}\n",
    "\n",
    "fig = plot_se_comparison(\n",
    "    coef_name='value',\n",
    "    estimates=estimates,\n",
    "    std_errors=std_errors,\n",
    "    methods=se_types,\n",
    "    title='Comparison of Standard Error Methods: Value Coefficient'\n",
    ")\n",
    "plt.savefig(FIG_PATH + 'se_comparison_value.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe59f6",
   "metadata": {},
   "source": [
    "### Application to Fixed Effects Model\n",
    "\n",
    "Now let's see how robust SEs work with entity fixed effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a943b32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:21.841840Z",
     "iopub.status.busy": "2026-02-16T22:57:21.841600Z",
     "iopub.status.idle": "2026-02-16T22:57:21.879568Z",
     "shell.execute_reply": "2026-02-16T22:57:21.878307Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estimate fixed effects model\n",
    "model_fe = FixedEffects(\"invest ~ value + capital\", data, \"firm_id\", \"year\")\n",
    "\n",
    "# Compare classical vs robust SEs\n",
    "result_fe_classical = model_fe.fit(cov_type='nonrobust')\n",
    "result_fe_robust = model_fe.fit(cov_type='hc3')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FIXED EFFECTS: CLASSICAL vs ROBUST SE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nClassical SE:\")\n",
    "print(result_fe_classical.summary())\n",
    "print(\"\\nRobust SE (HC3):\")\n",
    "print(result_fe_robust.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f6fd5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='comparison'></a>\n",
    "## 5. Comparison and Interpretation\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Magnitude of Correction**: How much do robust SEs differ from classical SEs?\n",
    "2. **Inference Impact**: Do conclusions change when using robust SEs?\n",
    "3. **Choice of Variant**: How much do HC0-HC3 differ in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a2be4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:21.881421Z",
     "iopub.status.busy": "2026-02-16T22:57:21.881268Z",
     "iopub.status.idle": "2026-02-16T22:57:21.886013Z",
     "shell.execute_reply": "2026-02-16T22:57:21.884817Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate SE ratios (robust/classical)\n",
    "print(\"=\" * 60)\n",
    "print(\"STANDARD ERROR RATIOS (Robust / Classical)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for var in ['value', 'capital']:\n",
    "    classical_se = results_dict['classical'].std_errors[var]\n",
    "    print(f\"\\nVariable: {var}\")\n",
    "    print(f\"  Classical SE: {classical_se:.6f}\")\n",
    "\n",
    "    for se_type in ['HC0', 'HC1', 'HC2', 'HC3']:\n",
    "        robust_se = results_dict[se_type].std_errors[var]\n",
    "        ratio = robust_se / classical_se\n",
    "        print(f\"  {se_type} SE: {robust_se:.6f} (ratio: {ratio:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ic77sc1swop",
   "metadata": {},
   "source": [
    "### 5.1 Understanding Leverage and HC2/HC3 Adjustments\n",
    "\n",
    "**Leverage** ($h_i$) measures the influence of observation $i$ on its own fitted value.\n",
    "\n",
    "High leverage observations:\n",
    "- Are far from the mean in X-space\n",
    "- Have large potential influence on regression\n",
    "- Need special treatment in SE calculation\n",
    "\n",
    "HC2 and HC3 adjust for leverage:\n",
    "- **HC2**: Divides residuals by $\\sqrt{1 - h_i}$\n",
    "- **HC3**: Divides residuals by $(1 - h_i)$ (more aggressive)\n",
    "\n",
    "Effect: Observations with high leverage get **larger weights** ‚Üí more conservative SEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hr9teqzjgp",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:21.887606Z",
     "iopub.status.busy": "2026-02-16T22:57:21.887484Z",
     "iopub.status.idle": "2026-02-16T22:57:21.894213Z",
     "shell.execute_reply": "2026-02-16T22:57:21.893144Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate leverage for Grunfeld data\n",
    "from scipy.linalg import inv\n",
    "\n",
    "# Reconstruct design matrix from data\n",
    "# We need: intercept, value, capital\n",
    "X_design = np.column_stack([\n",
    "    np.ones(len(data)),  # Intercept\n",
    "    data['value'].values,\n",
    "    data['capital'].values\n",
    "])\n",
    "\n",
    "# Calculate hat matrix diagonal: h_i = X_i (X'X)^(-1) X_i'\n",
    "XtX_inv = inv(X_design.T @ X_design)\n",
    "leverage = np.sum(X_design @ XtX_inv * X_design, axis=1)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LEVERAGE STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Mean leverage: {leverage.mean():.4f}\")\n",
    "print(f\"Max leverage: {leverage.max():.4f}\")\n",
    "print(f\"Min leverage: {leverage.min():.4f}\")\n",
    "print(f\"Std leverage: {leverage.std():.4f}\")\n",
    "print(f\"\\nHigh leverage threshold (2k/n): {2 * X_design.shape[1] / len(X_design):.4f}\")\n",
    "high_leverage = leverage > (2 * X_design.shape[1] / len(X_design))\n",
    "print(f\"Observations with high leverage: {high_leverage.sum()} ({high_leverage.sum()/len(leverage)*100:.1f}%)\")\n",
    "\n",
    "# Calculate HC adjustment factors\n",
    "hc1_factor = np.sqrt(len(leverage) / (len(leverage) - X_design.shape[1]))  # Constant\n",
    "hc2_factor = 1 / np.sqrt(1 - leverage)\n",
    "hc3_factor = 1 / (1 - leverage)\n",
    "\n",
    "print(f\"\\nAdjustment factors:\")\n",
    "print(f\"  HC1: {hc1_factor:.4f} (constant)\")\n",
    "print(f\"  HC2: {hc2_factor.mean():.4f} (mean), max={hc2_factor.max():.4f}\")\n",
    "print(f\"  HC3: {hc3_factor.mean():.4f} (mean), max={hc3_factor.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jaj0s3zx9gg",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:21.896162Z",
     "iopub.status.busy": "2026-02-16T22:57:21.896039Z",
     "iopub.status.idle": "2026-02-16T22:57:22.699713Z",
     "shell.execute_reply": "2026-02-16T22:57:22.698553Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize leverage and HC adjustments\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Leverage distribution\n",
    "ax = axes[0]\n",
    "ax.hist(leverage, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax.axvline(leverage.mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {leverage.mean():.4f}')\n",
    "ax.axvline(2 * X_design.shape[1] / len(X_design), color='orange', \n",
    "           linestyle='--', linewidth=2, label='High leverage threshold (2k/n)')\n",
    "ax.set_xlabel('Leverage (h)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribution of Leverage Values', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: HC adjustment factors vs leverage\n",
    "ax = axes[1]\n",
    "sorted_idx = np.argsort(leverage)\n",
    "ax.plot(leverage[sorted_idx], hc2_factor[sorted_idx], \n",
    "        label='HC2: 1/‚àö(1-h)', linewidth=2.5, color='green')\n",
    "ax.plot(leverage[sorted_idx], hc3_factor[sorted_idx], \n",
    "        label='HC3: 1/(1-h)', linewidth=2.5, color='red')\n",
    "ax.axhline(hc1_factor, color='blue', linestyle='--', linewidth=2, \n",
    "           label=f'HC1: {hc1_factor:.3f} (constant)')\n",
    "\n",
    "ax.set_xlabel('Leverage (h)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Adjustment Factor', fontsize=12, fontweight='bold')\n",
    "ax.set_title('HC2 and HC3 Leverage Adjustments', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'leverage_adjustments.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚Ä¢ HC1: Constant adjustment (simple degrees-of-freedom correction)\")\n",
    "print(\"‚Ä¢ HC2: Moderate leverage adjustment (‚àö correction)\")\n",
    "print(\"‚Ä¢ HC3: Aggressive leverage adjustment (more conservative)\")\n",
    "print(\"\\nAs leverage increases:\")\n",
    "print(\"  ‚Üí HC2/HC3 adjustment factors increase\")\n",
    "print(\"  ‚Üí Residuals get weighted more heavily\")\n",
    "print(\"  ‚Üí Standard errors become larger (more conservative)\")\n",
    "print(\"\\n‚úì Visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb7431",
   "metadata": {},
   "source": [
    "### Summary of Findings\n",
    "\n",
    "**When to Use Robust Standard Errors:**\n",
    "\n",
    "‚úÖ **Always use** when:\n",
    "- Heteroskedasticity is suspected or detected\n",
    "- Sample size is large (n > 50)\n",
    "- You want inference robust to misspecification\n",
    "\n",
    "‚ö†Ô∏è **Be cautious** when:\n",
    "- Sample size is very small (n < 30)\n",
    "- Model is severely misspecified\n",
    "- High leverage points are present\n",
    "\n",
    "**Which Variant?**\n",
    "\n",
    "- **HC0**: Original White, can underestimate SEs in small samples\n",
    "- **HC1**: Simple df correction, better in small samples\n",
    "- **HC2**: Leverage adjustment, good theoretical properties\n",
    "- **HC3**: **Recommended default** - best small-sample performance\n",
    "\n",
    "**PanelBox Default**: HC3 (when `cov_type='robust'`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "en4shusluz",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='practices'></a>\n",
    "## 5.2 Best Practices and Reporting Guidelines\n",
    "\n",
    "### When to Use Robust Standard Errors\n",
    "\n",
    "‚úÖ **ALWAYS use robust SEs** when:\n",
    "- Working with cross-sectional data\n",
    "- Heteroskedasticity is detected (White/BP test)\n",
    "- You want inference robust to misspecification\n",
    "- Sample size is moderate to large (n > 50)\n",
    "- Publishing in applied economics/finance journals\n",
    "\n",
    "‚ö†Ô∏è **Be cautious** when:\n",
    "- Sample size is very small (n < 30) ‚Äì consider bootstrap\n",
    "- Model is severely misspecified ‚Äì fix model first\n",
    "- Many high leverage points ‚Äì investigate outliers\n",
    "\n",
    "### Which HC Variant to Choose?\n",
    "\n",
    "**Decision Tree**:\n",
    "\n",
    "1. **Large samples (n > 500)**:\n",
    "   - Use HC0 or HC1 (similar performance)\n",
    "   - Stata default: HC1\n",
    "\n",
    "2. **Moderate samples (100 < n < 500)**:\n",
    "   - **Use HC1** (recommended default)\n",
    "   - Matches Stata `robust` option\n",
    "   - Good balance of properties\n",
    "\n",
    "3. **Small samples (50 < n < 100)**:\n",
    "   - Use HC2 or HC3\n",
    "   - HC3 is more conservative (recommended)\n",
    "\n",
    "4. **Very small samples (n < 50)**:\n",
    "   - **Use HC3** (Long & Ervin 2000 recommendation)\n",
    "   - Or consider bootstrap methods\n",
    "\n",
    "**PanelBox Default**: `cov_type='robust'` ‚Üí HC3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r6xlo5ch14f",
   "metadata": {},
   "source": [
    "### How to Report in Academic Papers\n",
    "\n",
    "#### Table Notes Example:\n",
    "\n",
    "```\n",
    "Note: Robust standard errors (HC1) in parentheses.  \n",
    "*, **, *** denote significance at 10%, 5%, and 1% levels.\n",
    "```\n",
    "\n",
    "#### Text Reporting (when results are robust):\n",
    "\n",
    "> \"Our results are robust to the choice of standard error method. Table A2 in the Appendix presents results using nonrobust, HC1, HC2, and HC3 standard errors. While robust standard errors are larger than classical (as expected given evidence of heteroskedasticity; White test: œá¬≤=28.4, p<0.001), all key coefficients remain statistically significant at conventional levels.\"\n",
    "\n",
    "#### Text Reporting (when results are SENSITIVE):\n",
    "\n",
    "> \"We note that the coefficient on firm value is significant using classical standard errors (Œ≤=0.110, SE=0.011, p=0.03) but not with robust standard errors (Œ≤=0.110, SE=0.018, p=0.12). Given strong evidence of heteroskedasticity (White test: œá¬≤=45.3, p<0.001; residual plots show clear fan pattern), we rely on robust inference and conclude that the effect of firm value on investment is **not statistically significant**.\"\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "‚ùå **Pitfall 1**: \"Robust = Better Model\"\n",
    "- Wrong: \"My model is robust because I used robust SEs\"\n",
    "- Right: \"My inference is valid under heteroskedasticity due to robust SEs\"\n",
    "- **Robustness** = trying different specifications, samples, estimation methods\n",
    "\n",
    "‚ùå **Pitfall 2**: Ignoring Large SE Differences\n",
    "- If robust SEs are 2x-3x larger than classical:\n",
    "  - Investigate why (severe heteroskedasticity? outliers?)\n",
    "  - May indicate model misspecification\n",
    "  - Consider: omitted variables, wrong functional form, outliers\n",
    "\n",
    "‚ùå **Pitfall 3**: Selective Reporting (p-hacking)\n",
    "- Don't try HC0, HC1, HC2, HC3 and report only the one with p<0.05\n",
    "- **Pre-specify** SE choice in analysis plan\n",
    "- Or report all variants as robustness check\n",
    "\n",
    "‚ùå **Pitfall 4**: Using Robust SEs as a \"Fix\" for Bad Models\n",
    "- Robust SEs correct inference, not bad modeling\n",
    "- If heteroskedasticity is severe, consider:\n",
    "  - Transform dependent variable (log, sqrt)\n",
    "  - Weighted least squares (WLS)\n",
    "  - Model variance explicitly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fa8a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercises'></a>\n",
    "## 6. Exercises\n",
    "\n",
    "### Exercise 1: Heteroskedasticity Diagnosis (Easy)\n",
    "\n",
    "**Task**: Generate a dataset with known heteroskedasticity and diagnose it.\n",
    "\n",
    "**Requirements**:\n",
    "1. Generate data with multiplicative heteroskedasticity: Œµ ~ N(0, œÉ¬≤¬∑x¬≤)\n",
    "2. Estimate model and create residual plot\n",
    "3. Perform White test\n",
    "4. Compare classical vs robust (HC3) SEs\n",
    "\n",
    "**Starter Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bs2mp26rf2o",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:22.701494Z",
     "iopub.status.busy": "2026-02-16T22:57:22.701345Z",
     "iopub.status.idle": "2026-02-16T22:57:22.705649Z",
     "shell.execute_reply": "2026-02-16T22:57:22.704548Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "np.random.seed(123)\n",
    "n = 200\n",
    "\n",
    "# Step 1: Generate heteroskedastic data\n",
    "x = np.random.uniform(1, 5, n)\n",
    "epsilon = np.random.normal(0, x**2, n)  # Variance ‚àù x¬≤\n",
    "y = 1 + 2*x + epsilon\n",
    "\n",
    "ex1_data = pd.DataFrame({'y': y, 'x': x})\n",
    "\n",
    "# Step 2: Estimate model\n",
    "# YOUR CODE: Estimate PooledOLS model\n",
    "\n",
    "# Step 3: Create residual plot\n",
    "# YOUR CODE: Plot residuals vs fitted values\n",
    "\n",
    "# Step 4: White test\n",
    "# YOUR CODE: Perform White test\n",
    "\n",
    "# Step 5: Compare SEs\n",
    "# YOUR CODE: Estimate with classical and HC3, compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79m7vdeubwn",
   "metadata": {},
   "source": [
    "### Exercise 2: Confidence Interval Coverage (Moderate)\n",
    "\n",
    "**Task**: Verify empirically that 95% CIs have correct coverage under heteroskedasticity.\n",
    "\n",
    "**Requirements**:\n",
    "1. Simulate 1000 datasets with heteroskedastic errors\n",
    "2. For each, construct 95% CI using classical and robust SEs\n",
    "3. Calculate coverage rate (% of times CI contains true Œ≤)\n",
    "4. **Expected**: Classical < 95%, Robust ‚âà 95%\n",
    "\n",
    "**Starter Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p8k2cuk0gqb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:22.707331Z",
     "iopub.status.busy": "2026-02-16T22:57:22.707190Z",
     "iopub.status.idle": "2026-02-16T22:57:22.799501Z",
     "shell.execute_reply": "2026-02-16T22:57:22.798343Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "from scipy import stats\n",
    "\n",
    "n_sims = 1000\n",
    "n_obs = 200\n",
    "true_beta = 1.5\n",
    "coverage_classical = []\n",
    "coverage_robust = []\n",
    "\n",
    "for sim in range(n_sims):\n",
    "    # Generate heteroskedastic data\n",
    "    x = np.random.uniform(1, 5, n_obs)\n",
    "    eps = np.random.normal(0, x**1.5, n_obs)\n",
    "    y = 1 + true_beta * x + eps\n",
    "    \n",
    "    sim_data = pd.DataFrame({'y': y, 'x': x})\n",
    "    \n",
    "    # YOUR CODE:\n",
    "    # 1. Estimate model with classical and robust SEs\n",
    "    # 2. Construct 95% CIs for both\n",
    "    # 3. Check if true_beta is in CI\n",
    "    # 4. Append to coverage_classical and coverage_robust lists\n",
    "    \n",
    "    pass  # Remove this and add your code\n",
    "\n",
    "# Calculate coverage rates\n",
    "# YOUR CODE: Calculate mean of coverage lists\n",
    "\n",
    "print(f\"Classical CI coverage: {np.mean(coverage_classical):.3f} (should be 0.95)\")\n",
    "print(f\"Robust CI coverage: {np.mean(coverage_robust):.3f} (should be 0.95)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hfnjleyx5vo",
   "metadata": {},
   "source": [
    "### Exercise 3: Real Data Application (Challenging)\n",
    "\n",
    "**Task**: Apply robust inference to wage panel data.\n",
    "\n",
    "**Requirements**:\n",
    "1. Load `wage_panel.csv` dataset\n",
    "2. Estimate wage equation: `wage ~ education + experience + tenure`\n",
    "3. Diagnose heteroskedasticity (visual + formal tests)\n",
    "4. Compare all HC variants (HC0, HC1, HC2, HC3)\n",
    "5. Create publication-ready table\n",
    "6. Write 1-paragraph interpretation\n",
    "\n",
    "**Starter Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7za2ogb0v69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T22:57:22.801750Z",
     "iopub.status.busy": "2026-02-16T22:57:22.801409Z",
     "iopub.status.idle": "2026-02-16T22:57:22.818838Z",
     "shell.execute_reply": "2026-02-16T22:57:22.817878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n",
    "# Load wage data\n",
    "wage_data = pd.read_csv(DATA_PATH + 'wage_panel.csv')\n",
    "\n",
    "print(\"Wage Panel Data:\")\n",
    "print(f\"Shape: {wage_data.shape}\")\n",
    "print(f\"\\nVariables: {list(wage_data.columns)}\")\n",
    "print(f\"\\nSample:\")\n",
    "display(wage_data.head())\n",
    "\n",
    "# YOUR CODE:\n",
    "# 1. Estimate PooledOLS: wage ~ education + experience + tenure\n",
    "# 2. Create residual plots\n",
    "# 3. Run heteroskedasticity tests\n",
    "# 4. Estimate with all HC variants\n",
    "# 5. Use StandardErrorComparison to create comparison table\n",
    "# 6. Plot results\n",
    "# 7. Write interpretation\n",
    "\n",
    "# Example structure:\n",
    "# model_wage = PooledOLS(\"wage ~ education + experience + tenure\", wage_data, \"entity_id\", \"time_id\")\n",
    "# result = model_wage.fit(cov_type='nonrobust')\n",
    "# ...\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INTERPRETATION (write your own!):\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "[Your interpretation here - discuss:]\n",
    "- Economic meaning of coefficients\n",
    "- Evidence of heteroskedasticity\n",
    "- Impact of SE choice on inference\n",
    "- Recommendation for preferred specification\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ttysxz2m94",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='summary'></a>\n",
    "## 7. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Heteroskedasticity** is common in real data and invalidates classical standard errors\n",
    "   - OLS coefficients remain unbiased\n",
    "   - But classical SEs are biased ‚Üí invalid t-tests and CIs\n",
    "\n",
    "2. **Diagnosis is important** but robust SEs are safe even when tests are negative\n",
    "   - Visual: Residual plots (look for cone/funnel patterns)\n",
    "   - Formal: White test, Breusch-Pagan test\n",
    "\n",
    "3. **Monte Carlo evidence** confirms:\n",
    "   - Classical SEs: Liberal tests (over-rejection) under heteroskedasticity\n",
    "   - Robust SEs: Maintain correct test size (~5%)\n",
    "\n",
    "4. **HC variants** (HC0-HC3) differ in leverage adjustment:\n",
    "   - HC0: Baseline (White 1980)\n",
    "   - HC1: DF correction **(Stata default, recommended for general use)**\n",
    "   - HC2: Moderate leverage adjustment\n",
    "   - HC3: Aggressive leverage adjustment **(PanelBox default, best for small samples)**\n",
    "\n",
    "5. **Reporting** multiple SE types demonstrates robustness of findings\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "**Classical Variance** (WRONG under heteroskedasticity):\n",
    "$$\\\\text{Var}(\\\\hat{\\\\beta}) = \\\\sigma^2 (X'X)^{-1}$$\n",
    "\n",
    "**Robust Variance (HC1)** (CORRECT under heteroskedasticity):\n",
    "$$\\\\text{Var}(\\\\hat{\\\\beta}) = \\\\frac{n}{n-k} (X'X)^{-1} \\\\left[\\\\sum_{i=1}^n \\\\hat{\\\\epsilon}_i^2 x_i x_i' \\\\right] (X'X)^{-1}$$\n",
    "\n",
    "### Decision Rules\n",
    "\n",
    "**When to use robust SEs?**\n",
    "- ‚úÖ Always in cross-sectional data\n",
    "- ‚úÖ When heteroskedasticity detected\n",
    "- ‚úÖ When publishing applied research\n",
    "\n",
    "**Which variant?**\n",
    "- Large samples (n>500): HC0 or HC1\n",
    "- Moderate samples (100<n<500): **HC1** (recommended)\n",
    "- Small samples (n<100): **HC3** (more conservative)\n",
    "\n",
    "**In PanelBox**:\n",
    "```python\n",
    "# Default robust (HC3)\n",
    "result = model.fit(cov_type='robust')\n",
    "\n",
    "# Specific variant\n",
    "result = model.fit(cov_type='hc1')  # Matches Stata\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q18s4uzimx",
   "metadata": {},
   "source": [
    "### Connection to Next Tutorials\n",
    "\n",
    "‚û°Ô∏è **Tutorial 02: Clustered Standard Errors**\n",
    "\n",
    "**Why?** Robust SEs (HC0-HC3) handle **heteroskedasticity** but assume **independence** across observations.\n",
    "\n",
    "**Problem in panel data**: Observations within the same entity (firm, individual, country) are often **correlated over time**.\n",
    "\n",
    "**Solution**: **Clustered standard errors** account for:\n",
    "- Within-cluster correlation (e.g., observations from same firm)\n",
    "- Both heteroskedasticity AND correlation\n",
    "\n",
    "**Example**:\n",
    "- Firm-level data: Cluster by firm_id\n",
    "- Country-year data: Cluster by country (or two-way clustering)\n",
    "\n",
    "**Preview**:\n",
    "```python\n",
    "# Next tutorial: Clustering\n",
    "result = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "\n",
    "# Or two-way clustering\n",
    "result = model.fit(cov_type='twoway')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "‚û°Ô∏è **Tutorial 03: HAC Standard Errors (Newey-West)**\n",
    "\n",
    "For time-series and panel data with **autocorrelation**, we'll learn:\n",
    "- Heteroskedasticity and Autocorrelation Consistent (HAC) SEs\n",
    "- Newey-West estimator\n",
    "- Choosing optimal lag length\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Path**:\n",
    "1. ‚úÖ **Tutorial 01**: Robust SEs (heteroskedasticity)\n",
    "2. ‚è≠Ô∏è **Tutorial 02**: Clustered SEs (correlation within clusters)\n",
    "3. ‚è≠Ô∏è **Tutorial 03**: HAC SEs (autocorrelation over time)\n",
    "4. ‚è≠Ô∏è **Tutorial 04**: Spatial SEs (spatial correlation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e3235",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='references'></a>\n",
    "## 7. References\n",
    "\n",
    "### Key Papers\n",
    "\n",
    "1. **White, H. (1980)**. \"A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity\". *Econometrica*, 48(4), 817-838.\n",
    "\n",
    "2. **MacKinnon, J. G., & White, H. (1985)**. \"Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties\". *Journal of Econometrics*, 29(3), 305-325.\n",
    "\n",
    "3. **Long, J. S., & Ervin, L. H. (2000)**. \"Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model\". *The American Statistician*, 54(3), 217-224.\n",
    "\n",
    "### Software Documentation\n",
    "\n",
    "- [PanelBox Documentation](https://panelbox.readthedocs.io/)\n",
    "- [Robust Covariance Guide](https://panelbox.readthedocs.io/robust-inference.html)\n",
    "\n",
    "### Next Tutorial\n",
    "\n",
    "‚û°Ô∏è **Tutorial 02**: Clustered Standard Errors for Panel Data\n",
    "\n",
    "---\n",
    "\n",
    "**End of Tutorial 01**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
