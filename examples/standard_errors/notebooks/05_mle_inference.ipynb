{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Tutorial 05: Inference in Maximum Likelihood Estimation\n",
    "\n",
    "**Author**: PanelBox Development Team  \n",
    "**Date**: 2026-02-16  \n",
    "**Estimated Duration**: 90-120 minutes  \n",
    "**Prerequisites**: Tutorials 01 (Robust Fundamentals) and 02 (Clustering Panels)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Understand** MLE standard errors and the role of the Hessian matrix\n",
    "2. **Implement** classical MLE inference using the information matrix\n",
    "3. **Apply** robust (sandwich) standard errors for misspecified models\n",
    "4. **Use** the delta method for nonlinear transformations (odds ratios, marginal effects)\n",
    "5. **Implement** cluster-robust MLE for panel data\n",
    "6. **Apply** bootstrap for nonlinear models\n",
    "7. **Compare** classical, robust, and bootstrap inference\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Data Loading](#setup)\n",
    "2. [MLE Basics: Likelihood, Score, and Hessian](#mle-basics)\n",
    "3. [Classical MLE Standard Errors](#classical)\n",
    "4. [Robust (Sandwich) Standard Errors](#robust)\n",
    "5. [Cluster-Robust MLE for Panel Data](#cluster)\n",
    "6. [Delta Method for Nonlinear Transformations](#delta)\n",
    "7. [Bootstrap for MLE](#bootstrap)\n",
    "8. [Comparison and Best Practices](#comparison)\n",
    "9. [Exercises](#exercises)\n",
    "10. [Summary and Key Takeaways](#summary)\n",
    "11. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='setup'></a>\n",
    "## 1. Setup and Data Loading\n",
    "\n",
    "We work with two datasets:\n",
    "1. **Credit Approval** (`credit_approval.csv`): 5000 cross-section observations, binary outcome `approved`\n",
    "2. **Health Insurance** (`health_insurance.csv`): 1000 individuals × 5 years, multinomial plan choice (we derive a binary indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical tools\n",
    "from scipy import stats\n",
    "from scipy.special import expit  # logistic CDF: Λ(z) = 1/(1+exp(-z))\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# PanelBox imports\n",
    "import sys\n",
    "sys.path.insert(0, '../../..')  # Ensure panelbox is on path\n",
    "import panelbox as pb\n",
    "from panelbox.models.discrete import PooledLogit, PooledProbit\n",
    "from panelbox.marginal_effects.discrete_me import compute_ame, compute_mem\n",
    "from panelbox.standard_errors.mle import (\n",
    "    sandwich_estimator,\n",
    "    cluster_robust_mle,\n",
    "    delta_method,\n",
    "    bootstrap_mle,\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = '../data/'\n",
    "FIG_PATH  = '../outputs/figures/05_mle/'\n",
    "\n",
    "import os\n",
    "os.makedirs(FIG_PATH, exist_ok=True)\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load Cross-Section: Credit Approval ─────────────────────────────────────\n",
    "credit = pd.read_csv(DATA_PATH + 'credit_approval.csv')\n",
    "# Add dummy panel identifiers so PooledLogit is happy\n",
    "credit['entity'] = credit['id']\n",
    "credit['time']   = 1\n",
    "\n",
    "print('Credit Approval dataset')\n",
    "print(f'  Shape : {credit.shape}')\n",
    "print(f'  Approval rate : {credit[\"approved\"].mean():.1%}')\n",
    "print(f'  Columns: {list(credit.columns)}')\n",
    "print()\n",
    "print(credit[['approved','income','age','debt_ratio','credit_score','employment_length']].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load Panel: Health Insurance Choice ─────────────────────────────────────\n",
    "health = pd.read_csv(DATA_PATH + 'health_insurance.csv')\n",
    "# Derive binary outcome: 1 if individual chose the highest-coverage plan (plan 3)\n",
    "health['high_coverage'] = (health['plan_choice'] == 3).astype(int)\n",
    "\n",
    "print('Health Insurance Panel dataset')\n",
    "print(f'  Shape          : {health.shape}')\n",
    "print(f'  Individuals    : {health[\"person_id\"].nunique()}')\n",
    "print(f'  Years          : {sorted(health[\"year\"].unique())}')\n",
    "print(f'  High-coverage rate : {health[\"high_coverage\"].mean():.1%}')\n",
    "print()\n",
    "print(health[['high_coverage','age','income','family_size','health_status']].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-mle-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='mle-basics'></a>\n",
    "## 2. MLE Basics: Likelihood, Score, and Hessian\n",
    "\n",
    "### 2.1 The MLE Framework\n",
    "\n",
    "**Maximum Likelihood** chooses parameters $\\hat{\\beta}$ to maximise the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{MLE} = \\arg\\max_\\beta \\; \\ell(\\beta) = \\arg\\max_\\beta \\sum_{i=1}^n \\log f(y_i \\mid x_i, \\beta)\n",
    "$$\n",
    "\n",
    "**Binary Logit** — the workhorse model for binary outcomes:\n",
    "\n",
    "$$\n",
    "P(y_i = 1 \\mid x_i) = \\Lambda(x_i'\\beta) = \\frac{\\exp(x_i'\\beta)}{1 + \\exp(x_i'\\beta)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ell(\\beta) = \\sum_{i=1}^n \\bigl[y_i \\log \\Lambda(x_i'\\beta) + (1-y_i)\\log(1-\\Lambda(x_i'\\beta))\\bigr]\n",
    "$$\n",
    "\n",
    "### 2.2 Why MLE Inference Differs from OLS\n",
    "\n",
    "**OLS**: Variance depends on residuals and $X$ through an explicit formula.\n",
    "\n",
    "**MLE**: Variance depends on the **curvature** of the log-likelihood at $\\hat\\beta$:\n",
    "- Steep curvature $\\Rightarrow$ precise estimate (small variance)\n",
    "- Flat curvature $\\Rightarrow$ imprecise estimate (large variance)\n",
    "\n",
    "> **Key insight**: Curvature = Information = Inverse of Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-curvature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualise: Steep vs Flat Log-Likelihood ──────────────────────────────────\n",
    "beta_grid = np.linspace(0, 4, 1000)\n",
    "\n",
    "# Steep: high curvature → low variance (well-identified parameter)\n",
    "ll_steep = -10 * (beta_grid - 2) ** 2\n",
    "# Flat: low curvature → high variance (poorly identified)\n",
    "ll_flat  =  -1 * (beta_grid - 2) ** 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, ll, label, color in zip(\n",
    "    axes,\n",
    "    [ll_steep, ll_flat],\n",
    "    ['Steep Log-Likelihood\\n(Low Variance, Precise Estimate)',\n",
    "     'Flat Log-Likelihood\\n(High Variance, Imprecise Estimate)'],\n",
    "    ['steelblue', 'darkorange']\n",
    "):\n",
    "    ax.plot(beta_grid, ll, color=color, linewidth=2.5)\n",
    "    ax.axvline(2, color='red', linestyle='--', linewidth=2, label='True $\\\\beta = 2$')\n",
    "    ax.set_title(label, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('$\\\\beta$', fontsize=12)\n",
    "    ax.set_ylabel('Log-Likelihood $\\\\ell(\\\\beta)$', fontsize=11)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Log-Likelihood Curvature Determines Precision of MLE',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'loglik_curvature.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Hessian (second derivative) at beta = 2:')\n",
    "print(f'  Steep: d²ℓ/dβ² = -20  →  Var(β̂) = 1/20 = {1/20:.3f}')\n",
    "print(f'  Flat : d²ℓ/dβ² =  -2  →  Var(β̂) = 1/2  = {1/2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-asym-md",
   "metadata": {},
   "source": [
    "### 2.3 Asymptotic Normality of MLE\n",
    "\n",
    "Under regularity conditions:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} \\xrightarrow{d} N\\!\\left(\\beta_0,\\; [\\mathcal{I}(\\beta_0)]^{-1}\\right)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{I}(\\beta) = -\\mathbb{E}[\\nabla^2 \\ell(\\beta)]$ is the **Fisher Information Matrix**.\n",
    "\n",
    "**Three equivalent estimators** of the information matrix:\n",
    "\n",
    "| Name | Formula | Used when |\n",
    "|------|---------|----------|\n",
    "| Observed Hessian | $-H(\\hat\\beta)$ | Most common |\n",
    "| Expected (Fisher) | $\\mathbb{E}[-H(\\beta)]$ | Analytic form available |\n",
    "| Outer-product of scores | $\\sum_i s_i s_i'$ | Misspecification robust |\n",
    "\n",
    "All three coincide under correct specification. They diverge when the model is misspecified — this divergence is what **robust (sandwich) SEs** exploit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-classical-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='classical'></a>\n",
    "## 3. Classical MLE Standard Errors\n",
    "\n",
    "### 3.1 The Information Matrix Estimator\n",
    "\n",
    "**Classical variance** uses the inverted (negative) Hessian:\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathrm{Var}}_{\\mathrm{classical}}(\\hat{\\beta}) = [-H(\\hat{\\beta})]^{-1}\n",
    "$$\n",
    "\n",
    "**Hessian for Logit** (analytical):\n",
    "\n",
    "$$\n",
    "H = -\\sum_{i=1}^n \\Lambda(x_i'\\beta)\\left[1 - \\Lambda(x_i'\\beta)\\right] x_i x_i'\n",
    "$$\n",
    "\n",
    "This is negative semi-definite, so $[-H]^{-1}$ is positive semi-definite — a valid covariance matrix.\n",
    "\n",
    "### 3.2 Estimation with PanelBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-classical-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Logit model on credit data (cross-section, dummy panel ids) ───────────────\n",
    "formula = 'approved ~ income + age + debt_ratio + credit_score + employment_length'\n",
    "\n",
    "logit_nonrob = PooledLogit(formula, credit, 'entity', 'time')\n",
    "res_nonrob   = logit_nonrob.fit(cov_type='nonrobust')\n",
    "\n",
    "print(res_nonrob.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-hessian-inspect-md",
   "metadata": {},
   "source": [
    "### 3.3 Inspecting the Hessian and Covariance Matrix\n",
    "\n",
    "The covariance matrix $[-H]^{-1}$ is stored in `result.cov_params`. Let's reconstruct it manually to understand the underlying computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-hessian-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Manual Hessian computation for Logit ─────────────────────────────────────\n",
    "from scipy.special import expit\n",
    "import patsy\n",
    "\n",
    "# Build design matrices (same formula)\n",
    "y_cr, X_cr = patsy.dmatrices(\n",
    "    'approved ~ income + age + debt_ratio + credit_score + employment_length',\n",
    "    credit, return_type='matrix'\n",
    ")\n",
    "y_cr = np.asarray(y_cr).ravel()\n",
    "X_cr = np.asarray(X_cr)\n",
    "beta_hat = res_nonrob.params.values  # MLE estimates\n",
    "var_names = res_nonrob.params.index.tolist()\n",
    "\n",
    "# Fitted probabilities at MLE\n",
    "eta   = X_cr @ beta_hat\n",
    "p_hat = expit(eta)          # Λ(η)\n",
    "w     = p_hat * (1 - p_hat) # Λ(η)(1 − Λ(η))\n",
    "\n",
    "# Hessian: H = -X'WX  (negative definite)\n",
    "H = -(X_cr.T * w) @ X_cr\n",
    "\n",
    "# Covariance: Var(β̂) = (-H)^{-1}\n",
    "vcov_classical = np.linalg.inv(-H)\n",
    "se_classical   = np.sqrt(np.diag(vcov_classical))\n",
    "\n",
    "print('Classical Standard Errors (manual vs PanelBox):')\n",
    "print(f'{\"Variable\":<22} {\"Manual SE\":>10} {\"PanelBox SE\":>12} {\"Match\":>8}')\n",
    "print('-' * 55)\n",
    "for i, v in enumerate(var_names):\n",
    "    pb_se  = res_nonrob.std_errors.iloc[i]\n",
    "    match  = abs(se_classical[i] - pb_se) < 1e-8\n",
    "    print(f'{v:<22} {se_classical[i]:>10.6f} {pb_se:>12.6f} {str(match):>8}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-classical-valid-md",
   "metadata": {},
   "source": [
    "### 3.4 When Classical SEs Are Valid\n",
    "\n",
    "Classical SEs require three conditions:\n",
    "1. **Correct specification**: The likelihood $f(y_i \\mid x_i, \\beta)$ is the true DGP\n",
    "2. **Independent observations**: No cross-observation correlation\n",
    "3. **Homogeneous parameters**: No unmodeled heterogeneity\n",
    "\n",
    "**Reality**: These conditions are almost never all satisfied. The consequence: classical SEs are **biased**.\n",
    "\n",
    "### 3.5 Monte Carlo: Classical SEs Under Misspecification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-mc-misspec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Monte Carlo: size distortion under misspecification ───────────────────────\n",
    "# True DGP includes x^2 but we estimate a linear model\n",
    "np.random.seed(42)\n",
    "n_sim, n_mc = 500, 200\n",
    "\n",
    "reject_classical = []\n",
    "reject_robust    = []\n",
    "\n",
    "for _ in range(n_mc):\n",
    "    x      = np.random.normal(0, 1, n_sim)\n",
    "    # True model: quadratic in x\n",
    "    eta_true = 0.5 + 1.0 * x + 0.5 * x**2\n",
    "    p_true   = expit(eta_true)\n",
    "    y        = np.random.binomial(1, p_true)\n",
    "\n",
    "    # Design matrix for MISSPECIFIED model (omits x^2)\n",
    "    X_sim = np.column_stack([np.ones(n_sim), x])\n",
    "\n",
    "    # Logit MLE via scipy\n",
    "    def neg_ll(beta):\n",
    "        eta = X_sim @ beta\n",
    "        return -np.sum(y * eta - np.log1p(np.exp(eta)))\n",
    "\n",
    "    opt = minimize(neg_ll, np.zeros(2), method='BFGS')\n",
    "    if not opt.success:\n",
    "        continue\n",
    "    b = opt.x\n",
    "\n",
    "    p_fit = expit(X_sim @ b)\n",
    "    w_fit = p_fit * (1 - p_fit)\n",
    "    H_fit = -(X_sim.T * w_fit) @ X_sim\n",
    "\n",
    "    # Classical SE\n",
    "    try:\n",
    "        vcov_c = np.linalg.inv(-H_fit)\n",
    "        se_c   = np.sqrt(vcov_c[1, 1])\n",
    "        t_c    = b[1] / se_c\n",
    "        reject_classical.append(abs(t_c) > 1.96)\n",
    "    except np.linalg.LinAlgError:\n",
    "        pass\n",
    "\n",
    "    # Robust (sandwich) SE\n",
    "    scores = (y - p_fit)[:, None] * X_sim\n",
    "    S_fit  = scores.T @ scores\n",
    "    H_inv  = np.linalg.inv(-H_fit)\n",
    "    vcov_r = H_inv @ S_fit @ H_inv\n",
    "    se_r   = np.sqrt(vcov_r[1, 1])\n",
    "    t_r    = b[1] / se_r\n",
    "    reject_robust.append(abs(t_r) > 1.96)\n",
    "\n",
    "print('Rejection rates at nominal 5% level (testing H0: β_x = 0 when true β_x ≠ 0)')\n",
    "print(f'  Null is FALSE here — both should reject often (test of power)')\n",
    "print()\n",
    "print('Comparing CLASSICAL vs ROBUST for the x coefficient under misspecification:')\n",
    "print(f'  Classical SE reject rate : {np.mean(reject_classical):.3f}')\n",
    "print(f'  Robust SE reject rate    : {np.mean(reject_robust):.3f}')\n",
    "print()\n",
    "print('Key: Under misspecification, the two SEs diverge.')\n",
    "print('     Robust SEs give more honest coverage of the pseudo-true parameter.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-robust-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='robust'></a>\n",
    "## 4. Robust (Sandwich) Standard Errors\n",
    "\n",
    "### 4.1 The Huber-White Sandwich Estimator\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathrm{Var}}_{\\mathrm{robust}}(\\hat{\\beta}) = H^{-1}\\, S\\, H^{-1}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $H = -\\nabla^2 \\ell(\\hat{\\beta})$ **Hessian** (\"Bread\")\n",
    "- $S = \\displaystyle\\sum_{i=1}^n s_i s_i'$ **Outer product of scores** (\"Meat\")\n",
    "- $s_i = \\nabla_\\beta \\log f(y_i \\mid x_i, \\hat{\\beta})$ individual **score**\n",
    "\n",
    "**For Logit**: $s_i = \\bigl(y_i - \\Lambda(x_i'\\hat\\beta)\\bigr)\\, x_i$\n",
    "\n",
    "**Robustness**: Valid even when the model is misspecified — it does not require the two expressions for the information matrix ($H$ and $S$) to be equal.\n",
    "\n",
    "### 4.2 Using PanelBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-robust-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Fit with all three SE types ───────────────────────────────────────────────\n",
    "logit_rob  = PooledLogit(formula, credit, 'entity', 'time')\n",
    "res_rob    = logit_rob.fit(cov_type='robust')\n",
    "\n",
    "print('=== Robust (Sandwich) SE Results ===')\n",
    "print(res_rob.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-se-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Comparison table: Classical vs Robust ────────────────────────────────────\n",
    "comp = pd.DataFrame({\n",
    "    'Coefficient'  : res_nonrob.params,\n",
    "    'SE Classical' : res_nonrob.std_errors,\n",
    "    'SE Robust'    : res_rob.std_errors,\n",
    "    'Ratio Rob/Cls': res_rob.std_errors / res_nonrob.std_errors,\n",
    "    't Classical'  : res_nonrob.params / res_nonrob.std_errors,\n",
    "    't Robust'     : res_rob.params    / res_rob.std_errors,\n",
    "})\n",
    "\n",
    "print('Classical vs Robust Standard Errors — Credit Approval Logit')\n",
    "print('=' * 80)\n",
    "print(comp.round(4).to_string())\n",
    "print()\n",
    "print('Interpretation:')\n",
    "print('  Ratio > 1: Robust SE > Classical SE (classical underestimates uncertainty)')\n",
    "print('  Ratio < 1: Classical SE > Robust SE (rare; occurs with highly efficient models)')\n",
    "print('  Large ratio: Signals model misspecification or heteroskedasticity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-se-bar-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Bar chart: Classical vs Robust SEs ───────────────────────────────────────\n",
    "vars_plot = [v for v in res_nonrob.params.index if v != 'Intercept']\n",
    "x_pos  = np.arange(len(vars_plot))\n",
    "width  = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.bar(x_pos - width/2, res_nonrob.std_errors[vars_plot], width,\n",
    "       label='Classical', color='steelblue', edgecolor='black', alpha=0.8)\n",
    "ax.bar(x_pos + width/2, res_rob.std_errors[vars_plot], width,\n",
    "       label='Robust', color='darkorange', edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Variable', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Standard Error', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Classical vs Robust Standard Errors — Logit Model (Credit Approval)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(vars_plot, rotation=30, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'classical_vs_robust_se.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-robust-validity-md",
   "metadata": {},
   "source": [
    "### 4.3 Sandwich vs Classical: When Does It Matter?\n",
    "\n",
    "The ratio $SE_{robust} / SE_{classical}$ is informative:\n",
    "\n",
    "| Ratio | Interpretation | Action |\n",
    "|-------|----------------|--------|\n",
    "| ≈ 1.0 | Model well-specified | Both valid; classical slightly more efficient |\n",
    "| 1.1–1.5 | Mild misspecification | Use robust |\n",
    "| > 1.5 | Notable misspecification | Use robust; consider respecifying |\n",
    "| < 1.0 | Unusual (may indicate small-sample issues) | Inspect carefully |\n",
    "\n",
    "> **Rule of thumb**: Always use `cov_type='robust'` or better. Classical SEs are only presented for educational comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-cluster-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='cluster'></a>\n",
    "## 5. Cluster-Robust MLE for Panel Data\n",
    "\n",
    "### 5.1 The Problem: Correlated Observations in Panels\n",
    "\n",
    "In the health insurance panel, the same individual is observed 5 times. Their choices are correlated across years — robust SEs that assume independence are still invalid!\n",
    "\n",
    "**Solution**: Cluster-robust sandwich estimator:\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathrm{Var}}_{\\mathrm{cluster}}(\\hat{\\beta}) = H^{-1}\\!\\left[\\sum_{g=1}^{G} S_g S_g'\\right]H^{-1}\n",
    "$$\n",
    "\n",
    "where $S_g = \\displaystyle\\sum_{i \\in g} s_i$ is the **sum of scores within cluster** $g$.\n",
    "\n",
    "This allows arbitrary correlation within clusters while maintaining independence across clusters.\n",
    "\n",
    "**Degrees-of-freedom correction** (always applied in PanelBox):\n",
    "\n",
    "$$\n",
    "\\text{adj} = \\frac{G}{G-1} \\times \\frac{N-1}{N-K}\n",
    "$$\n",
    "\n",
    "### 5.2 Estimation on the Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cluster-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Logit on panel data: health insurance choice ──────────────────────────────\n",
    "formula_panel = 'high_coverage ~ age + income + family_size + health_status'\n",
    "\n",
    "logit_nonrob_p = PooledLogit(formula_panel, health, 'person_id', 'year')\n",
    "logit_rob_p    = PooledLogit(formula_panel, health, 'person_id', 'year')\n",
    "logit_clust_p  = PooledLogit(formula_panel, health, 'person_id', 'year')\n",
    "\n",
    "res_nonrob_p = logit_nonrob_p.fit(cov_type='nonrobust')\n",
    "res_rob_p    = logit_rob_p.fit(cov_type='robust')\n",
    "res_clust_p  = logit_clust_p.fit(cov_type='cluster')  # cluster by entity (default)\n",
    "\n",
    "print('Cluster-Robust Results — Health Insurance Panel')\n",
    "print(res_clust_p.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cluster-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Three-way comparison table ────────────────────────────────────────────────\n",
    "panel_vars = [v for v in res_clust_p.params.index if v != 'Intercept']\n",
    "\n",
    "comp_panel = pd.DataFrame({\n",
    "    'Coef.'      : res_clust_p.params[panel_vars],\n",
    "    'SE Classic' : res_nonrob_p.std_errors[panel_vars],\n",
    "    'SE Robust'  : res_rob_p.std_errors[panel_vars],\n",
    "    'SE Cluster' : res_clust_p.std_errors[panel_vars],\n",
    "    'Clust/Class': res_clust_p.std_errors[panel_vars] / res_nonrob_p.std_errors[panel_vars],\n",
    "})\n",
    "\n",
    "G = health['person_id'].nunique()\n",
    "print(f'Health Insurance Panel — G = {G} clusters (persons)')\n",
    "print('=' * 72)\n",
    "print(comp_panel.round(4).to_string())\n",
    "print()\n",
    "print('Observation: Clustered SEs are substantially larger than both classical')\n",
    "print('and robust SEs, reflecting within-person correlation across years.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cluster-diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cluster diagnostics ───────────────────────────────────────────────────────\n",
    "cluster_sizes = health.groupby('person_id').size()\n",
    "\n",
    "print('Cluster (person) diagnostics:')\n",
    "print(f'  Number of clusters (G) : {G}')\n",
    "print(f'  Min cluster size       : {cluster_sizes.min()}')\n",
    "print(f'  Mean cluster size      : {cluster_sizes.mean():.1f}')\n",
    "print(f'  Max cluster size       : {cluster_sizes.max()}')\n",
    "print(f'  Balanced               : {cluster_sizes.nunique() == 1}')\n",
    "print()\n",
    "\n",
    "if G < 20:\n",
    "    print('WARNING: Too few clusters. Consider cluster bootstrap.')\n",
    "elif G < 50:\n",
    "    print('CAUTION: Modest number of clusters. Verify robustness.')\n",
    "else:\n",
    "    print('Sufficient clusters for reliable asymptotic inference.')\n",
    "\n",
    "# Visualise SE comparison for panel\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "x_pos = np.arange(len(panel_vars))\n",
    "w = 0.25\n",
    "\n",
    "ax.bar(x_pos - w, res_nonrob_p.std_errors[panel_vars], w,\n",
    "       label='Classical', color='steelblue', edgecolor='black', alpha=0.8)\n",
    "ax.bar(x_pos,     res_rob_p.std_errors[panel_vars], w,\n",
    "       label='Robust', color='darkorange', edgecolor='black', alpha=0.8)\n",
    "ax.bar(x_pos + w, res_clust_p.std_errors[panel_vars], w,\n",
    "       label='Clustered (by person)', color='green', edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Variable', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Standard Error', fontsize=12, fontweight='bold')\n",
    "ax.set_title('SE Comparison: Classical / Robust / Clustered\\nHealth Insurance Panel Logit',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(panel_vars, rotation=30, ha='right')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'cluster_se_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-delta-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='delta'></a>\n",
    "## 6. Delta Method for Nonlinear Transformations\n",
    "\n",
    "### 6.1 The Problem\n",
    "\n",
    "We often report transformations of MLE coefficients:\n",
    "- **Odds ratios**: $OR = \\exp(\\beta)$\n",
    "- **Marginal effects**: $ME_j = \\frac{1}{n}\\sum_i \\Lambda'(x_i'\\beta)\\beta_j$\n",
    "- **Elasticities**: $\\varepsilon = \\beta_j \\cdot \\bar{x}_j / \\bar{P}$\n",
    "\n",
    "**Challenge**: $\\mathrm{Var}(\\exp(\\hat\\beta)) \\neq \\exp(\\mathrm{Var}(\\hat\\beta))$\n",
    "\n",
    "### 6.2 Delta Method Theory\n",
    "\n",
    "For a smooth transformation $g: \\mathbb{R}^k \\to \\mathbb{R}^m$, the **first-order Taylor approximation** gives:\n",
    "\n",
    "$$\n",
    "g(\\hat{\\beta}) \\approx g(\\beta_0) + J(\\beta_0)(\\hat{\\beta} - \\beta_0)\n",
    "$$\n",
    "\n",
    "where $J(\\beta) = \\nabla_\\beta g(\\beta)$ is the $m \\times k$ **Jacobian**. Therefore:\n",
    "\n",
    "$$\n",
    "\\boxed{\\widehat{\\mathrm{Var}}\\bigl(g(\\hat{\\beta})\\bigr) \\approx J(\\hat{\\beta})\\; \\widehat{\\mathrm{Var}}(\\hat{\\beta})\\; J(\\hat{\\beta})'}\n",
    "$$\n",
    "\n",
    "### 6.3 Application: Odds Ratios in Logit\n",
    "\n",
    "**Odds ratio**: $OR_j = \\exp(\\hat\\beta_j)$\n",
    "\n",
    "**Jacobian** (element-wise): $\\partial OR_j / \\partial \\beta_j = \\exp(\\beta_j) = OR_j$\n",
    "\n",
    "**SE of OR**: $SE(OR_j) = OR_j \\cdot SE(\\hat\\beta_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-delta-or",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Delta method: Odds Ratios ─────────────────────────────────────────────────\n",
    "# Use the robust result on credit data\n",
    "beta_hat = res_rob.params.values\n",
    "vcov_rob = res_rob.cov_params.values  # k×k covariance matrix\n",
    "var_names = res_rob.params.index.tolist()\n",
    "\n",
    "# Transformation g(β) = exp(β)  [element-wise]\n",
    "def odds_ratio_transform(beta):\n",
    "    return np.exp(beta)\n",
    "\n",
    "# PanelBox delta_method: returns covariance matrix of g(β̂)\n",
    "vcov_or = delta_method(vcov_rob, odds_ratio_transform, beta_hat)\n",
    "se_or   = np.sqrt(np.diag(vcov_or))\n",
    "or_vals = np.exp(beta_hat)\n",
    "\n",
    "# 95% CI for OR (log-scale is more accurate, then exponentiate)\n",
    "ci_or_lower = np.exp(beta_hat - 1.96 * res_rob.std_errors.values)\n",
    "ci_or_upper = np.exp(beta_hat + 1.96 * res_rob.std_errors.values)\n",
    "\n",
    "or_table = pd.DataFrame({\n",
    "    'Coefficient': beta_hat,\n",
    "    'Odds Ratio' : or_vals,\n",
    "    'OR SE (delta)': se_or,\n",
    "    'OR 95% CI lower': ci_or_lower,\n",
    "    'OR 95% CI upper': ci_or_upper,\n",
    "}, index=var_names)\n",
    "\n",
    "print('Odds Ratios with Delta Method Standard Errors')\n",
    "print('(Based on robust / sandwich covariance matrix)')\n",
    "print('=' * 80)\n",
    "print(or_table.round(4).to_string())\n",
    "print()\n",
    "print('Interpretation:')\n",
    "print('  OR > 1: Factor increases odds of approval')\n",
    "print('  OR < 1: Factor decreases odds of approval')\n",
    "print('  OR = 1: No effect on approval odds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-delta-or-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Forest plot: Odds Ratios with 95% CIs ────────────────────────────────────\n",
    "vars_or = [v for v in var_names if v != 'Intercept']\n",
    "idx_or  = [var_names.index(v) for v in vars_or]\n",
    "\n",
    "or_vals_plot   = or_vals[idx_or]\n",
    "ci_lower_plot  = ci_or_lower[idx_or]\n",
    "ci_upper_plot  = ci_or_upper[idx_or]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "y_pos = np.arange(len(vars_or))\n",
    "\n",
    "ax.scatter(or_vals_plot, y_pos, color='steelblue', s=100, zorder=5)\n",
    "ax.hlines(y_pos, ci_lower_plot, ci_upper_plot, color='steelblue', linewidth=2)\n",
    "ax.axvline(1, color='red', linestyle='--', linewidth=2, label='OR = 1 (no effect)')\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(vars_or, fontsize=11)\n",
    "ax.set_xlabel('Odds Ratio (95% CI)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Odds Ratios — Credit Approval Logit\\n(Robust SEs, Delta Method CIs)',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'odds_ratios_forest.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ame-md",
   "metadata": {},
   "source": [
    "### 6.4 Application: Average Marginal Effects\n",
    "\n",
    "**Marginal effect** of variable $j$ for observation $i$:\n",
    "\n",
    "$$\n",
    "ME_{ij} = \\Lambda'(x_i'\\hat\\beta) \\cdot \\hat\\beta_j = \\Lambda(x_i'\\hat\\beta)\\bigl[1-\\Lambda(x_i'\\hat\\beta)\\bigr] \\hat\\beta_j\n",
    "$$\n",
    "\n",
    "**Average Marginal Effect (AME)** — average over all observations:\n",
    "\n",
    "$$\n",
    "\\widehat{AME}_j = \\frac{1}{n} \\sum_{i=1}^n ME_{ij}\n",
    "$$\n",
    "\n",
    "Standard errors via **delta method** account for estimation uncertainty in $\\hat\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ame",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Average Marginal Effects ──────────────────────────────────────────────────\n",
    "ame_result = compute_ame(res_rob)\n",
    "\n",
    "print('Average Marginal Effects — Credit Approval Logit (Robust SEs)')\n",
    "ame_df = ame_result.summary()\n",
    "print(ame_df.round(5).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ame-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── AME horizontal bar chart with 95% CIs ────────────────────────────────────\n",
    "me_vals  = ame_result.marginal_effects\n",
    "me_se    = ame_result.std_errors\n",
    "me_ci    = ame_result.conf_int()\n",
    "\n",
    "# Exclude intercept if present\n",
    "me_vars  = [v for v in me_vals.index if v.lower() != 'intercept']\n",
    "me_vals  = me_vals[me_vars]\n",
    "me_se    = me_se[me_vars]\n",
    "me_ci    = me_ci.loc[me_vars]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "y_pos = np.arange(len(me_vars))\n",
    "colors = ['steelblue' if v > 0 else 'darkorange' for v in me_vals]\n",
    "\n",
    "ax.barh(y_pos, me_vals, xerr=1.96 * me_se, color=colors,\n",
    "        edgecolor='black', alpha=0.8, capsize=5)\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(me_vars, fontsize=11)\n",
    "ax.set_xlabel('Average Marginal Effect on P(Approved)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Average Marginal Effects with 95% Confidence Intervals\\n(Delta Method, Robust SEs)',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'ame_barplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Interpretation:')\n",
    "print('  AME = change in P(approved) for a 1-unit increase in the variable')\n",
    "print('  Averaged over all 5000 observations in the sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-bootstrap-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='bootstrap'></a>\n",
    "## 7. Bootstrap for MLE\n",
    "\n",
    "### 7.1 When Bootstrap Is Necessary\n",
    "\n",
    "| Situation | Recommended approach |\n",
    "|-----------|---------------------|\n",
    "| Asymmetric sampling distribution (OR, RR) | Percentile/BCa bootstrap |\n",
    "| Small samples (n < 200) | Bootstrap |\n",
    "| Complex transformations | Bootstrap |\n",
    "| Few clusters (G < 20) | Cluster bootstrap |\n",
    "| Large n, simple transformation | Delta method |\n",
    "\n",
    "### 7.2 Nonparametric Bootstrap Algorithm\n",
    "\n",
    "1. Draw $n$ observations **with replacement** from the sample\n",
    "2. Re-estimate the model: obtain $\\hat\\beta^{*(b)}$\n",
    "3. Repeat $B$ times (typically $B = 999$ or $1999$)\n",
    "4. Use the empirical distribution of $\\{\\hat\\beta^{*(b)}\\}_{b=1}^B$:\n",
    "   - $SE_{boot} = \\mathrm{sd}(\\hat\\beta^{*(b)})$\n",
    "   - Percentile CI: $[q_{2.5}, q_{97.5}]$ of bootstrap distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-bootstrap-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Bootstrap SEs for Logit — Credit Approval ────────────────────────────────\n",
    "import patsy\n",
    "\n",
    "y_boot_cr, X_boot_cr = patsy.dmatrices(\n",
    "    'approved ~ income + age + debt_ratio + credit_score + employment_length',\n",
    "    credit, return_type='matrix'\n",
    ")\n",
    "y_boot_cr = np.asarray(y_boot_cr).ravel()\n",
    "X_boot_cr = np.asarray(X_boot_cr)\n",
    "\n",
    "def estimate_logit(y, X):\n",
    "    \"\"\"Logit MLE: returns parameter vector.\"\"\"\n",
    "    def neg_ll(beta):\n",
    "        eta = X @ beta\n",
    "        # Numerically stable log-likelihood\n",
    "        return -np.sum(y * eta - np.log1p(np.exp(np.clip(eta, -500, 500))))\n",
    "    result = minimize(neg_ll, np.zeros(X.shape[1]), method='BFGS',\n",
    "                      options={'maxiter': 1000, 'disp': False})\n",
    "    return result.x\n",
    "\n",
    "# Bootstrap (B=499 for speed; use B=999 in practice)\n",
    "boot_result = bootstrap_mle(\n",
    "    estimate_logit, y_boot_cr, X_boot_cr,\n",
    "    n_bootstrap=499, seed=42\n",
    ")\n",
    "\n",
    "var_names_boot = res_rob.params.index.tolist()\n",
    "\n",
    "print('Bootstrap Standard Errors (B=499)')\n",
    "print(f'{\"Variable\":<22} {\"SE Classical\":>13} {\"SE Robust\":>10} {\"SE Bootstrap\":>13}')\n",
    "print('-' * 60)\n",
    "for i, v in enumerate(var_names_boot):\n",
    "    print(f'{v:<22} {res_nonrob.std_errors.iloc[i]:>13.5f}'\n",
    "          f' {res_rob.std_errors.iloc[i]:>10.5f}'\n",
    "          f' {boot_result.std_errors[i]:>13.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-boot-ci-md",
   "metadata": {},
   "source": [
    "### 7.3 Bootstrap Confidence Intervals\n",
    "\n",
    "Three CI methods, each with different properties:\n",
    "\n",
    "| Method | Formula | When to use |\n",
    "|--------|---------|-------------|\n",
    "| **Normal** | $\\hat\\beta \\pm 1.96 \\cdot SE_{boot}$ | Symmetric distribution |\n",
    "| **Percentile** | $[q_{2.5}, q_{97.5}]$ of $\\hat\\beta^*$ | Asymmetric distribution |\n",
    "| **BCa** | Adjusted percentile (bias+acceleration) | Most accurate, asymmetric |\n",
    "\n",
    "For **odds ratios** (always positive, skewed), percentile or BCa CIs are preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-boot-ci",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Reconstruct bootstrap distribution and compute CIs ────────────────────────\n",
    "# Re-run bootstrap to store individual estimates\n",
    "np.random.seed(42)\n",
    "n_obs_cr = len(y_boot_cr)\n",
    "B = 499\n",
    "boot_estimates = []\n",
    "\n",
    "for _ in range(B):\n",
    "    idx = np.random.choice(n_obs_cr, size=n_obs_cr, replace=True)\n",
    "    try:\n",
    "        b_est = estimate_logit(y_boot_cr[idx], X_boot_cr[idx])\n",
    "        boot_estimates.append(b_est)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "boot_arr = np.array(boot_estimates)  # shape (B_valid, k)\n",
    "beta_orig = res_rob.params.values\n",
    "boot_se   = boot_arr.std(axis=0)\n",
    "\n",
    "# ── Confidence intervals ──────────────────────────────────────────────────────\n",
    "alpha = 0.05\n",
    "\n",
    "# Normal CI\n",
    "ci_normal_lo = beta_orig - 1.96 * boot_se\n",
    "ci_normal_hi = beta_orig + 1.96 * boot_se\n",
    "\n",
    "# Percentile CI\n",
    "ci_pct_lo = np.percentile(boot_arr, 100 * alpha/2, axis=0)\n",
    "ci_pct_hi = np.percentile(boot_arr, 100 * (1 - alpha/2), axis=0)\n",
    "\n",
    "# BCa CI (simplified: percentile with bias-correction only)\n",
    "# Bias correction z0\n",
    "z0 = stats.norm.ppf((boot_arr < beta_orig).mean(axis=0).clip(1e-6, 1-1e-6))\n",
    "z_alpha    = stats.norm.ppf(alpha / 2)\n",
    "z_1malpha  = stats.norm.ppf(1 - alpha / 2)\n",
    "p_lo = stats.norm.cdf(2*z0 + z_alpha)\n",
    "p_hi = stats.norm.cdf(2*z0 + z_1malpha)\n",
    "ci_bca_lo  = np.array([np.percentile(boot_arr[:, j], 100*p_lo[j])\n",
    "                        for j in range(len(beta_orig))])\n",
    "ci_bca_hi  = np.array([np.percentile(boot_arr[:, j], 100*p_hi[j])\n",
    "                        for j in range(len(beta_orig))])\n",
    "\n",
    "print('95% Bootstrap Confidence Intervals for Logit Coefficients')\n",
    "print(f'{\"Variable\":<22} {\"Normal CI\":>22} {\"Percentile CI\":>22} {\"BCa CI\":>22}')\n",
    "print('-' * 90)\n",
    "for i, v in enumerate(var_names_boot):\n",
    "    print(f'{v:<22} '\n",
    "          f'[{ci_normal_lo[i]:7.4f}, {ci_normal_hi[i]:7.4f}]  '\n",
    "          f'[{ci_pct_lo[i]:7.4f}, {ci_pct_hi[i]:7.4f}]  '\n",
    "          f'[{ci_bca_lo[i]:7.4f}, {ci_bca_hi[i]:7.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-boot-dist-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Bootstrap distribution: histogram + Q-Q plot for income ──────────────────\n",
    "# Select a coefficient of interest (income)\n",
    "try:\n",
    "    param_idx = var_names_boot.index('income')\n",
    "except ValueError:\n",
    "    param_idx = 1\n",
    "\n",
    "boot_param = boot_arr[:, param_idx]\n",
    "orig_val   = beta_orig[param_idx]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(boot_param, bins=40, density=True, alpha=0.7,\n",
    "             edgecolor='black', color='steelblue', label='Bootstrap dist.')\n",
    "axes[0].axvline(orig_val, color='red', linewidth=2.5, linestyle='--', label='Original estimate')\n",
    "axes[0].axvline(ci_pct_lo[param_idx], color='green', linewidth=2, linestyle=':',\n",
    "                label='Percentile 95% CI')\n",
    "axes[0].axvline(ci_pct_hi[param_idx], color='green', linewidth=2, linestyle=':')\n",
    "axes[0].set_xlabel(f'Bootstrap $\\\\hat{{\\\\beta}}_{{\\\\mathrm{{{var_names_boot[param_idx]}}}}}$',\n",
    "                   fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].set_title(f'Bootstrap Distribution: {var_names_boot[param_idx]}', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(boot_param, dist='norm', plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot: Normality of Bootstrap Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Bootstrap Inference Diagnostics', fontsize=13, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'bootstrap_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "skewness = stats.skew(boot_param)\n",
    "print(f'Skewness of bootstrap distribution: {skewness:.3f}')\n",
    "if abs(skewness) > 0.3:\n",
    "    print('  Substantial skewness detected: use Percentile or BCa CIs (not Normal CI)')\n",
    "else:\n",
    "    print('  Distribution is approximately symmetric: Normal CI is acceptable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-cluster-boot-md",
   "metadata": {},
   "source": [
    "### 7.4 Cluster Bootstrap for Panel Data\n",
    "\n",
    "**Algorithm**: Instead of resampling observations, resample **entire clusters** (persons). This preserves within-person dependence.\n",
    "\n",
    "Particularly useful when:\n",
    "- $G < 20$ clusters (analytical cluster SEs unreliable)\n",
    "- Cluster size is highly unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cluster-boot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cluster bootstrap — health insurance panel ────────────────────────────────\n",
    "y_h, X_h = patsy.dmatrices(\n",
    "    'high_coverage ~ age + income + family_size + health_status',\n",
    "    health, return_type='matrix'\n",
    ")\n",
    "y_h = np.asarray(y_h).ravel()\n",
    "X_h = np.asarray(X_h)\n",
    "cluster_ids = health['person_id'].values\n",
    "\n",
    "# Cluster bootstrap (B=199 for speed)\n",
    "boot_cluster_result = bootstrap_mle(\n",
    "    estimate_logit, y_h, X_h,\n",
    "    n_bootstrap=199,\n",
    "    cluster_ids=cluster_ids,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "var_names_panel = res_clust_p.params.index.tolist()\n",
    "\n",
    "print('Cluster Bootstrap vs Cluster-Robust SEs — Health Panel')\n",
    "print(f'{\"Variable\":<22} {\"Cluster-Robust SE\":>18} {\"Cluster Bootstrap SE\":>22}')\n",
    "print('-' * 65)\n",
    "for i, v in enumerate(var_names_panel):\n",
    "    print(f'{v:<22} {res_clust_p.std_errors.iloc[i]:>18.5f}'\n",
    "          f' {boot_cluster_result.std_errors[i]:>22.5f}')\n",
    "\n",
    "print()\n",
    "print('Cluster bootstrap is the gold standard when G < 20.')\n",
    "print(f'Here G = {G}, so both methods should agree closely.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-comparison-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='comparison'></a>\n",
    "## 8. Comparison and Best Practices\n",
    "\n",
    "### 8.1 Summary Table\n",
    "\n",
    "| Method | Variance Formula | Assumptions | Pros | Cons | When to use |\n",
    "|--------|-----------------|-------------|------|------|-------------|\n",
    "| **Classical** | $[-H]^{-1}$ | Correct spec., independence | Efficient if valid | Biased under misspec. | Never (educational only) |\n",
    "| **Robust (sandwich)** | $H^{-1}SH^{-1}$ | Independence | Valid under misspec. | Assumes indep. | Cross-section |\n",
    "| **Cluster-robust** | $H^{-1}[\\sum_g S_g S_g']H^{-1}$ | Indep. clusters | Handles within-cluster corr. | Need $G\\geq 20$ | Panel data |\n",
    "| **Bootstrap** | $\\mathrm{Var}(\\hat\\beta^*)$ | Minimal | Works w/ few clusters, skewed dist. | Computationally costly | Small $n$, $G < 20$, transformations |\n",
    "\n",
    "### 8.2 Decision Tree\n",
    "\n",
    "```\n",
    "Nonlinear model (Logit / Probit / Tobit / Count)\n",
    "         │\n",
    "         ├── Cross-sectional data\n",
    "         │       └── Robust (sandwich) SEs\n",
    "         │\n",
    "         └── Panel data\n",
    "                 ├── G >= 20 clusters → Cluster-robust SEs\n",
    "                 └── G <  20 clusters → Cluster bootstrap\n",
    "\n",
    "Reporting transformations (OR, ME, elasticity)?\n",
    "         ├── Large n, symmetric → Delta method\n",
    "         └── Small n, or skewed → Bootstrap CIs (percentile/BCa)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-final-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Final visual: all SE methods side by side ─────────────────────────────────\n",
    "# Focus on panel model where we have all four\n",
    "panel_vars_plot = [v for v in var_names_panel if v.lower() != 'intercept']\n",
    "idx_plot = [var_names_panel.index(v) for v in panel_vars_plot]\n",
    "\n",
    "se_c  = res_nonrob_p.std_errors[panel_vars_plot].values\n",
    "se_r  = res_rob_p.std_errors[panel_vars_plot].values\n",
    "se_cl = res_clust_p.std_errors[panel_vars_plot].values\n",
    "se_b  = boot_cluster_result.std_errors[idx_plot]\n",
    "\n",
    "x_pos = np.arange(len(panel_vars_plot))\n",
    "w = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "ax.bar(x_pos - 1.5*w, se_c,  w, label='Classical', color='steelblue',   edgecolor='black', alpha=0.85)\n",
    "ax.bar(x_pos - 0.5*w, se_r,  w, label='Robust',    color='darkorange',  edgecolor='black', alpha=0.85)\n",
    "ax.bar(x_pos + 0.5*w, se_cl, w, label='Clustered', color='green',       edgecolor='black', alpha=0.85)\n",
    "ax.bar(x_pos + 1.5*w, se_b,  w, label='Cluster Bootstrap', color='purple', edgecolor='black', alpha=0.85)\n",
    "\n",
    "ax.set_xlabel('Variable', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Standard Error', fontsize=12, fontweight='bold')\n",
    "ax.set_title('All SE Methods — Health Insurance Panel Logit\\n'\n",
    "             'Classical / Robust / Cluster-Robust / Cluster Bootstrap',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(panel_vars_plot, rotation=30, ha='right')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'all_se_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Summary:')\n",
    "print('  Classical < Robust < Clustered (expected hierarchy in panel data)')\n",
    "print('  Cluster Bootstrap serves as a cross-check on cluster-robust SEs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-reporting-md",
   "metadata": {},
   "source": [
    "### 8.3 Reporting Standards\n",
    "\n",
    "**In papers**:\n",
    "- \"Standard errors are robust (Huber-White sandwich)\" — for cross-section\n",
    "- \"Standard errors are clustered by *person* (1,000 clusters)\" — for panel\n",
    "- \"95% confidence intervals based on 999 cluster bootstrap replications\" — for few clusters\n",
    "\n",
    "**For transformations**:\n",
    "- \"Marginal effects computed via delta method with robust standard errors\"\n",
    "- \"Odds ratios with 95% CIs based on log-scale robust SEs\"\n",
    "\n",
    "**Always include**: (1) method name, (2) number of clusters if applicable, (3) number of bootstrap replications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-exercises-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='exercises'></a>\n",
    "## 9. Exercises\n",
    "\n",
    "### Exercise 1: Classical vs Robust SEs — Probit Model (Easy)\n",
    "\n",
    "**Task**:\n",
    "1. Estimate a **Probit** model (not Logit) on `credit_approval.csv` using `PooledProbit`\n",
    "2. Obtain classical and robust SEs\n",
    "3. Identify which coefficient shows the largest ratio $SE_{robust}/SE_{classical}$\n",
    "4. Interpret: What does a large ratio suggest about that variable?\n",
    "\n",
    "**Hint**: Use `from panelbox.models.discrete import PooledProbit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ex1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Classical vs Robust SEs with Probit\n",
    "# ── YOUR CODE HERE ──────────────────────────────────────────────────────────\n",
    "\n",
    "# Step 1: Import PooledProbit and fit the model\n",
    "from panelbox.models.discrete import PooledProbit\n",
    "# probit_classic = PooledProbit(...)\n",
    "# res_probit_classic = probit_classic.fit(cov_type='nonrobust')\n",
    "\n",
    "# Step 2: Fit with robust SEs\n",
    "# probit_robust = PooledProbit(...)\n",
    "# res_probit_robust = probit_robust.fit(cov_type='robust')\n",
    "\n",
    "# Step 3: Build comparison table and find largest ratio\n",
    "# ...\n",
    "\n",
    "# Step 4: Print interpretation\n",
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "print('Exercise 1 — complete the code above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ex2-md",
   "metadata": {},
   "source": [
    "### Exercise 2: Delta Method for Income Elasticity (Moderate)\n",
    "\n",
    "**Task**: Compute the **income elasticity** of the probability of approval.\n",
    "\n",
    "**Definition**:\n",
    "$$\n",
    "\\varepsilon_{income} = \\hat\\beta_{income} \\cdot \\frac{\\bar{x}_{income}}{\\bar{P}}\n",
    "$$\n",
    "\n",
    "where $\\bar P = \\frac{1}{n}\\sum_i \\hat P_i$ is the average predicted probability.\n",
    "\n",
    "**Steps**:\n",
    "1. Estimate Logit on `credit_approval.csv` with robust SEs (already done: `res_rob`)\n",
    "2. Define the elasticity as a function of `beta`\n",
    "3. Use `delta_method` from `panelbox.standard_errors.mle` to get SE\n",
    "4. Construct 95% CI\n",
    "5. Interpret: A 1% increase in income → how many % change in approval probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ex2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Income elasticity via delta method\n",
    "# ── YOUR CODE HERE ──────────────────────────────────────────────────────────\n",
    "\n",
    "# Available objects: res_rob, X_cr, y_cr, var_names_boot\n",
    "\n",
    "# Step 1: Compute mean income and mean predicted probability\n",
    "# income_idx = var_names_boot.index('income')\n",
    "# x_bar_income = credit['income'].mean()\n",
    "# p_bar = expit(X_cr @ res_rob.params.values).mean()\n",
    "\n",
    "# Step 2: Define elasticity as function of beta\n",
    "# def income_elasticity(beta):\n",
    "#     beta_income = beta[income_idx]\n",
    "#     # ε = β_income * x̄_income / P̄\n",
    "#     return np.array([beta_income * x_bar_income / p_bar])\n",
    "\n",
    "# Step 3: Apply delta method\n",
    "# vcov_elast = delta_method(res_rob.cov_params.values, income_elasticity, res_rob.params.values)\n",
    "# se_elast   = np.sqrt(vcov_elast[0, 0])\n",
    "# elast_val  = income_elasticity(res_rob.params.values)[0]\n",
    "\n",
    "# Step 4: 95% CI\n",
    "# ci_lo = elast_val - 1.96 * se_elast\n",
    "# ci_hi = elast_val + 1.96 * se_elast\n",
    "\n",
    "# Step 5: Print interpretation\n",
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "print('Exercise 2 — complete the code above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ex3-md",
   "metadata": {},
   "source": [
    "### Exercise 3: Bootstrap vs Cluster-Robust with Few Clusters (Challenging)\n",
    "\n",
    "**Task**: Demonstrate when bootstrap is superior to cluster-robust SEs.\n",
    "\n",
    "**Steps**:\n",
    "1. Simulate a panel: $G = 10$ clusters, $T = 50$ time periods, binary outcome\n",
    "   - Within-cluster correlation $\\rho = 0.4$ (strong clustering)\n",
    "2. Estimate Logit with (a) cluster-robust SEs and (b) cluster bootstrap ($B = 999$)\n",
    "3. Run Monte Carlo (200 replications):\n",
    "   - Check empirical coverage of 95% CI for $\\beta_{x}$ under each method\n",
    "   - Nominal coverage = 95%\n",
    "4. Compare: Which method is closer to 95%?\n",
    "5. Write a short conclusion (3-4 sentences)\n",
    "\n",
    "**Deliverable**: Coverage table and written conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ex3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Few clusters — bootstrap vs cluster-robust\n",
    "# ── YOUR CODE HERE ──────────────────────────────────────────────────────────\n",
    "\n",
    "# Step 1: Simulate panel with G=10, T=50, rho=0.4\n",
    "# G, T_obs, rho = 10, 50, 0.4\n",
    "# beta_true = np.array([0.0, 1.0])  # intercept, slope\n",
    "# ...\n",
    "\n",
    "# Step 2: Estimation functions for cluster-robust and cluster-bootstrap SEs\n",
    "# ...\n",
    "\n",
    "# Step 3: Monte Carlo loop (200 replications)\n",
    "# coverage_clustered  = []\n",
    "# coverage_bootstrap  = []\n",
    "# for rep in range(200):\n",
    "#     # generate data\n",
    "#     # fit model\n",
    "#     # compute CIs\n",
    "#     # check coverage\n",
    "\n",
    "# Step 4: Report results\n",
    "# print(f'Cluster-robust 95% CI coverage: {np.mean(coverage_clustered):.3f}')\n",
    "# print(f'Cluster bootstrap 95% CI coverage: {np.mean(coverage_bootstrap):.3f}')\n",
    "\n",
    "# Step 5: Written conclusion\n",
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "print('Exercise 3 — complete the code above.')\n",
    "print()\n",
    "print('CONCLUSION:')\n",
    "print('[Your 3-4 sentence conclusion here]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='summary'></a>\n",
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Classical MLE SEs** are based on the inverted Hessian $[-H]^{-1}$ and valid only under correct specification\n",
    "2. **Robust (sandwich) SEs** use $H^{-1}SH^{-1}$ and are valid even under misspecification\n",
    "3. **Cluster-robust SEs** aggregate scores within clusters to handle within-cluster correlation (essential for panels)\n",
    "4. **Delta method** propagates parameter uncertainty to nonlinear transformations (OR, AME, elasticities)\n",
    "5. **Bootstrap** is flexible: works with few clusters, non-normal sampling distributions, and complex transformations\n",
    "6. **Never rely on classical SEs** in applied work — use robust or better\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "**Classical**: $\\widehat{\\mathrm{Var}}_{\\mathrm{cls}} = [-H(\\hat\\beta)]^{-1}$\n",
    "\n",
    "**Sandwich**: $\\widehat{\\mathrm{Var}}_{\\mathrm{rob}} = H^{-1}\\, S\\, H^{-1}, \\quad S = \\textstyle\\sum_i s_i s_i'$\n",
    "\n",
    "**Cluster**: $\\widehat{\\mathrm{Var}}_{\\mathrm{cl}} = H^{-1}\\left[\\sum_g S_g S_g'\\right]H^{-1}, \\quad S_g = \\textstyle\\sum_{i\\in g} s_i$\n",
    "\n",
    "**Delta method**: $\\widehat{\\mathrm{Var}}\\bigl(g(\\hat\\beta)\\bigr) \\approx J\\, \\widehat{\\mathrm{Var}}(\\hat\\beta)\\, J'$\n",
    "\n",
    "### PanelBox Cheat Sheet\n",
    "\n",
    "```python\n",
    "from panelbox.models.discrete import PooledLogit\n",
    "from panelbox.marginal_effects.discrete_me import compute_ame\n",
    "from panelbox.standard_errors.mle import delta_method, bootstrap_mle\n",
    "\n",
    "# Estimation\n",
    "model = PooledLogit(formula, data, entity_col, time_col)\n",
    "res_robust  = model.fit(cov_type='robust')   # cross-section\n",
    "res_cluster = model.fit(cov_type='cluster')  # panel (cluster by entity)\n",
    "\n",
    "# Marginal effects\n",
    "ame = compute_ame(res_robust)\n",
    "ame.summary()\n",
    "\n",
    "# Delta method\n",
    "vcov_or = delta_method(res_robust.cov_params.values, np.exp, res_robust.params.values)\n",
    "\n",
    "# Bootstrap\n",
    "boot = bootstrap_mle(estimate_func, y, X, n_bootstrap=999,\n",
    "                     cluster_ids=entity_ids, seed=42)\n",
    "```\n",
    "\n",
    "### Connection to Next Tutorial\n",
    "\n",
    "**Tutorial 06**: Bootstrap for Quantile Regression\n",
    "- Quantile regression has no closed-form asymptotic variance\n",
    "- Bootstrap is the primary inference method\n",
    "- Extends ideas from this notebook to distributional effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-references-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='references'></a>\n",
    "## 11. References\n",
    "\n",
    "### Foundational Papers\n",
    "\n",
    "1. **White, H. (1982)**. \"Maximum likelihood estimation of misspecified models.\" *Econometrica*, 50(1), 1–25.  \n",
    "   *The original paper establishing the sandwich (robust) covariance matrix for MLE.*\n",
    "\n",
    "2. **Huber, P. J. (1967)**. \"The behavior of maximum likelihood estimates under nonstandard conditions.\" *Proceedings of the Fifth Berkeley Symposium*, 1, 221–233.  \n",
    "   *Foundation for robustness of MLE under misspecification.*\n",
    "\n",
    "3. **Cameron, A. C., & Miller, D. L. (2015)**. \"A practitioner's guide to cluster-robust inference.\" *Journal of Human Resources*, 50(2), 317–372.  \n",
    "   *Comprehensive review of clustering, including few-cluster problems.*\n",
    "\n",
    "4. **Efron, B. (1987)**. \"Better bootstrap confidence intervals.\" *Journal of the American Statistical Association*, 82(397), 171–185.  \n",
    "   *Introduces BCa confidence intervals.*\n",
    "\n",
    "### Textbooks\n",
    "\n",
    "1. **Cameron, A. C., & Trivedi, P. K. (2005)**. *Microeconometrics: Methods and Applications*. Cambridge University Press. [Chapters 5, 10, 11]  \n",
    "   *Comprehensive treatment of MLE, robust inference, and bootstrapping for microeconometrics.*\n",
    "\n",
    "2. **Wooldridge, J. M. (2010)**. *Econometric Analysis of Cross Section and Panel Data* (2nd ed.). MIT Press. [Chapters 13, 15]  \n",
    "   *Standard graduate reference for MLE in panel data contexts.*\n",
    "\n",
    "3. **Efron, B., & Tibshirani, R. J. (1994)**. *An Introduction to the Bootstrap*. CRC Press.  \n",
    "   *The definitive reference for bootstrap methods.*\n",
    "\n",
    "### PanelBox Documentation\n",
    "\n",
    "- MLE models: `panelbox.readthedocs.io/models/discrete.html`\n",
    "- Standard errors: `panelbox.readthedocs.io/inference/standard-errors.html`\n",
    "- Marginal effects: `panelbox.readthedocs.io/inference/marginal-effects.html`\n",
    "\n",
    "---\n",
    "\n",
    "**End of Tutorial 05**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
