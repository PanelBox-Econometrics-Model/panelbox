{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# Tutorial 07: Synthesis - Comparing All Standard Error Methods\n",
    "\n",
    "**Author**: PanelBox Development Team  \n",
    "**Date**: 2026-02-16  \n",
    "**Estimated Duration**: 120-150 minutes  \n",
    "**Difficulty Level**: Advanced (synthesis)  \n",
    "**Prerequisites**: Notebooks 01-06 (all SE methods)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Synthesize** all standard error methods learned in previous notebooks\n",
    "2. **Apply** a systematic comparison framework using `StandardErrorComparison`\n",
    "3. **Develop** decision-making intuition for choosing SE methods\n",
    "4. **Identify** when SE choice critically affects inference\n",
    "5. **Create** publication-ready tables with multiple SE specifications\n",
    "6. **Follow** best practices for reporting robust inference\n",
    "7. **Conduct** comprehensive sensitivity analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction: The SE Decision Tree](#section1)\n",
    "2. [Application 1: Financial Panel - Two-Way Clustering](#section2)\n",
    "3. [Application 2: Macro Panel - Driscoll-Kraay vs PCSE](#section3)\n",
    "4. [Application 3: Wage Panel - Cluster-Robust vs Quantile](#section4)\n",
    "5. [Decision-Making Framework](#section5)\n",
    "6. [Publication-Ready Tables](#section6)\n",
    "7. [Case Study: When SE Choice Matters Most](#section7)\n",
    "8. [Exercises](#section8)\n",
    "9. [Summary and Takeaways](#section9)\n",
    "10. [References](#section10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='setup'></a>\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PanelBox imports\n",
    "import sys\n",
    "sys.path.insert(0, '../../../desenvolvimento')\n",
    "import panelbox as pb\n",
    "from panelbox.models.static import FixedEffects, PooledOLS\n",
    "from panelbox.models.quantile import PooledQuantile\n",
    "from panelbox.inference.quantile import QuantileBootstrap\n",
    "from panelbox.standard_errors.comparison import StandardErrorComparison\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = '../data/'\n",
    "FIG_PATH = '../outputs/figures/07_comparison/'\n",
    "\n",
    "import os\n",
    "os.makedirs(FIG_PATH, exist_ok=True)\n",
    "os.makedirs('../outputs/reports/', exist_ok=True)\n",
    "\n",
    "print(\"Setup complete! All libraries loaded.\")\n",
    "print(f\"Output directory: {FIG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section1'></a>\n",
    "## 1. Introduction: The SE Decision Tree\n",
    "\n",
    "### 1.1 Review: Taxonomy of Standard Error Methods\n",
    "\n",
    "After six tutorials covering individual methods, let's synthesize everything into a unified framework.\n",
    "\n",
    "**Complete Map of Standard Errors in Panel Data**:\n",
    "\n",
    "```\n",
    "Standard Errors in Panel Data\n",
    "│\n",
    "├─ HETEROSKEDASTICITY ONLY\n",
    "│  ├─ HC0 (White 1980)                  → Basic sandwich\n",
    "│  ├─ HC1 (DF-corrected)                → Stata default 'robust'\n",
    "│  ├─ HC2 (Leverage-adjusted)           → Moderate correction\n",
    "│  └─ HC3 (Aggressive leverage)         → Best small samples\n",
    "│\n",
    "├─ TEMPORAL CORRELATION\n",
    "│  ├─ Cluster by Entity                 → Most common in micro panels\n",
    "│  ├─ Newey-West HAC                    → Time series / short panels\n",
    "│  └─ Driscoll-Kraay HAC               → Panel with cross-section corr.\n",
    "│\n",
    "├─ CROSS-SECTIONAL CORRELATION\n",
    "│  ├─ Cluster by Time                   → Common shocks approach\n",
    "│  ├─ Driscoll-Kraay HAC               → Handles both dimensions\n",
    "│  └─ PCSE                             → Political science default\n",
    "│\n",
    "├─ BOTH TEMPORAL & CROSS-SECTIONAL\n",
    "│  ├─ Two-Way Clustering               → Finance panels standard\n",
    "│  ├─ Driscoll-Kraay HAC               → Macro panels\n",
    "│  └─ Spatial HAC                      → Geographic data\n",
    "│\n",
    "└─ NONLINEAR MODELS\n",
    "   ├─ MLE Sandwich (robust)             → Logit, Probit, Poisson\n",
    "   ├─ MLE Cluster-robust               → Panel nonlinear models\n",
    "   └─ Bootstrap                         → Quantile regression\n",
    "```\n",
    "\n",
    "### 1.2 Key Questions for Choosing SE Method\n",
    "\n",
    "Before selecting a SE method, answer these diagnostic questions:\n",
    "\n",
    "| # | Question | Implication |\n",
    "|---|----------|-------------|\n",
    "| 1 | Data type: Cross-section, Time series, or Panel? | Determines applicable methods |\n",
    "| 2 | Panel type: Micro (N large, T small) or Macro (N small, T large)? | Cluster vs HAC |\n",
    "| 3 | Heteroskedasticity present? | Almost always yes → use robust |\n",
    "| 4 | Temporal correlation within entities? | Cluster by entity |\n",
    "| 5 | Cross-sectional correlation (same time period)? | Two-way or Driscoll-Kraay |\n",
    "| 6 | Spatial correlation? | Spatial HAC |\n",
    "| 7 | Number of clusters G ≥ 20? | Required for cluster asymptotics |\n",
    "\n",
    "### 1.3 Objectives of This Notebook\n",
    "\n",
    "We apply these principles to **three real empirical problems**:\n",
    "\n",
    "| Application | Dataset | Primary Issue | Methods Compared |\n",
    "|-------------|---------|---------------|------------------|\n",
    "| **Financial Panel** | Stock returns (50 firms, 120 months) | Both temporal & cross-section corr. | Robust, Cluster, Two-Way, Driscoll-Kraay |\n",
    "| **Macro Panel** | Growth (30 countries, 40 years) | Cross-section + temporal corr. | Driscoll-Kraay, PCSE, Two-Way Cluster |\n",
    "| **Wage Panel** | Wages (2000 individuals, 5 years) | Temporal corr. within persons | Cluster, Robust, Quantile Bootstrap |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decision-flowchart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the SE Decision Framework\n",
    "fig, ax = plt.subplots(figsize=(16, 11))\n",
    "ax.set_xlim(0, 16)\n",
    "ax.set_ylim(0, 11)\n",
    "ax.axis('off')\n",
    "\n",
    "def add_box(ax, x, y, w, h, text, color, fontsize=9.5, alpha=0.85, bold=False):\n",
    "    box = FancyBboxPatch((x - w/2, y - h/2), w, h,\n",
    "                          boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor='#333333', linewidth=1.5,\n",
    "                          alpha=alpha)\n",
    "    ax.add_patch(box)\n",
    "    weight = 'bold' if bold else 'normal'\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=fontsize,\n",
    "            fontweight=weight, wrap=True,\n",
    "            multialignment='center')\n",
    "\n",
    "def add_arrow(ax, x1, y1, x2, y2, label='', color='#555555'):\n",
    "    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                arrowprops=dict(arrowstyle='->', color=color, lw=1.8))\n",
    "    if label:\n",
    "        mx, my = (x1+x2)/2, (y1+y2)/2\n",
    "        ax.text(mx + 0.15, my, label, fontsize=8, color=color, fontstyle='italic')\n",
    "\n",
    "# Start\n",
    "add_box(ax, 8, 10.2, 3.2, 0.7, 'Panel Data Model\\n(Regression)', '#4C72B0', fontsize=10, bold=True)\n",
    "\n",
    "# Q1: Panel type\n",
    "add_box(ax, 8, 9.0, 4.0, 0.7, 'Panel Type?', '#FFA500', fontsize=9.5, alpha=0.9, bold=True)\n",
    "add_arrow(ax, 8, 9.85, 8, 9.35)\n",
    "\n",
    "# Micro panel\n",
    "add_box(ax, 3.5, 7.8, 3.8, 0.65,\n",
    "        'Micro Panel\\n(N large, T small)', '#90EE90', fontsize=9)\n",
    "add_arrow(ax, 6.0, 9.0, 5.4, 7.8, 'N>>T')\n",
    "\n",
    "# Macro panel\n",
    "add_box(ax, 12.5, 7.8, 3.8, 0.65,\n",
    "        'Macro Panel\\n(N small, T large)', '#FFB6C1', fontsize=9)\n",
    "add_arrow(ax, 10.0, 9.0, 11.6, 7.8, 'T>>N')\n",
    "\n",
    "# Micro: clustering questions\n",
    "add_box(ax, 2.0, 6.5, 3.2, 0.6, 'G ≥ 20 clusters?', '#FFA500', fontsize=9, alpha=0.8, bold=False)\n",
    "add_box(ax, 5.0, 6.5, 3.2, 0.6, 'Cross-section\\ncorrelation?', '#FFA500', fontsize=9, alpha=0.8)\n",
    "add_arrow(ax, 3.5, 7.48, 2.0, 6.8)\n",
    "add_arrow(ax, 3.5, 7.48, 5.0, 6.8)\n",
    "\n",
    "# Micro solutions\n",
    "add_box(ax, 1.0, 5.3, 2.4, 0.65,\n",
    "        'Bootstrap\\n(small G)', '#87CEEB', fontsize=8.5)\n",
    "add_box(ax, 3.2, 5.3, 2.8, 0.65,\n",
    "        'Cluster by Entity\\n(HC1 baseline)', '#3CB371', fontsize=8.5, alpha=0.9)\n",
    "add_box(ax, 5.8, 5.3, 2.8, 0.65,\n",
    "        'Two-Way\\nClustering', '#2E8B57', fontsize=8.5, alpha=0.9)\n",
    "add_arrow(ax, 2.0, 6.2, 1.0, 5.65, 'No')\n",
    "add_arrow(ax, 2.0, 6.2, 3.2, 5.65, 'Yes')\n",
    "add_arrow(ax, 5.0, 6.2, 5.8, 5.65, 'Yes')\n",
    "add_arrow(ax, 5.0, 6.2, 3.2, 5.65, 'No')\n",
    "\n",
    "# Macro: method questions  \n",
    "add_box(ax, 11.0, 6.5, 3.2, 0.6, 'T > N?', '#FFA500', fontsize=9, alpha=0.8, bold=False)\n",
    "add_box(ax, 14.0, 6.5, 3.2, 0.6, 'T > 20?', '#FFA500', fontsize=9, alpha=0.8)\n",
    "add_arrow(ax, 12.5, 7.48, 11.0, 6.8)\n",
    "add_arrow(ax, 12.5, 7.48, 14.0, 6.8)\n",
    "\n",
    "# Macro solutions\n",
    "add_box(ax, 10.0, 5.3, 2.8, 0.65,\n",
    "        'Driscoll-Kraay\\n(primary)', '#DC143C', fontsize=8.5, alpha=0.9)\n",
    "add_box(ax, 13.0, 5.3, 2.8, 0.65,\n",
    "        'PCSE\\n(T >> N)', '#FF6347', fontsize=8.5, alpha=0.9)\n",
    "add_box(ax, 15.2, 5.3, 1.8, 0.65,\n",
    "        'Cluster\\nOnly', '#FFA07A', fontsize=8.5, alpha=0.9)\n",
    "add_arrow(ax, 11.0, 6.2, 10.0, 5.65, 'No/T≈N')\n",
    "add_arrow(ax, 11.0, 6.2, 13.0, 5.65, 'Yes')\n",
    "add_arrow(ax, 14.0, 6.2, 13.0, 5.65, 'Yes')\n",
    "add_arrow(ax, 14.0, 6.2, 15.2, 5.65, 'No')\n",
    "\n",
    "# Special cases row\n",
    "add_box(ax, 8, 4.0, 14, 0.65,\n",
    "        'Special Cases: Nonlinear Models → MLE Sandwich/Cluster-Robust  |  '\n",
    "        'Quantile Regression → Cluster Bootstrap  |  Spatial Data → Spatial HAC',\n",
    "        '#D8BFD8', fontsize=8.5, alpha=0.9)\n",
    "\n",
    "# Best practices row\n",
    "add_box(ax, 8, 3.0, 14, 0.65,\n",
    "        'ALWAYS: Report primary + robustness check  |  '\n",
    "        'NEVER use nonrobust in panel data  |  '\n",
    "        'G < 20 clusters → Bootstrap or aggregate',\n",
    "        '#FFFACD', fontsize=8.5, alpha=0.9, bold=False)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='#FFA500', label='Decision node'),\n",
    "    mpatches.Patch(facecolor='#3CB371', label='Micro panel method'),\n",
    "    mpatches.Patch(facecolor='#DC143C', label='Macro panel method'),\n",
    "    mpatches.Patch(facecolor='#D8BFD8', label='Special cases'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower left', fontsize=9,\n",
    "          bbox_to_anchor=(0.01, 0.01))\n",
    "\n",
    "ax.set_title('Standard Error Decision Flowchart for Panel Data',\n",
    "             fontsize=16, fontweight='bold', pad=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'decision_flowchart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Decision flowchart saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section2'></a>\n",
    "## 2. Application 1: Financial Panel — Two-Way Clustering\n",
    "\n",
    "### 2.1 Context and Research Question\n",
    "\n",
    "**Research Question**: Do market risk and value-to-book ratio explain stock excess returns?\n",
    "\n",
    "**Data Structure**:\n",
    "- N = 50 firms, T = 120 months (10 years) → balanced panel\n",
    "- Variables: `returns` (excess return), `market_ret` (market premium), `book_to_market` (value factor)\n",
    "\n",
    "**Expected Correlations**:\n",
    "- **Temporal (within firm)**: Momentum, persistent idiosyncratic risk\n",
    "- **Cross-sectional (same month)**: Market shocks, macroeconomic news\n",
    "\n",
    "> **Key Insight**: Both correlations present → Classical or even firm-clustered SEs are inadequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fin-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load financial panel\n",
    "fin_data = pd.read_csv(DATA_PATH + 'financial_panel.csv')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINANCIAL PANEL DATA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {fin_data.shape}\")\n",
    "print(f\"Firms: {fin_data['firm_id'].nunique()}\")\n",
    "print(f\"Months: {fin_data['month'].nunique()}\")\n",
    "print(f\"Variables: {list(fin_data.columns)}\")\n",
    "print()\n",
    "\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(fin_data[['returns', 'market_ret', 'book_to_market']].describe().round(4))\n",
    "\n",
    "# Note on collinearity: 'size' is time-invariant within firms → absorbed by FE\n",
    "# We use market_ret and book_to_market (both vary within firms over time)\n",
    "size_within_std = fin_data.groupby('firm_id')['size'].std().mean()\n",
    "print(f\"\\nNote: 'size' within-firm std = {size_within_std:.4f} → time-invariant, absorbed by FE\")\n",
    "print(\"Model uses: returns ~ market_ret + book_to_market\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fin-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed effects model (firm + month fixed effects)\n",
    "fe_fin = FixedEffects(\"returns ~ market_ret + book_to_market\",\n",
    "                      fin_data, \"firm_id\", \"month\")\n",
    "\n",
    "# Estimate with different SE methods\n",
    "results_fin = {}\n",
    "\n",
    "# 1. Non-robust (baseline — incorrect for panel data)\n",
    "results_fin['nonrobust'] = fe_fin.fit(cov_type='nonrobust')\n",
    "print(\"[1/6] Non-robust: done\")\n",
    "\n",
    "# 2. Robust HC1 (ignores correlation structure)\n",
    "results_fin['robust'] = fe_fin.fit(cov_type='robust')\n",
    "print(\"[2/6] Robust (HC1): done\")\n",
    "\n",
    "# 3. Cluster by firm (captures within-firm temporal correlation)\n",
    "results_fin['cluster_firm'] = fe_fin.fit(cov_type='clustered')\n",
    "print(\"[3/6] Cluster by firm: done\")\n",
    "\n",
    "# 4. Two-way clustering (firm + month — captures both dimensions)\n",
    "results_fin['twoway'] = fe_fin.fit(cov_type='twoway')\n",
    "print(\"[4/6] Two-way clustering: done\")\n",
    "\n",
    "# 5. Driscoll-Kraay (alternative to two-way for macro-style panels)\n",
    "results_fin['driscoll_kraay'] = fe_fin.fit(cov_type='driscoll_kraay', max_lags=6)\n",
    "print(\"[5/6] Driscoll-Kraay (lags=6): done\")\n",
    "\n",
    "# 6. Newey-West HAC\n",
    "results_fin['newey_west'] = fe_fin.fit(cov_type='newey_west', max_lags=6)\n",
    "print(\"[6/6] Newey-West (lags=6): done\")\n",
    "\n",
    "print(\"\\nAll financial panel estimations complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fin-compare-header",
   "metadata": {},
   "source": [
    "### 2.2 Comprehensive SE Comparison — Financial Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fin-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison using StandardErrorComparison\n",
    "comp_fin = StandardErrorComparison(results_fin['twoway'])\n",
    "\n",
    "result_fin = comp_fin.compare_all(\n",
    "    se_types=['nonrobust', 'robust', 'clustered', 'twoway', 'driscoll_kraay', 'newey_west']\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINANCIAL PANEL: STANDARD ERROR COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# SE comparison table\n",
    "se_table = result_fin.se_comparison.copy()\n",
    "se_table.columns = ['NonRobust', 'Robust', 'Cluster\\n(Firm)', 'Two-Way', 'Driscoll\\nKraay', 'Newey\\nWest']\n",
    "print(\"Standard Errors by Method:\")\n",
    "print(se_table.round(5).to_string())\n",
    "print()\n",
    "\n",
    "# SE ratios relative to non-robust\n",
    "print(\"SE Ratios Relative to Non-Robust:\")\n",
    "print(result_fin.se_ratios.round(3).to_string())\n",
    "print()\n",
    "\n",
    "# Significance comparison\n",
    "print(\"Significance Stars (* p<0.10, ** p<0.05, *** p<0.01):\")\n",
    "print(result_fin.significance.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fin-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify the inflation: nonrobust vs twoway vs driscoll_kraay\n",
    "print(\"=\" * 70)\n",
    "print(\"KEY FINDING: SE INFLATION IN FINANCIAL PANEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for var in ['market_ret', 'book_to_market']:\n",
    "    se_nr = results_fin['nonrobust'].std_errors[var]\n",
    "    se_rob = results_fin['robust'].std_errors[var]\n",
    "    se_cl = results_fin['cluster_firm'].std_errors[var]\n",
    "    se_tw = results_fin['twoway'].std_errors[var]\n",
    "    se_dk = results_fin['driscoll_kraay'].std_errors[var]\n",
    "\n",
    "    print(f\"\\n{var}:\")\n",
    "    print(f\"  Non-robust:     {se_nr:.5f} (baseline = 1.00x)\")\n",
    "    print(f\"  Robust:         {se_rob:.5f} ({se_rob/se_nr:.2f}x vs nonrobust)\")\n",
    "    print(f\"  Cluster (firm): {se_cl:.5f} ({se_cl/se_nr:.2f}x vs nonrobust)\")\n",
    "    print(f\"  Two-Way:        {se_tw:.5f} ({se_tw/se_nr:.2f}x vs nonrobust)\")\n",
    "    print(f\"  Driscoll-Kraay: {se_dk:.5f} ({se_dk/se_nr:.2f}x vs nonrobust)\")\n",
    "\n",
    "print()\n",
    "print(\"Observations:\")\n",
    "print(\"  1. Two-way SEs are substantially larger than robust SEs\")\n",
    "print(\"  2. Firm clustering > Non-robust (temporal autocorrelation present)\")\n",
    "print(\"  3. Driscoll-Kraay ≈ Two-Way (both capture multi-dimensional correlation)\")\n",
    "print(\"  4. Ignoring correlation structure → over-rejection of null hypotheses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fin-viz-header",
   "metadata": {},
   "source": [
    "### 2.3 Visualization: SE Comparison Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fin-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SE comparison for financial panel\n",
    "vars_to_plot = ['market_ret', 'book_to_market']\n",
    "method_keys = ['nonrobust', 'robust', 'cluster_firm', 'twoway', 'driscoll_kraay', 'newey_west']\n",
    "method_labels = ['Non\\nRobust', 'Robust\\n(HC1)', 'Cluster\\n(Firm)', 'Two-\\nWay', 'Driscoll\\nKraay', 'Newey\\nWest']\n",
    "colors = ['#aaaaaa', '#5B9BD5', '#FFA500', '#C00000', '#70AD47', '#9B59B6']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for ax, var in zip(axes, vars_to_plot):\n",
    "    ses = [results_fin[k].std_errors[var] for k in method_keys]\n",
    "    bars = ax.bar(method_labels, ses, color=colors,\n",
    "                  edgecolor='black', linewidth=1.2, alpha=0.85)\n",
    "\n",
    "    # Annotate with values\n",
    "    for bar, se in zip(bars, ses):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height * 1.01,\n",
    "                f'{se:.4f}', ha='center', va='bottom', fontsize=8.5,\n",
    "                fontweight='bold')\n",
    "\n",
    "    # Highlight non-robust as dangerous reference\n",
    "    ax.axhline(y=ses[0], color='red', linestyle='--', alpha=0.5,\n",
    "               linewidth=1.5, label='Non-robust (baseline)')\n",
    "\n",
    "    ax.set_ylabel('Standard Error', fontsize=12)\n",
    "    ax.set_title(f'SE Comparison: {var}', fontsize=13, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Financial Panel: Standard Error Comparison Across Methods\\n'\n",
    "             '(50 Firms × 120 Months)',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'fin_se_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Figure saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fin-signif-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient plots with CIs under each SE method\n",
    "fig = comp_fin.plot_comparison(result=result_fin, figsize=(14, 9))\n",
    "plt.suptitle('Financial Panel: Coefficients and 95% CIs by SE Method',\n",
    "             fontsize=13, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'fin_ci_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Confidence interval comparison saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fin-rec",
   "metadata": {},
   "source": [
    "### 2.4 Sensitivity Analysis and Recommendation\n",
    "\n",
    "**Does SE choice change inference?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fin-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 85)\n",
    "print(\"FINANCIAL PANEL: INFERENCE SENSITIVITY\")\n",
    "print(\"=\" * 85)\n",
    "print(f\"{'Method':<20} | {'Var':<17} | {'Coef':>8} | {'SE':>8} | {'t-stat':>7} | {'p-value':>8} | Sig\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "method_display = {\n",
    "    'nonrobust': 'Non-Robust',\n",
    "    'robust': 'Robust (HC1)',\n",
    "    'cluster_firm': 'Cluster (Firm)',\n",
    "    'twoway': 'Two-Way',\n",
    "    'driscoll_kraay': 'Driscoll-Kraay',\n",
    "    'newey_west': 'Newey-West'\n",
    "}\n",
    "\n",
    "for method, res in results_fin.items():\n",
    "    for var in ['market_ret', 'book_to_market']:\n",
    "        coef = res.params[var]\n",
    "        se = res.std_errors[var]\n",
    "        t = coef / se\n",
    "        p = res.pvalues[var]\n",
    "        sig = '***' if p < 0.01 else ('**' if p < 0.05 else ('*' if p < 0.10 else 'ns'))\n",
    "        print(f\"{method_display[method]:<20} | {var:<17} | {coef:>8.4f} | {se:>8.5f} | {t:>7.2f} | {p:>8.4f} | {sig}\")\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 85)\n",
    "print(\"RECOMMENDATION: FINANCIAL PANEL\")\n",
    "print(\"=\" * 85)\n",
    "print(\"\"\"\n",
    "Primary specification: Two-way clustering (firm × month)\n",
    "  - Accounts for both within-firm temporal correlation AND cross-firm shocks\n",
    "  - Standard in financial economics (Petersen 2009, Thompson 2011)\n",
    "  - N_firms=50, N_months=120 — both above G=20 minimum\n",
    "\n",
    "Robustness check: Driscoll-Kraay with automatic lag selection\n",
    "  - Provides independent confirmation of two-way cluster results\n",
    "  - HAC kernel approach versus clustering approach\n",
    "\n",
    "Table note: 'Standard errors two-way clustered by firm and month\n",
    "  (50 firms, 120 months). * p<0.10, ** p<0.05, *** p<0.01.'\n",
    "\n",
    "AVOID: Non-robust or simple robust SEs — substantially understate uncertainty!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section3'></a>\n",
    "## 3. Application 2: Macro Panel — Driscoll-Kraay vs PCSE\n",
    "\n",
    "### 3.1 Context: Trade and Economic Growth\n",
    "\n",
    "**Research Question**: Does trade openness promote economic growth?\n",
    "\n",
    "**Data Structure**:\n",
    "- N = 30 countries, T = 40 years → balanced macro panel  \n",
    "- Variables: `gdp_growth`, `investment`, `education`, `openness`\n",
    "\n",
    "**Expected Features**:\n",
    "- **Global shocks** (financial crises, commodity cycles) → cross-section correlation\n",
    "- **Country-specific trends** → temporal correlation within countries\n",
    "- T = 40 > N = 30 → PCSE condition T > N satisfied\n",
    "\n",
    "**Applicable Methods**: Cluster by country, Driscoll-Kraay, PCSE, Two-Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load macro panel\n",
    "macro_data = pd.read_csv(DATA_PATH + 'macro_growth.csv')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MACRO PANEL DATA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {macro_data.shape}\")\n",
    "print(f\"Countries: {macro_data['country_id'].nunique()}\")\n",
    "print(f\"Years: {macro_data['year'].max() - macro_data['year'].min() + 1} \" +\n",
    "      f\"({macro_data['year'].min()}–{macro_data['year'].max()})\")\n",
    "print(f\"Variables: {list(macro_data.columns)}\")\n",
    "\n",
    "# Check balance\n",
    "balance = macro_data.groupby('country_id').size()\n",
    "print(f\"\\nBalance check:\")\n",
    "print(f\"  Min obs per country: {balance.min()}\")\n",
    "print(f\"  Max obs per country: {balance.max()}\")\n",
    "print(f\"  Balanced: {balance.nunique() == 1}\")\n",
    "\n",
    "# T vs N check for PCSE\n",
    "T = macro_data['year'].nunique()\n",
    "N = macro_data['country_id'].nunique()\n",
    "print(f\"\\nT = {T}, N = {N}, T > N: {T > N} → PCSE condition satisfied\")\n",
    "\n",
    "print()\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(macro_data[['gdp_growth', 'investment', 'education', 'openness']].describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed effects model\n",
    "fe_macro = FixedEffects(\"gdp_growth ~ investment + education + openness\",\n",
    "                        macro_data, \"country_id\", \"year\")\n",
    "\n",
    "results_macro = {}\n",
    "\n",
    "# 1. Non-robust\n",
    "results_macro['nonrobust'] = fe_macro.fit(cov_type='nonrobust')\n",
    "print(\"[1/6] Non-robust: done\")\n",
    "\n",
    "# 2. Robust (HC1)\n",
    "results_macro['robust'] = fe_macro.fit(cov_type='robust')\n",
    "print(\"[2/6] Robust: done\")\n",
    "\n",
    "# 3. Cluster by country (within-country temporal correlation)\n",
    "results_macro['cluster_country'] = fe_macro.fit(cov_type='clustered')\n",
    "print(\"[3/6] Cluster by country: done\")\n",
    "\n",
    "# 4. Two-way clustering (country + year)\n",
    "results_macro['twoway'] = fe_macro.fit(cov_type='twoway')\n",
    "print(\"[4/6] Two-way clustering: done\")\n",
    "\n",
    "# 5. Driscoll-Kraay (T=40 — well above T>20 requirement)\n",
    "results_macro['driscoll_kraay'] = fe_macro.fit(cov_type='driscoll_kraay', max_lags=4)\n",
    "print(\"[5/6] Driscoll-Kraay (lags=4): done\")\n",
    "\n",
    "# 6. PCSE (T=40 > N=30 ✓)\n",
    "results_macro['pcse'] = fe_macro.fit(cov_type='pcse')\n",
    "print(\"[6/6] PCSE: done\")\n",
    "\n",
    "print(\"\\nAll macro panel estimations complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison using StandardErrorComparison\n",
    "comp_macro = StandardErrorComparison(results_macro['driscoll_kraay'])\n",
    "result_macro = comp_macro.compare_all(\n",
    "    se_types=['nonrobust', 'robust', 'clustered', 'twoway', 'driscoll_kraay', 'pcse']\n",
    ")\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"MACRO PANEL: STANDARD ERROR COMPARISON\")\n",
    "print(\"=\" * 90)\n",
    "print()\n",
    "\n",
    "se_display = result_macro.se_comparison.copy()\n",
    "se_display.columns = ['NonRobust', 'Robust', 'Cluster\\n(Country)', 'Two-Way', 'Driscoll\\nKraay', 'PCSE']\n",
    "print(\"Standard Errors:\")\n",
    "print(se_display.round(5).to_string())\n",
    "print()\n",
    "\n",
    "print(\"SE Ratios (relative to Non-Robust):\")\n",
    "print(result_macro.se_ratios.round(3).to_string())\n",
    "print()\n",
    "\n",
    "print(\"Significance (* p<0.10, ** p<0.05, *** p<0.01):\")\n",
    "print(result_macro.significance.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-dk-pcse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus: Driscoll-Kraay vs PCSE vs Cluster\n",
    "print(\"=\" * 85)\n",
    "print(\"MACRO PANEL: INFERENCE SENSITIVITY TABLE\")\n",
    "print(\"=\" * 85)\n",
    "print(f\"{'Method':<22} | {'Variable':<12} | {'Coef':>8} | {'SE':>8} | {'t-stat':>7} | {'p-value':>8} | Sig\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "macro_method_display = {\n",
    "    'nonrobust': 'Non-Robust',\n",
    "    'robust': 'Robust (HC1)',\n",
    "    'cluster_country': 'Cluster (Country)',\n",
    "    'twoway': 'Two-Way',\n",
    "    'driscoll_kraay': 'Driscoll-Kraay',\n",
    "    'pcse': 'PCSE'\n",
    "}\n",
    "\n",
    "for method, res in results_macro.items():\n",
    "    for var in ['investment', 'education', 'openness']:\n",
    "        coef = res.params[var]\n",
    "        se = res.std_errors[var]\n",
    "        t = coef / se\n",
    "        p = res.pvalues[var]\n",
    "        sig = '***' if p < 0.01 else ('**' if p < 0.05 else ('*' if p < 0.10 else 'ns'))\n",
    "        print(f\"{macro_method_display[method]:<22} | {var:<12} | {coef:>8.4f} | {se:>8.5f} | {t:>7.2f} | {p:>8.4f} | {sig}\")\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "print(\"\\nKey: consistent significance across DK and PCSE = robust finding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dk-vs-pcse",
   "metadata": {},
   "source": [
    "### 3.2 Driscoll-Kraay vs PCSE: When to Use Which?\n",
    "\n",
    "| Criterion | Driscoll-Kraay | PCSE |\n",
    "|-----------|----------------|------|\n",
    "| **T requirement** | T > 20 (practical) | T > N (ideally T >> N) |\n",
    "| **Cross-section correlation** | Yes (via HAC) | Yes (estimates full Σ) |\n",
    "| **Temporal correlation** | Yes (HAC kernel) | Optional (AR1 spec) |\n",
    "| **Asymptotics** | T → ∞ (N fixed) | T → ∞, T/N → ∞ |\n",
    "| **Typical use** | Macro panels, DID | Political science (legislator voting) |\n",
    "| **This dataset** | ✅ T=40 > 20 | ✅ T=40 > N=30 |\n",
    "\n",
    "**Practical Rules**:\n",
    "- T < 2N: Prefer Driscoll-Kraay (safer asymptotics)\n",
    "- T > 2N: PCSE acceptable, estimate full covariance matrix\n",
    "- Always: Compare both as robustness check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: SE comparison across methods for macro panel\n",
    "macro_method_keys = ['nonrobust', 'robust', 'cluster_country', 'twoway', 'driscoll_kraay', 'pcse']\n",
    "macro_labels = ['Non\\nRobust', 'Robust\\n(HC1)', 'Cluster\\n(Country)', 'Two-\\nWay', 'Driscoll\\nKraay', 'PCSE']\n",
    "colors_macro = ['#aaaaaa', '#5B9BD5', '#FFA500', '#C00000', '#70AD47', '#9B59B6']\n",
    "\n",
    "vars_macro = ['investment', 'education', 'openness']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for ax, var in zip(axes, vars_macro):\n",
    "    ses = [results_macro[k].std_errors[var] for k in macro_method_keys]\n",
    "    bars = ax.bar(macro_labels, ses, color=colors_macro,\n",
    "                  edgecolor='black', linewidth=1.2, alpha=0.85)\n",
    "\n",
    "    for bar, se in zip(bars, ses):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() * 1.01,\n",
    "                f'{se:.4f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "    ax.set_ylabel('Standard Error', fontsize=11)\n",
    "    ax.set_title(f'SE Comparison: {var}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Macro Panel: Standard Error Comparison\\n(30 Countries × 40 Years)',\n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'macro_se_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Figure saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-rec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RECOMMENDATION: MACRO PANEL\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Primary specification: Driscoll-Kraay with lag selection\n",
    "  - Handles both cross-section and temporal correlation via HAC\n",
    "  - Does NOT require T > N (works with T > 20)\n",
    "  - More robust to misspecification of correlation structure\n",
    "\n",
    "Robustness check: PCSE\n",
    "  - T=40 > N=30 → PCSE condition satisfied\n",
    "  - Estimates full cross-section error covariance matrix\n",
    "  - Compare results with DK — should be similar if both assumptions hold\n",
    "\n",
    "Secondary robustness: Two-way clustering\n",
    "  - Simpler assumption, good if G_country ≥ 20 (✓ N=30) and G_year ≥ 20 (✓ T=40)\n",
    "\n",
    "AVOID: Simple robust SEs — ignore both temporal and cross-section correlation!\n",
    "\n",
    "Table note: 'Standard errors are Driscoll-Kraay (4 lags). PCSE results\n",
    "  in Appendix Table A1 confirm all main conclusions.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4-header",
   "metadata": {},
   "source": "---\n\n<a id='section4'></a>\n## 4. Application 3: Wage Panel — Cluster-Robust vs Quantile Regression\n\n### 4.1 Context: Returns to Education and Experience\n\n**Research Question**: How do education and experience affect wages?\n\n**Data Structure**:\n- N = 2000 individuals, T = 5 years → typical micro panel\n\n**Expected Features**:\n- **Strong within-person correlation**: Persistent wage levels, unobserved ability\n- **Time-invariant variables**: Education, gender, union status don't change across years\n  → Use PooledOLS (not FE) to retain between-person identification\n- **Micro panel**: N >> T → cluster by entity is the right approach\n- **Extension**: Quantile regression to capture heterogeneous returns"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wage-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wage panel\n",
    "wage_data = pd.read_csv(DATA_PATH + 'wage_panel.csv')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WAGE PANEL DATA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {wage_data.shape}\")\n",
    "print(f\"Individuals: {wage_data['person_id'].nunique()}\")\n",
    "print(f\"Years: {wage_data['year'].nunique()} ({wage_data['year'].min()}–{wage_data['year'].max()})\")\n",
    "print(f\"Variables: {list(wage_data.columns)}\")\n",
    "print()\n",
    "\n",
    "# Check within-person wage persistence (motivation for clustering)\n",
    "wage_data['log_wage'] = np.log(wage_data['wage'])\n",
    "within_corr = wage_data.groupby('person_id')['log_wage'].apply(\n",
    "    lambda x: x.autocorr() if len(x) > 1 else np.nan\n",
    ").mean()\n",
    "print(f\"Mean within-person wage autocorrelation: {within_corr:.3f}\")\n",
    "print(\"(High autocorrelation → clustering is essential)\")\n",
    "print()\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(wage_data[['log_wage', 'education', 'experience']].describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wage-estimate",
   "metadata": {},
   "outputs": [],
   "source": "# Wage panel: most variables are time-invariant (education, female, union)\n# → Use PooledOLS with cluster-robust SEs (standard Mincer wage equation approach)\n# Note: PooledOLS preserves between-person variation needed to identify education/gender effects\npool_wage = PooledOLS(\"log_wage ~ education + experience + female + union\",\n                      wage_data, \"person_id\", \"year\")\n\nresults_wage = {}\n\n# 1. Non-robust\nresults_wage['nonrobust'] = pool_wage.fit(cov_type='nonrobust')\nprint(\"[1/4] Non-robust: done\")\n\n# 2. Robust\nresults_wage['robust'] = pool_wage.fit(cov_type='robust')\nprint(\"[2/4] Robust (HC1): done\")\n\n# 3. Cluster by person (correct for micro panel — absorbs within-person correlation)\nresults_wage['cluster_person'] = pool_wage.fit(cov_type='clustered')\nprint(\"[3/4] Cluster by person: done\")\n\n# 4. Two-way clustering (person + year)\nresults_wage['twoway'] = pool_wage.fit(cov_type='twoway')\nprint(\"[4/4] Two-way clustering: done\")\n\nprint(\"\\nWage panel estimations complete.\")\nprint(\"\\nCoefficients:\")\nfor var, coef in results_wage['cluster_person'].params.items():\n    se = results_wage['cluster_person'].std_errors[var]\n    print(f\"  {var:<12}: {coef:.5f}  (SE: {se:.5f})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wage-compare",
   "metadata": {},
   "outputs": [],
   "source": "# SE comparison — wage panel\ncomp_wage = StandardErrorComparison(results_wage['cluster_person'])\nresult_wage = comp_wage.compare_all(\n    se_types=['nonrobust', 'robust', 'clustered', 'twoway']\n)\n\nprint(\"=\" * 70)\nprint(\"WAGE PANEL: STANDARD ERROR COMPARISON (PooledOLS)\")\nprint(\"=\" * 70)\nprint()\n\nse_w = result_wage.se_comparison.copy()\nse_w.columns = ['NonRobust', 'Robust', 'Cluster\\n(Person)', 'Two-Way']\nprint(\"Standard Errors:\")\nprint(se_w.round(5).to_string())\nprint()\n\nprint(\"SE Ratios relative to Non-Robust:\")\nprint(result_wage.se_ratios.round(3).to_string())\nprint()\n\nprint(\"Significance (* p<0.10, ** p<0.05, *** p<0.01):\")\nprint(result_wage.significance.to_string())\nprint()\n\n# Inflation factor — education and experience\nfor var in ['education', 'experience']:\n    se_nr = results_wage['nonrobust'].std_errors[var]\n    se_cl = results_wage['cluster_person'].std_errors[var]\n    print(f\"{var}: Cluster SE is {se_cl/se_nr:.2f}x larger than non-robust SE\")\nprint(\"(Reflects strong within-person serial correlation)\")"
  },
  {
   "cell_type": "markdown",
   "id": "wage-qr-header",
   "metadata": {},
   "source": "### 4.2 Extension: Quantile Regression with Cluster-Robust SE\n\nPooledOLS estimates the **average** effect. Quantile regression reveals **heterogeneous effects** across the wage distribution — e.g., whether education premiums differ for low vs. high earners.\n\nWe use `PooledQuantile` with cluster-robust SEs (clustered by person) to account for within-person correlation across years."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wage-qr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PooledQuantile (takes numpy arrays)\n",
    "y_wage = wage_data['log_wage'].values\n",
    "X_wage = np.column_stack([\n",
    "    np.ones(len(wage_data)),           # Intercept\n",
    "    wage_data['education'].values,\n",
    "    wage_data['experience'].values\n",
    "])\n",
    "entity_ids = wage_data['person_id'].values\n",
    "time_ids = wage_data['year'].values\n",
    "\n",
    "# Estimate quantile regression at three quantiles\n",
    "quantiles = [0.25, 0.50, 0.75]\n",
    "qr_results = {}\n",
    "\n",
    "for tau in quantiles:\n",
    "    qr = PooledQuantile(\n",
    "        endog=y_wage,\n",
    "        exog=X_wage,\n",
    "        entity_id=entity_ids,\n",
    "        time_id=time_ids,\n",
    "        quantiles=tau\n",
    "    )\n",
    "    res = qr.fit(se_type='cluster')\n",
    "    qr_results[tau] = res\n",
    "    print(f\"Q{int(tau*100)}: params={res.params.flatten().round(4)}, \"\n",
    "          f\"se={res.std_errors.round(4)}\")\n",
    "\n",
    "print(\"\\nQuantile estimates complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wage-qr-boot",
   "metadata": {},
   "outputs": [],
   "source": "# Compare median quantile regression with cluster SE vs PooledOLS cluster SE\n# (QuantileBootstrap not compatible with PooledQuantile in this version)\n# Instead we compare cluster-robust SEs across methods\n\nprint(\"=\" * 70)\nprint(\"COMPARISON: PooledOLS Cluster vs Quantile Regression (Cluster SE)\")\nprint(\"=\" * 70)\nprint(f\"{'Variable':<15} | {'OLS Coef':>9} | {'OLS SE':>9} | {'QR(0.5)':>9} | {'QR SE':>9}\")\nprint(\"-\" * 60)\n\n# Median QR already fitted in qr_results — extract cluster SEs\n# Indices: 0=Intercept, 1=education, 2=experience\nfor i, var in enumerate(['education', 'experience']):\n    ols_coef = results_wage['cluster_person'].params[var]\n    ols_se = results_wage['cluster_person'].std_errors[var]\n    qr_coef = qr_results[0.50].params.flatten()[i+1]  # skip intercept\n    qr_se = qr_results[0.50].std_errors[i+1]\n    print(f\"{var:<15} | {ols_coef:>9.5f} | {ols_se:>9.5f} | {qr_coef:>9.5f} | {qr_se:>9.5f}\")\n\nprint()\nprint(\"Notes:\")\nprint(\"  QR(0.5) = median regression estimate\")\nprint(\"  QR SEs = cluster-robust (clustered by entity)\")\nprint(\"  Close agreement between OLS and QR(0.5) suggests homogeneous returns\")\nprint()\n\n# Compute inflation factor of cluster vs nonrobust for QR\nqr_nr = PooledQuantile(\n    endog=y_wage, exog=X_wage,\n    entity_id=entity_ids, time_id=time_ids,\n    quantiles=0.50\n)\nres_qr_nr = qr_nr.fit(se_type='nonrobust')\nres_qr_cl = qr_results[0.50]\n\nprint(\"QR SE Inflation (cluster vs. nonrobust):\")\nfor i, var in enumerate(['education', 'experience']):\n    ratio = res_qr_cl.std_errors[i+1] / res_qr_nr.std_errors[i+1]\n    print(f\"  {var}: {ratio:.2f}x larger cluster SE\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wage-qr-viz",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize quantile coefficients vs PooledOLS\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\nfor ax_idx, (var_idx, var_name) in enumerate(zip([1, 2], ['Education', 'Experience'])):\n    ax = axes[ax_idx]\n\n    # Quantile estimates with cluster SEs\n    qr_coefs = [qr_results[tau].params.flatten()[var_idx] for tau in quantiles]\n    qr_ses = [qr_results[tau].std_errors[var_idx] for tau in quantiles]\n\n    ax.errorbar(quantiles, qr_coefs,\n                yerr=[1.96 * se for se in qr_ses],\n                fmt='o-', color='#C00000', linewidth=2, markersize=8,\n                capsize=5, label='Quantile Regression ± 1.96 SE (cluster)', zorder=5)\n\n    # PooledOLS cluster estimate (constant across distribution)\n    ols_coef = results_wage['cluster_person'].params[var_name.lower()]\n    ols_se = results_wage['cluster_person'].std_errors[var_name.lower()]\n    ax.axhline(ols_coef, color='#5B9BD5', linestyle='--', linewidth=2.5,\n               label=f'OLS (cluster) = {ols_coef:.4f}', zorder=3)\n    ax.fill_between([0.20, 0.80],\n                    ols_coef - 1.96*ols_se, ols_coef + 1.96*ols_se,\n                    alpha=0.15, color='#5B9BD5', label='OLS 95% CI')\n\n    ax.set_xlabel('Quantile', fontsize=12)\n    ax.set_ylabel('Coefficient', fontsize=12)\n    ax.set_title(f'Returns to {var_name}\\nAcross Wage Distribution', fontsize=12, fontweight='bold')\n    ax.set_xticks(quantiles)\n    ax.set_xticklabels(['Q25\\n(25th)', 'Q50\\n(Median)', 'Q75\\n(75th)'])\n    ax.legend(fontsize=9)\n    ax.grid(alpha=0.3)\n\nplt.suptitle('Wage Panel: Quantile Regression vs PooledOLS\\n'\n             'Cluster-Robust SEs',\n             fontsize=13, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.savefig(FIG_PATH + 'wage_quantile_vs_ols.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Figure saved.\")"
  },
  {
   "cell_type": "markdown",
   "id": "section5-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section5'></a>\n",
    "## 5. Decision-Making Framework\n",
    "\n",
    "### 5.1 Comprehensive Decision Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decision-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive decision table\n",
    "decision_table = pd.DataFrame({\n",
    "    'Data Structure': [\n",
    "        'Cross-section only',\n",
    "        'Time series only',\n",
    "        'Micro panel (N large, T small)',\n",
    "        'Micro panel + cross-section corr.',\n",
    "        'Macro panel (N small, T large)',\n",
    "        'Macro panel (T >> N)',\n",
    "        'Spatial data',\n",
    "        'Nonlinear model (Logit, Probit)',\n",
    "        'Quantile regression'\n",
    "    ],\n",
    "    'Primary Method': [\n",
    "        'Robust (HC1)',\n",
    "        'Newey-West HAC',\n",
    "        'Cluster by entity',\n",
    "        'Two-way clustering',\n",
    "        'Driscoll-Kraay',\n",
    "        'PCSE',\n",
    "        'Spatial HAC',\n",
    "        'MLE Sandwich',\n",
    "        'Cluster Bootstrap'\n",
    "    ],\n",
    "    'Robustness Check': [\n",
    "        'HC2, HC3',\n",
    "        'Different lags',\n",
    "        'Two-way clustering',\n",
    "        'Driscoll-Kraay',\n",
    "        'PCSE (if T>N)',\n",
    "        'Driscoll-Kraay',\n",
    "        'Vary cutoff distance',\n",
    "        'Cluster-robust MLE',\n",
    "        'Pairs bootstrap'\n",
    "    ],\n",
    "    'Key Requirement': [\n",
    "        'n > 50',\n",
    "        'T > 50',\n",
    "        'G ≥ 20 entities',\n",
    "        'G ≥ 20 in both dims',\n",
    "        'T > 20',\n",
    "        'T > N (ideally T > 2N)',\n",
    "        'Geographic coordinates',\n",
    "        'Model correctly specified',\n",
    "        'B ≥ 499 bootstrap reps'\n",
    "    ],\n",
    "    'Example Dataset': [\n",
    "        'CPS wage survey',\n",
    "        'GDP quarterly',\n",
    "        'Wage panel (N=2000)',\n",
    "        'Financial panel (N=50)',\n",
    "        'Macro growth (N=30, T=40)',\n",
    "        'Legislative voting',\n",
    "        'Regional housing prices',\n",
    "        'Binary outcome panels',\n",
    "        'Wage quantiles'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"DECISION FRAMEWORK: CHOOSING STANDARD ERROR METHOD\")\n",
    "print(\"=\" * 120)\n",
    "print(decision_table.to_string(index=False))\n",
    "print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "red-flags-header",
   "metadata": {},
   "source": [
    "### 5.2 Red Flags: When to Be Cautious\n",
    "\n",
    "**Warning Signs That SE Choice Is Critical**:\n",
    "\n",
    "| Red Flag | Threshold | Implication |\n",
    "|----------|-----------|-------------|\n",
    "| SE ratio (cluster/robust) | > 2x | Strong temporal correlation — clustering essential |\n",
    "| Significance flip | Any | SE choice affects conclusions — report both |\n",
    "| Few clusters | G < 20 | Cluster asymptotics unreliable — use bootstrap |\n",
    "| Short T with DK | T < 20 | Asymptotic approximation poor — use clustering |\n",
    "| T < N with PCSE | T < N | Σ matrix singular — use Driscoll-Kraay |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "red-flags-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_red_flags(result_baseline, result_alternative,\n",
    "                    name_baseline='Robust', name_alternative='Cluster',\n",
    "                    se_threshold=2.0, p_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Compare two SE methods and identify red flags.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result_baseline : PanelResults\n",
    "        Baseline SE method (usually robust)\n",
    "    result_alternative : PanelResults\n",
    "        Alternative SE method (e.g., clustered)\n",
    "    name_baseline : str\n",
    "        Name for baseline\n",
    "    name_alternative : str\n",
    "        Name for alternative\n",
    "    se_threshold : float\n",
    "        Ratio threshold for flagging large SE differences\n",
    "    p_threshold : float\n",
    "        Significance threshold for checking inference flips\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with flags and details\n",
    "    \"\"\"\n",
    "    print(f\"RED FLAG CHECK: {name_baseline} vs {name_alternative}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    flags = {}\n",
    "\n",
    "    for var in result_baseline.params.index:\n",
    "        se_b = result_baseline.std_errors[var]\n",
    "        se_a = result_alternative.std_errors[var]\n",
    "        ratio = se_a / se_b\n",
    "\n",
    "        p_b = result_baseline.pvalues[var]\n",
    "        p_a = result_alternative.pvalues[var]\n",
    "        sig_b = p_b < p_threshold\n",
    "        sig_a = p_a < p_threshold\n",
    "\n",
    "        large_diff = ratio > se_threshold\n",
    "        sig_flip = sig_b != sig_a\n",
    "\n",
    "        flag_markers = []\n",
    "        if large_diff:\n",
    "            flag_markers.append(f\"[!] SE ratio={ratio:.2f}x > {se_threshold}x\")\n",
    "        if sig_flip:\n",
    "            prev_sig = \"significant\" if sig_b else \"NOT significant\"\n",
    "            new_sig = \"significant\" if sig_a else \"NOT significant\"\n",
    "            flag_markers.append(f\"[!] Inference flips: {prev_sig} → {new_sig}\")\n",
    "\n",
    "        status = \"OK\" if not flag_markers else \"FLAG\"\n",
    "        print(f\"  {var:<20}: ratio={ratio:.3f}x  {name_baseline}:p={p_b:.4f}  \"\n",
    "              f\"{name_alternative}:p={p_a:.4f}  → {status}\")\n",
    "\n",
    "        if flag_markers:\n",
    "            for m in flag_markers:\n",
    "                print(f\"    {m}\")\n",
    "\n",
    "        flags[var] = {\n",
    "            'ratio': ratio, 'large_diff': large_diff,\n",
    "            'sig_flip': sig_flip, 'flags': flag_markers\n",
    "        }\n",
    "\n",
    "    total_flags = sum(1 for v in flags.values() if v['flags'])\n",
    "    print(f\"\\nSummary: {total_flags}/{len(flags)} variables flagged\")\n",
    "    if total_flags > 0:\n",
    "        print(\"WARNING: SE choice may affect inference — report multiple methods!\")\n",
    "    else:\n",
    "        print(\"OK: Inference consistent across SE methods.\")\n",
    "\n",
    "    return flags\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"APPLICATION 1: Financial Panel\")\n",
    "print(\"=\" * 70)\n",
    "flags_fin = check_red_flags(\n",
    "    results_fin['robust'], results_fin['twoway'],\n",
    "    'Robust', 'Two-Way', se_threshold=2.0\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"APPLICATION 2: Macro Panel\")\n",
    "print(\"=\" * 70)\n",
    "flags_macro = check_red_flags(\n",
    "    results_macro['robust'], results_macro['driscoll_kraay'],\n",
    "    'Robust', 'Driscoll-Kraay', se_threshold=2.0\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"APPLICATION 3: Wage Panel\")\n",
    "print(\"=\" * 70)\n",
    "flags_wage = check_red_flags(\n",
    "    results_wage['nonrobust'], results_wage['cluster_person'],\n",
    "    'Non-Robust', 'Cluster (Person)', se_threshold=2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-practices",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BEST PRACTICES CHECKLIST\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Before Running Regressions:\n",
    "  [ ] Identify data structure (cross-section / time series / panel)\n",
    "  [ ] Determine N vs T ratio\n",
    "  [ ] Check number of clusters (G ≥ 20?)\n",
    "  [ ] Check T > N for PCSE (if applicable)\n",
    "  [ ] Pre-specify SE method in analysis plan\n",
    "\n",
    "When Reporting Results:\n",
    "  [ ] Estimate with at least 3 SE methods\n",
    "  [ ] Use StandardErrorComparison for systematic check\n",
    "  [ ] Run check_red_flags() against baseline method\n",
    "  [ ] Report primary specification + robustness check\n",
    "  [ ] Specify SE method in table notes\n",
    "  [ ] Report number of clusters if applicable\n",
    "  [ ] Discuss if conclusions sensitive to SE choice\n",
    "\n",
    "In Tables:\n",
    "  [ ] Standard errors in parentheses (not t-statistics)\n",
    "  [ ] Specify SE method in table notes\n",
    "  [ ] Include number of clusters / observations\n",
    "  [ ] Consistent significance levels (*, **, ***)\n",
    "  [ ] Consider multi-column format for robustness\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section6'></a>\n",
    "## 6. Publication-Ready Tables\n",
    "\n",
    "### 6.1 Regression Table with Multiple SE Specifications\n",
    "\n",
    "The standard format in applied economics shows the same model with different SE methods as separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pub-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_table(models, model_names, title, dep_var, decimals=4,\n",
    "                             sig_levels=(0.10, 0.05, 0.01)):\n",
    "    \"\"\"\n",
    "    Create a publication-ready regression table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models : list of PanelResults\n",
    "        List of fitted model results\n",
    "    model_names : list of str\n",
    "        Column headers\n",
    "    title : str\n",
    "        Table title\n",
    "    dep_var : str\n",
    "        Dependent variable description\n",
    "    decimals : int\n",
    "        Decimal places for display\n",
    "    sig_levels : tuple\n",
    "        Significance thresholds (10%, 5%, 1%)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str : Formatted table\n",
    "    \"\"\"\n",
    "    def stars(p):\n",
    "        if p < sig_levels[2]: return '***'\n",
    "        if p < sig_levels[1]: return '**'\n",
    "        if p < sig_levels[0]: return '*'\n",
    "        return ''\n",
    "\n",
    "    vars_list = list(models[0].params.index)\n",
    "    n_models = len(models)\n",
    "    col_w = 16\n",
    "    var_w = 22\n",
    "    total_w = var_w + col_w * n_models + 2\n",
    "\n",
    "    lines = []\n",
    "    lines.append('=' * total_w)\n",
    "    lines.append(title.center(total_w))\n",
    "    lines.append(f\"Dependent Variable: {dep_var}\".center(total_w))\n",
    "    lines.append('-' * total_w)\n",
    "\n",
    "    # Column headers\n",
    "    header = ' ' * var_w\n",
    "    for i, name in enumerate(model_names, 1):\n",
    "        header += f\"({i}){name:<{col_w-3}}\"\n",
    "    lines.append(header)\n",
    "    lines.append('-' * total_w)\n",
    "\n",
    "    # Coefficients and SEs\n",
    "    for var in vars_list:\n",
    "        coef_line = f\"{var:<{var_w}}\"\n",
    "        se_line = ' ' * var_w\n",
    "        for res in models:\n",
    "            coef = res.params[var]\n",
    "            se = res.std_errors[var]\n",
    "            p = res.pvalues[var]\n",
    "            coef_str = f\"{coef:.{decimals}f}{stars(p)}\"\n",
    "            se_str = f\"({se:.{decimals}f})\"\n",
    "            coef_line += f\"{coef_str:^{col_w}}\"\n",
    "            se_line += f\"{se_str:^{col_w}}\"\n",
    "        lines.append(coef_line)\n",
    "        lines.append(se_line)\n",
    "        lines.append('')\n",
    "\n",
    "    lines.append('-' * total_w)\n",
    "\n",
    "    # Model statistics\n",
    "    stat_rows = [\n",
    "        ('Observations', [f\"{res.nobs:,}\" for res in models]),\n",
    "        ('Entities', [f\"{res.n_entities}\" for res in models]),\n",
    "        ('R-squared (within)', [f\"{res.rsquared_within:.3f}\" for res in models]),\n",
    "        ('SE method', model_names),\n",
    "    ]\n",
    "    for label, values in stat_rows:\n",
    "        row = f\"{label:<{var_w}}\"\n",
    "        for v in values:\n",
    "            row += f\"{v:^{col_w}}\"\n",
    "        lines.append(row)\n",
    "\n",
    "    lines.append('=' * total_w)\n",
    "    lines.append(f\"Notes: * p<{sig_levels[0]}, ** p<{sig_levels[1]}, *** p<{sig_levels[2]}.\")\n",
    "    lines.append(\"Standard errors in parentheses.\")\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# Financial panel table: 3 SE specifications\n",
    "models_to_report = [\n",
    "    results_fin['robust'],\n",
    "    results_fin['cluster_firm'],\n",
    "    results_fin['twoway']\n",
    "]\n",
    "model_names = ['Robust', 'Cluster(Firm)', 'Two-Way']\n",
    "\n",
    "fin_table = create_regression_table(\n",
    "    models=models_to_report,\n",
    "    model_names=model_names,\n",
    "    title='Fama-French Factor Model',\n",
    "    dep_var='Stock Excess Return'\n",
    ")\n",
    "\n",
    "print(fin_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robustness-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness appendix table — ALL SE methods\n",
    "all_models = [\n",
    "    results_fin['nonrobust'],\n",
    "    results_fin['robust'],\n",
    "    results_fin['cluster_firm'],\n",
    "    results_fin['twoway'],\n",
    "    results_fin['driscoll_kraay'],\n",
    "    results_fin['newey_west']\n",
    "]\n",
    "all_names = ['Classical', 'Robust', 'Cluster\\nFirm', 'Two-\\nWay', 'Driscoll\\nKraay', 'Newey\\nWest']\n",
    "\n",
    "robustness_table = create_regression_table(\n",
    "    models=all_models,\n",
    "    model_names=all_names,\n",
    "    title='Appendix: Robustness — All Standard Error Methods',\n",
    "    dep_var='Stock Excess Return'\n",
    ")\n",
    "\n",
    "print(robustness_table)\n",
    "\n",
    "# Save to file\n",
    "with open('../outputs/reports/robustness_table.txt', 'w') as f:\n",
    "    f.write(robustness_table)\n",
    "print(\"\\nRobustness table saved to ../outputs/reports/robustness_table.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latex-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latex_table(models, model_names, title, dep_var, decimals=4,\n",
    "                        sig_levels=(0.10, 0.05, 0.01)):\n",
    "    \"\"\"\n",
    "    Generate LaTeX code for regression table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models : list of PanelResults\n",
    "    model_names : list of str\n",
    "    title : str\n",
    "    dep_var : str\n",
    "    decimals : int\n",
    "    sig_levels : tuple\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str : LaTeX table\n",
    "    \"\"\"\n",
    "    def stars(p):\n",
    "        if p < sig_levels[2]: return '$^{***}$'\n",
    "        if p < sig_levels[1]: return '$^{**}$'\n",
    "        if p < sig_levels[0]: return '$^{*}$'\n",
    "        return ''\n",
    "\n",
    "    n_models = len(models)\n",
    "    col_spec = 'l' + 'c' * n_models\n",
    "\n",
    "    lines = [\n",
    "        r'\\begin{table}[htbp]',\n",
    "        r'\\centering',\n",
    "        r'\\caption{' + title + '}',\n",
    "        r'\\label{tab:regression}',\n",
    "        r'\\begin{tabular}{' + col_spec + '}',\n",
    "        r'\\hline\\hline',\n",
    "    ]\n",
    "\n",
    "    # Header\n",
    "    header = 'Variable & ' + ' & '.join(f'({i+1}) {name}'\n",
    "                                         for i, name in enumerate(model_names))\n",
    "    lines.append(header + r' \\\\')\n",
    "    lines.append(r'\\hline')\n",
    "\n",
    "    vars_list = list(models[0].params.index)\n",
    "    for var in vars_list:\n",
    "        coef_row = var.replace('_', '\\_') + ' & '\n",
    "        se_row = ' & '\n",
    "        for res in models:\n",
    "            coef = res.params[var]\n",
    "            se = res.std_errors[var]\n",
    "            p = res.pvalues[var]\n",
    "            coef_row += f\"{coef:.{decimals}f}{stars(p)} & \"\n",
    "            se_row += f\"({se:.{decimals}f}) & \"\n",
    "        lines.append(coef_row.rstrip(' & ') + r' \\\\')\n",
    "        lines.append(se_row.rstrip(' & ') + r' \\\\')\n",
    "        lines.append('')\n",
    "\n",
    "    lines.append(r'\\hline')\n",
    "    obs_row = 'Observations & ' + ' & '.join(f\"{m.nobs:,}\" for m in models)\n",
    "    r2_row = 'R$^2$ (within) & ' + ' & '.join(f\"{m.rsquared_within:.3f}\" for m in models)\n",
    "    lines.append(obs_row + r' \\\\')\n",
    "    lines.append(r2_row + r' \\\\')\n",
    "    lines.append(r'\\hline\\hline')\n",
    "    lines.append(r'\\end{tabular}')\n",
    "    lines.append(r'\\begin{tablenotes}')\n",
    "    lines.append(r'\\small')\n",
    "    lines.append(r'\\item \\textit{Notes}: Dependent variable: ' + dep_var +\n",
    "                 r'. Standard errors in parentheses. '\n",
    "                 r'$^{*}$ p$<$0.10, $^{**}$ p$<$0.05, $^{***}$ p$<$0.01.')\n",
    "    lines.append(r'\\end{tablenotes}')\n",
    "    lines.append(r'\\end{table}')\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "latex_table = create_latex_table(\n",
    "    models=models_to_report,\n",
    "    model_names=['Robust', 'Cluster (Firm)', 'Two-Way'],\n",
    "    title='Fama-French Factor Model: Return Determinants',\n",
    "    dep_var='Stock Excess Return'\n",
    ")\n",
    "\n",
    "with open('../outputs/reports/regression_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to ../outputs/reports/regression_table.tex\")\n",
    "print()\n",
    "print(\"LaTeX output preview:\")\n",
    "print(\"-\" * 50)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section7'></a>\n",
    "## 7. Case Study: When SE Choice Matters Most\n",
    "\n",
    "### 7.1 Monte Carlo: Type I Error Under Misspecification\n",
    "\n",
    "**Objective**: Demonstrate empirically that using wrong SEs leads to inflated Type I error rates (false discoveries).\n",
    "\n",
    "**Design**:\n",
    "- Panel with N=50 entities, T=20 periods\n",
    "- True β = 1.0 (test H₀: β = 1.0, which is TRUE)\n",
    "- Errors autocorrelated within entities (ρ = 0.5)\n",
    "- Expected rejection rate: 5% (nominal α = 0.05)\n",
    "- 1,000 Monte Carlo replications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "montecarlo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo simulation\n",
    "np.random.seed(42)\n",
    "N_entities = 50\n",
    "T_periods = 20\n",
    "n_sim = 1000\n",
    "true_beta = 1.0\n",
    "autocorr_rho = 0.5\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MONTE CARLO SIMULATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"N = {N_entities} entities, T = {T_periods} periods\")\n",
    "print(f\"True β = {true_beta} (H₀: β = {true_beta} is TRUE)\")\n",
    "print(f\"Autocorrelation ρ = {autocorr_rho}\")\n",
    "print(f\"Replications = {n_sim}\")\n",
    "print(f\"Nominal significance level = 0.05\")\n",
    "print(f\"\\nRunning simulation...\")\n",
    "\n",
    "reject_nonrobust = []\n",
    "reject_robust = []\n",
    "reject_cluster = []\n",
    "\n",
    "for sim in range(n_sim):\n",
    "    rows = []\n",
    "    for i in range(N_entities):\n",
    "        x_i = np.random.normal(0, 1, T_periods)\n",
    "        # Autocorrelated errors within entity\n",
    "        eps_i = np.zeros(T_periods)\n",
    "        eps_i[0] = np.random.normal(0, 1)\n",
    "        for t in range(1, T_periods):\n",
    "            eps_i[t] = autocorr_rho * eps_i[t-1] + np.sqrt(1 - autocorr_rho**2) * np.random.normal(0, 1)\n",
    "        y_i = true_beta * x_i + eps_i\n",
    "\n",
    "        for t in range(T_periods):\n",
    "            rows.append({'entity': i, 'time': t, 'y': y_i[t], 'x': x_i[t]})\n",
    "\n",
    "    sim_df = pd.DataFrame(rows)\n",
    "\n",
    "    fe_sim = FixedEffects(\"y ~ x\", sim_df, \"entity\", \"time\")\n",
    "\n",
    "    res_nr = fe_sim.fit(cov_type='nonrobust')\n",
    "    res_rob = fe_sim.fit(cov_type='robust')\n",
    "    res_cl = fe_sim.fit(cov_type='clustered')\n",
    "\n",
    "    # Test H0: beta = true_beta (TRUE null)\n",
    "    t_nr = (res_nr.params['x'] - true_beta) / res_nr.std_errors['x']\n",
    "    t_rob = (res_rob.params['x'] - true_beta) / res_rob.std_errors['x']\n",
    "    t_cl = (res_cl.params['x'] - true_beta) / res_cl.std_errors['x']\n",
    "\n",
    "    reject_nonrobust.append(abs(t_nr) > 1.96)\n",
    "    reject_robust.append(abs(t_rob) > 1.96)\n",
    "    reject_cluster.append(abs(t_cl) > 1.96)\n",
    "\n",
    "    if (sim + 1) % 250 == 0:\n",
    "        print(f\"  {sim + 1}/{n_sim} replications done\")\n",
    "\n",
    "print(\"Simulation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "montecarlo-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Type I error rates\n",
    "rate_nr = np.mean(reject_nonrobust)\n",
    "rate_rob = np.mean(reject_robust)\n",
    "rate_cl = np.mean(reject_cluster)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TYPE I ERROR RATES (H₀ is TRUE — should reject ≈ 5%)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Non-robust SEs:      {rate_nr:.3f} ({rate_nr*100:.1f}%)\", end='')\n",
    "print(f\" {'← OVER-REJECTS!' if rate_nr > 0.07 else '✓'}\")\n",
    "print(f\"  Robust (HC1) SEs:    {rate_rob:.3f} ({rate_rob*100:.1f}%)\", end='')\n",
    "print(f\" {'← OVER-REJECTS!' if rate_rob > 0.07 else '✓'}\")\n",
    "print(f\"  Cluster SEs:         {rate_cl:.3f} ({rate_cl*100:.1f}%)\", end='')\n",
    "print(f\" {'← OVER-REJECTS!' if rate_cl > 0.07 else '← correct size ✓' if 0.03 <= rate_cl <= 0.07 else '← conservative'}\")\n",
    "print()\n",
    "print(f\"  Excess false positives (Robust vs Cluster): \"\n",
    "      f\"{(rate_rob - rate_cl)*100:.1f} percentage points\")\n",
    "print(f\"  Multiplier: Robust rejects {rate_rob/rate_cl:.1f}x more than Cluster\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"IMPLICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "With autocorrelated panel errors (ρ = {autocorr_rho}):\n",
    "  - Non-robust SEs reject {rate_nr*100:.1f}% instead of 5% → {rate_nr/0.05:.1f}x over-rejection\n",
    "  - Robust HC1 SEs reject {rate_rob*100:.1f}% instead of 5% → {rate_rob/0.05:.1f}x over-rejection\n",
    "  - Cluster SEs reject {rate_cl*100:.1f}% → approximately correct size\n",
    "\n",
    "In a published sample of 100 studies:\n",
    "  - Using robust SEs: expect ~{int(rate_rob*100)} false discoveries\n",
    "  - Using cluster SEs: expect ~{int(rate_cl*100)} false discoveries\n",
    "\n",
    "This is the statistical mechanism behind replication crises in social science!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "montecarlo-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Monte Carlo results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Type I Error Rates\n",
    "ax = axes[0]\n",
    "methods = ['Non-Robust', 'Robust\\n(HC1)', 'Cluster\\n(Entity)']\n",
    "rates = [rate_nr, rate_rob, rate_cl]\n",
    "colors_bar = ['#d62728', '#FF7F0E', '#2ca02c']\n",
    "\n",
    "bars = ax.bar(methods, rates, color=colors_bar, alpha=0.8,\n",
    "              edgecolor='black', linewidth=1.5)\n",
    "ax.axhline(0.05, color='black', linestyle='--', linewidth=2.5,\n",
    "           label='Nominal 5% level', zorder=10)\n",
    "ax.axhspan(0.03, 0.07, alpha=0.1, color='green', label='Acceptable range')\n",
    "\n",
    "for bar, rate in zip(bars, rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.005,\n",
    "            f'{rate:.3f}\\n({rate*100:.1f}%)',\n",
    "            ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.set_ylabel('Rejection Rate', fontsize=12)\n",
    "ax.set_title(f'Type I Error Rates\\nH₀ is TRUE (β={true_beta}), ρ={autocorr_rho}',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.set_ylim(0, max(rates) * 1.4)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Distribution of t-statistics under H0\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Run one more simulation to get t-statistic distributions\n",
    "np.random.seed(99)\n",
    "t_nr_dist, t_cl_dist = [], []\n",
    "\n",
    "for sim in range(500):\n",
    "    rows = []\n",
    "    for i in range(N_entities):\n",
    "        x_i = np.random.normal(0, 1, T_periods)\n",
    "        eps_i = np.zeros(T_periods)\n",
    "        eps_i[0] = np.random.normal(0, 1)\n",
    "        for t in range(1, T_periods):\n",
    "            eps_i[t] = autocorr_rho * eps_i[t-1] + np.sqrt(1 - autocorr_rho**2) * np.random.normal(0, 1)\n",
    "        y_i = true_beta * x_i + eps_i\n",
    "        for t in range(T_periods):\n",
    "            rows.append({'entity': i, 'time': t, 'y': y_i[t], 'x': x_i[t]})\n",
    "\n",
    "    sim_df = pd.DataFrame(rows)\n",
    "    fe_sim = FixedEffects(\"y ~ x\", sim_df, \"entity\", \"time\")\n",
    "    res_nr = fe_sim.fit(cov_type='nonrobust')\n",
    "    res_cl = fe_sim.fit(cov_type='clustered')\n",
    "    t_nr_dist.append((res_nr.params['x'] - true_beta) / res_nr.std_errors['x'])\n",
    "    t_cl_dist.append((res_cl.params['x'] - true_beta) / res_cl.std_errors['x'])\n",
    "\n",
    "# Plot t-statistic distributions\n",
    "from scipy import stats as scipy_stats\n",
    "x_range = np.linspace(-5, 5, 200)\n",
    "normal_pdf = scipy_stats.norm.pdf(x_range)\n",
    "\n",
    "ax2.hist(t_nr_dist, bins=40, alpha=0.5, color='#d62728',\n",
    "         label='Non-Robust t-stats', density=True)\n",
    "ax2.hist(t_cl_dist, bins=40, alpha=0.5, color='#2ca02c',\n",
    "         label='Cluster t-stats', density=True)\n",
    "ax2.plot(x_range, normal_pdf, 'k-', linewidth=2.5, label='Standard Normal')\n",
    "ax2.axvline(-1.96, color='black', linestyle=':', linewidth=1.5)\n",
    "ax2.axvline(1.96, color='black', linestyle=':', linewidth=1.5, label='±1.96 critical values')\n",
    "\n",
    "ax2.set_xlabel('t-statistic', fontsize=12)\n",
    "ax2.set_ylabel('Density', fontsize=12)\n",
    "ax2.set_title('Distribution of t-statistics under H₀\\n'\n",
    "              '(Should follow Standard Normal)',\n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.set_xlim(-5, 5)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Monte Carlo Evidence: Consequences of Wrong SE Method\\n'\n",
    "             f'(N={N_entities}, T={T_periods}, ρ={autocorr_rho}, B={n_sim} replications)',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_PATH + 'monte_carlo_type1_error.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Monte Carlo figure saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7-discussion",
   "metadata": {},
   "source": [
    "### 7.2 Discussion: SE Choice and the Replication Crisis\n",
    "\n",
    "The Monte Carlo evidence above illustrates a key mechanism behind the **replication crisis** in social science:\n",
    "\n",
    "1. **Publication bias** → journals favor statistically significant results\n",
    "2. **Wrong SE choice** → inflated test statistics → spurious significance\n",
    "3. **Replication attempts** use correct SEs → null results\n",
    "4. **Conclusion**: Original \"finding\" was a Type I error\n",
    "\n",
    "**Prevention**:\n",
    "- Pre-register analysis including SE method choice\n",
    "- Report multiple SE methods as robustness checks\n",
    "- Follow field conventions (cluster in panel data, always)\n",
    "- Report exact SE method in all tables\n",
    "\n",
    "**Key Reference**: Cameron & Miller (2015) — the definitive practitioner's guide to cluster-robust inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section8'></a>\n",
    "## 8. Exercises\n",
    "\n",
    "### Exercise 1: Complete SE Analysis (Moderate)\n",
    "\n",
    "**Task**: Apply the full SE comparison framework to a new dataset.\n",
    "\n",
    "**Requirements**:\n",
    "1. Load `agricultural_panel.csv` from the data directory\n",
    "2. Estimate a model of your choice (explore the variables first)\n",
    "3. Estimate with nonrobust, robust, clustered, and twoway SEs\n",
    "4. Create a comparison using `StandardErrorComparison`\n",
    "5. Run `check_red_flags()` comparing robust vs clustered\n",
    "6. Recommend primary SE method with justification\n",
    "\n",
    "**Deliverable**: Summary table + 1-paragraph recommendation\n",
    "\n",
    "**Starter Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Complete SE Analysis\n",
    "# Load agricultural panel data\n",
    "agri_data = pd.read_csv(DATA_PATH + 'agricultural_panel.csv')\n",
    "print(\"Agricultural Panel Data:\")\n",
    "print(f\"Shape: {agri_data.shape}\")\n",
    "print(f\"Variables: {list(agri_data.columns)}\")\n",
    "print()\n",
    "print(agri_data.head())\n",
    "\n",
    "# YOUR CODE:\n",
    "# Step 1: Explore N, T, variables\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Step 2: Choose dependent and independent variables, build formula\n",
    "# formula = \"...\"\n",
    "# entity_col = \"...\"\n",
    "# time_col = \"...\"\n",
    "\n",
    "# Step 3: Estimate FixedEffects model with 4 SE methods\n",
    "# fe_agri = FixedEffects(formula, agri_data, entity_col, time_col)\n",
    "# results_agri = {}\n",
    "# results_agri['nonrobust'] = fe_agri.fit(cov_type='nonrobust')\n",
    "# results_agri['robust'] = fe_agri.fit(cov_type='robust')\n",
    "# results_agri['clustered'] = fe_agri.fit(cov_type='clustered')\n",
    "# results_agri['twoway'] = fe_agri.fit(cov_type='twoway')\n",
    "\n",
    "# Step 4: StandardErrorComparison\n",
    "# comp_agri = StandardErrorComparison(results_agri['clustered'])\n",
    "# result_agri = comp_agri.compare_all(se_types=['nonrobust', 'robust', 'clustered', 'twoway'])\n",
    "# comp_agri.summary(result_agri)\n",
    "\n",
    "# Step 5: Red flag check\n",
    "# check_red_flags(results_agri['robust'], results_agri['clustered'])\n",
    "\n",
    "# Step 6: Write recommendation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"YOUR RECOMMENDATION (write here):\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Based on the data structure (N=?, T=?) and the comparison results:\n",
    "  Primary SE method: [...]\n",
    "  Reasoning: [...]\n",
    "  Robustness check: [...]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise2-header",
   "metadata": {},
   "source": [
    "### Exercise 2: Sensitivity Analysis (Moderate)\n",
    "\n",
    "**Task**: Determine whether key conclusions depend on SE method choice.\n",
    "\n",
    "**Requirements**:\n",
    "1. Use the macro panel (`macro_growth.csv`)\n",
    "2. Focus on the coefficient on `openness`\n",
    "3. Estimate with 5 different SE methods\n",
    "4. Create a table showing: Coef, SE, t-stat, p-value, Significance for each method\n",
    "5. Conclusion: Is the openness-growth relationship robust to SE choice?\n",
    "\n",
    "**Starter Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Sensitivity Analysis\n",
    "# We already have results_macro from Section 3\n",
    "# Focus on 'openness' coefficient sensitivity\n",
    "\n",
    "# YOUR CODE:\n",
    "# Step 1: Extract openness results from all macro methods\n",
    "# Step 2: Build sensitivity table (Coef, SE, t-stat, p-value, Sig)\n",
    "# Step 3: Visualize with coefficient plot (estimate + CI by method)\n",
    "# Step 4: Conclusion\n",
    "\n",
    "print(\"Exercise 2: Sensitivity Analysis — 'openness' coefficient\")\n",
    "print()\n",
    "\n",
    "# Starter: extract openness results\n",
    "var = 'openness'\n",
    "print(f\"{'Method':<22} | {'Coef':>8} | {'SE':>8} | {'t-stat':>7} | {'p-value':>8} | Sig\")\n",
    "print(\"-\" * 70)\n",
    "for method, res in results_macro.items():\n",
    "    coef = res.params[var]\n",
    "    se = res.std_errors[var]\n",
    "    t = coef / se\n",
    "    p = res.pvalues[var]\n",
    "    sig = '***' if p < 0.01 else ('**' if p < 0.05 else ('*' if p < 0.10 else 'ns'))\n",
    "    print(f\"{macro_method_display[method]:<22} | {coef:>8.5f} | {se:>8.5f} | {t:>7.2f} | {p:>8.4f} | {sig}\")\n",
    "\n",
    "print()\n",
    "print(\"YOUR CONCLUSION (complete this):\")\n",
    "print(\"\"\"\n",
    "The openness coefficient is [significant/not significant] across SE methods.\n",
    "The most appropriate SE method for this data is [...] because [...].\n",
    "The conclusion that trade openness [affects/does not affect] growth is [robust/sensitive]\n",
    "to SE choice.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise3-header",
   "metadata": {},
   "source": [
    "### Exercise 3: Build an SE Selection Function (Advanced)\n",
    "\n",
    "**Task**: Create a function that recommends an SE method based on data characteristics.\n",
    "\n",
    "**Requirements**:\n",
    "1. Function takes panel dataset and returns recommended SE method + rationale\n",
    "2. Decision logic based on N, T, and user-specified characteristics\n",
    "3. Validate with all three datasets used in this notebook\n",
    "\n",
    "**Starter Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: SE Selection Function\n",
    "\n",
    "def recommend_se_method(data, entity_col, time_col,\n",
    "                         has_cross_section_corr=None,\n",
    "                         has_temporal_corr=None,\n",
    "                         is_nonlinear=False,\n",
    "                         min_clusters=20):\n",
    "    \"\"\"\n",
    "    Recommend standard error method based on data characteristics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Panel dataset\n",
    "    entity_col : str\n",
    "        Entity identifier column\n",
    "    time_col : str\n",
    "        Time identifier column\n",
    "    has_cross_section_corr : bool or None\n",
    "        Whether cross-section correlation is expected\n",
    "    has_temporal_corr : bool or None\n",
    "        Whether temporal correlation is expected\n",
    "    is_nonlinear : bool\n",
    "        Whether using nonlinear model\n",
    "    min_clusters : int\n",
    "        Minimum clusters for asymptotic validity\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys: 'primary', 'robustness', 'rationale', 'warnings'\n",
    "    \"\"\"\n",
    "    N = data[entity_col].nunique()\n",
    "    T = data[time_col].nunique()\n",
    "\n",
    "    recommendation = {\n",
    "        'N': N, 'T': T,\n",
    "        'primary': None,\n",
    "        'robustness': None,\n",
    "        'rationale': [],\n",
    "        'warnings': []\n",
    "    }\n",
    "\n",
    "    # YOUR CODE: Implement decision logic\n",
    "    # Hints:\n",
    "    # - Micro panel: N >> T → cluster by entity\n",
    "    # - Macro panel: T >> N → Driscoll-Kraay\n",
    "    # - Cross-section correlation? → two-way or DK\n",
    "    # - Few clusters (G < min_clusters)? → bootstrap warning\n",
    "    # - T > N? → PCSE viable\n",
    "\n",
    "    # Placeholder (replace with your implementation):\n",
    "    if N >= T:\n",
    "        recommendation['primary'] = 'clustered'\n",
    "        recommendation['robustness'] = 'twoway'\n",
    "        recommendation['rationale'].append(f'Micro panel: N={N} >> T={T} → cluster by entity')\n",
    "    else:\n",
    "        recommendation['primary'] = 'driscoll_kraay'\n",
    "        recommendation['robustness'] = 'pcse' if T > N else 'clustered'\n",
    "        recommendation['rationale'].append(f'Macro panel: T={T} > N={N} → Driscoll-Kraay')\n",
    "\n",
    "    if N < min_clusters:\n",
    "        recommendation['warnings'].append(f'Only {N} entities < {min_clusters} → cluster asymptotics may be unreliable')\n",
    "\n",
    "    # YOUR CODE: Add more decision rules here\n",
    "\n",
    "    return recommendation\n",
    "\n",
    "\n",
    "# Test with all three datasets\n",
    "print(\"SE Method Recommendations:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "datasets = [\n",
    "    (fin_data, 'firm_id', 'month', 'Financial Panel'),\n",
    "    (macro_data, 'country_id', 'year', 'Macro Panel'),\n",
    "    (wage_data, 'person_id', 'year', 'Wage Panel')\n",
    "]\n",
    "\n",
    "for df, ent, t, name in datasets:\n",
    "    rec = recommend_se_method(df, ent, t)\n",
    "    print(f\"\\n{name}: N={rec['N']}, T={rec['T']}\")\n",
    "    print(f\"  Primary: {rec['primary']}\")\n",
    "    print(f\"  Robustness: {rec['robustness']}\")\n",
    "    for r in rec['rationale']:\n",
    "        print(f\"  Rationale: {r}\")\n",
    "    for w in rec['warnings']:\n",
    "        print(f\"  WARNING: {w}\")\n",
    "\n",
    "print()\n",
    "print(\"EXTEND THIS FUNCTION: Add logic for cross-section correlation,\")\n",
    "print(\"nonlinear models, spatial data, and bootstrap recommendations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section9'></a>\n",
    "## 9. Summary and Takeaways\n",
    "\n",
    "### What We Learned Across All 7 Tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all tutorials and methods\n",
    "tutorial_summary = pd.DataFrame({\n",
    "    'Tutorial': [\n",
    "        '01 Robust', '02 Clustering', '03 HAC',\n",
    "        '04 Spatial', '05 MLE Inference', '06 Bootstrap',\n",
    "        '07 Synthesis'\n",
    "    ],\n",
    "    'Methods Covered': [\n",
    "        'HC0, HC1, HC2, HC3',\n",
    "        'Cluster (entity, time, two-way)',\n",
    "        'Newey-West, Driscoll-Kraay',\n",
    "        'Spatial HAC (geographic)',\n",
    "        'MLE Sandwich, cluster MLE',\n",
    "        'Pairs, cluster, wild bootstrap',\n",
    "        'All methods compared'\n",
    "    ],\n",
    "    'Key Use Case': [\n",
    "        'Heteroskedasticity (always)',\n",
    "        'Within-cluster correlation',\n",
    "        'Temporal + cross-sec. correlation',\n",
    "        'Geographic proximity',\n",
    "        'Logit, Probit, Poisson',\n",
    "        'Quantile regression, small G',\n",
    "        'Capstone: decision framework'\n",
    "    ],\n",
    "    'Key Finding': [\n",
    "        'HC3 best in small samples',\n",
    "        'G ≥ 20 for valid asymptotics',\n",
    "        'DK for macro; NW for time series',\n",
    "        'Spatial cutoff choice matters',\n",
    "        'MLE cluster often needed',\n",
    "        'B ≥ 499 for stable results',\n",
    "        'No one-size-fits-all — context!'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\" * 110)\n",
    "print(\"COMPLETE SERIES SUMMARY: 7 TUTORIALS ON STANDARD ERRORS\")\n",
    "print(\"=\" * 110)\n",
    "print(tutorial_summary.to_string(index=False))\n",
    "print(\"=\" * 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"THE 8 COMMANDMENTS OF STANDARD ERRORS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. NEVER use non-robust SEs in panel data (heteroskedasticity is pervasive)\n",
    "\n",
    "2. ALWAYS use cluster-robust SEs for micro panels (N large, T small)\n",
    "   → Cluster by entity captures within-entity temporal correlation\n",
    "\n",
    "3. PREFER Driscoll-Kraay for macro panels (N small, T large)\n",
    "   → Also handles cross-sectional correlation via HAC\n",
    "\n",
    "4. USE two-way clustering for financial panels\n",
    "   → Both firm and time dimensions correlated (Petersen 2009)\n",
    "\n",
    "5. CHECK cluster asymptotics: G ≥ 20 required\n",
    "   → With fewer clusters, use bootstrap or aggregate\n",
    "\n",
    "6. VERIFY T > N before using PCSE\n",
    "   → If T ≈ N or T < N, Driscoll-Kraay is safer\n",
    "\n",
    "7. REPORT primary + robustness SE specification\n",
    "   → Use StandardErrorComparison for systematic checks\n",
    "\n",
    "8. DISCUSS when SE choice affects conclusions\n",
    "   → Transparency essential for replicability\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SIMPLIFIED DECISION TREE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Panel Data\n",
    "    │\n",
    "    ├─→ N large, T small (micro panel)\n",
    "    │       │\n",
    "    │       ├─→ G ≥ 20? YES → Cluster by entity (primary)\n",
    "    │       │               → Two-way cluster (robustness)\n",
    "    │       │\n",
    "    │       └─→ G < 20?  → Bootstrap (cluster or pairs)\n",
    "    │\n",
    "    └─→ N small, T large (macro panel)\n",
    "            │\n",
    "            ├─→ T > N? YES → PCSE (primary if T >> N)\n",
    "            │               → Driscoll-Kraay (always valid check)\n",
    "            │\n",
    "            └─→ T ≤ N?  → Driscoll-Kraay (primary)\n",
    "                         → Cluster by entity (secondary)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section10-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section10'></a>\n",
    "## 10. References\n",
    "\n",
    "### Synthesis Papers\n",
    "\n",
    "1. **Angrist, J. D., & Pischke, J.-S. (2009)**. *Mostly Harmless Econometrics*. Princeton University Press. [Chapter 8: Nonstandard Standard Errors]\n",
    "\n",
    "2. **Cameron, A. C., & Miller, D. L. (2015)**. \"A practitioner's guide to cluster-robust inference.\" *Journal of Human Resources*, 50(2), 317–372.\n",
    "   - *The definitive practical reference for clustering in panel data.*\n",
    "\n",
    "3. **Petersen, M. A. (2009)**. \"Estimating standard errors in finance panel data sets: Comparing approaches.\" *Review of Financial Studies*, 22(1), 435–480.\n",
    "   - *Key reference for two-way clustering in finance.*\n",
    "\n",
    "4. **Thompson, S. B. (2011)**. \"Simple formulas for standard errors that cluster by both firm and time.\" *Journal of Financial Economics*, 99(1), 1–10.\n",
    "\n",
    "### Method-Specific Papers\n",
    "\n",
    "5. **Driscoll, J. C., & Kraay, A. C. (1998)**. \"Consistent covariance matrix estimation with spatially dependent panel data.\" *Review of Economics and Statistics*, 80(4), 549–560.\n",
    "\n",
    "6. **Beck, N., & Katz, J. N. (1995)**. \"What to do (and not to do) with time-series cross-section data.\" *American Political Science Review*, 89(3), 634–647. [PCSE]\n",
    "\n",
    "7. **White, H. (1980)**. \"A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity.\" *Econometrica*, 48(4), 817–838. [HC0]\n",
    "\n",
    "8. **MacKinnon, J. G., & White, H. (1985)**. \"Some heteroskedasticity-consistent covariance matrix estimators with improved finite sample properties.\" *Journal of Econometrics*, 29(3), 305–325. [HC1-HC3]\n",
    "\n",
    "### For Reporting and Reproducibility\n",
    "\n",
    "9. **Ioannidis, J. P. A. (2005)**. \"Why most published research findings are false.\" *PLOS Medicine*, 2(8), e124.\n",
    "   - *Context for understanding why correct SEs matter for scientific validity.*\n",
    "\n",
    "10. **Open Science Collaboration (2015)**. \"Estimating the reproducibility of psychological science.\" *Science*, 349(6251), aac4716.\n",
    "\n",
    "---\n",
    "\n",
    "### PanelBox API Reference\n",
    "\n",
    "```python\n",
    "# Standard error methods summary\n",
    "cov_types = {\n",
    "    'nonrobust':      'Classical OLS (baseline, often wrong)',\n",
    "    'robust':         'HC1 heteroskedasticity-robust',\n",
    "    'hc0':            'HC0 (White 1980)',\n",
    "    'hc2':            'HC2 (leverage-adjusted)',\n",
    "    'hc3':            'HC3 (aggressive leverage, best small samples)',\n",
    "    'clustered':      'Cluster by entity (temporal correlation)',\n",
    "    'twoway':         'Two-way cluster (entity + time)',\n",
    "    'driscoll_kraay': 'Driscoll-Kraay HAC (macro panels)',\n",
    "    'newey_west':     'Newey-West HAC (time series / short panels)',\n",
    "    'pcse':           'Panel-corrected SE (requires T > N)',\n",
    "}\n",
    "\n",
    "# Usage\n",
    "model = FixedEffects(formula, data, entity_col, time_col)\n",
    "results = model.fit(cov_type='clustered')\n",
    "\n",
    "# Comparison\n",
    "comp = StandardErrorComparison(results)\n",
    "result = comp.compare_all()  # or compare_all(se_types=['robust', 'clustered', 'twoway'])\n",
    "comp.summary(result)\n",
    "comp.plot_comparison(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**End of Tutorial 07 — Standard Errors Series Complete!**\n",
    "\n",
    "You have now completed a comprehensive survey of standard error methods for panel data. The key message throughout: **always match your SE method to the correlation structure in your data**, use `StandardErrorComparison` to systematically check robustness, and be transparent about SE choices in your reporting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
