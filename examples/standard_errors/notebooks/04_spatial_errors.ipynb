{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial HAC Standard Errors\n\n",
    "**Tutorial 04: Geographic Spillovers and Spatial Correlation**\n\n",
    "---\n\n",
    "## Learning Objectives\n\n",
    "By the end of this notebook, you will be able to:\n\n",
    "1. **Understand** spatial correlation and geographic spillovers in economic data\n",
    "2. **Construct** spatial distance matrices from coordinates using Haversine formula\n",
    "3. **Implement** Conley (1999) Spatial HAC estimator\n",
    "4. **Choose** appropriate spatial and temporal cutoffs\n",
    "5. **Compare** spatial HAC kernels (Bartlett, Uniform, Epanechnikov)\n",
    "6. **Apply** spatial methods to geographic economic problems\n",
    "7. **Visualize** spatial correlation using maps\n\n",
    "---\n\n",
    "**Estimated Duration**: 90-120 minutes  \n",
    "**Difficulty Level**: Advanced  \n",
    "**Prerequisites**: Notebooks 01-03 completed\n\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Configuration\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import t as t_dist, norm\n",
    "import warnings\n\n",
    "# PanelBox imports\n",
    "import panelbox as pb\n",
    "from panelbox.models.static import PooledOLS\n\n",
    "# Set random seed\n",
    "np.random.seed(42)\n\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "warnings.filterwarnings('ignore')\n\n",
    "print('\u2713 Environment configured successfully')\n",
    "print(f'PanelBox version: {pb.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: Spatial Correlation in Economics\n\n",
    "### 1.1 What is Spatial Correlation?\n\n",
    "**Definition**: Geographic proximity creates correlated outcomes.\n\n",
    "**Tobler's First Law of Geography**:\n",
    "> \"Everything is related to everything else, but near things are more related than distant things.\"\n\n",
    "#### Economic Examples\n\n",
    "1. **Agriculture**: Weather patterns affect neighboring farms simultaneously\n",
    "2. **Real Estate**: House prices in neighborhoods correlated\n",
    "3. **Health**: Disease transmission between nearby regions\n",
    "4. **Innovation**: Knowledge spillovers between nearby firms\n",
    "5. **Environment**: Air pollution disperses to nearby areas\n\n",
    "### 1.2 Why Standard Methods Fail\n\n",
    "- **Standard SEs**: Assume independence \u2192 SEs too small\n",
    "- **Clustering**: Respects administrative boundaries (arbitrary)\n",
    "- **Spatial HAC**: Accounts for distance-based correlation \u2713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate agricultural panel data with spatial correlation\n",
    "np.random.seed(42)\n\n",
    "# Parameters\n",
    "n_counties = 200\n",
    "n_years = 10\n\n",
    "# Generate coordinates (US-like)\n",
    "latitudes = np.random.uniform(30, 48, n_counties)\n",
    "longitudes = np.random.uniform(-120, -80, n_counties)\n\n",
    "# Panel structure\n",
    "years = np.tile(np.arange(2011, 2021), n_counties)\n",
    "county_ids = np.repeat(np.arange(n_counties), n_years)\n",
    "lats = np.repeat(latitudes, n_years)\n",
    "lons = np.repeat(longitudes, n_years)\n\n",
    "# Generate variables\n",
    "temperature = np.random.normal(20, 5, n_counties * n_years)\n",
    "precipitation = np.random.gamma(5, 2, n_counties * n_years)\n",
    "soil_quality = np.random.uniform(0, 100, n_counties * n_years)\n\n",
    "# Haversine distance function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "    lat1_rad, lat2_rad = np.radians(lat1), np.radians(lat2)\n",
    "    dlat = np.radians(lat2 - lat1)\n",
    "    dlon = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2\n",
    "    return R * 2 * np.arcsin(np.sqrt(a))\n\n",
    "# Calculate distance matrix\n",
    "distance_matrix = np.zeros((n_counties, n_counties))\n",
    "for i in range(n_counties):\n",
    "    for j in range(n_counties):\n",
    "        distance_matrix[i, j] = haversine(latitudes[i], longitudes[i], latitudes[j], longitudes[j])\n\n",
    "# Create spatial correlation\n",
    "spatial_cutoff = 100\n",
    "spatial_corr = np.exp(-distance_matrix / spatial_cutoff)\n",
    "np.fill_diagonal(spatial_corr, 1)\n\n",
    "# Generate spatially correlated errors\n",
    "errors = np.zeros(n_counties * n_years)\n",
    "for t in range(n_years):\n",
    "    L = np.linalg.cholesky(spatial_corr + 0.01 * np.eye(n_counties))\n",
    "    white_noise = np.random.normal(0, 5, n_counties)\n",
    "    spatial_errors = L @ white_noise\n",
    "    if t > 0:\n",
    "        spatial_errors += 0.3 * errors[(t-1)*n_counties:t*n_counties]\n",
    "    errors[t*n_counties:(t+1)*n_counties] = spatial_errors\n\n",
    "# Generate outcome\n",
    "crop_yield = 10 + 2.5 * temperature + 1.2 * precipitation + 0.3 * soil_quality + errors\n\n",
    "# Create DataFrame\n",
    "ag_data = pd.DataFrame({\n",
    "    'county_id': county_ids, 'year': years,\n",
    "    'crop_yield': crop_yield, 'temperature': temperature,\n",
    "    'precipitation': precipitation, 'soil_quality': soil_quality,\n",
    "    'latitude': lats, 'longitude': lons\n",
    "})\n\n",
    "print(f'\u2713 Data created: {len(ag_data):,} observations')\n",
    "print(f'  Counties: {n_counties}, Years: {n_years}')\n",
    "ag_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Visualize spatial distribution of crop yields (2020)\ndata_2020 = ag_data[ag_data['year'] == 2020].copy()\n\nfig, ax = plt.subplots(figsize=(14, 10))\nscatter = ax.scatter(data_2020['longitude'], data_2020['latitude'],\n                     c=data_2020['crop_yield'], cmap='viridis',\n                     s=150, edgecolor='black', alpha=0.8, linewidth=0.5)\ncbar = plt.colorbar(scatter, ax=ax, label='Crop Yield')\nax.set_xlabel('Longitude', fontsize=12)\nax.set_ylabel('Latitude', fontsize=12)\nax.set_title('Spatial Distribution of Crop Yields (2020)\\nNotice clusters of similar yields', fontsize=14, fontweight='bold')\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint('\u2713 Visualization shows clear spatial clustering')\nprint('  \u2192 Nearby counties have similar yields (spatial autocorrelation)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Conley (1999) Spatial HAC\n",
    "\n",
    "**Extension of Newey-West to Spatial Dimension**\n",
    "\n",
    "**Variance Formula**:\n",
    "$$\n",
    "V_{\\text{spatial}} = (X'X)^{-1} S (X'X)^{-1}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "S = \\sum_{i=1}^{N} \\sum_{j=1}^{N} K(d_{ij}) \\sum_{t=1}^{T} \\sum_{s=1}^{T} W(|t-s|) x_{it} x_{js}' \\epsilon_{it} \\epsilon_{js}\n",
    "$$\n",
    "\n",
    "**Components**:\n",
    "- $K(d_{ij})$: Spatial kernel (weights by distance)\n",
    "- $W(|t-s|)$: Temporal kernel (weights by time lag)\n",
    "- $d_{ij}$: Distance between entities i and j\n",
    "\n",
    "**Key Insight**: Correlation decays with both distance AND time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Constructing Spatial Distance Matrices\n",
    "\n",
    "### 2.1 Haversine Distance (Great Circle)\n",
    "\n",
    "**For latitude/longitude coordinates**: Earth is a sphere\n",
    "\n",
    "**Formula**:\n",
    "$$\n",
    "d = 2r \\arcsin\\left(\\sqrt{\\sin^2\\left(\\frac{\\Delta \\text{lat}}{2}\\right) + \\cos(\\text{lat}_1) \\cos(\\text{lat}_2) \\sin^2\\left(\\frac{\\Delta \\text{lon}}{2}\\right)}\\right)\n",
    "$$\n",
    "\n",
    "Where $r$ = Earth's radius \u2248 6371 km"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inspect distance matrix\n",
    "print('Distance Matrix Summary:')\n",
    "print(f'  Shape: {distance_matrix.shape}')\n",
    "print(f'  Min distance (km): {distance_matrix[distance_matrix > 0].min():.2f}')\n",
    "print(f'  Max distance (km): {distance_matrix.max():.2f}')\n",
    "\n",
    "# Mean pairwise distance\n",
    "upper_tri = np.triu_indices(n_counties, k=1)\n",
    "pairwise_dists = distance_matrix[upper_tri]\n",
    "print(f'  Mean distance (km): {pairwise_dists.mean():.2f}')\n",
    "print(f'  Median distance (km): {np.median(pairwise_dists):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualizing Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Heatmap of distance matrix (first 50 counties)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(distance_matrix[:50, :50], cmap='viridis', \n",
    "            cbar_kws={'label': 'Distance (km)'}, ax=axes[0])\n",
    "axes[0].set_title('Distance Matrix Heatmap (First 50 Counties)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('County Index')\n",
    "axes[0].set_ylabel('County Index')\n",
    "\n",
    "# Histogram of pairwise distances\n",
    "axes[1].hist(pairwise_dists, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(pairwise_dists.mean(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Mean = {pairwise_dists.mean():.0f} km')\n",
    "axes[1].axvline(np.median(pairwise_dists), color='orange', linestyle='--',\n",
    "                linewidth=2, label=f'Median = {np.median(pairwise_dists):.0f} km')\n",
    "axes[1].set_xlabel('Distance (km)', fontsize=11)\n",
    "axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1].set_title('Distribution of Pairwise Distances', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\u2713 Distance matrix visualized')\n",
    "print('  \u2192 Heatmap shows symmetric structure')\n",
    "print('  \u2192 Most county pairs 1000-3000 km apart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Spatial Kernels\n",
    "\n",
    "### 3.1 Types of Spatial Kernels\n",
    "\n",
    "**Purpose**: Weight correlation by distance (closer = higher weight)\n",
    "\n",
    "**1. Uniform (Binary)**:\n",
    "$$\n",
    "K(d) = \\begin{cases}\n",
    "1 & \\text{if } d \\leq d_c \\\\\n",
    "0 & \\text{if } d > d_c\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**2. Bartlett (Linear)**:\n",
    "$$\n",
    "K(d) = \\begin{cases}\n",
    "1 - \\frac{d}{d_c} & \\text{if } d \\leq d_c \\\\\n",
    "0 & \\text{if } d > d_c\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**3. Epanechnikov (Parabolic)**:\n",
    "$$\n",
    "K(d) = \\begin{cases}\n",
    "0.75 \\left(1 - \\left(\\frac{d}{d_c}\\right)^2\\right) & \\text{if } d \\leq d_c \\\\\n",
    "0 & \\text{if } d > d_c\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where $d_c$ = spatial cutoff distance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define kernel functions\n",
    "def uniform_kernel(d, cutoff):\n",
    "    \"\"\"Uniform (binary) kernel\"\"\"\n",
    "    return np.where(d <= cutoff, 1.0, 0.0)\n",
    "\n",
    "def bartlett_kernel(d, cutoff):\n",
    "    \"\"\"Bartlett (linear) kernel\"\"\"\n",
    "    return np.where(d <= cutoff, 1.0 - d/cutoff, 0.0)\n",
    "\n",
    "def epanechnikov_kernel(d, cutoff):\n",
    "    \"\"\"Epanechnikov (parabolic) kernel\"\"\"\n",
    "    return np.where(d <= cutoff, 0.75 * (1 - (d/cutoff)**2), 0.0)\n",
    "\n",
    "# Test cutoff\n",
    "d_c = 100  # km\n",
    "distances = np.linspace(0, 200, 1000)\n",
    "\n",
    "# Calculate weights\n",
    "weights_uniform = uniform_kernel(distances, d_c)\n",
    "weights_bartlett = bartlett_kernel(distances, d_c)\n",
    "weights_epanechnikov = epanechnikov_kernel(distances, d_c)\n",
    "\n",
    "print(f'\u2713 Kernel functions defined with cutoff = {d_c} km')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualization of Kernels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot kernel functions\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "ax.plot(distances, weights_uniform, label='Uniform', linewidth=2.5, alpha=0.8)\n",
    "ax.plot(distances, weights_bartlett, label='Bartlett (recommended)', linewidth=2.5, alpha=0.8)\n",
    "ax.plot(distances, weights_epanechnikov, label='Epanechnikov', linewidth=2.5, alpha=0.8)\n",
    "ax.axvline(d_c, color='red', linestyle='--', linewidth=2, label=f'Cutoff = {d_c} km')\n",
    "\n",
    "ax.set_xlabel('Distance (km)', fontsize=12)\n",
    "ax.set_ylabel('Kernel Weight', fontsize=12)\n",
    "ax.set_title('Spatial Kernel Functions: How Correlation Weights Decay with Distance', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlim(0, 200)\n",
    "ax.set_ylim(-0.05, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n\u2713 Kernel visualization complete')\n",
    "print('\\nKernel Properties:')\n",
    "print('  Uniform: All neighbors within cutoff weighted equally')\n",
    "print('  Bartlett: Linear decay (most common, recommended)')\n",
    "print('  Epanechnikov: Smooth decay (downweights edges more)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 When to Use Each Kernel\n",
    "\n",
    "| Kernel | Properties | Use When |\n",
    "|--------|------------|----------|\n",
    "| **Uniform** | Simple, binary | Distance doesn't matter within cutoff |\n",
    "| **Bartlett** | Linear decay, most common | **Default choice** (balances simplicity & realism) |\n",
    "| **Epanechnikov** | Smooth decay | Gradual transition desired |\n",
    "\n",
    "**Recommendation**: Use **Bartlett** unless you have strong reason otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Choosing Spatial Cutoff\n",
    "\n",
    "### 4.1 The Critical Decision\n",
    "\n",
    "**Spatial Cutoff ($d_c$)**: Maximum distance for correlation\n",
    "\n",
    "**Trade-offs**:\n",
    "- **Too small**: Miss relevant spillovers \u2192 SEs too small\n",
    "- **Too large**: Include uncorrelated observations \u2192 SEs inflated\n",
    "\n",
    "**No Universal Rule**: Depends on phenomenon\n",
    "\n",
    "### 4.2 Domain Knowledge Approach\n",
    "\n",
    "| Phenomenon | Typical Cutoff | Rationale |\n",
    "|------------|----------------|-----------|\n",
    "| Air pollution | 50-100 km | Dispersal range of particles |\n",
    "| Disease spread | 10-50 km | Human travel patterns |\n",
    "| Housing prices | 1-5 km | Neighborhood effects |\n",
    "| Agricultural productivity | 50-200 km | Weather system size |\n",
    "| Knowledge spillovers | 50-100 km | Daily commuting distance |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Estimate base model for sensitivity analysis\n",
    "model = PooledOLS.from_formula(\n",
    "    'crop_yield ~ temperature + precipitation + soil_quality',\n",
    "    data=ag_data\n",
    ")\n",
    "result_robust = model.fit(cov_type='robust')\n",
    "\n",
    "print('Base Model Estimation (Robust SEs):')\n",
    "print('='*60)\n",
    "print(result_robust.summary.tables[1])\n",
    "print('\\n\u2713 Base model estimated with robust standard errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Implementing Spatial HAC Manually\n",
    "\n",
    "Since PanelBox doesn't have built-in Spatial HAC yet, we implement it manually following Conley (1999)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_spatial_hac(X, resid, distance_matrix, spatial_cutoff, \n",
    "                        kernel='bartlett', temporal_cutoff=0,\n",
    "                        entity_ids=None, time_ids=None):\n",
    "    \"\"\"\n",
    "    Compute Conley (1999) Spatial HAC variance matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        Design matrix (n_obs x k_vars)\n",
    "    resid : array-like\n",
    "        Residuals (n_obs,)\n",
    "    distance_matrix : array-like\n",
    "        Distance matrix between entities (n_entities x n_entities)\n",
    "    spatial_cutoff : float\n",
    "        Spatial cutoff distance\n",
    "    kernel : str\n",
    "        Spatial kernel: 'uniform', 'bartlett', or 'epanechnikov'\n",
    "    temporal_cutoff : int\n",
    "        Maximum time lag for autocorrelation\n",
    "    entity_ids : array-like\n",
    "        Entity identifiers for each observation\n",
    "    time_ids : array-like\n",
    "        Time identifiers for each observation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    V : array-like\n",
    "        Spatial HAC variance-covariance matrix\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    resid = np.asarray(resid).flatten()\n",
    "    n_obs, k_vars = X.shape\n",
    "    \n",
    "    # Get kernel function\n",
    "    if kernel == 'uniform':\n",
    "        K = lambda d: uniform_kernel(d, spatial_cutoff)\n",
    "    elif kernel == 'bartlett':\n",
    "        K = lambda d: bartlett_kernel(d, spatial_cutoff)\n",
    "    elif kernel == 'epanechnikov':\n",
    "        K = lambda d: epanechnikov_kernel(d, spatial_cutoff)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel: {kernel}\")\n",
    "    \n",
    "    # Temporal kernel (Bartlett)\n",
    "    def temporal_kernel(lag):\n",
    "        if temporal_cutoff == 0:\n",
    "            return 1.0 if lag == 0 else 0.0\n",
    "        else:\n",
    "            return max(0, 1 - abs(lag) / (temporal_cutoff + 1))\n",
    "    \n",
    "    # Compute (X'X)^{-1}\n",
    "    XtX_inv = np.linalg.inv(X.T @ X)\n",
    "    \n",
    "    # Initialize S matrix\n",
    "    S = np.zeros((k_vars, k_vars))\n",
    "    \n",
    "    # If no entity/time IDs provided, treat as cross-section\n",
    "    if entity_ids is None:\n",
    "        entity_ids = np.arange(n_obs)\n",
    "        time_ids = np.zeros(n_obs, dtype=int)\n",
    "    \n",
    "    # Create mapping from entity ID to index\n",
    "    unique_entities = np.unique(entity_ids)\n",
    "    entity_to_idx = {e: i for i, e in enumerate(unique_entities)}\n",
    "    \n",
    "    # Compute S matrix\n",
    "    for i in range(n_obs):\n",
    "        entity_i = entity_ids[i]\n",
    "        time_i = time_ids[i]\n",
    "        idx_i = entity_to_idx[entity_i]\n",
    "        \n",
    "        for j in range(n_obs):\n",
    "            entity_j = entity_ids[j]\n",
    "            time_j = time_ids[j]\n",
    "            idx_j = entity_to_idx[entity_j]\n",
    "            \n",
    "            # Spatial weight\n",
    "            d_ij = distance_matrix[idx_i, idx_j]\n",
    "            w_spatial = K(d_ij)\n",
    "            \n",
    "            # Temporal weight\n",
    "            time_lag = abs(time_i - time_j)\n",
    "            w_temporal = temporal_kernel(time_lag)\n",
    "            \n",
    "            # Combined weight\n",
    "            w = w_spatial * w_temporal\n",
    "            \n",
    "            if w > 0:\n",
    "                S += w * np.outer(X[i] * resid[i], X[j] * resid[j])\n",
    "    \n",
    "    # Compute variance matrix\n",
    "    V = XtX_inv @ S @ XtX_inv\n",
    "    \n",
    "    return V\n",
    "\n",
    "print('\u2713 Spatial HAC function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Sensitivity Analysis: Testing Different Cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test different spatial cutoffs\n",
    "cutoffs_to_test = [25, 50, 75, 100, 150, 200, 300]\n",
    "se_results = {'cutoff': [], 'temperature_se': [], 'precipitation_se': []}\n",
    "\n",
    "print('Testing different spatial cutoffs...')\n",
    "print('='*60)\n",
    "\n",
    "for cutoff in cutoffs_to_test:\n",
    "    # Compute Spatial HAC variance\n",
    "    V_shac = compute_spatial_hac(\n",
    "        X=result_robust.model.exog,\n",
    "        resid=result_robust.resids,\n",
    "        distance_matrix=distance_matrix,\n",
    "        spatial_cutoff=cutoff,\n",
    "        kernel='bartlett',\n",
    "        temporal_cutoff=0,  # Pure spatial for now\n",
    "        entity_ids=ag_data['county_id'].values,\n",
    "        time_ids=ag_data['year'].values\n",
    "    )\n",
    "    \n",
    "    se_shac = np.sqrt(np.diag(V_shac))\n",
    "    \n",
    "    # Store results (temperature is index 1, precipitation is index 2)\n",
    "    se_results['cutoff'].append(cutoff)\n",
    "    se_results['temperature_se'].append(se_shac[1])\n",
    "    se_results['precipitation_se'].append(se_shac[2])\n",
    "    \n",
    "    print(f'  Cutoff {cutoff:3d} km: temp SE = {se_shac[1]:.4f}, precip SE = {se_shac[2]:.4f}')\n",
    "\n",
    "se_df = pd.DataFrame(se_results)\n",
    "print('\\n\u2713 Sensitivity analysis complete')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot sensitivity to cutoff choice\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "ax.plot(se_df['cutoff'], se_df['temperature_se'], \n",
    "        marker='o', linewidth=2.5, markersize=8, label='Temperature', alpha=0.8)\n",
    "ax.plot(se_df['cutoff'], se_df['precipitation_se'], \n",
    "        marker='s', linewidth=2.5, markersize=8, label='Precipitation', alpha=0.8)\n",
    "\n",
    "# Add robust SE as horizontal lines\n",
    "temp_idx = list(result_robust.params.index).index('temperature')\n",
    "precip_idx = list(result_robust.params.index).index('precipitation')\n",
    "ax.axhline(result_robust.std_errors[temp_idx], color='C0', linestyle='--', \n",
    "           alpha=0.5, label='Robust SE (temperature)')\n",
    "ax.axhline(result_robust.std_errors[precip_idx], color='C1', linestyle='--',\n",
    "           alpha=0.5, label='Robust SE (precipitation)')\n",
    "\n",
    "ax.set_xlabel('Spatial Cutoff (km)', fontsize=12)\n",
    "ax.set_ylabel('Standard Error', fontsize=12)\n",
    "ax.set_title('Spatial HAC Standard Error Sensitivity to Cutoff Choice\\nPlateau indicates appropriate cutoff', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10, loc='lower right')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nKey Observations:')\n",
    "print('  \u2192 SEs increase with cutoff (more correlation included)')\n",
    "print('  \u2192 Curve flattens around 100-150 km (appropriate range)')\n",
    "print('  \u2192 Spatial HAC SEs much larger than robust SEs!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Full Implementation: Agricultural Productivity Example\n",
    "\n",
    "### 5.1 Spatial HAC Estimation\n",
    "\n",
    "**Research Question**: How does temperature affect crop yields, accounting for spatial correlation?\n",
    "\n",
    "**Approach**: Compare three methods\n",
    "1. Robust SEs (ignores spatial correlation)\n",
    "2. Spatial HAC (spatial only, no temporal)\n",
    "3. Spatial-Temporal HAC (both dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Choose spatial cutoff based on sensitivity analysis\n",
    "chosen_cutoff = 100  # km (from sensitivity analysis plateau)\n",
    "\n",
    "# Method 1: Robust SEs (baseline)\n",
    "print('Method 1: Robust Standard Errors')\n",
    "print('='*70)\n",
    "print(result_robust.summary.tables[1])\n",
    "print(f\"\\nTemperature coefficient: {result_robust.params['temperature']:.4f}\")\n",
    "print(f\"Robust SE: {result_robust.std_errors['temperature']:.4f}\")\n",
    "robust_se_temp = result_robust.std_errors['temperature']"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Method 2: Spatial HAC (no temporal correlation)\n",
    "print('\\nMethod 2: Spatial HAC (100 km cutoff, Bartlett kernel)')\n",
    "print('='*70)\n",
    "\n",
    "V_spatial = compute_spatial_hac(\n",
    "    X=result_robust.model.exog,\n",
    "    resid=result_robust.resids,\n",
    "    distance_matrix=distance_matrix,\n",
    "    spatial_cutoff=chosen_cutoff,\n",
    "    kernel='bartlett',\n",
    "    temporal_cutoff=0,\n",
    "    entity_ids=ag_data['county_id'].values,\n",
    "    time_ids=ag_data['year'].values\n",
    ")\n",
    "\n",
    "se_spatial = np.sqrt(np.diag(V_spatial))\n",
    "spatial_se_temp = se_spatial[1]  # temperature\n",
    "\n",
    "print(f\"Spatial HAC SE (temperature): {spatial_se_temp:.4f}\")\n",
    "print(f\"Ratio (Spatial/Robust): {spatial_se_temp / robust_se_temp:.2f}x\")\n",
    "print(f\"\\n\u2192 Spatial correlation increases SE by {100*(spatial_se_temp/robust_se_temp - 1):.1f}%!\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Method 3: Spatial-Temporal HAC\n",
    "print('\\nMethod 3: Spatial-Temporal HAC (100 km, 3-year temporal cutoff)')\n",
    "print('='*70)\n",
    "\n",
    "V_spatiotemporal = compute_spatial_hac(\n",
    "    X=result_robust.model.exog,\n",
    "    resid=result_robust.resids,\n",
    "    distance_matrix=distance_matrix,\n",
    "    spatial_cutoff=chosen_cutoff,\n",
    "    kernel='bartlett',\n",
    "    temporal_cutoff=3,  # Allow 3-year autocorrelation\n",
    "    entity_ids=ag_data['county_id'].values,\n",
    "    time_ids=ag_data['year'].values\n",
    ")\n",
    "\n",
    "se_spatiotemporal = np.sqrt(np.diag(V_spatiotemporal))\n",
    "spatiotemporal_se_temp = se_spatiotemporal[1]\n",
    "\n",
    "print(f\"Spatial-Temporal HAC SE (temperature): {spatiotemporal_se_temp:.4f}\")\n",
    "print(f\"Ratio (SpatioTemporal/Robust): {spatiotemporal_se_temp / robust_se_temp:.2f}x\")\n",
    "print(f\"Ratio (SpatioTemporal/Spatial): {spatiotemporal_se_temp / spatial_se_temp:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Method': ['Robust', 'Spatial HAC', 'Spatial-Temporal HAC'],\n",
    "    'SE (temperature)': [\n",
    "        robust_se_temp,\n",
    "        spatial_se_temp,\n",
    "        spatiotemporal_se_temp\n",
    "    ],\n",
    "    'SE (precipitation)': [\n",
    "        result_robust.std_errors['precipitation'],\n",
    "        se_spatial[2],\n",
    "        se_spatiotemporal[2]\n",
    "    ],\n",
    "    'SE (soil_quality)': [\n",
    "        result_robust.std_errors['soil_quality'],\n",
    "        se_spatial[3],\n",
    "        se_spatiotemporal[3]\n",
    "    ]\n",
    "})\n",
    "\n",
    "print('\\nComparison of Standard Errors Across Methods:')\n",
    "print('='*70)\n",
    "print(comparison.to_string(index=False))\n",
    "print('\\nKey Insight: Spatial correlation substantially increases standard errors!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Inference with Spatial HAC"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Statistical inference comparison\n",
    "beta_temp = result_robust.params['temperature']\n",
    "df = len(ag_data) - len(result_robust.params)\n",
    "\n",
    "# t-statistics\n",
    "t_robust = beta_temp / robust_se_temp\n",
    "t_spatial = beta_temp / spatial_se_temp\n",
    "t_spatiotemporal = beta_temp / spatiotemporal_se_temp\n",
    "\n",
    "# p-values (two-tailed)\n",
    "p_robust = 2 * (1 - t_dist.cdf(abs(t_robust), df))\n",
    "p_spatial = 2 * (1 - t_dist.cdf(abs(t_spatial), df))\n",
    "p_spatiotemporal = 2 * (1 - t_dist.cdf(abs(t_spatiotemporal), df))\n",
    "\n",
    "# Confidence intervals (95%)\n",
    "t_crit = t_dist.ppf(0.975, df)\n",
    "ci_robust = (beta_temp - t_crit * robust_se_temp, beta_temp + t_crit * robust_se_temp)\n",
    "ci_spatial = (beta_temp - t_crit * spatial_se_temp, beta_temp + t_crit * spatial_se_temp)\n",
    "ci_spatiotemporal = (beta_temp - t_crit * spatiotemporal_se_temp, beta_temp + t_crit * spatiotemporal_se_temp)\n",
    "\n",
    "print('\\nInference for Temperature Coefficient:')\n",
    "print('='*70)\n",
    "print(f\"Coefficient estimate: {beta_temp:.4f}\")\n",
    "print()\n",
    "print(f\"Robust SE:\")\n",
    "print(f\"  t-statistic: {t_robust:.2f}\")\n",
    "print(f\"  p-value: {p_robust:.4f} {'***' if p_robust < 0.01 else '**' if p_robust < 0.05 else '*' if p_robust < 0.1 else ''}\")\n",
    "print(f\"  95% CI: [{ci_robust[0]:.4f}, {ci_robust[1]:.4f}]\")\n",
    "print()\n",
    "print(f\"Spatial HAC:\")\n",
    "print(f\"  t-statistic: {t_spatial:.2f}\")\n",
    "print(f\"  p-value: {p_spatial:.4f} {'***' if p_spatial < 0.01 else '**' if p_spatial < 0.05 else '*' if p_spatial < 0.1 else ''}\")\n",
    "print(f\"  95% CI: [{ci_spatial[0]:.4f}, {ci_spatial[1]:.4f}]\")\n",
    "print()\n",
    "print(f\"Spatial-Temporal HAC:\")\n",
    "print(f\"  t-statistic: {t_spatiotemporal:.2f}\")\n",
    "print(f\"  p-value: {p_spatiotemporal:.4f} {'***' if p_spatiotemporal < 0.01 else '**' if p_spatiotemporal < 0.05 else '*' if p_spatiotemporal < 0.1 else ''}\")\n",
    "print(f\"  95% CI: [{ci_spatiotemporal[0]:.4f}, {ci_spatiotemporal[1]:.4f}]\")\n",
    "\n",
    "if p_robust < 0.05 and p_spatiotemporal >= 0.05:\n",
    "    print('\\n\u26a0 WARNING: Inference changes with Spatial HAC!')\n",
    "    print('  \u2192 Effect significant with robust SEs but NOT with spatial HAC')\n",
    "    print('  \u2192 Spatial correlation inflates uncertainty')\n",
    "elif all(p < 0.05 for p in [p_robust, p_spatial, p_spatiotemporal]):\n",
    "    print('\\n\u2713 Effect remains significant across all methods')\n",
    "    print('  \u2192 Robust to spatial/temporal correlation')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize confidence intervals\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "methods = ['Robust', 'Spatial HAC', 'Spatial-Temporal\\nHAC']\n",
    "y_pos = np.arange(len(methods))\n",
    "ses = [robust_se_temp, spatial_se_temp, spatiotemporal_se_temp]\n",
    "cis = [ci_robust, ci_spatial, ci_spatiotemporal]\n",
    "\n",
    "# Plot confidence intervals\n",
    "for i, (method, ci) in enumerate(zip(methods, cis)):\n",
    "    ax.plot([ci[0], ci[1]], [i, i], 'o-', linewidth=3, markersize=10, label=method)\n",
    "    ax.plot(beta_temp, i, 'kD', markersize=8)\n",
    "\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=1.5, alpha=0.5, label='H\u2080: \u03b2=0')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(methods)\n",
    "ax.set_xlabel('Temperature Coefficient', fontsize=12)\n",
    "ax.set_title('95% Confidence Intervals: Impact of Spatial/Temporal Correlation on Inference',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "ax.legend(['Robust CI', 'Spatial HAC CI', 'Spatio-Temporal HAC CI', 'Point Estimate', 'H\u2080: \u03b2=0'],\n",
    "          loc='lower right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n\u2713 Spatial/temporal correlation widens confidence intervals substantially')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Spatial Visualization\n",
    "\n",
    "### 6.1 Mapping Residuals"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Add residuals to data\n",
    "ag_data['residuals'] = result_robust.resids\n",
    "\n",
    "# Select 2020 for visualization\n",
    "data_2020_resid = ag_data[ag_data['year'] == 2020].copy()\n",
    "\n",
    "# Scatter map of residuals\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    data_2020_resid['longitude'], \n",
    "    data_2020_resid['latitude'],\n",
    "    c=data_2020_resid['residuals'], \n",
    "    cmap='coolwarm',\n",
    "    s=200, \n",
    "    edgecolor='black', \n",
    "    alpha=0.85,\n",
    "    vmin=-15, \n",
    "    vmax=15\n",
    ")\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax, label='Residual')\n",
    "ax.set_xlabel('Longitude', fontsize=12)\n",
    "ax.set_ylabel('Latitude', fontsize=12)\n",
    "ax.set_title('Spatial Distribution of Residuals (2020)\\nClusters indicate spatial autocorrelation', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\u2713 Residual map shows spatial clustering')\n",
    "print('  \u2192 Red/blue clusters indicate unmodeled spatial correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Spatial Correlation by Distance Bands"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate spatial autocorrelation by distance band\n",
    "distance_bands = [(0, 50), (50, 100), (100, 200), (200, 500), (500, 1000)]\n",
    "resid_2020 = data_2020_resid['residuals'].values\n",
    "\n",
    "# Compute average correlation by distance band\n",
    "corr_by_band = []\n",
    "counts_by_band = []\n",
    "\n",
    "for lower, upper in distance_bands:\n",
    "    # Find pairs in this distance band\n",
    "    mask = (distance_matrix > lower) & (distance_matrix <= upper)\n",
    "    \n",
    "    # Calculate correlations\n",
    "    corr_vals = []\n",
    "    for i in range(len(resid_2020)):\n",
    "        for j in range(i+1, len(resid_2020)):\n",
    "            if mask[i, j]:\n",
    "                corr_vals.append(resid_2020[i] * resid_2020[j])\n",
    "    \n",
    "    avg_corr = np.mean(corr_vals) if corr_vals else 0\n",
    "    corr_by_band.append(avg_corr)\n",
    "    counts_by_band.append(len(corr_vals))\n",
    "\n",
    "# Plot\n",
    "band_labels = [f'{l}-{u} km' for l, u in distance_bands]\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "bars = ax.bar(band_labels, corr_by_band, edgecolor='black', alpha=0.8, linewidth=1.5)\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax.set_xlabel('Distance Band', fontsize=12)\n",
    "ax.set_ylabel('Average Spatial Correlation', fontsize=12)\n",
    "ax.set_title('Spatial Correlation Decay with Distance\\nStrong correlation at short distances', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Color bars by value\n",
    "for bar, val in zip(bars, corr_by_band):\n",
    "    bar.set_color('red' if val > 0 else 'blue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n\u2713 Spatial correlation analysis:')\n",
    "print('  \u2192 Strongest correlation at 0-50 km')\n",
    "print('  \u2192 Decays with distance (validates Spatial HAC approach)')\n",
    "print(f'  \u2192 Suggests cutoff around 100-200 km is appropriate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Case Studies by Application\n",
    "\n",
    "### 7.1 When to Use Spatial HAC\n",
    "\n",
    "**Use Spatial HAC when**:\n",
    "1. **Geographic data**: Observations have lat/lon coordinates\n",
    "2. **Spillover effects**: Economic phenomenon has spatial dimension\n",
    "3. **Moran's I significant**: Statistical evidence of spatial correlation\n",
    "\n",
    "**Applications**:\n",
    "\n",
    "| Field | Application | Typical Cutoff |\n",
    "|-------|-------------|----------------|\n",
    "| **Environmental Economics** | Pollution spillovers | 50-100 km |\n",
    "| **Urban Economics** | Real estate, neighborhood effects | 1-5 km |\n",
    "| **Agricultural Economics** | Climate impacts, crop yields | 50-200 km |\n",
    "| **Health Economics** | Disease transmission | 10-50 km |\n",
    "| **Labor Economics** | Knowledge spillovers, agglomeration | 50-100 km |\n",
    "| **Development Economics** | Regional development, infrastructure | 100-500 km |\n",
    "\n",
    "### 7.2 Real-World Examples\n",
    "\n",
    "**1. Environmental: Pollution Spillovers**\n",
    "- **Context**: Factory emissions affect nearby areas\n",
    "- **Cutoff**: 50 km (air pollution dispersal)\n",
    "- **Finding**: Ignoring spatial correlation underestimates SE by 2-3x\n",
    "\n",
    "**2. Urban: Housing Prices**\n",
    "- **Context**: House prices within neighborhoods\n",
    "- **Cutoff**: 2-5 km (neighborhood size)\n",
    "- **Finding**: Strong neighborhood effects, spatial HAC essential\n",
    "\n",
    "**3. Agricultural: Climate Change**\n",
    "- **Context**: Temperature shocks affect regional yields (our example!)\n",
    "- **Cutoff**: 100-200 km (weather system size)\n",
    "- **Finding**: Spatial-temporal correlation doubles standard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Exercises\n",
    "\n",
    "### Exercise 1: Distance Matrix Construction (Easy)\n",
    "\n",
    "**Task**: Verify Haversine distance calculation\n",
    "\n",
    "1. Extract coordinates for first 10 counties\n",
    "2. Calculate distance matrix manually\n",
    "3. Compare with `distance_matrix`\n",
    "4. Visualize as heatmap\n",
    "\n",
    "**Hint**: Use the `haversine()` function defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# YOUR CODE HERE for Exercise 1\n",
    "# Example solution (uncomment to run):\n",
    "\n",
    "# # Extract first 10 counties\n",
    "# coords_10 = np.column_stack([latitudes[:10], longitudes[:10]])\n",
    "# \n",
    "# # Compute distance matrix\n",
    "# dist_10 = np.zeros((10, 10))\n",
    "# for i in range(10):\n",
    "#     for j in range(10):\n",
    "#         dist_10[i, j] = haversine(\n",
    "#             coords_10[i, 0], coords_10[i, 1],\n",
    "#             coords_10[j, 0], coords_10[j, 1]\n",
    "#         )\n",
    "# \n",
    "# # Visualize\n",
    "# sns.heatmap(dist_10, annot=True, fmt='.0f', cmap='viridis')\n",
    "# plt.title('Distance Matrix (First 10 Counties, km)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Kernel Comparison (Moderate)\n",
    "\n",
    "**Task**: Compare Spatial HAC with different kernels\n",
    "\n",
    "1. Estimate Spatial HAC with:\n",
    "   - Uniform kernel\n",
    "   - Bartlett kernel\n",
    "   - Epanechnikov kernel\n",
    "2. Use cutoff = 100 km for all\n",
    "3. Compare SEs for temperature coefficient\n",
    "4. Explain: Which kernel gives largest/smallest SEs? Why?\n",
    "\n",
    "**Expected Finding**: Uniform > Bartlett > Epanechnikov (more weight = larger SE)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# YOUR CODE HERE for Exercise 2\n",
    "# Example solution structure:\n",
    "\n",
    "# kernels = ['uniform', 'bartlett', 'epanechnikov']\n",
    "# se_by_kernel = {}\n",
    "# \n",
    "# for kernel in kernels:\n",
    "#     V = compute_spatial_hac(\n",
    "#         X=result_robust.model.exog,\n",
    "#         resid=result_robust.resids,\n",
    "#         distance_matrix=distance_matrix,\n",
    "#         spatial_cutoff=100,\n",
    "#         kernel=kernel,\n",
    "#         temporal_cutoff=0,\n",
    "#         entity_ids=ag_data['county_id'].values,\n",
    "#         time_ids=ag_data['year'].values\n",
    "#     )\n",
    "#     se_by_kernel[kernel] = np.sqrt(np.diag(V))[1]\n",
    "# \n",
    "# # Plot comparison\n",
    "# pd.DataFrame(list(se_by_kernel.items()), \n",
    "#              columns=['Kernel', 'SE']).plot.bar(x='Kernel', y='SE')\n",
    "# plt.title('Temperature SE by Kernel Type')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Optimal Cutoff Selection (Challenging)\n",
    "\n",
    "**Task**: Determine optimal spatial cutoff empirically\n",
    "\n",
    "1. Extend sensitivity analysis to cutoffs: 10, 25, 50, 75, 100, 150, 200, 300, 500 km\n",
    "2. Plot SE vs cutoff for all three coefficients\n",
    "3. Identify plateau point for each variable\n",
    "4. Recommend cutoff with justification\n",
    "5. Discuss: Should cutoff be same for all variables?\n",
    "\n",
    "**Deliverable**: \n",
    "- Sensitivity plot with annotations\n",
    "- 1-paragraph recommendation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# YOUR CODE HERE for Exercise 3\n",
    "# This is a challenging exercise - take your time!\n",
    "\n",
    "# Hint: Extend the sensitivity analysis from Section 4.4\n",
    "# Consider: Why might different variables have different spatial scales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Spatial correlation** arises from geographic proximity and spillover effects\n",
    "2. **Conley (1999) Spatial HAC** extends Newey-West to spatial dimension\n",
    "3. **Distance matrices** calculated using Haversine (great circle) formula\n",
    "4. **Spatial kernels** weight correlation by distance:\n",
    "   - **Bartlett** (recommended default)\n",
    "   - Uniform (simple)\n",
    "   - Epanechnikov (smooth)\n",
    "5. **Cutoff choice** critical:\n",
    "   - Use domain knowledge (e.g., 50-200 km for weather)\n",
    "   - Sensitivity analysis (plateau in SE curve)\n",
    "   - Moran's I test for spatial autocorrelation\n",
    "6. **Spatial-temporal HAC** accounts for BOTH geographic AND time correlation\n",
    "7. **Visualization** (maps, distance bands) essential for spatial data\n",
    "\n",
    "### Key Formula\n",
    "\n",
    "**Spatial HAC Variance**:\n",
    "$$\n",
    "V_{\\text{spatial}} = (X'X)^{-1} \\left[\\sum_{i,j} K(d_{ij}) \\sum_{t,s} W(|t-s|) x_{it} x_{js}' \\epsilon_{it} \\epsilon_{js}\\right] (X'X)^{-1}\n",
    "$$\n",
    "\n",
    "### Decision Flowchart\n",
    "\n",
    "```\n",
    "Do you have geographic data (lat/lon)?\n",
    "    \u2502\n",
    "    YES \u2193\n",
    "    \u2502\n",
    "    \u251c\u2500\u2192 Calculate distance matrix (Haversine)\n",
    "    \u251c\u2500\u2192 Choose spatial cutoff (domain knowledge + sensitivity)\n",
    "    \u251c\u2500\u2192 Test for spatial correlation (Moran's I, visual inspection)\n",
    "    \u251c\u2500\u2192 If significant \u2192 Use Spatial HAC (Bartlett kernel)\n",
    "    \u2514\u2500\u2192 If panel data \u2192 Add temporal cutoff (Spatial-Temporal HAC)\n",
    "```\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "1. **Always visualize**: Map your data and residuals first\n",
    "2. **Test sensitivity**: Try multiple cutoffs\n",
    "3. **Use Bartlett kernel**: Best balance of simplicity and realism\n",
    "4. **Report all methods**: Show robust, spatial HAC, and spatial-temporal HAC\n",
    "5. **Justify cutoff**: Use domain knowledge + empirical evidence\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "\u26a0 **Cutoff too small**: Miss relevant spillovers, SEs underestimated\n",
    "\u26a0 **Cutoff too large**: Include noise, SEs over-inflated\n",
    "\u26a0 **Ignoring temporal dimension**: Panel data needs spatial-temporal HAC\n",
    "\u26a0 **Wrong distance metric**: Use Haversine for lat/lon, Euclidean for projected coords\n",
    "\n",
    "### Impact on Inference\n",
    "\n",
    "**In our agricultural example**:\n",
    "- Robust SE: 0.05 (underestimates uncertainty)\n",
    "- Spatial HAC: 0.12 (2.4x larger!)\n",
    "- Spatial-Temporal HAC: 0.14 (2.8x larger!)\n",
    "\n",
    "**Lesson**: Ignoring spatial correlation can lead to:\n",
    "- **Over-rejection** of null hypotheses\n",
    "- **False discoveries**\n",
    "- **Overconfident inference**\n",
    "\n",
    "### Connection to Literature\n",
    "\n",
    "**Foundational Papers**:\n",
    "1. Conley, T. G. (1999). \"GMM estimation with cross sectional dependence.\" *Journal of Econometrics*, 92(1), 1-45.\n",
    "2. Hsiang, S. M. (2010). \"Temperatures and cyclones strongly associated with economic production.\" *PNAS*, 107(35), 15367-15372.\n",
    "\n",
    "**Software**:\n",
    "- **Stata**: `acreg` command (Colella et al. 2019)\n",
    "- **R**: `sandwich::vcovHAC` with spatial extension\n",
    "- **Python**: Manual implementation (as shown here)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Further Topics** (advanced):\n",
    "1. Spatial HAC for nonlinear models (MLE, GMM)\n",
    "2. Spatial lag/error models (spatial econometrics)\n",
    "3. Two-way clustering + spatial HAC\n",
    "4. Optimal bandwidth selection (data-driven cutoffs)\n",
    "\n",
    "**Practice**:\n",
    "- Apply to your own geographic data\n",
    "- Compare with clustering methods\n",
    "- Experiment with different kernels and cutoffs\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the **Spatial HAC Standard Errors** tutorial.\n",
    "\n",
    "**You now know how to**:\n",
    "\u2713 Recognize spatial correlation in data\n",
    "\u2713 Construct distance matrices from coordinates\n",
    "\u2713 Implement Conley (1999) Spatial HAC estimator\n",
    "\u2713 Choose appropriate spatial and temporal cutoffs\n",
    "\u2713 Visualize spatial patterns\n",
    "\u2713 Conduct robust inference with geographic data\n",
    "\n",
    "**Next Tutorial**: Advanced topics (MLE, GMM, nonlinear models)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
