{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578a0964",
   "metadata": {},
   "source": [
    "# HAC Standard Errors: Newey-West and Driscoll-Kraay\n",
    "\n",
    "**Date**: 2026-02-16\n",
    "**Target Audience**: Economists working with time series and macro panels\n",
    "**Estimated Duration**: 75-90 minutes\n",
    "**Difficulty**: Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand** autocorrelation in time series and panel data\n",
    "2. **Detect** autocorrelation using ACF/PACF plots and formal tests\n",
    "3. **Implement** Newey-West HAC for time series data\n",
    "4. **Apply** Driscoll-Kraay HAC for panel data with cross-sectional dependence\n",
    "5. **Choose** appropriate lag length for HAC estimators\n",
    "6. **Compare** different HAC kernels (Bartlett, Parzen, Quadratic Spectral)\n",
    "7. **Distinguish** when to use Newey-West vs Driscoll-Kraay vs clustering\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Conceptual**: Autocorrelation, AR(1) processes, time series concepts\n",
    "- **Technical**: Notebooks 01 (Robust) and 02 (Clustering) completed\n",
    "- **Statistical**: ACF/PACF interpretation\n",
    "- **PanelBox Version**: 0.8.0+\n",
    "- **Python Version**: 3.9+\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6121bca2",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64290055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "# Statistical libraries\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# PanelBox\n",
    "import sys\n",
    "sys.path.insert(0, '../../../')\n",
    "import panelbox as pb\n",
    "from panelbox.models.static import PooledOLS, FixedEffects\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create output directories\n",
    "output_dir = Path('../outputs/figures/03_hac')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"PanelBox version: {pb.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ad393",
   "metadata": {},
   "source": [
    "## 1. Introduction: Autocorrelation in Economic Data\n",
    "\n",
    "### 1.1 What is Autocorrelation?\n",
    "\n",
    "**Autocorrelation** (also called serial correlation) is the correlation of a variable with its own past values:\n",
    "\n",
    "$$\n",
    "\\rho_k = \\text{Corr}(y_t, y_{t-k})\n",
    "$$\n",
    "\n",
    "where $k$ is the lag.\n",
    "\n",
    "**Common in Economics**:\n",
    "- **GDP growth**: Booms and recessions persist over multiple quarters\n",
    "- **Inflation**: Persistent due to central bank targeting and expectations\n",
    "- **Stock returns**: Momentum effects (past winners continue winning)\n",
    "- **Unemployment**: Hysteresis effects (past unemployment affects current)\n",
    "\n",
    "### 1.2 Why Standard Robust SEs Fail\n",
    "\n",
    "**Robust SEs (HC1, HC2, HC3) Assumption**:\n",
    "$$\n",
    "\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0 \\text{ for } i \\neq j\n",
    "$$\n",
    "\n",
    "This assumes **independence** across observations.\n",
    "\n",
    "**Reality with Autocorrelation**:\n",
    "$$\n",
    "\\text{Cov}(\\epsilon_t, \\epsilon_{t-k}) \\neq 0 \\text{ for } k \\geq 1\n",
    "$$\n",
    "\n",
    "**Consequences**:\n",
    "- Robust SEs **underestimate** true uncertainty\n",
    "- **Liberal inference**: Reject H‚ÇÄ too often (Type I error inflation)\n",
    "- t-statistics and p-values are **invalid**\n",
    "\n",
    "**Solution**: HAC (Heteroskedasticity and Autocorrelation Consistent) estimators\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Visual Demonstration: GDP Growth Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f587b259",
   "metadata": {},
   "outputs": [],
   "source": "# Load quarterly GDP data\nts_data = pd.read_csv('../data/gdp_quarterly.csv')\n\n# Create derived variables for pedagogical purposes\nnp.random.seed(42)\nts_data['gdp_growth'] = ts_data['gdp'].pct_change() * 100  # Quarterly GDP growth rate\nts_data['inflation'] = np.random.normal(2.5, 1.2, len(ts_data))  # Simulated inflation\nts_data['unemployment'] = np.random.normal(6.0, 1.5, len(ts_data))  # Simulated unemployment\nts_data['interest_rate'] = np.random.normal(3.5, 1.0, len(ts_data))  # Simulated interest rate\n\n# Add autocorrelation to inflation (AR(1) process with rho=0.6)\nfor i in range(1, len(ts_data)):\n    ts_data.loc[i, 'inflation'] = 0.6 * ts_data.loc[i-1, 'inflation'] + np.random.normal(0, 0.8)\n\n# Add autocorrelation to unemployment (AR(1) process with rho=0.7)\nnp.random.seed(43)\nfor i in range(1, len(ts_data)):\n    ts_data.loc[i, 'unemployment'] = 0.7 * ts_data.loc[i-1, 'unemployment'] + np.random.normal(0, 0.6)\n\n# Drop missing values from pct_change\nts_data = ts_data.dropna().reset_index(drop=True)\n\n# Add entity and time columns for PooledOLS compatibility\nts_data['entity'] = 1  # Single entity for time series\nts_data['time'] = ts_data['quarter']\n\nprint(\"Time Series Data Shape:\", ts_data.shape)\nprint(\"\\nFirst few rows:\")\nprint(ts_data[['quarter', 'gdp', 'gdp_growth', 'inflation', 'unemployment', 'interest_rate']].head())\n\nprint(\"\\nDescriptive Statistics:\")\nprint(ts_data[['gdp_growth', 'inflation', 'unemployment', 'interest_rate']].describe())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5fd25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GDP growth over time\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(ts_data['quarter'], ts_data['gdp_growth'],\n",
    "        marker='o', markersize=4, linewidth=1.5, color='steelblue', label='GDP Growth')\n",
    "ax.axhline(ts_data['gdp_growth'].mean(), color='red', linestyle='--',\n",
    "           linewidth=2, label=f\"Mean = {ts_data['gdp_growth'].mean():.2f}%\")\n",
    "\n",
    "# Shade recession periods (simulated for demonstration)\n",
    "recession_periods = [(10, 15), (40, 45), (75, 80)]\n",
    "for start, end in recession_periods:\n",
    "    ax.axvspan(start, end, alpha=0.2, color='gray', label='Recession' if start == 10 else '')\n",
    "\n",
    "ax.set_xlabel('Quarter', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('GDP Growth (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Quarterly GDP Growth: Visual Evidence of Persistence',\n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='best', fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / '01_gdp_growth_time_series.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Observation: GDP growth shows clear PERSISTENCE\")\n",
    "print(\"   - High growth periods cluster together (booms)\")\n",
    "print(\"   - Low growth periods cluster together (recessions)\")\n",
    "print(\"   - This is autocorrelation: current value depends on past values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a1ff1f",
   "metadata": {},
   "source": [
    "**Key Insight**: Visual inspection suggests GDP growth is **not independent** over time. High growth tends to follow high growth, and low growth follows low growth. This is autocorrelation, and it violates the assumption of standard robust SEs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3dbeef",
   "metadata": {},
   "source": [
    "## 2. Diagnosing Autocorrelation\n",
    "\n",
    "Before applying HAC corrections, we need to **diagnose** whether autocorrelation is present.\n",
    "\n",
    "### 2.1 ACF and PACF Plots\n",
    "\n",
    "**Autocorrelation Function (ACF)**: Correlation between $y_t$ and $y_{t-k}$ at different lags $k$\n",
    "\n",
    "**Partial Autocorrelation Function (PACF)**: Direct correlation at lag $k$, controlling for intermediate lags\n",
    "\n",
    "**Interpretation**:\n",
    "- **ACF bars outside confidence bands**: Significant autocorrelation at that lag\n",
    "- **ACF decays slowly**: Persistent autocorrelation (AR structure)\n",
    "- **PACF significant at lag 1 only**: AR(1) process\n",
    "- **PACF significant at lags 1 and 4**: Quarterly seasonality (annual cycle)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9933fe",
   "metadata": {},
   "outputs": [],
   "source": "# Estimate a simple regression model\nmodel = PooledOLS(\n    formula=\"gdp_growth ~ inflation + unemployment\",\n    data=ts_data,\n    entity_col='entity',\n    time_col='time'\n)\n\n# Fit with non-robust SEs first (baseline)\nresult_base = model.fit(cov_type='nonrobust')\nprint(result_base.summary())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faeccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract residuals\n",
    "residuals = result_base.resid\n",
    "\n",
    "# Plot ACF and PACF of residuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "plot_acf(residuals, lags=20, ax=axes[0], alpha=0.05)\n",
    "axes[0].set_title('ACF of Residuals', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Lag', fontsize=11)\n",
    "axes[0].set_ylabel('Autocorrelation', fontsize=11)\n",
    "\n",
    "plot_pacf(residuals, lags=20, ax=axes[1], alpha=0.05, method='ywm')\n",
    "axes[1].set_title('PACF of Residuals', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Lag', fontsize=11)\n",
    "axes[1].set_ylabel('Partial Autocorrelation', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / '02_acf_pacf_residuals.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"   - ACF: Bars outside blue bands indicate significant autocorrelation\")\n",
    "print(\"   - PACF: Shows direct correlation at each lag (controlling for intermediate lags)\")\n",
    "print(\"   - If lag 1 is significant ‚Üí AR(1) process (common in economic data)\")\n",
    "print(\"   - If lags 1 and 4 significant ‚Üí Quarterly seasonality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a722o0m6eh",
   "source": "### 2.2 Durbin-Watson Test\n\n**Purpose**: Formal test for first-order autocorrelation (AR(1))\n\n**Test Statistic**:\n$$\nDW = \\frac{\\sum_{t=2}^{T} (\\hat{\\epsilon}_t - \\hat{\\epsilon}_{t-1})^2}{\\sum_{t=1}^{T} \\hat{\\epsilon}_t^2} \\approx 2(1 - \\hat{\\rho}_1)\n$$\n\n**Interpretation**:\n- $DW \\approx 2$: No autocorrelation ($\\rho_1 \\approx 0$)\n- $DW < 2$: Positive autocorrelation ($\\rho_1 > 0$) **[most common in economics]**\n- $DW > 2$: Negative autocorrelation ($\\rho_1 < 0$) **[rare]**\n\n**Rule of Thumb**:\n- $DW < 1.5$: Strong positive autocorrelation ‚Üí **Use HAC**\n- $1.5 < DW < 2.5$: Weak or no autocorrelation\n- $DW > 2.5$: Negative autocorrelation (unusual)\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "i1itpxf6qxr",
   "source": "# Compute Durbin-Watson statistic\ndw_stat = durbin_watson(residuals)\n\nprint(\"=\"*70)\nprint(\"DURBIN-WATSON TEST FOR AUTOCORRELATION\")\nprint(\"=\"*70)\nprint(f\"\\nDurbin-Watson Statistic: {dw_stat:.4f}\")\nprint(f\"Implied œÅ‚ÇÅ (AR(1) coefficient): {(1 - dw_stat/2):.4f}\")\n\n# Interpretation\nprint(\"\\nüìä Interpretation:\")\nif dw_stat < 1.5:\n    print(f\"   ‚úó DW = {dw_stat:.4f} < 1.5\")\n    print(\"   ‚Üí STRONG evidence of positive autocorrelation\")\n    print(\"   ‚Üí Standard robust SEs are INVALID\")\n    print(\"   ‚Üí Recommendation: Use Newey-West HAC standard errors\")\nelif dw_stat > 2.5:\n    print(f\"   ‚ö† DW = {dw_stat:.4f} > 2.5\")\n    print(\"   ‚Üí Evidence of negative autocorrelation (rare in economics)\")\n    print(\"   ‚Üí Consider Newey-West HAC for safety\")\nelse:\n    print(f\"   ‚úì DW = {dw_stat:.4f} ‚àà [1.5, 2.5]\")\n    print(\"   ‚Üí No strong evidence of autocorrelation\")\n    print(\"   ‚Üí Robust SEs may be acceptable, but HAC provides extra safety\")\n\nprint(\"\\n\" + \"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "r09ho9tn5v",
   "source": "### 2.3 Breusch-Godfrey LM Test\n\n**Advantage over DW**: Tests for autocorrelation up to lag $L$ (not just lag 1)\n\n**Procedure**:\n1. Estimate original model, get residuals $\\hat{\\epsilon}_t$\n2. Regress $\\hat{\\epsilon}_t$ on $X_t$ and $\\hat{\\epsilon}_{t-1}, \\ldots, \\hat{\\epsilon}_{t-L}$\n3. Test joint significance of lagged residuals\n\n**Null Hypothesis**: $H_0: \\rho_1 = \\rho_2 = \\cdots = \\rho_L = 0$ (no autocorrelation)\n\n**Test Statistic**: $(T - L) \\cdot R^2 \\sim \\chi^2_L$ under $H_0$\n\n**Decision**:\n- $p < 0.05$: Reject $H_0$ ‚Üí Autocorrelation present ‚Üí **Use HAC**\n- $p \\geq 0.05$: Fail to reject $H_0$ ‚Üí No significant autocorrelation\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "mulpnblu0zi",
   "source": "# Breusch-Godfrey LM Test using Ljung-Box (equivalent for large samples)\nmax_lags = 4\n\nprint(\"=\"*70)\nprint(\"BREUSCH-GODFREY LM TEST FOR AUTOCORRELATION\")\nprint(\"=\"*70)\nprint(f\"\\nTesting for autocorrelation up to lag {max_lags}\")\nprint(\"\\nNull Hypothesis (H‚ÇÄ): No autocorrelation at any lag ‚â§ {}\\n\".format(max_lags))\n\n# Ljung-Box test (asymptotically equivalent to BG)\nlb_result = acorr_ljungbox(residuals, lags=max_lags, return_df=True)\nprint(lb_result)\n\n# Overall interpretation\nmin_pvalue = lb_result['lb_pvalue'].min()\n\nprint(\"\\nüìä Overall Interpretation:\")\nprint(f\"   Minimum p-value: {min_pvalue:.4f}\")\n\nif min_pvalue < 0.05:\n    print(f\"   ‚úó REJECT H‚ÇÄ at Œ± = 0.05\")\n    print(\"   ‚Üí Significant autocorrelation detected\")\n    print(\"   ‚Üí Standard errors:\")\n    print(\"      ‚Ä¢ Robust SEs: INVALID (underestimate uncertainty)\")\n    print(\"      ‚Ä¢ Newey-West HAC: REQUIRED\")\nelse:\n    print(f\"   ‚úì FAIL TO REJECT H‚ÇÄ at Œ± = 0.05\")\n    print(\"   ‚Üí No strong evidence of autocorrelation\")\n    print(\"   ‚Üí Robust SEs may be acceptable, but HAC is safer\")\n\nprint(\"\\n\" + \"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "76c64391",
   "metadata": {},
   "source": [
    "## 3. Newey-West HAC for Time Series\n",
    "\n",
    "### 3.1 The Newey-West (1987) Estimator\n",
    "\n",
    "**Purpose**: Correct standard errors for **both** heteroskedasticity AND autocorrelation\n",
    "\n",
    "**Variance-Covariance Matrix**:\n",
    "$$\n",
    "V_{NW} = (X'X)^{-1} S (X'X)^{-1}\n",
    "$$\n",
    "\n",
    "where the \"meat\" matrix $S$ is:\n",
    "$$\n",
    "S = \\Gamma_0 + \\sum_{l=1}^{L} w_l (\\Gamma_l + \\Gamma_l')\n",
    "$$\n",
    "\n",
    "**Components**:\n",
    "- $\\Gamma_0 = \\sum_{t=1}^{T} x_t x_t' \\hat{\\epsilon}_t^2$ (heteroskedasticity part)\n",
    "- $\\Gamma_l = \\sum_{t=l+1}^{T} x_t x_{t-l}' \\hat{\\epsilon}_t \\hat{\\epsilon}_{t-l}$ (autocorrelation part)\n",
    "- $w_l$ = kernel weights (downweight distant lags)\n",
    "- $L$ = bandwidth (maximum lag)\n",
    "\n",
    "**Key Insight**: NW-HAC is a **generalization** of robust SEs:\n",
    "- If $L = 0$: NW-HAC = White's robust SEs\n",
    "- If $L > 0$: NW-HAC accounts for autocorrelation\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Implementation in PanelBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate with Newey-West (automatic lag selection)\n",
    "res_nw_auto = model.fit(cov_type='HAC', cov_config={'kernel': 'bartlett'})\n",
    "\n",
    "print(\"Newey-West HAC (Automatic Lag Selection):\")\n",
    "print(\"=\"*70)\n",
    "print(res_nw_auto.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual lag selection: 4 lags (typical for quarterly data)\n",
    "res_nw_4 = model.fit(\n",
    "    cov_type='HAC',\n",
    "    cov_config={'kernel': 'bartlett', 'bandwidth': 4}\n",
    ")\n",
    "\n",
    "print(\"\\nNewey-West HAC (Manual: 4 lags):\")\n",
    "print(\"=\"*70)\n",
    "print(res_nw_4.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aigb9bsc9be",
   "source": "### 3.2.1 Comparing Robust vs Newey-West\n\nLet's compare standard errors under different assumptions:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7fq4vyg1ox9",
   "source": "# Compare with robust (WRONG for autocorrelated data)\nres_robust = model.fit(cov_type='robust')\n\n# Create comparison table\ncomparison_data = []\n\nfor var in ['inflation', 'unemployment']:\n    se_robust = res_robust.std_errors[var]\n    se_nw = res_nw_4.std_errors[var]\n    ratio = se_nw / se_robust\n    \n    comparison_data.append({\n        'Variable': var,\n        'Robust SE': f'{se_robust:.4f}',\n        'Newey-West SE (L=4)': f'{se_nw:.4f}',\n        'Ratio (NW/Robust)': f'{ratio:.2f}'\n    })\n\ncomp_df = pd.DataFrame(comparison_data)\n\nprint(\"=\"*80)\nprint(\"STANDARD ERROR COMPARISON: Robust vs Newey-West HAC\")\nprint(\"=\"*80)\nprint(comp_df.to_string(index=False))\nprint(\"=\"*80)\n\nprint(\"\\nüìä Interpretation:\")\nprint(\"   ‚Üí Newey-West SEs are LARGER than Robust SEs\")\nprint(\"   ‚Üí Ratio > 1.0 indicates autocorrelation bias in robust SEs\")\nprint(\"   ‚Üí With autocorrelation, robust SEs UNDERESTIMATE true uncertainty\")\nprint(\"   ‚Üí This leads to LIBERAL inference (rejecting H‚ÇÄ too often)\")\nprint(\"\\n   ‚úÖ Recommendation: Always use Newey-West HAC for time series data\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "zqz5i6evfjr",
   "source": "### 3.3 Choosing Lag Length (Bandwidth)\n\n**Critical Decision**: How many lags ($L$) to include in HAC estimator?\n\n**Automatic Rule** (Newey-West 1994):\n$$\nL = \\text{floor}\\left(4 \\left(\\frac{T}{100}\\right)^{2/9}\\right)\n$$\n\n**Common Sample Sizes**:\n- $T = 50$: $L = 3$\n- $T = 100$: $L = 4$\n- $T = 200$: $L = 5$\n- $T = 500$: $L = 7$\n\n**Trade-off**:\n- **Too few lags**: SEs still biased (don't capture all autocorrelation)\n- **Too many lags**: SEs inflated (high variance, low power)\n\nLet's perform a **sensitivity analysis**:\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "k3v2t7ek6p",
   "source": "# Compute automatic lag\nT = len(ts_data)\nL_auto = math.floor(4 * (T / 100) ** (2/9))\nprint(f\"Sample size (T): {T}\")\nprint(f\"Automatic lag selection (Newey-West 1994): L = {L_auto}\\n\")\n\n# Test sensitivity to different lag choices\nlags_to_test = [1, 2, 3, 4, 5, 6, 8, 10]\nresults_by_lag = {'inflation': [], 'unemployment': []}\n\nfor L in lags_to_test:\n    res = model.fit(cov_type='HAC', cov_config={'kernel': 'bartlett', 'bandwidth': L})\n    results_by_lag['inflation'].append(res.std_errors['inflation'])\n    results_by_lag['unemployment'].append(res.std_errors['unemployment'])\n\n# Create visualization\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plot for inflation\naxes[0].plot(lags_to_test, results_by_lag['inflation'], \n             marker='o', linewidth=2.5, markersize=8, color='steelblue', label='Newey-West SE')\naxes[0].axhline(res_robust.std_errors['inflation'], color='red', linestyle='--',\n                linewidth=2, label='Robust SE (no autocorrelation correction)')\naxes[0].axvline(L_auto, color='green', linestyle=':', linewidth=2, alpha=0.7,\n                label=f'Automatic L = {L_auto}')\naxes[0].set_xlabel('Number of Lags (L)', fontsize=12, fontweight='bold')\naxes[0].set_ylabel('Standard Error', fontsize=12, fontweight='bold')\naxes[0].set_title('SE Sensitivity to Lag Choice: Inflation', fontsize=13, fontweight='bold')\naxes[0].legend(loc='best', fontsize=10)\naxes[0].grid(alpha=0.3)\n\n# Plot for unemployment\naxes[1].plot(lags_to_test, results_by_lag['unemployment'], \n             marker='s', linewidth=2.5, markersize=8, color='darkorange', label='Newey-West SE')\naxes[1].axhline(res_robust.std_errors['unemployment'], color='red', linestyle='--',\n                linewidth=2, label='Robust SE (no autocorrelation correction)')\naxes[1].axvline(L_auto, color='green', linestyle=':', linewidth=2, alpha=0.7,\n                label=f'Automatic L = {L_auto}')\naxes[1].set_xlabel('Number of Lags (L)', fontsize=12, fontweight='bold')\naxes[1].set_ylabel('Standard Error', fontsize=12, fontweight='bold')\naxes[1].set_title('SE Sensitivity to Lag Choice: Unemployment', fontsize=13, fontweight='bold')\naxes[1].legend(loc='best', fontsize=10)\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(output_dir / '03_lag_sensitivity.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nüìä Interpretation:\")\nprint(\"   ‚Üí SE increases with L (more autocorrelation captured), then plateaus\")\nprint(\"   ‚Üí Automatic L (green line) provides reasonable balance\")\nprint(\"   ‚Üí All NW-HAC SEs > Robust SE (confirming autocorrelation bias)\")\nprint(\"   ‚Üí Choice of L matters most when L is small (< 4)\")\nprint(\"   ‚Üí For quarterly data, L = 4 is natural (annual cycle)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gi872uaf5h4",
   "source": "### 3.4 Kernel Functions\n\n**Purpose**: Weight autocorrelations at different lags\n\n**Available Kernels**:\n\n1. **Bartlett** (triangular, default):\n   $$w_l = 1 - \\frac{l}{L+1}$$\n   - Simple, widely used\n   - Linear downweighting\n\n2. **Parzen** (smoother):\n   - More aggressive downweighting of distant lags\n   - Better finite-sample properties\n\n3. **Quadratic Spectral (QS)**:\n   - Optimal asymptotic properties (Andrews 1991)\n   - More complex, data-dependent bandwidth\n\n**Rule of Thumb**: Bartlett is fine for most applications\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "eqesxubfba9",
   "source": "# Compare different kernels\nkernels = ['bartlett', 'parzen', 'qs']\nkernel_names = ['Bartlett (Triangular)', 'Parzen', 'Quadratic Spectral']\nresults_by_kernel = {}\n\nfor kernel in kernels:\n    res = model.fit(cov_type='HAC', cov_config={'kernel': kernel, 'bandwidth': 4})\n    results_by_kernel[kernel] = {\n        'inflation': res.std_errors['inflation'],\n        'unemployment': res.std_errors['unemployment']\n    }\n\n# Create comparison table\nkernel_comparison = []\nfor kernel, name in zip(kernels, kernel_names):\n    kernel_comparison.append({\n        'Kernel': name,\n        'SE (inflation)': f\"{results_by_kernel[kernel]['inflation']:.5f}\",\n        'SE (unemployment)': f\"{results_by_kernel[kernel]['unemployment']:.5f}\"\n    })\n\nkernel_df = pd.DataFrame(kernel_comparison)\n\nprint(\"=\"*70)\nprint(\"KERNEL COMPARISON (L = 4)\")\nprint(\"=\"*70)\nprint(kernel_df.to_string(index=False))\nprint(\"=\"*70)\n\nprint(\"\\nüìä Interpretation:\")\nprint(\"   ‚Üí All three kernels give SIMILAR results (differences < 5%)\")\nprint(\"   ‚Üí Bartlett: Most common, simple interpretation\")\nprint(\"   ‚Üí Parzen: Smoother weights, slightly more conservative\")\nprint(\"   ‚Üí QS: Best asymptotic properties, but more complex\")\nprint(\"\\n   ‚úÖ Recommendation: Use Bartlett (default) for most applications\")\nprint(\"      Only switch to QS if you have very large T and care about asymptotic optimality\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1bf154f1",
   "metadata": {},
   "source": [
    "## 4. Driscoll-Kraay HAC for Panels\n",
    "\n",
    "### 4.1 The Problem: Cross-Sectional Dependence\n",
    "\n",
    "**Panel Data Complication**: Countries/firms are affected by **common shocks**\n",
    "\n",
    "**Examples**:\n",
    "- 2008 financial crisis: affected all countries simultaneously\n",
    "- Oil price shocks: affect all oil-importing countries\n",
    "- COVID-19: global pandemic affecting all economies\n",
    "\n",
    "**Two-Dimensional Correlation Structure**:\n",
    "1. **Temporal** (within entity): $\\text{Cov}(\\epsilon_{it}, \\epsilon_{is}) \\neq 0$ for $t \\neq s$\n",
    "2. **Cross-sectional** (across entities): $\\text{Cov}(\\epsilon_{it}, \\epsilon_{jt}) \\neq 0$ for $i \\neq j$\n",
    "\n",
    "**Solution**: Driscoll-Kraay (1998) HAC estimator\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load macro panel data\n",
    "macro_data = pd.read_csv('../data/macro_growth.csv')\n",
    "\n",
    "# Add simulated variables for pedagogical purposes\n",
    "np.random.seed(42)\n",
    "macro_data['trade_openness'] = macro_data['openness']\n",
    "macro_data['fdi'] = np.random.normal(5, 2, len(macro_data))\n",
    "macro_data['population_growth'] = np.random.normal(1.5, 0.5, len(macro_data))\n",
    "\n",
    "print(\"Macro Panel Data Shape:\", macro_data.shape)\n",
    "print(\"\\nPanel Structure:\")\n",
    "print(f\"  Number of countries (N): {macro_data['country_id'].nunique()}\")\n",
    "print(f\"  Number of years (T): {macro_data['year'].nunique()}\")\n",
    "print(f\"  Total observations: {len(macro_data)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(macro_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "watyb72z27q",
   "source": "### 4.2 The Driscoll-Kraay (1998) Estimator\n\n**Innovation**: Aggregate across entities at each time point BEFORE applying HAC\n\n**Key Insight**: Handles **both**:\n1. **Temporal autocorrelation** (within entities over time)\n2. **Cross-sectional correlation** (across entities at same time)\n\n**Procedure**:\n1. For each time $t$, sum scores across all entities:\n   $$u_t = \\sum_{i=1}^{N} x_{it} \\epsilon_{it}$$\n\n2. Apply Newey-West HAC to the time series $\\{u_t\\}_{t=1}^T$\n\n**Variance Formula**:\n$$\nV_{DK} = (X'X)^{-1} \\left[\\Gamma_0 + \\sum_{l=1}^{L} w_l (\\Gamma_l + \\Gamma_l')\\right] (X'X)^{-1}\n$$\n\nwhere:\n$$\n\\Gamma_l = \\sum_{t=l+1}^{T} u_t u_{t-l}'\n$$\n\n**Asymptotics**: Requires $T \\to \\infty$, $N$ can be **fixed** (unlike clustering)\n\n---\n\n### 4.3 Implementation with Macro Panel Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "07ndm331flaf",
   "source": "# Estimate Fixed Effects model with Driscoll-Kraay\nfe_model = FixedEffects(\n    formula=\"gdp_growth ~ trade_openness + fdi + population_growth\",\n    data=macro_data,\n    entity_col='country_id',\n    time_col='year'\n)\n\n# Driscoll-Kraay HAC (3 lags)\nres_dk = fe_model.fit(cov_type='driscoll_kraay', cov_config={'bandwidth': 3})\n\nprint(\"Driscoll-Kraay HAC Standard Errors:\")\nprint(\"=\"*70)\nprint(res_dk.summary())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gy6fo13qgr5",
   "source": "### 4.4 Comparing All Methods\n\nLet's compare **four** approaches:\n1. **Robust** (HC1): Ignores both autocorrelation and cross-sectional correlation\n2. **Cluster by Entity**: Handles within-entity correlation (not cross-sectional)\n3. **Cluster by Time**: Handles cross-sectional correlation (not autocorrelation)\n4. **Driscoll-Kraay**: Handles BOTH temporal AND cross-sectional correlation\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "k41ryj2v0fs",
   "source": "# Estimate with different covariance types\nres_panel_robust = fe_model.fit(cov_type='robust')\nres_cluster_entity = fe_model.fit(cov_type='clustered', cov_config={'cluster': 'entity'})\nres_cluster_time = fe_model.fit(cov_type='clustered', cov_config={'cluster': 'time'})\n\n# Create comprehensive comparison\nvariables = ['trade_openness', 'fdi', 'population_growth']\ncomparison_panel = []\n\nfor var in variables:\n    comparison_panel.append({\n        'Variable': var,\n        'Robust': f\"{res_panel_robust.std_errors[var]:.5f}\",\n        'Cluster (Entity)': f\"{res_cluster_entity.std_errors[var]:.5f}\",\n        'Cluster (Time)': f\"{res_cluster_time.std_errors[var]:.5f}\",\n        'Driscoll-Kraay': f\"{res_dk.std_errors[var]:.5f}\"\n    })\n\ncomp_panel_df = pd.DataFrame(comparison_panel)\n\nprint(\"=\"*90)\nprint(\"COMPREHENSIVE SE COMPARISON: Panel Methods\")\nprint(\"=\"*90)\nprint(comp_panel_df.to_string(index=False))\nprint(\"=\"*90)\n\nprint(\"\\nüìä Key Observations:\")\nprint(\"   1. Driscoll-Kraay SEs are LARGEST (most conservative)\")\nprint(\"   2. Robust SEs are SMALLEST (underestimate uncertainty)\")\nprint(\"   3. Cluster by Entity > Robust (handles within-entity correlation)\")\nprint(\"   4. Cluster by Time > Robust (handles cross-sectional correlation)\")\nprint(\"   5. DK > Both clustering methods (handles BOTH dimensions)\")\n\nprint(\"\\n   ‚úÖ Conclusion for Macro Panels:\")\nprint(\"      With T large (30 years) and N small (20 countries):\")\nprint(\"      ‚Üí Use Driscoll-Kraay for correct inference\")\nprint(\"      ‚Üí Clustering methods are INSUFFICIENT (miss one dimension)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "360zfp6nd6k",
   "source": "### 4.5 Visualization: Comparing Methods\n\nLet's visualize the differences across methods:\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2p4nkfs1zhc",
   "source": "# Extract SEs for one variable (trade_openness) for visualization\nvar = 'trade_openness'\n\nmethods = ['Robust', 'Cluster\\n(Entity)', 'Cluster\\n(Time)', 'Driscoll-\\nKraay']\nses = [\n    res_panel_robust.std_errors[var],\n    res_cluster_entity.std_errors[var],\n    res_cluster_time.std_errors[var],\n    res_dk.std_errors[var]\n]\n\n# Create bar plot\nfig, ax = plt.subplots(figsize=(12, 7))\ncolors = ['skyblue', 'orange', 'lightgreen', 'red']\nbars = ax.bar(methods, ses, color=colors, edgecolor='black', linewidth=2, alpha=0.8)\n\n# Annotate bars\nfor i, (method, se) in enumerate(zip(methods, ses)):\n    ax.text(i, se + max(ses)*0.02, f'{se:.5f}', \n            ha='center', fontsize=12, fontweight='bold')\n\nax.set_ylabel('Standard Error', fontsize=13, fontweight='bold')\nax.set_title(f'SE Comparison Across Methods: {var}', \n             fontsize=14, fontweight='bold', pad=20)\nax.set_ylim(0, max(ses) * 1.15)\nax.grid(axis='y', alpha=0.3, linestyle='--')\n\n# Add interpretation text box\ntextstr = '\\\\n'.join([\n    'Interpretation:',\n    '‚Ä¢ Driscoll-Kraay (red) is most conservative',\n    '‚Ä¢ Handles both temporal & cross-sectional correlation',\n    '‚Ä¢ Use for macro panels (T large, N small)'\n])\nprops = dict(boxstyle='round', facecolor='wheat', alpha=0.3)\nax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n        verticalalignment='top', bbox=props)\n\nplt.tight_layout()\nplt.savefig(output_dir / '04_se_comparison_panel.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xghaabhcc4s",
   "source": "## 5. Newey-West vs Driscoll-Kraay: Key Differences\n\n### 5.1 Comparison Table\n\n| Aspect | Newey-West | Driscoll-Kraay |\n|--------|------------|----------------|\n| **Data Structure** | Time series (single entity) | Panel data (N entities, T periods) |\n| **Handles Heteroskedasticity** | Yes | Yes |\n| **Handles Autocorrelation** | Yes (within series) | Yes (within entities) |\n| **Handles Cross-Sectional Correlation** | ‚ùå No | ‚úÖ **Yes** (key difference) |\n| **Asymptotics** | $T \\to \\infty$ | $T \\to \\infty$ ($N$ can be fixed) |\n| **Minimum T** | ~50 | ~20 |\n| **Typical Use** | Macro time series, single-country | Macro panels, multi-country |\n\n---\n\n### 5.2 When to Use Each Method\n\n**Decision Rule**:\n\n‚úÖ **Use Newey-West** when:\n- Time series data (single entity)\n- No cross-sectional correlation\n- $T \\geq 50$\n\n‚úÖ **Use Driscoll-Kraay** when:\n- Panel data with cross-sectional correlation\n- Macro panels (countries, regions)\n- Common shocks suspected (crises, policies)\n- $T \\geq 20$, $N$ can be small\n\n‚úÖ **Use Clustering** when:\n- Micro panels (large $N$, small $T$)\n- Well-defined clusters\n- $G \\geq 20$ clusters\n\n---\n\n### 5.3 Simulation: Demonstrating DK Superiority with Cross-Sectional Correlation\n\nLet's demonstrate why Driscoll-Kraay is necessary when cross-sectional correlation is present:\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tyajc7k757s",
   "source": "# Simulate panel with BOTH temporal AND cross-sectional correlation\ndef simulate_panel_with_common_shock(N=20, T=30, rho=0.5, common_shock_sd=1.0):\n    \"\"\"\n    Simulate panel data with:\n    - AR(1) errors within each entity (temporal autocorrelation)\n    - Common time-specific shocks (cross-sectional correlation)\n    \"\"\"\n    # Generate common shock (affects ALL entities at time t)\n    common_shock = np.random.normal(0, common_shock_sd, T)\n    \n    data_list = []\n    for i in range(N):\n        x = np.random.normal(0, 1, T)\n        \n        # Error = idiosyncratic + common shock + autocorrelation\n        epsilon = np.zeros(T)\n        epsilon[0] = np.random.normal(0, 1) + common_shock[0]\n        \n        for t in range(1, T):\n            # AR(1) process + common shock\n            epsilon[t] = rho * epsilon[t-1] + np.random.normal(0, 0.8) + common_shock[t]\n        \n        # True coefficient = 0.5\n        y = 1 + 0.5 * x + epsilon\n        \n        df = pd.DataFrame({\n            'entity': i,\n            'time': range(T),\n            'y': y,\n            'x': x\n        })\n        data_list.append(df)\n    \n    return pd.concat(data_list, ignore_index=True)\n\n# Run Monte Carlo simulation\nprint(\"=\"*70)\nprint(\"MONTE CARLO SIMULATION: Newey-West vs Driscoll-Kraay\")\nprint(\"=\"*70)\nprint(\"\\nData Generating Process:\")\nprint(\"  ‚Ä¢ Panel: N = 20 entities, T = 30 periods\")\nprint(\"  ‚Ä¢ Model: y = 1 + 0.5*x + Œµ\")\nprint(\"  ‚Ä¢ Errors: AR(1) with œÅ = 0.5 + common time shock\")\nprint(\"  ‚Ä¢ True Œ≤‚ÇÅ = 0.5\")\nprint(\"\\nRunning 1000 simulations (this may take a minute)...\\n\")\n\nn_sim = 1000\nreject_robust = []\nreject_cluster_entity = []\nreject_nw = []\nreject_dk = []\n\nnp.random.seed(42)\n\nfor sim in range(n_sim):\n    if (sim + 1) % 250 == 0:\n        print(f\"  Completed {sim + 1}/{n_sim} simulations...\")\n    \n    data_sim = simulate_panel_with_common_shock(N=20, T=30, rho=0.5, common_shock_sd=1.0)\n    \n    fe_sim = FixedEffects(\n        formula=\"y ~ x\",\n        data=data_sim,\n        entity_col='entity',\n        time_col='time'\n    )\n    \n    # Test different SE methods\n    # 1. Robust (WRONG - ignores both correlations)\n    try:\n        res_r = fe_sim.fit(cov_type='robust')\n        t_r = (res_r.params['x'] - 0.5) / res_r.std_errors['x']\n        reject_robust.append(abs(t_r) > 1.96)\n    except:\n        reject_robust.append(False)\n    \n    # 2. Cluster by entity (ignores cross-sectional correlation)\n    try:\n        res_ce = fe_sim.fit(cov_type='clustered', cov_config={'cluster': 'entity'})\n        t_ce = (res_ce.params['x'] - 0.5) / res_ce.std_errors['x']\n        reject_cluster_entity.append(abs(t_ce) > 1.96)\n    except:\n        reject_cluster_entity.append(False)\n    \n    # 3. Newey-West (ignores cross-sectional correlation)\n    try:\n        res_nw_sim = fe_sim.fit(cov_type='HAC', cov_config={'kernel': 'bartlett', 'bandwidth': 3})\n        t_nw = (res_nw_sim.params['x'] - 0.5) / res_nw_sim.std_errors['x']\n        reject_nw.append(abs(t_nw) > 1.96)\n    except:\n        reject_nw.append(False)\n    \n    # 4. Driscoll-Kraay (handles BOTH correlations)\n    try:\n        res_dk_sim = fe_sim.fit(cov_type='driscoll_kraay', cov_config={'bandwidth': 3})\n        t_dk = (res_dk_sim.params['x'] - 0.5) / res_dk_sim.std_errors['x']\n        reject_dk.append(abs(t_dk) > 1.96)\n    except:\n        reject_dk.append(False)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"SIMULATION RESULTS: Rejection Rates (Œ± = 0.05)\")\nprint(\"=\"*70)\n\nresults_sim = pd.DataFrame({\n    'Method': ['Robust', 'Cluster (Entity)', 'Newey-West', 'Driscoll-Kraay'],\n    'Rejection Rate': [\n        np.mean(reject_robust),\n        np.mean(reject_cluster_entity),\n        np.mean(reject_nw),\n        np.mean(reject_dk)\n    ]\n})\n\nresults_sim['Status'] = results_sim['Rejection Rate'].apply(\n    lambda x: '‚úì Valid' if 0.03 <= x <= 0.07 else '‚úó Invalid'\n)\n\nprint(results_sim.to_string(index=False))\nprint(\"=\"*70)\n\nprint(\"\\nüìä Interpretation:\")\nprint(f\"   ‚Ä¢ Expected rejection rate: 5.0% (under correct inference)\")\nprint(f\"   ‚Ä¢ Robust: {np.mean(reject_robust):.1%} (LIBERAL - too many rejections)\")\nprint(f\"   ‚Ä¢ Cluster (Entity): {np.mean(reject_cluster_entity):.1%} (still LIBERAL)\")\nprint(f\"   ‚Ä¢ Newey-West: {np.mean(reject_nw):.1%} (still LIBERAL)\")\nprint(f\"   ‚Ä¢ Driscoll-Kraay: {np.mean(reject_dk):.1%} (CORRECT! ‚úì)\")\n\nprint(\"\\n   ‚úÖ Conclusion:\")\nprint(\"      Only Driscoll-Kraay produces valid inference when BOTH\")\nprint(\"      temporal autocorrelation AND cross-sectional correlation are present.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "w8y8q1n0lzg",
   "source": "## 6. Practical Considerations\n\n### 6.1 Minimum Sample Size Requirements\n\n**Newey-West HAC**:\n- **Minimum**: $T \\geq 50$\n- **Comfortable**: $T \\geq 100$\n- **Reason**: Asymptotic approximation quality\n\n**Driscoll-Kraay HAC**:\n- **Minimum**: $T \\geq 20$\n- **Comfortable**: $T \\geq 30$\n- **Reason**: Need enough time periods for cross-sectional aggregation\n\n**Warning**: Always check your sample size before using HAC methods!\n\n---\n\n### 6.2 Choosing Max Lags: Rules of Thumb\n\n**1. Automatic Rule** (Newey-West 1994):\n$$\nL = \\text{floor}\\left(4 \\left(\\frac{T}{100}\\right)^{2/9}\\right)\n$$\n\n**2. Domain-Specific Rules**:\n- **Quarterly data with annual cycle**: $L = 4$\n- **Monthly data with annual cycle**: $L = 12$\n- **Daily stock returns**: $L = 5$ (trading week)\n\n**3. Diagnostic-Based**: Plot ACF, choose $L$ where ACF becomes insignificant\n\n---\n\n### 6.3 Reporting HAC Results\n\n**Good Practice**:\n\n‚úÖ **Always report**:\n1. Which HAC method used (Newey-West or Driscoll-Kraay)\n2. Number of lags ($L$) and how chosen (automatic vs manual)\n3. Kernel function (Bartlett, Parzen, QS)\n\n**Example Reporting**:\n\n> \"Standard errors are Newey-West HAC with automatic lag selection ($L = 4$) using the Bartlett kernel, robust to heteroskedasticity and autocorrelation.\"\n\n> \"Standard errors are Driscoll-Kraay with 3 lags, robust to heteroskedasticity, autocorrelation, and cross-sectional dependence.\"\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "m367hmhqgsf",
   "source": "## 7. Case Studies\n\n### 7.1 Case Study 1: Monetary Policy Impact (Time Series)\n\n**Research Question**: Does interest rate affect inflation?\n\n**Context**: Single-country quarterly macroeconomic data\n\n**Appropriate Method**: Newey-West HAC\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "p08tp5j20je",
   "source": "# Case Study 1: Monetary Policy\nprint(\"=\"*70)\nprint(\"CASE STUDY 1: Monetary Policy Impact on Inflation\")\nprint(\"=\"*70)\n\npolicy_model = PooledOLS(\n    formula=\"inflation ~ interest_rate + unemployment\",\n    data=ts_data,\n    entity_col='entity',\n    time_col='time'\n)\n\nres_policy = policy_model.fit(cov_type='HAC', cov_config={'kernel': 'bartlett'})\n\nprint(\"\\nEstimation Results:\")\nprint(res_policy.summary())\n\nprint(\"\\nüìä Interpretation:\")\nprint(f\"   ‚Ä¢ Interest rate coefficient: {res_policy.params['interest_rate']:.4f}\")\nprint(f\"   ‚Ä¢ Standard error (Newey-West): {res_policy.std_errors['interest_rate']:.4f}\")\nprint(f\"   ‚Ä¢ t-statistic: {res_policy.params['interest_rate'] / res_policy.std_errors['interest_rate']:.4f}\")\n\nif abs(res_policy.params['interest_rate'] / res_policy.std_errors['interest_rate']) > 1.96:\n    print(\"   ‚Ä¢ Statistically significant at 5% level ‚úì\")\nelse:\n    print(\"   ‚Ä¢ Not statistically significant at 5% level\")\n\nprint(\"\\n‚úÖ Reporting:\")\nprint('   \"We estimate the effect of interest rates on inflation using quarterly')\nprint('    data. Standard errors are Newey-West HAC with automatic lag selection')\nprint('    (L=4), robust to heteroskedasticity and autocorrelation.\"')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "38i6ivmequy",
   "source": "### 7.2 Case Study 2: Trade and Growth (Macro Panel)\n\n**Research Question**: Does trade openness promote economic growth?\n\n**Context**: Panel of 20 countries over 30 years\n\n**Appropriate Method**: Driscoll-Kraay HAC (handles global shocks)\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "666fm8y1ah",
   "source": "# Case Study 2: Trade and Growth\nprint(\"=\"*70)\nprint(\"CASE STUDY 2: Trade Openness and Economic Growth\")\nprint(\"=\"*70)\n\ntrade_model = FixedEffects(\n    formula=\"gdp_growth ~ trade_openness + fdi\",\n    data=macro_data,\n    entity_col='country_id',\n    time_col='year'\n)\n\nres_trade = trade_model.fit(cov_type='driscoll_kraay', cov_config={'bandwidth': 3})\n\nprint(\"\\nEstimation Results:\")\nprint(res_trade.summary())\n\nprint(\"\\nüìä Interpretation:\")\nprint(f\"   ‚Ä¢ Trade openness coefficient: {res_trade.params['trade_openness']:.4f}\")\nprint(f\"   ‚Ä¢ Standard error (Driscoll-Kraay): {res_trade.std_errors['trade_openness']:.4f}\")\nprint(f\"   ‚Ä¢ t-statistic: {res_trade.params['trade_openness'] / res_trade.std_errors['trade_openness']:.4f}\")\n\nif abs(res_trade.params['trade_openness'] / res_trade.std_errors['trade_openness']) > 1.96:\n    print(\"   ‚Ä¢ Statistically significant at 5% level ‚úì\")\nelse:\n    print(\"   ‚Ä¢ Not statistically significant at 5% level\")\n\nprint(\"\\n‚úÖ Reporting:\")\nprint('   \"We estimate the effect of trade openness on GDP growth using a panel')\nprint('    of 20 countries over 30 years. We use entity fixed effects to control')\nprint('    for time-invariant country characteristics. Standard errors are')\nprint('    Driscoll-Kraay with 3 lags, robust to heteroskedasticity,')\nprint('    autocorrelation, and cross-sectional dependence due to global shocks.\"')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "iy3snauipsr",
   "source": "## 8. Exercises\n\n### Exercise 1: ACF Diagnosis and Newey-West (Easy)\n\n**Task**: Practice diagnosing autocorrelation and applying Newey-West\n\n**Steps**:\n1. Load `gdp_quarterly.csv`\n2. Estimate: `inflation ~ unemployment`\n3. Plot ACF of residuals\n4. Determine appropriate lag length from ACF\n5. Estimate with Newey-West using your chosen lag\n6. Compare SEs: robust vs Newey-West\n7. Report your findings\n\n**Deliverable**: Write 2-3 sentences explaining:\n- Is autocorrelation present?\n- How much do SEs change?\n- What does this mean for inference?\n\n---\n\n### Exercise 2: Driscoll-Kraay Necessity (Moderate)\n\n**Task**: Test whether cross-sectional correlation matters\n\n**Steps**:\n1. Load `macro_panel.csv`\n2. Estimate FE model: `gdp_growth ~ trade_openness + fdi`\n3. Compare 4 methods: Robust, Cluster (Entity), Cluster (Time), Driscoll-Kraay\n4. Create comparison table\n5. Identify which variable has largest DK/Robust ratio\n6. Explain: Why does DK give larger SEs than clustering?\n\n**Deliverable**: Short write-up (1 paragraph) explaining when Driscoll-Kraay is necessary\n\n---\n\n### Exercise 3: Monte Carlo Simulation (Challenging)\n\n**Task**: Replicate and extend the simulation from Section 5.3\n\n**Steps**:\n1. Modify the simulation function to vary AR(1) coefficient ($\\rho$)\n2. Run simulations for $\\rho \\in \\{0, 0.3, 0.5, 0.7, 0.9\\}$\n3. For each $\\rho$, calculate rejection rates for all 4 methods\n4. Plot rejection rate vs $\\rho$ for each method\n5. Identify when methods start to fail\n\n**Expected Result**: As $\\rho$ increases:\n- Robust becomes increasingly liberal\n- Newey-West stays close to 5% (if cross-sectional correlation = 0)\n- Driscoll-Kraay stays close to 5% (always)\n\n**Deliverable**: \n- Plot showing rejection rates vs $\\rho$\n- 1-paragraph explanation of why autocorrelation matters for inference\n\n---\n\n**Space for your work:**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "i30oqndl4d",
   "source": "# Space for Exercise 1\n\n# Your code here:\n# 1. Load data\n# 2. Estimate model\n# 3. Plot ACF\n# 4. Apply Newey-West\n# 5. Compare SEs",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fb1718d2",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. ‚úÖ **Autocorrelation** invalidates standard and robust SEs (underestimate uncertainty)\n",
    "2. ‚úÖ **Newey-West HAC** handles heteroskedasticity + autocorrelation (time series)\n",
    "3. ‚úÖ **Driscoll-Kraay HAC** additionally handles cross-sectional correlation (panels)\n",
    "4. ‚úÖ **Lag selection** is critical: use automatic rule or domain knowledge\n",
    "5. ‚úÖ **Minimum T requirements**: NW needs $T > 50$, DK needs $T > 20$\n",
    "6. ‚úÖ **Choose between HAC and clustering** based on $(N, T)$ structure\n",
    "\n",
    "---\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "**Newey-West HAC Variance**:\n",
    "$$\n",
    "V_{NW} = (X'X)^{-1} \\left[\\Gamma_0 + \\sum_{l=1}^{L} w_l (\\Gamma_l + \\Gamma_l')\\right] (X'X)^{-1}\n",
    "$$\n",
    "\n",
    "**Automatic Lag Selection**:\n",
    "$$\n",
    "L = \\text{floor}\\left(4 \\left(\\frac{T}{100}\\right)^{2/9}\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Decision Flowchart\n",
    "\n",
    "```\n",
    "Data Structure?\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚Üí Time Series (T > 50)\n",
    "    ‚îÇ       ‚îÇ\n",
    "    ‚îÇ       ‚îî‚îÄ‚Üí Newey-West HAC\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚Üí Panel (N small, T > 20)\n",
    "    ‚îÇ       ‚îÇ\n",
    "    ‚îÇ       ‚îú‚îÄ‚Üí Cross-sectional correlation? YES ‚Üí Driscoll-Kraay\n",
    "    ‚îÇ       ‚îî‚îÄ‚Üí NO ‚Üí Cluster by entity\n",
    "    ‚îÇ\n",
    "    ‚îî‚îÄ‚Üí Panel (N large, T < 20)\n",
    "            ‚îÇ\n",
    "            ‚îî‚îÄ‚Üí Cluster by entity (G ‚â• 20)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3104401d",
   "metadata": {},
   "source": [
    "## 10. References\n",
    "\n",
    "### Foundational Papers\n",
    "\n",
    "1. **Newey, W. K., & West, K. D. (1987)**. \"A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix.\" *Econometrica*, 55(3), 703-708.\n",
    "\n",
    "2. **Newey, W. K., & West, K. D. (1994)**. \"Automatic lag selection in covariance matrix estimation.\" *Review of Economic Studies*, 61(4), 631-653.\n",
    "\n",
    "3. **Driscoll, J. C., & Kraay, A. C. (1998)**. \"Consistent covariance matrix estimation with spatially dependent panel data.\" *Review of Economics and Statistics*, 80(4), 549-560.\n",
    "\n",
    "4. **Andrews, D. W. K. (1991)**. \"Heteroskedasticity and autocorrelation consistent covariance matrix estimation.\" *Econometrica*, 59(3), 817-858.\n",
    "\n",
    "---\n",
    "\n",
    "### Textbooks\n",
    "\n",
    "1. **Wooldridge, J. M. (2010)**. *Econometric Analysis of Cross Section and Panel Data* (2nd ed.). MIT Press.\n",
    "2. **Hamilton, J. D. (1994)**. *Time Series Analysis*. Princeton University Press.\n",
    "3. **Greene, W. H. (2018)**. *Econometric Analysis* (8th ed.). Pearson.\n",
    "\n",
    "---\n",
    "\n",
    "### Online Resources\n",
    "\n",
    "- **PanelBox Documentation**: `panelbox.readthedocs.io/standard_errors/hac.html`\n",
    "- **StatsModels HAC**: `statsmodels.org/stable/generated/statsmodels.stats.sandwich_covariance.cov_hac.html`\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook 03: HAC Standard Errors**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
