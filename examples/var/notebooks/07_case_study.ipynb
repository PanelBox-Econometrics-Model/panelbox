{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell_01",
   "metadata": {},
   "source": "# Tutorial 07: Complete Case Study — Monetary Policy Transmission\n\n**Estimated duration**: 180–240 minutes | **Level**: Capstone\n\n---\n\n## Learning Objectives\n\nThis capstone tutorial integrates **all** techniques from Tutorials 01–06 into a coherent empirical research workflow. By the end, you will be able to:\n\n1. Formulate a complete Panel VAR research design grounded in macroeconomic theory\n2. Conduct thorough exploratory data analysis on a multi-country macro panel\n3. Test for stationarity and select optimal lag order\n4. Estimate a 4-variable Panel VAR with appropriate standard errors\n5. Apply Granger causality and Dumitrescu-Hurlin tests to identify causal linkages\n6. Estimate and interpret Cholesky, cumulative, and generalized impulse response functions\n7. Decompose forecast error variance to quantify monetary policy's explanatory power\n8. Investigate cross-country heterogeneity in policy transmission\n9. Conduct robustness checks across alternative specifications\n10. Synthesize findings into a coherent economic narrative with policy implications\n\n## Research Question\n\n> **\"How do monetary policy shocks (interest rate changes) transmit to the real economy (GDP growth, inflation, unemployment)?\"**\n\n## Outline\n\n| Section | Topic | Duration |\n|---------|-------|----------|\n| 1 | Research Problem Setup | 20 min |\n| 2 | Exploratory Data Analysis | 25 min |\n| 3 | Stationarity Testing | 20 min |\n| 4 | Model Specification | 30 min |\n| 5 | Granger Causality Analysis | 25 min |\n| 6 | Impulse Response Functions | 40 min |\n| 7 | Variance Decomposition | 25 min |\n| 8 | Heterogeneity Analysis | 30 min |\n| 9 | Robustness Checks | 20 min |\n| 10 | Summary and Conclusions | 20 min |\n| 11 | Export Results | 10 min |\n| -- | Exercises | 45+ min |\n\n## References\n\n- Christiano, L. J., Eichenbaum, M., & Evans, C. L. (1999). Monetary policy shocks: What have we learned and to what end? *Handbook of Macroeconomics*, 1, 65–148.\n- Sims, C. A. (1992). Interpreting the macroeconomic time series facts: The effects of monetary policy. *European Economic Review*, 36(5), 975–1000.\n- Romer, C. D., & Romer, D. H. (2004). A new measure of monetary shocks. *American Economic Review*, 94(4), 1055–1084.\n- Dumitrescu, E. I., & Hurlin, C. (2012). Testing for Granger non-causality in heterogeneous panels. *Economic Modelling*, 29(4), 1450–1460.\n- Lutkepohl, H. (2005). *New Introduction to Multiple Time Series Analysis*. Springer."
  },
  {
   "cell_type": "markdown",
   "id": "cell_02",
   "metadata": {},
   "source": "---\n\n## Setup"
  },
  {
   "cell_type": "code",
   "id": "cell_03",
   "metadata": {},
   "source": "# ============================================================\n# Setup\n# ============================================================\nimport sys\nimport os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n%matplotlib inline\n\n# Reproducibility\nnp.random.seed(42)\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# Add project root and utilities to path\nproject_root = Path('../../../').resolve()\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\nsys.path.insert(0, '../utils')\n\n# PanelBox imports\nfrom panelbox.var import PanelVARData, PanelVAR\n\n# Tutorial utilities\nfrom visualization_helpers import (\n    plot_irf_grid, plot_irf_comparison, plot_fevd_stacked,\n    plot_coefficient_heatmap, plot_stability_diagram, set_academic_style\n)\nfrom diagnostic_tools import (\n    residual_diagnostics, model_comparison_table, granger_causality_summary\n)\n\n# Apply academic style\nset_academic_style()\n\n# Output directories\nfor subdir in ['irfs', 'fevds', 'causality_networks', 'diagnostics']:\n    os.makedirs(f'../outputs/figures/{subdir}', exist_ok=True)\nos.makedirs('../outputs/tables', exist_ok=True)\nos.makedirs('../outputs/results', exist_ok=True)\n\nprint('Setup complete.')\nprint(f'Project root: {project_root}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_04",
   "metadata": {},
   "source": "---\n\n## Section 1: Research Problem Setup (20 min)\n\n### 1.1 Economic Context: The Monetary Transmission Mechanism\n\nMonetary policy is one of the primary tools available to central banks for macroeconomic stabilization. The **monetary transmission mechanism** describes the channels through which changes in the policy interest rate propagate to the real economy.\n\nThe key channels include:\n\n1. **Interest rate channel**: Higher policy rates raise borrowing costs, reducing investment and consumption (IS curve)\n2. **Exchange rate channel**: Higher rates attract capital inflows, appreciating the currency and reducing net exports\n3. **Credit channel**: Tighter monetary policy reduces bank lending and credit availability\n4. **Expectations channel**: Central bank actions signal future policy stance, affecting inflation expectations\n\n### 1.2 The Taylor Rule and Central Bank Objectives\n\nThe **Taylor Rule** (Taylor, 1993) describes how central banks typically set interest rates:\n\n$$i_t = r^* + \\pi_t + 0.5(\\pi_t - \\pi^*) + 0.5(y_t - y^*)$$\n\nwhere:\n- $i_t$ = nominal interest rate\n- $r^*$ = equilibrium real interest rate\n- $\\pi_t$ = current inflation, $\\pi^*$ = target inflation\n- $y_t - y^*$ = output gap\n\nThis implies **bidirectional causality**: the central bank reacts to inflation and output, while its rate decisions affect both.\n\n### 1.3 Research Hypotheses\n\nWe formalize four testable hypotheses:\n\n| # | Hypothesis | Expected Sign | Expected Timing |\n|---|-----------|---------------|----------------|\n| H1 | A contractionary monetary shock (rate increase) reduces GDP growth | Negative | Peak at 4–8 quarters |\n| H2 | A contractionary monetary shock reduces inflation | Negative (after possible price puzzle) | Peak at 6–12 quarters |\n| H3 | A contractionary monetary shock increases unemployment | Positive | Peak at 4–10 quarters |\n| H4 | Inflation Granger-causes interest rates (Taylor rule feedback) | Bidirectional | Lagged 1–2 quarters |\n\n### 1.4 Panel VAR Framework\n\nWe model the monetary transmission mechanism as a 4-variable Panel VAR:\n\n$$Y_{i,t} = \\mu_i + A_1 Y_{i,t-1} + A_2 Y_{i,t-2} + u_{i,t}$$\n\nwhere $Y_{i,t} = [\\text{interest\\_rate}, \\text{inflation}, \\text{gdp\\_growth}, \\text{unemployment}]'$ for country $i$ at time $t$, and $\\mu_i$ represents country-specific fixed effects.\n\n### 1.5 Expected Relationships\n\n| Impulse (Shock) | Response | Expected Sign | Economic Mechanism |\n|:---|:---|:---:|:---|\n| interest_rate $\\uparrow$ | gdp_growth | $-$ | Higher borrowing costs reduce investment and consumption |\n| interest_rate $\\uparrow$ | inflation | $-$ (lagged) | Demand contraction reduces price pressures |\n| interest_rate $\\uparrow$ | unemployment | $+$ | Okun's law: lower output leads to job losses |\n| inflation $\\uparrow$ | interest_rate | $+$ | Taylor rule: central bank tightens policy |\n| gdp_growth $\\uparrow$ | interest_rate | $+$ | Taylor rule: closing output gap triggers tightening |"
  },
  {
   "cell_type": "markdown",
   "id": "cell_05",
   "metadata": {},
   "source": "---\n\n## Section 2: Exploratory Data Analysis (25 min)\n\n### 2.1 Load and Inspect Data"
  },
  {
   "cell_type": "code",
   "id": "cell_06",
   "metadata": {},
   "source": "# Load the monetary policy panel dataset\ndf = pd.read_csv('../data/monetary_policy.csv')\n\nprint(f'Dataset shape: {df.shape}')\nprint(f'Columns: {list(df.columns)}')\nprint(f'Countries: {df[\"country\"].nunique()}')\nprint(f'Quarters: {df[\"quarter\"].nunique()}')\nprint(f'\\nCountry list ({df[\"country\"].nunique()}):')\nprint(sorted(df['country'].unique().tolist()))\nprint(f'\\nQuarter range: {df[\"quarter\"].min()} to {df[\"quarter\"].max()}')\nprint(f'\\nFirst rows:')\ndf.head(8)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_07",
   "metadata": {},
   "source": "# Panel balance check\nobs_per_country = df.groupby('country').size()\nprint('Observations per country:')\nprint(obs_per_country.describe())\nprint(f'\\nBalanced panel: {obs_per_country.nunique() == 1}')\nprint(f'N x T = {df[\"country\"].nunique()} x {df[\"quarter\"].nunique()} = '\n      f'{df[\"country\"].nunique() * df[\"quarter\"].nunique()}')\nprint(f'Actual rows: {len(df)}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_08",
   "metadata": {},
   "source": "### 2.2 Summary Statistics"
  },
  {
   "cell_type": "code",
   "id": "cell_09",
   "metadata": {},
   "source": "# Overall summary statistics\nvariables = ['gdp_growth', 'inflation', 'interest_rate', 'unemployment']\n\nprint('Overall Summary Statistics')\nprint('=' * 70)\nprint(df[variables].describe().round(4).to_string())\n\n# Summary by country (selected)\nprint('\\n\\nMeans by Selected Countries:')\nprint('=' * 70)\nfor country in ['USA', 'DEU', 'JPN', 'MEX', 'KOR']:\n    cdf = df[df['country'] == country]\n    means = cdf[variables].mean()\n    print(f'  {country}: ' + ', '.join(f'{v}={means[v]:.3f}' for v in variables))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_10",
   "metadata": {},
   "source": "# Cross-sectional variation: barplots of country means\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\n\nfor idx, var in enumerate(variables):\n    ax = axes[idx]\n    country_means = df.groupby('country')[var].mean().sort_values()\n    colors = sns.color_palette('husl', len(country_means))\n    ax.barh(country_means.index, country_means.values, color=colors)\n    ax.set_xlabel(var.replace('_', ' ').title(), fontsize=10)\n    ax.set_title(f'Mean {var.replace(\"_\", \" \").title()}', fontsize=11, fontweight='bold')\n    ax.tick_params(axis='y', labelsize=7)\n\nfig.suptitle('Cross-Sectional Variation in Macroeconomic Variables',\n             fontsize=14, fontweight='bold', y=1.02)\nfig.tight_layout()\nfig.savefig('../outputs/figures/diagnostics/07_cross_section_variation.png',\n            dpi=150, bbox_inches='tight')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_11",
   "metadata": {},
   "source": "### 2.3 Time Series Plots for Sample Countries"
  },
  {
   "cell_type": "code",
   "id": "cell_12",
   "metadata": {},
   "source": "# Time series for USA, DEU, JPN\nsample_countries = ['USA', 'DEU', 'JPN']\ncolors_ts = {'USA': '#2166ac', 'DEU': '#b2182b', 'JPN': '#4dac26'}\n\nfig, axes = plt.subplots(4, 1, figsize=(14, 14), sharex=True)\n\nfor var_idx, var in enumerate(variables):\n    ax = axes[var_idx]\n    for country in sample_countries:\n        cdf = df[df['country'] == country].sort_values('quarter')\n        ax.plot(range(len(cdf)), cdf[var].values,\n                label=country, color=colors_ts[country], linewidth=1.5)\n    ax.set_ylabel(var.replace('_', ' ').title(), fontsize=11)\n    ax.set_title(f'{var.replace(\"_\", \" \").title()}', fontsize=12, fontweight='bold')\n    ax.legend(fontsize=9, loc='best')\n    ax.grid(True, alpha=0.3)\n    ax.axhline(y=0, color='black', linewidth=0.5, linestyle='--', alpha=0.4)\n\n# Label x-axis with quarter ticks\nquarters_list = df[df['country'] == 'USA'].sort_values('quarter')['quarter'].values\ntick_pos = range(0, len(quarters_list), 8)\ntick_lab = [quarters_list[i] for i in tick_pos]\naxes[-1].set_xticks(list(tick_pos))\naxes[-1].set_xticklabels(tick_lab, rotation=45, fontsize=8)\naxes[-1].set_xlabel('Quarter', fontsize=11)\n\nfig.suptitle('Macroeconomic Variables: USA, Germany, Japan (2000-2019)',\n             fontsize=14, fontweight='bold', y=1.01)\nfig.tight_layout()\nfig.savefig('../outputs/figures/diagnostics/07_time_series_sample.png',\n            dpi=150, bbox_inches='tight')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_13",
   "metadata": {},
   "source": "### 2.4 Correlation Matrix"
  },
  {
   "cell_type": "code",
   "id": "cell_14",
   "metadata": {},
   "source": "# Correlation heatmap\ncorr = df[variables].corr()\n\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(corr, annot=True, fmt='.3f', cmap='RdBu_r', center=0,\n            vmin=-1, vmax=1, square=True, linewidths=0.5, ax=ax,\n            cbar_kws={'label': 'Correlation'})\nax.set_title('Unconditional Correlation Matrix (Pooled)',\n             fontsize=14, fontweight='bold')\nfig.tight_layout()\nfig.savefig('../outputs/figures/diagnostics/07_correlation_matrix.png',\n            dpi=150, bbox_inches='tight')\nplt.show()\n\nprint('Key correlations:')\nprint(f'  interest_rate vs inflation:    {corr.loc[\"interest_rate\", \"inflation\"]:.3f}')\nprint(f'  interest_rate vs gdp_growth:   {corr.loc[\"interest_rate\", \"gdp_growth\"]:.3f}')\nprint(f'  interest_rate vs unemployment: {corr.loc[\"interest_rate\", \"unemployment\"]:.3f}')\nprint(f'  gdp_growth vs unemployment:    {corr.loc[\"gdp_growth\", \"unemployment\"]:.3f}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_15",
   "metadata": {},
   "source": "---\n\n## Section 3: Stationarity Testing (20 min)\n\nBefore estimating the Panel VAR, we need to verify that the variables are stationary. Non-stationary variables would require differencing or a VECM framework.\n\n### 3.1 Formal Panel Unit Root Tests\n\nWe apply panel unit root tests to each variable. These tests have more power than individual time-series tests because they pool information across cross-sectional units."
  },
  {
   "cell_type": "code",
   "id": "cell_16",
   "metadata": {},
   "source": "# Formal panel unit root tests using panelbox diagnostics\nfrom panelbox.diagnostics.unit_root import panel_unit_root_test\n\nprint('Panel Unit Root Tests')\nprint('=' * 80)\n\nfor var in variables:\n    try:\n        ur_result = panel_unit_root_test(\n            df, variable=var, entity_col='country', time_col='quarter',\n            test='all', trend='c'\n        )\n        print(f'\\n--- {var} ---')\n        print(ur_result.summary_table())\n    except Exception as e:\n        print(f'\\n--- {var} ---')\n        print(f'  Formal test failed: {e}')\n        # Fallback: descriptive autocorrelation assessment\n        autocorrs = []\n        for country in df['country'].unique():\n            series = df[df['country'] == country].sort_values('quarter')[var].values\n            if len(series) > 2:\n                rho = np.corrcoef(series[:-1], series[1:])[0, 1]\n                autocorrs.append(rho)\n        mean_rho = np.mean(autocorrs)\n        verdict = 'Likely stationary' if mean_rho < 0.95 else 'High persistence'\n        print(f'  Descriptive: mean AR(1) rho = {mean_rho:.4f} => {verdict}')\n\nprint('\\n' + '=' * 80)\nprint('Conclusion: Growth rates and policy rates are typically I(0).')\nprint('We proceed with the Panel VAR in levels specification.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_17",
   "metadata": {},
   "source": "# Supplementary: Within-entity autocorrelation check\n# High rho (close to 1.0) is consistent with a unit root; moderate rho confirms stationarity\nprint('Supplementary: Within-Entity First-Order Autocorrelation')\nprint('=' * 65)\n\nfor var in variables:\n    autocorrs = []\n    for country in df['country'].unique():\n        series = df[df['country'] == country].sort_values('quarter')[var].values\n        if len(series) > 2:\n            rho = np.corrcoef(series[:-1], series[1:])[0, 1]\n            autocorrs.append(rho)\n    \n    mean_rho = np.mean(autocorrs)\n    max_rho = np.max(autocorrs)\n    verdict = 'Consistent with I(0)' if mean_rho < 0.95 else 'High persistence'\n    print(f'  {var:>16s}: mean rho(1) = {mean_rho:.4f}, max = {max_rho:.4f}  => {verdict}')\n\nprint('\\nBoth formal tests and descriptive analysis confirm stationarity.')\nprint('We proceed with the Panel VAR specification in levels.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_18",
   "metadata": {},
   "source": "---\n\n## Section 4: Model Specification (30 min)\n\n### 4.1 Lag Selection\n\nWe use information criteria (AIC, BIC, HQIC) to select the optimal lag order, testing lags 1 through 8."
  },
  {
   "cell_type": "code",
   "id": "cell_19",
   "metadata": {},
   "source": "# Define endogenous variables in economic ordering\nendog_vars = ['interest_rate', 'inflation', 'gdp_growth', 'unemployment']\n\n# Create PanelVARData with 1 lag for the model object used for lag selection\ndata_prelim = PanelVARData(\n    df,\n    endog_vars=endog_vars,\n    entity_col='country',\n    time_col='quarter',\n    lags=1\n)\nmodel_prelim = PanelVAR(data_prelim)\n\n# Select optimal lag order\nlag_results = model_prelim.select_lag_order(max_lags=8)\n\nprint(lag_results.summary())\nprint(f'\\nSelected lags by criterion: {lag_results.selected}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_20",
   "metadata": {},
   "source": "# Display criteria DataFrame\nprint('Information Criteria by Lag Order:')\nprint(lag_results.criteria_df.to_string())\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor col in ['AIC', 'BIC', 'HQIC']:\n    if col in lag_results.criteria_df.columns:\n        vals = lag_results.criteria_df[col].values.astype(float)\n        lags_arr = lag_results.criteria_df.index.values if 'lags' not in lag_results.criteria_df.columns else lag_results.criteria_df['lags'].values\n        ax.plot(lags_arr, vals, marker='o', linewidth=2, markersize=8,\n                label=f'{col} (best: p={lag_results.selected.get(col, \"?\")})')\n\nax.set_xlabel('Number of Lags (p)', fontsize=12)\nax.set_ylabel('Information Criterion Value', fontsize=12)\nax.set_title('Lag Order Selection: Information Criteria', fontsize=14, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\nfig.tight_layout()\nfig.savefig('../outputs/figures/diagnostics/07_lag_selection.png', dpi=150, bbox_inches='tight')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_21",
   "metadata": {},
   "source": "### 4.2 Model Estimation\n\nWe estimate the Panel VAR(2) with Driscoll-Kraay standard errors, which are robust to both cross-sectional dependence and heteroskedasticity. We use 2 lags as a reasonable baseline consistent with the quarterly frequency and the monetary policy literature."
  },
  {
   "cell_type": "code",
   "id": "cell_22",
   "metadata": {},
   "source": "# Estimate the main model with 2 lags and Driscoll-Kraay SE\ndata = PanelVARData(\n    df,\n    endog_vars=endog_vars,\n    entity_col='country',\n    time_col='quarter',\n    lags=2\n)\nmodel = PanelVAR(data)\nresults = model.fit(method='ols', cov_type='driscoll_kraay')\n\nprint('Panel VAR(2) Estimation Results')\nprint('=' * 60)\nprint(f'  Endogenous variables: {endog_vars}')\nprint(f'  Number of variables (K): {results.K}')\nprint(f'  Number of lags (p):      {results.p}')\nprint(f'  Number of entities (N):  {results.N}')\nprint(f'  Observations used:       {results.n_obs}')\nprint(f'  Covariance type:         driscoll_kraay')\nprint(f'\\nModel stability:')\nprint(f'  Stable: {results.is_stable()}')\nprint(f'  Max eigenvalue modulus: {results.max_eigenvalue_modulus:.6f}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_23",
   "metadata": {},
   "source": "# Display the full summary\nprint(results.summary())",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_24",
   "metadata": {},
   "source": "### 4.3 Stability Check and Diagnostics"
  },
  {
   "cell_type": "code",
   "id": "cell_25",
   "metadata": {},
   "source": "# Stability diagram: eigenvalues on unit circle\nfig = plot_stability_diagram(results,\n    save_path='../outputs/figures/diagnostics/07_stability.png')\nplt.show()\n\nif results.is_stable():\n    print('The VAR system is STABLE: all eigenvalues inside the unit circle.')\n    print('IRFs will converge to zero at long horizons.')\nelse:\n    print('WARNING: The system is UNSTABLE. IRFs may not converge.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_26",
   "metadata": {},
   "source": "# Residual covariance matrix\nSigma = results.Sigma\n\nprint('Residual Covariance Matrix (Sigma):')\nprint('=' * 60)\nsigma_df = pd.DataFrame(Sigma, index=endog_vars, columns=endog_vars)\nprint(sigma_df.round(6))\n\n# Residual correlation matrix\nD_inv = np.diag(1.0 / np.sqrt(np.diag(Sigma)))\ncorr_resid = D_inv @ Sigma @ D_inv\n\nprint('\\nResidual Correlation Matrix:')\nprint('=' * 60)\ncorr_resid_df = pd.DataFrame(corr_resid, index=endog_vars, columns=endog_vars)\nprint(corr_resid_df.round(4))\n\nprint('\\nOff-diagonal correlations indicate contemporaneous co-movement.')\nprint('The Cholesky decomposition will orthogonalize these for structural identification.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_27",
   "metadata": {},
   "source": "# Coefficient heatmaps\nfig = plot_coefficient_heatmap(results, lag=1,\n    save_path='../outputs/figures/diagnostics/07_coeff_lag1.png')\nplt.show()\n\nfig = plot_coefficient_heatmap(results, lag=2,\n    save_path='../outputs/figures/diagnostics/07_coeff_lag2.png')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_28",
   "metadata": {},
   "source": "# Residual diagnostics\ndiag = residual_diagnostics(results, save_dir='../outputs/figures/diagnostics')\n\nprint('Residual Diagnostics Summary')\nprint('=' * 80)\nprint(f'{\"Variable\":>16s} {\"Mean\":>10s} {\"Std\":>10s} {\"Skew\":>10s} {\"Kurt\":>10s} '\n      f'{\"LB p-val\":>10s} {\"JB p-val\":>10s}')\nprint('-' * 80)\nfor var_name, d in diag.items():\n    print(f'{var_name:>16s} {d[\"mean\"]:>10.4f} {d[\"std\"]:>10.4f} '\n          f'{d[\"skewness\"]:>10.4f} {d[\"kurtosis\"]:>10.4f} '\n          f'{d[\"ljung_box_pvalue\"]:>10.4f} {d[\"jarque_bera_pvalue\"]:>10.4f}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_29",
   "metadata": {},
   "source": "---\n\n## Section 5: Granger Causality Analysis (25 min)\n\nGranger causality tests whether lagged values of one variable help predict another, beyond what is already captured by the other variable's own lags. This tests the **predictive content**, not structural causation.\n\n### 5.1 Pairwise Granger Causality Tests"
  },
  {
   "cell_type": "code",
   "id": "cell_30",
   "metadata": {},
   "source": "# Test key economic hypotheses\nhypotheses = [\n    ('interest_rate', 'inflation', 'Monetary policy -> price stability'),\n    ('interest_rate', 'gdp_growth', 'Monetary transmission to output'),\n    ('interest_rate', 'unemployment', 'Monetary policy -> labor market'),\n    ('inflation', 'interest_rate', 'Taylor rule: CB reacts to inflation'),\n    ('gdp_growth', 'interest_rate', 'Taylor rule: CB reacts to output'),\n    ('gdp_growth', 'unemployment', \"Okun's law\"),\n    ('inflation', 'gdp_growth', 'Phillips curve / supply shocks'),\n    ('unemployment', 'inflation', 'Wage-price spiral'),\n]\n\npairwise_results = []\nprint('Pairwise Granger Causality Tests')\nprint('=' * 100)\n\nfor cause, effect, label in hypotheses:\n    gc = results.granger_causality(cause=cause, effect=effect)\n    stars = '***' if gc.p_value < 0.01 else '**' if gc.p_value < 0.05 else '*' if gc.p_value < 0.10 else ''\n    print(f'  {cause:>16s} -> {effect:<16s}  Wald={gc.wald_stat:8.2f}  '\n          f'p={gc.p_value:.4f} {stars:<3s}  ({label})')\n    pairwise_results.append({\n        'Cause': cause, 'Effect': effect, 'Hypothesis': label,\n        'Wald_stat': gc.wald_stat, 'p_value': gc.p_value,\n        'Significant_5pct': gc.p_value < 0.05,\n    })\n\nprint('=' * 100)\nprint('Significance: *** p<0.01, ** p<0.05, * p<0.10')\npw_df = pd.DataFrame(pairwise_results).sort_values('p_value')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_31",
   "metadata": {},
   "source": "### 5.2 Full Granger Causality Matrix"
  },
  {
   "cell_type": "code",
   "id": "cell_32",
   "metadata": {},
   "source": "# Full K x K Granger causality p-value matrix\ngc_matrix = results.granger_causality_matrix(significance_level=0.05)\n\nprint('Granger Causality P-Value Matrix (Row causes Column):')\nprint(gc_matrix.round(4).to_string())\n\nsig_count = (gc_matrix < 0.05).sum().sum()\ntotal_pairs = gc_matrix.notna().sum().sum()\nprint(f'\\nSignificant at 5%: {sig_count} out of {total_pairs} pairs')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_33",
   "metadata": {},
   "source": "# Heatmap of Granger causality p-values\nfig, ax = plt.subplots(figsize=(9, 7))\n\nmask = gc_matrix.isna()\nsns.heatmap(\n    gc_matrix, annot=True, fmt='.3f', cmap='coolwarm_r',\n    vmin=0, vmax=0.20, mask=mask, linewidths=0.5, linecolor='gray',\n    square=True, cbar_kws={'label': 'P-value', 'shrink': 0.8}, ax=ax,\n)\nax.set_title('Granger Causality P-Value Matrix\\n(rows cause columns)',\n             fontsize=14, fontweight='bold', pad=15)\nax.set_xlabel('Effect variable', fontsize=12)\nax.set_ylabel('Cause variable', fontsize=12)\n\nfig.tight_layout()\nfig.savefig('../outputs/figures/causality_networks/07_gc_matrix.png',\n            dpi=150, bbox_inches='tight')\nplt.show()\nprint('Granger causality heatmap saved.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4pt3euqfjbj",
   "source": "# Causality network visualization\ntry:\n    from panelbox.var import plot_causality_network\n    \n    fig_net = plot_causality_network(\n        gc_matrix,\n        threshold=0.05,\n        layout='circular',\n        backend='matplotlib',\n        title='Granger Causality Network (5% significance)',\n        figsize=(10, 8),\n        show=False\n    )\n    if fig_net is not None:\n        fig_net.savefig('../outputs/figures/causality_networks/07_gc_network.png',\n                        dpi=150, bbox_inches='tight')\n        plt.show()\n        print('Network visualization saved.')\nexcept Exception as e:\n    # Fallback: create a simple directed network using matplotlib\n    import matplotlib.patches as mpatches\n    \n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    var_labels = list(gc_matrix.columns)\n    n_vars = len(var_labels)\n    angles = np.linspace(0, 2 * np.pi, n_vars, endpoint=False)\n    radius = 2.0\n    positions = {var_labels[i]: (radius * np.cos(angles[i]), radius * np.sin(angles[i]))\n                 for i in range(n_vars)}\n    \n    # Draw nodes\n    for var, (x, y) in positions.items():\n        circle = plt.Circle((x, y), 0.4, color='#2166ac', alpha=0.8, zorder=5)\n        ax.add_patch(circle)\n        ax.text(x, y, var.replace('_', '\\n'), ha='center', va='center',\n                fontsize=8, fontweight='bold', color='white', zorder=6)\n    \n    # Draw edges for significant causal links\n    for cause in var_labels:\n        for effect in var_labels:\n            if cause != effect:\n                pval = gc_matrix.loc[cause, effect]\n                if pd.notna(pval) and pval < 0.05:\n                    x1, y1 = positions[cause]\n                    x2, y2 = positions[effect]\n                    dx, dy = x2 - x1, y2 - y1\n                    dist = np.sqrt(dx**2 + dy**2)\n                    # Shorten arrows to avoid overlapping with nodes\n                    shrink = 0.45 / dist\n                    ax.annotate('', xy=(x2 - dx * shrink, y2 - dy * shrink),\n                                xytext=(x1 + dx * shrink, y1 + dy * shrink),\n                                arrowprops=dict(arrowstyle='->', color='#b2182b',\n                                                lw=2.0, connectionstyle='arc3,rad=0.15'))\n                    # Label with p-value\n                    mid_x = (x1 + x2) / 2 + 0.15 * dy / dist\n                    mid_y = (y1 + y2) / 2 - 0.15 * dx / dist\n                    ax.text(mid_x, mid_y, f'p={pval:.3f}', fontsize=7,\n                            ha='center', va='center',\n                            bbox=dict(boxstyle='round,pad=0.2', facecolor='lightyellow',\n                                      edgecolor='gray', alpha=0.8))\n    \n    ax.set_xlim(-3.5, 3.5)\n    ax.set_ylim(-3.5, 3.5)\n    ax.set_aspect('equal')\n    ax.set_title('Granger Causality Network (5% significance)\\nArrows indicate direction of causality',\n                 fontsize=14, fontweight='bold')\n    ax.axis('off')\n    fig.tight_layout()\n    fig.savefig('../outputs/figures/causality_networks/07_gc_network.png',\n                dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f'Causality network saved (fallback method). Error was: {e}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell_34",
   "metadata": {},
   "source": "### 5.3 Dumitrescu-Hurlin Tests\n\nThe Dumitrescu-Hurlin (2012) test allows for heterogeneous causal relationships across countries. The null hypothesis is that no country exhibits Granger causality; the alternative is that at least some do."
  },
  {
   "cell_type": "code",
   "id": "cell_35",
   "metadata": {},
   "source": "# Dumitrescu-Hurlin tests for key pairs\ndh_pairs = [\n    ('interest_rate', 'gdp_growth'),\n    ('interest_rate', 'inflation'),\n    ('inflation', 'interest_rate'),\n    ('gdp_growth', 'unemployment'),\n]\n\nprint('Dumitrescu-Hurlin Heterogeneous Granger Causality Tests')\nprint('=' * 85)\n\nfor cause, effect in dh_pairs:\n    dh = results.dumitrescu_hurlin(cause=cause, effect=effect)\n    \n    # Use recommended statistic\n    if hasattr(dh, 'recommended_stat') and dh.recommended_stat == 'Z_tilde':\n        z_stat, z_pval = dh.Z_tilde_stat, dh.Z_tilde_pvalue\n        stat_name = 'Z_tilde'\n    else:\n        z_stat = getattr(dh, 'Z_bar_stat', dh.Z_tilde_stat)\n        z_pval = getattr(dh, 'Z_bar_pvalue', dh.Z_tilde_pvalue)\n        stat_name = 'Z_bar' if hasattr(dh, 'Z_bar_stat') else 'Z_tilde'\n    \n    stars = '***' if z_pval < 0.01 else '**' if z_pval < 0.05 else '*' if z_pval < 0.10 else ''\n    print(f'  {cause:>16s} -> {effect:<16s}  W_bar={dh.W_bar:7.2f}  '\n          f'{stat_name}={z_stat:7.2f}  p={z_pval:.4f} {stars}')\n\nprint('=' * 85)\nprint('\\nThe DH test accounts for heterogeneity across countries.')\nprint('Rejection indicates causality in at least some countries.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_36",
   "metadata": {},
   "source": "---\n\n## Section 6: Impulse Response Functions (40 min)\n\nIRFs trace the dynamic response of each variable to a one-standard-deviation structural shock. We use the **Cholesky decomposition** for identification.\n\n### 6.1 Identification: Economic Ordering\n\nWe order the variables as:\n\n$$\\text{interest\\_rate} \\to \\text{inflation} \\to \\text{gdp\\_growth} \\to \\text{unemployment}$$\n\n**Justification:**\n1. **Interest rate first**: Central banks set policy based on last period's information (predetermined within the quarter)\n2. **Inflation second**: Prices adjust faster than real activity within the quarter\n3. **GDP growth third**: Output responds to both monetary policy and prices with a lag\n4. **Unemployment last**: Labor markets are the slowest to adjust (hiring/firing frictions)\n\nThis ordering is standard in the monetary policy VAR literature (Christiano, Eichenbaum, Evans, 1999)."
  },
  {
   "cell_type": "code",
   "id": "cell_37",
   "metadata": {},
   "source": "# Compute Cholesky IRFs with bootstrap confidence intervals\nordering = ['interest_rate', 'inflation', 'gdp_growth', 'unemployment']\n\nirf = results.irf(\n    periods=20,\n    method='cholesky',\n    order=ordering,\n    ci_method='bootstrap',\n    n_bootstrap=500,\n    ci_level=0.95,\n    seed=42,\n    verbose=False\n)\n\nprint(f'IRF computed successfully.')\nprint(f'  Method: {irf.method}')\nprint(f'  Periods: {irf.periods}')\nprint(f'  Variables: {irf.var_names}')\nprint(f'  IRF matrix shape: {irf.irf_matrix.shape}')\nprint(f'  Bootstrap replications: 500')\nprint(f'  CI available: {irf.ci_lower is not None}')\nif irf.ci_lower is not None:\n    print(f'  CI lower shape: {irf.ci_lower.shape}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_38",
   "metadata": {},
   "source": "### 6.2 Monetary Policy Shock: All Responses"
  },
  {
   "cell_type": "code",
   "id": "cell_39",
   "metadata": {},
   "source": "# Plot all responses to a monetary policy shock\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\nresponse_vars = ['interest_rate', 'inflation', 'gdp_growth', 'unemployment']\ntitles = {\n    'interest_rate': 'Interest Rate (own response)',\n    'inflation': 'Inflation (price puzzle?)',\n    'gdp_growth': 'GDP Growth (IS curve)',\n    'unemployment': 'Unemployment (Okun\\'s law)',\n}\ncolors_resp = ['#2166ac', '#d6604d', '#4dac26', '#7b3294']\nh = np.arange(irf.periods + 1)\n\nfor idx, var in enumerate(response_vars):\n    ax = axes[idx // 2, idx % 2]\n    irf_vals = irf[var, 'interest_rate']\n    \n    # Confidence intervals\n    r_idx = list(irf.var_names).index(var)\n    i_idx = list(irf.var_names).index('interest_rate')\n    if irf.ci_lower is not None:\n        ci_lo = irf.ci_lower[:, r_idx, i_idx]\n        ci_hi = irf.ci_upper[:, r_idx, i_idx]\n        ax.fill_between(h, ci_lo, ci_hi, alpha=0.2, color=colors_resp[idx])\n    \n    ax.plot(h, irf_vals, color=colors_resp[idx], linewidth=2.2, marker='o', markersize=3)\n    ax.axhline(y=0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\n    \n    # Mark peak\n    peak_h = np.argmax(np.abs(irf_vals))\n    peak_val = irf_vals[peak_h]\n    if peak_h > 0:\n        ax.annotate(f'Peak: {peak_val:.4f} at h={peak_h}',\n                    xy=(peak_h, peak_val),\n                    xytext=(peak_h + 2, peak_val + 0.01 * np.sign(peak_val)),\n                    fontsize=9, arrowprops=dict(arrowstyle='->', color='gray'),\n                    bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow',\n                              edgecolor='gray'))\n    \n    ax.set_title(titles[var], fontsize=12, fontweight='bold')\n    ax.set_xlabel('Horizon (quarters)', fontsize=10)\n    ax.set_ylabel('Response', fontsize=10)\n    ax.grid(True, alpha=0.3)\n\nfig.suptitle('Monetary Policy Shock: Impulse Response Functions\\n'\n             '(Cholesky, 95% Bootstrap CI)',\n             fontsize=14, fontweight='bold', y=1.02)\nfig.tight_layout()\nfig.savefig('../outputs/figures/irfs/07_monetary_shock_all.png',\n            dpi=150, bbox_inches='tight')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_40",
   "metadata": {},
   "source": "# Detailed response table at key horizons\nprint('Responses to Monetary Policy Shock at Key Horizons')\nprint('=' * 90)\nprint(f'{\"Horizon\":>8s}', end='')\nfor var in response_vars:\n    print(f'{var:>18s}', end='')\nprint()\nprint('-' * 90)\n\nfor horizon in [0, 1, 2, 4, 8, 12, 16, 20]:\n    print(f'{horizon:>8d}', end='')\n    for var in response_vars:\n        val = irf[var, 'interest_rate'][horizon]\n        print(f'{val:>18.6f}', end='')\n    print()\n\nprint('\\n\\nKey Economic Results')\nprint('=' * 60)\nfor var in ['inflation', 'gdp_growth', 'unemployment']:\n    vals = irf[var, 'interest_rate']\n    peak_idx = np.argmax(np.abs(vals))\n    print(f'\\n  {var}:')\n    print(f'    Impact effect (h=0):  {vals[0]:.6f}')\n    print(f'    Peak effect:          {vals[peak_idx]:.6f} at h={peak_idx}')\n    print(f'    Long-run (h=20):      {vals[-1]:.6f}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_41",
   "metadata": {},
   "source": "### 6.3 Full IRF Grid"
  },
  {
   "cell_type": "code",
   "id": "cell_42",
   "metadata": {},
   "source": "# Full K x K IRF grid\nfig = plot_irf_grid(irf, save_path='../outputs/figures/irfs/07_irf_full_grid.png')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_43",
   "metadata": {},
   "source": "### 6.4 Cumulative IRFs\n\nSince GDP growth is a flow variable (rate of change), the cumulative IRF captures the total effect on the GDP **level**."
  },
  {
   "cell_type": "code",
   "id": "cell_44",
   "metadata": {},
   "source": "# Compute cumulative IRFs\nirf_cumulative = results.irf(\n    periods=20,\n    method='cholesky',\n    order=ordering,\n    cumulative=True\n)\n\n# Compare level vs cumulative for GDP growth\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nvals_level = irf['gdp_growth', 'interest_rate']\naxes[0].plot(h, vals_level, color='#2166ac', linewidth=2, marker='o', markersize=3)\naxes[0].fill_between(h, 0, vals_level, alpha=0.15, color='#2166ac')\naxes[0].axhline(y=0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\naxes[0].set_title('Level IRF (period-by-period)', fontsize=12, fontweight='bold')\naxes[0].set_xlabel('Horizon (quarters)', fontsize=11)\naxes[0].set_ylabel('Response of GDP Growth', fontsize=11)\naxes[0].grid(True, alpha=0.3)\n\nvals_cum = irf_cumulative['gdp_growth', 'interest_rate']\naxes[1].plot(h, vals_cum, color='#b2182b', linewidth=2, marker='s', markersize=3)\naxes[1].fill_between(h, 0, vals_cum, alpha=0.15, color='#b2182b')\naxes[1].axhline(y=0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\naxes[1].set_title('Cumulative IRF (effect on GDP level)', fontsize=12, fontweight='bold')\naxes[1].set_xlabel('Horizon (quarters)', fontsize=11)\naxes[1].set_ylabel('Cumulative Response', fontsize=11)\naxes[1].grid(True, alpha=0.3)\n\nfig.suptitle('GDP Growth Response to Monetary Shock: Level vs Cumulative',\n             fontsize=14, fontweight='bold', y=1.02)\nfig.tight_layout()\nfig.savefig('../outputs/figures/irfs/07_level_vs_cumulative.png',\n            dpi=150, bbox_inches='tight')\nplt.show()\n\nprint('Dynamic Multipliers (Cumulative IRF of GDP Growth):')\nprint(f'  Short-run (h=4):  {vals_cum[4]:.6f}')\nprint(f'  Medium-run (h=8): {vals_cum[8]:.6f}')\nprint(f'  Long-run (h=20):  {vals_cum[-1]:.6f}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_45",
   "metadata": {},
   "source": "---\n\n## Section 7: Variance Decomposition (25 min)\n\nFEVD tells us what fraction of the forecast error variance in each variable is attributable to shocks in other variables.\n\n### 7.1 Compute FEVD"
  },
  {
   "cell_type": "code",
   "id": "cell_46",
   "metadata": {},
   "source": "# Compute Cholesky FEVD\nfevd = results.fevd(periods=20, method='cholesky', order=ordering)\n\nprint(f'FEVD computed successfully.')\nprint(f'  Method: {fevd.method}')\nprint(f'  Decomposition shape: {fevd.decomposition.shape}')\nprint(f'  Variables: {fevd.var_names}')\nprint()\nprint(fevd.summary(horizons=[1, 4, 8, 12, 20]))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_47",
   "metadata": {},
   "source": "# Detailed FEVD for GDP growth\ngdp_idx = list(fevd.var_names).index('gdp_growth')\nir_idx_fevd = list(fevd.var_names).index('interest_rate')\n\nprint('Variance Decomposition of GDP Growth')\nprint('=' * 80)\nprint(f'{\"Horizon\":>8s}', end='')\nfor var in fevd.var_names:\n    print(f'{var:>18s}', end='')\nprint(f'{\"Sum\":>8s}')\nprint('-' * 80)\n\nfor horizon in [1, 4, 8, 12, 20]:\n    print(f'{horizon:>8d}', end='')\n    row_sum = 0\n    for j in range(fevd.K):\n        val = fevd.decomposition[horizon, gdp_idx, j]\n        row_sum += val\n        print(f'{val*100:>17.2f}%', end='')\n    print(f'{row_sum*100:>7.1f}%')\n\nprint(f'\\nMonetary policy (interest rate) explains '\n      f'{fevd.decomposition[20, gdp_idx, ir_idx_fevd]*100:.1f}% of GDP variance at h=20.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_48",
   "metadata": {},
   "source": "# Stacked area charts for all variables\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\n\ncolors_fevd = sns.color_palette('husl', n_colors=fevd.K)\nhorizons_plot = np.arange(fevd.periods + 1)\n\nfor idx, var in enumerate(list(fevd.var_names)):\n    ax = axes[idx // 2, idx % 2]\n    var_idx_local = list(fevd.var_names).index(var)\n    decomp = fevd.decomposition[:, var_idx_local, :]\n    \n    ax.stackplot(\n        horizons_plot,\n        *[decomp[:, j] for j in range(fevd.K)],\n        labels=list(fevd.var_names),\n        colors=colors_fevd,\n        alpha=0.85\n    )\n    ax.set_xlabel('Horizon', fontsize=10)\n    ax.set_ylabel('Share', fontsize=10)\n    ax.set_title(f'FEVD: {var}', fontsize=12, fontweight='bold')\n    ax.set_xlim(0, fevd.periods)\n    ax.set_ylim(0, 1.0)\n    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n    ax.grid(True, alpha=0.3, axis='y')\n    if idx == 1:\n        ax.legend(loc='center left', bbox_to_anchor=(1.02, 0.5),\n                  title='Shock', fontsize=9, title_fontsize=10)\n\nfig.suptitle('Forecast Error Variance Decomposition (Cholesky)',\n             fontsize=14, fontweight='bold', y=1.02)\nfig.tight_layout()\nfig.savefig('../outputs/figures/fevds/07_fevd_all.png',\n            dpi=150, bbox_inches='tight')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_49",
   "metadata": {},
   "source": "# Built-in FEVD plot for GDP growth\nfig = fevd.plot(variables=['gdp_growth'], backend='matplotlib', show=False)\nif fig is not None:\n    fig.savefig('../outputs/figures/fevds/07_fevd_gdp_builtin.png',\n                dpi=150, bbox_inches='tight')\n    plt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_50",
   "metadata": {},
   "source": "# Policy dominance summary\nprint('Share of Variance Explained by Monetary Shocks at h=20')\nprint('=' * 55)\nfor i, var in enumerate(fevd.var_names):\n    share = fevd.decomposition[20, i, ir_idx_fevd] * 100\n    bar = '#' * int(share)\n    print(f'  {var:>16s}: {share:5.1f}%  {bar}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_51",
   "metadata": {},
   "source": "---\n\n## Section 8: Heterogeneity Analysis (30 min)\n\nDo monetary policy shocks transmit differently in advanced vs. emerging/transition economies? We estimate separate Panel VARs for each group.\n\n### 8.1 Define Country Groups"
  },
  {
   "cell_type": "code",
   "id": "cell_52",
   "metadata": {},
   "source": "# Define country groups\nadvanced = ['USA', 'DEU', 'JPN', 'GBR', 'FRA']\nemerging = ['MEX', 'HUN', 'POL', 'CZE', 'KOR']\n\ndf_advanced = df[df['country'].isin(advanced)].copy()\ndf_emerging = df[df['country'].isin(emerging)].copy()\n\nprint(f'Advanced economies: {advanced}')\nprint(f'  Observations: {len(df_advanced)} ({df_advanced[\"country\"].nunique()} countries)')\nprint(f'\\nEmerging/transition economies: {emerging}')\nprint(f'  Observations: {len(df_emerging)} ({df_emerging[\"country\"].nunique()} countries)')\n\nprint('\\n\\nVariable Means by Group:')\nprint('=' * 65)\nprint(f'{\"Variable\":>16s} {\"Advanced\":>12s} {\"Emerging\":>12s} {\"Difference\":>12s}')\nprint('-' * 65)\nfor var in variables:\n    adv_mean = df_advanced[var].mean()\n    emg_mean = df_emerging[var].mean()\n    print(f'{var:>16s} {adv_mean:>12.3f} {emg_mean:>12.3f} {emg_mean - adv_mean:>12.3f}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_53",
   "metadata": {},
   "source": "# Estimate separate VARs\ndata_adv = PanelVARData(df_advanced, endog_vars=endog_vars,\n    entity_col='country', time_col='quarter', lags=2)\nmodel_adv = PanelVAR(data_adv)\nresults_adv = model_adv.fit(method='ols', cov_type='driscoll_kraay')\n\nprint('Advanced Economies VAR(2):')\nprint(f'  N={results_adv.N}, n_obs={results_adv.n_obs}')\nprint(f'  Stable: {results_adv.is_stable()}')\nprint(f'  Max eigenvalue modulus: {results_adv.max_eigenvalue_modulus:.4f}')\n\ndata_emg = PanelVARData(df_emerging, endog_vars=endog_vars,\n    entity_col='country', time_col='quarter', lags=2)\nmodel_emg = PanelVAR(data_emg)\nresults_emg = model_emg.fit(method='ols', cov_type='driscoll_kraay')\n\nprint(f'\\nEmerging Economies VAR(2):')\nprint(f'  N={results_emg.N}, n_obs={results_emg.n_obs}')\nprint(f'  Stable: {results_emg.is_stable()}')\nprint(f'  Max eigenvalue modulus: {results_emg.max_eigenvalue_modulus:.4f}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_54",
   "metadata": {},
   "source": "# Compute IRFs for both groups\nirf_adv = results_adv.irf(\n    periods=20, method='cholesky', order=ordering,\n    ci_method='bootstrap', n_bootstrap=500, ci_level=0.95,\n    seed=42, verbose=False\n)\nirf_emg = results_emg.irf(\n    periods=20, method='cholesky', order=ordering,\n    ci_method='bootstrap', n_bootstrap=500, ci_level=0.95,\n    seed=42, verbose=False\n)\nprint('IRFs computed for both groups (B=500 bootstrap replications).')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_55",
   "metadata": {},
   "source": "# Compare IRFs: Advanced vs Emerging\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nh = np.arange(irf_adv.periods + 1)\n\nfor idx, var in enumerate(response_vars):\n    ax = axes[idx // 2, idx % 2]\n    \n    # Advanced\n    vals_adv = irf_adv[var, 'interest_rate']\n    r_adv = list(irf_adv.var_names).index(var)\n    i_adv = list(irf_adv.var_names).index('interest_rate')\n    if irf_adv.ci_lower is not None:\n        ax.fill_between(h, irf_adv.ci_lower[:, r_adv, i_adv],\n                        irf_adv.ci_upper[:, r_adv, i_adv],\n                        alpha=0.15, color='#2166ac')\n    ax.plot(h, vals_adv, color='#2166ac', linewidth=2, label='Advanced', linestyle='-')\n    \n    # Emerging\n    vals_emg = irf_emg[var, 'interest_rate']\n    r_emg = list(irf_emg.var_names).index(var)\n    i_emg = list(irf_emg.var_names).index('interest_rate')\n    if irf_emg.ci_lower is not None:\n        ax.fill_between(h, irf_emg.ci_lower[:, r_emg, i_emg],\n                        irf_emg.ci_upper[:, r_emg, i_emg],\n                        alpha=0.15, color='#b2182b')\n    ax.plot(h, vals_emg, color='#b2182b', linewidth=2, label='Emerging', linestyle='--')\n    \n    ax.axhline(y=0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\n    ax.set_title(f'Response: {var}', fontsize=12, fontweight='bold')\n    ax.set_xlabel('Horizon (quarters)', fontsize=10)\n    ax.set_ylabel('Response', fontsize=10)\n    ax.legend(fontsize=9)\n    ax.grid(True, alpha=0.3)\n\nfig.suptitle('Monetary Shock Transmission: Advanced vs Emerging Economies\\n'\n             '(Cholesky IRFs with 95% Bootstrap CI)',\n             fontsize=14, fontweight='bold', y=1.03)\nfig.tight_layout()\nfig.savefig('../outputs/figures/irfs/07_heterogeneity_comparison.png',\n            dpi=150, bbox_inches='tight')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_56",
   "metadata": {},
   "source": "# Quantitative comparison at key horizons\nprint('Heterogeneity in Monetary Transmission')\nprint('=' * 85)\nprint(f'{\"Variable\":>16s} {\"Group\":>12s} {\"h=0\":>10s} {\"h=4\":>10s} '\n      f'{\"h=8\":>10s} {\"h=12\":>10s} {\"h=20\":>10s}')\nprint('-' * 85)\n\nfor var in ['inflation', 'gdp_growth', 'unemployment']:\n    for group_name, irf_obj in [('Advanced', irf_adv), ('Emerging', irf_emg)]:\n        vals = irf_obj[var, 'interest_rate']\n        print(f'{var:>16s} {group_name:>12s}', end='')\n        for horizon in [0, 4, 8, 12, 20]:\n            print(f'{vals[horizon]:>10.4f}', end='')\n        print()\n    print()\n\nprint('Larger responses in emerging economies may reflect less anchored expectations.')\nprint('Faster convergence in advanced economies suggests more credible monetary policy.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_57",
   "metadata": {},
   "source": "---\n\n## Section 9: Robustness Checks (20 min)\n\n### 9.1 Alternative Cholesky Orderings"
  },
  {
   "cell_type": "code",
   "id": "cell_58",
   "metadata": {},
   "source": "# Three alternative orderings\nalternative_orderings = {\n    'Baseline': ['interest_rate', 'inflation', 'gdp_growth', 'unemployment'],\n    'Alt 1 (output first)': ['gdp_growth', 'inflation', 'interest_rate', 'unemployment'],\n    'Alt 2 (policy last)': ['gdp_growth', 'unemployment', 'inflation', 'interest_rate'],\n}\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\ncolors_ord = ['#2166ac', '#b2182b', '#4dac26']\nstyles = ['-', '--', '-.']\nh = np.arange(21)\n\nfor resp_idx, resp_var in enumerate(['inflation', 'gdp_growth', 'unemployment']):\n    ax = axes[resp_idx]\n    for ord_idx, (label, ord_list) in enumerate(alternative_orderings.items()):\n        irf_alt = results.irf(periods=20, method='cholesky', order=ord_list)\n        vals = irf_alt[resp_var, 'interest_rate']\n        ax.plot(h, vals, color=colors_ord[ord_idx], linewidth=2,\n                linestyle=styles[ord_idx], label=label)\n    \n    ax.axhline(y=0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\n    ax.set_title(f'{resp_var} response', fontsize=12, fontweight='bold')\n    ax.set_xlabel('Horizon (quarters)', fontsize=10)\n    ax.set_ylabel('Response', fontsize=10)\n    ax.legend(fontsize=8, loc='best')\n    ax.grid(True, alpha=0.3)\n\nfig.suptitle('Robustness: IRFs across Alternative Cholesky Orderings\\n'\n             '(Response to Interest Rate Shock)',\n             fontsize=14, fontweight='bold', y=1.04)\nfig.tight_layout()\nfig.savefig('../outputs/figures/irfs/07_ordering_robustness.png',\n            dpi=150, bbox_inches='tight')\nplt.show()\n\nprint('If IRFs are similar across orderings, the identification is robust.')\nprint('Large differences suggest sensitivity to the recursive assumption.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_59",
   "metadata": {},
   "source": "### 9.2 Generalized IRFs (Order-Invariant)"
  },
  {
   "cell_type": "code",
   "id": "cell_60",
   "metadata": {},
   "source": "# Generalized IRFs (Pesaran-Shin, order-invariant)\nirf_generalized = results.irf(periods=20, method='generalized')\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nh = np.arange(21)\n\nfor idx, var in enumerate(['inflation', 'gdp_growth', 'unemployment']):\n    ax = axes[idx]\n    chol_vals = irf[var, 'interest_rate']\n    gen_vals = irf_generalized[var, 'interest_rate']\n    \n    ax.plot(h, chol_vals, color='#2166ac', linewidth=2, label='Cholesky', linestyle='-')\n    ax.plot(h, gen_vals, color='#b2182b', linewidth=2, label='Generalized', linestyle='--')\n    ax.axhline(y=0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\n    ax.set_title(f'{var}', fontsize=12, fontweight='bold')\n    ax.set_xlabel('Horizon', fontsize=10)\n    ax.set_ylabel('Response', fontsize=10)\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n\nfig.suptitle('Cholesky vs Generalized IRFs: Response to Interest Rate Shock',\n             fontsize=14, fontweight='bold', y=1.02)\nfig.tight_layout()\nfig.savefig('../outputs/figures/irfs/07_cholesky_vs_generalized.png',\n            dpi=150, bbox_inches='tight')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_61",
   "metadata": {},
   "source": "### 9.3 Sensitivity to Lag Order"
  },
  {
   "cell_type": "code",
   "id": "cell_62",
   "metadata": {},
   "source": "# Compare IRFs across lag orders p=1, 2, 3, 4\nlag_orders = [1, 2, 3, 4]\nlag_colors = ['#1b9e77', '#d95f02', '#7570b3', '#e7298a']\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nh = np.arange(21)\n\nfor resp_idx, resp_var in enumerate(['inflation', 'gdp_growth', 'unemployment']):\n    ax = axes[resp_idx]\n    for lag_idx, p_lags in enumerate(lag_orders):\n        try:\n            data_p = PanelVARData(df, endog_vars=endog_vars, entity_col='country',\n                                 time_col='quarter', lags=p_lags)\n            model_p = PanelVAR(data_p)\n            results_p = model_p.fit(method='ols', cov_type='driscoll_kraay')\n            irf_p = results_p.irf(periods=20, method='cholesky', order=ordering)\n            vals = irf_p[resp_var, 'interest_rate']\n            stable_label = 'S' if results_p.is_stable() else 'U'\n            ax.plot(h, vals, color=lag_colors[lag_idx], linewidth=2,\n                    label=f'p={p_lags} [{stable_label}]')\n        except Exception as e:\n            print(f'  Lag {p_lags}: Error - {e}')\n    \n    ax.axhline(y=0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\n    ax.set_title(f'{resp_var}', fontsize=12, fontweight='bold')\n    ax.set_xlabel('Horizon', fontsize=10)\n    ax.set_ylabel('Response', fontsize=10)\n    ax.legend(fontsize=9)\n    ax.grid(True, alpha=0.3)\n\nfig.suptitle('Robustness: IRFs across Lag Orders (p=1,2,3,4)\\n'\n             'S=Stable, U=Unstable',\n             fontsize=14, fontweight='bold', y=1.04)\nfig.tight_layout()\nfig.savefig('../outputs/figures/irfs/07_lag_robustness.png',\n            dpi=150, bbox_inches='tight')\nplt.show()\n\nprint('If results are qualitatively similar across lag orders, findings are robust.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_63",
   "metadata": {},
   "source": "---\n\n## Section 10: Summary and Conclusions (20 min)\n\n### Key Findings"
  },
  {
   "cell_type": "code",
   "id": "cell_64",
   "metadata": {},
   "source": "# Automated summary\nprint('=' * 70)\nprint('SUMMARY OF KEY FINDINGS')\nprint('=' * 70)\n\n# 1. Monetary Policy Effectiveness\nprint('\\n1. MONETARY POLICY EFFECTIVENESS')\nprint('-' * 40)\ngdp_resp = irf['gdp_growth', 'interest_rate']\ngdp_peak_h = np.argmax(np.abs(gdp_resp))\nprint(f'   GDP growth peak response: {gdp_resp[gdp_peak_h]:.4f} at h={gdp_peak_h}')\nprint(f'   GDP growth impact (h=0):  {gdp_resp[0]:.4f}')\nunemp_resp = irf['unemployment', 'interest_rate']\nunemp_peak_h = np.argmax(np.abs(unemp_resp))\nprint(f'   Unemployment peak:        {unemp_resp[unemp_peak_h]:.4f} at h={unemp_peak_h}')\n\n# 2. Transmission Timing\nprint('\\n2. TRANSMISSION TIMING')\nprint('-' * 40)\nfor var in ['inflation', 'gdp_growth', 'unemployment']:\n    vals = irf[var, 'interest_rate']\n    peak_h = np.argmax(np.abs(vals))\n    peak_val = vals[peak_h]\n    half_target = peak_val / 2\n    half_life = peak_h\n    for hh in range(peak_h, len(vals)):\n        if abs(vals[hh]) <= abs(half_target):\n            half_life = hh\n            break\n    print(f'   {var:>16s}: peak at h={peak_h}, half-life ~h={half_life}')\n\n# 3. Variance Decomposition\nprint('\\n3. VARIANCE DECOMPOSITION (h=20)')\nprint('-' * 40)\nfor i, var in enumerate(fevd.var_names):\n    ir_share = fevd.decomposition[20, i, ir_idx_fevd] * 100\n    print(f'   {var:>16s}: {ir_share:5.1f}% from monetary shocks')\n\n# 4. Heterogeneity\nprint('\\n4. HETEROGENEITY: ADVANCED vs EMERGING')\nprint('-' * 40)\nfor var in ['gdp_growth', 'inflation']:\n    adv_peak = np.max(np.abs(irf_adv[var, 'interest_rate']))\n    emg_peak = np.max(np.abs(irf_emg[var, 'interest_rate']))\n    ratio = emg_peak / adv_peak if adv_peak > 0 else float('inf')\n    print(f'   {var:>16s}: Adv peak={adv_peak:.4f}, Emg peak={emg_peak:.4f} (ratio={ratio:.2f})')\n\n# 5. Granger Causality\nprint('\\n5. GRANGER CAUSALITY HIGHLIGHTS')\nprint('-' * 40)\nfor _, row in pw_df.head(5).iterrows():\n    stars = '***' if row['p_value'] < 0.01 else '**' if row['p_value'] < 0.05 else '*' if row['p_value'] < 0.10 else ''\n    print(f'   {row[\"Cause\"]:>16s} -> {row[\"Effect\"]:<16s} p={row[\"p_value\"]:.4f} {stars}')\n\nprint('\\n' + '=' * 70)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_65",
   "metadata": {},
   "source": "### Economic Interpretation\n\n**Monetary Policy Effectiveness:**\nA contractionary monetary shock (interest rate increase) leads to a decline in GDP growth and an increase in unemployment, consistent with standard macroeconomic theory (IS curve). The peak effect on GDP typically occurs with a lag of several quarters, reflecting the well-known \"long and variable lags\" of monetary policy (Friedman, 1961).\n\n**Inflation Response:**\nThe inflation response to a monetary tightening may exhibit a \"price puzzle\" (initial increase) in the short run, a common finding in VAR-based monetary policy analysis (Sims, 1992). The contractionary effect on inflation becomes evident at longer horizons.\n\n**Transmission Heterogeneity:**\nAdvanced and emerging economies show different magnitudes and timing in their responses, reflecting differences in:\n- Financial market development and the interest rate channel\n- Exchange rate regimes and pass-through\n- Central bank credibility and expectations anchoring\n- Structural rigidities in labor and product markets\n\n**Policy Implications:**\n1. Central banks should account for the delayed effects of monetary policy when setting rates\n2. The heterogeneity across country groups suggests that \"one size fits all\" monetary policy rules are inappropriate\n3. Variance decomposition reveals the relative importance of monetary vs. supply shocks for output stabilization"
  },
  {
   "cell_type": "markdown",
   "id": "cell_66",
   "metadata": {},
   "source": "---\n\n## Section 11: Export Results (10 min)"
  },
  {
   "cell_type": "code",
   "id": "cell_67",
   "metadata": {},
   "source": "# Save Granger causality matrix\ngc_matrix.to_csv('../outputs/tables/07_granger_causality_matrix.csv')\nprint('Saved: ../outputs/tables/07_granger_causality_matrix.csv')\n\n# Save pairwise Granger results\npw_df.to_csv('../outputs/tables/07_pairwise_granger.csv', index=False)\nprint('Saved: ../outputs/tables/07_pairwise_granger.csv')\n\n# Save IRF at key horizons\nirf_records = []\nfor var in response_vars:\n    for horizon in [0, 1, 2, 4, 8, 12, 16, 20]:\n        irf_records.append({\n            'response': var, 'impulse': 'interest_rate',\n            'horizon': horizon,\n            'irf_value': irf[var, 'interest_rate'][horizon],\n        })\nirf_export_df = pd.DataFrame(irf_records)\nirf_export_df.to_csv('../outputs/tables/07_irf_monetary_shock.csv', index=False)\nprint('Saved: ../outputs/tables/07_irf_monetary_shock.csv')\n\n# Save FEVD at key horizons\nfevd_records = []\nfor i, var in enumerate(fevd.var_names):\n    for horizon in [1, 4, 8, 12, 20]:\n        for j, shock_var in enumerate(fevd.var_names):\n            fevd_records.append({\n                'variable': var, 'shock': shock_var,\n                'horizon': horizon,\n                'share': fevd.decomposition[horizon, i, j],\n            })\nfevd_export_df = pd.DataFrame(fevd_records)\nfevd_export_df.to_csv('../outputs/tables/07_fevd_all.csv', index=False)\nprint('Saved: ../outputs/tables/07_fevd_all.csv')\n\n# Save heterogeneity comparison\nhetero_records = []\nfor var in ['inflation', 'gdp_growth', 'unemployment']:\n    for group_name, irf_obj in [('Advanced', irf_adv), ('Emerging', irf_emg)]:\n        vals = irf_obj[var, 'interest_rate']\n        for horizon in [0, 4, 8, 12, 20]:\n            hetero_records.append({\n                'variable': var, 'group': group_name,\n                'horizon': horizon, 'irf_value': vals[horizon],\n            })\nhetero_df = pd.DataFrame(hetero_records)\nhetero_df.to_csv('../outputs/tables/07_heterogeneity_irf.csv', index=False)\nprint('Saved: ../outputs/tables/07_heterogeneity_irf.csv')\n\nprint('\\nAll results exported successfully.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e4ilua0ae6g",
   "source": "# Generate HTML report\nimport html as html_module\nfrom datetime import datetime\n\nhtml_content = f\"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Monetary Policy Transmission Analysis</title>\n    <style>\n        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; color: #333; }}\n        h1 {{ color: #2166ac; border-bottom: 3px solid #2166ac; padding-bottom: 10px; }}\n        h2 {{ color: #4393c3; margin-top: 30px; }}\n        h3 {{ color: #666; }}\n        table {{ border-collapse: collapse; width: 100%; margin: 15px 0; }}\n        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: right; }}\n        th {{ background-color: #2166ac; color: white; }}\n        tr:nth-child(even) {{ background-color: #f2f2f2; }}\n        .summary-box {{ background: #f0f7ff; border: 1px solid #b2d3ea; border-radius: 8px; padding: 15px; margin: 15px 0; }}\n        .finding {{ background: #fff8e1; border-left: 4px solid #ffc107; padding: 10px 15px; margin: 10px 0; }}\n        .footer {{ text-align: center; color: #999; margin-top: 40px; font-size: 0.9em; }}\n        img {{ max-width: 100%; height: auto; margin: 10px 0; border: 1px solid #eee; }}\n    </style>\n</head>\n<body>\n<h1>Monetary Policy Transmission Analysis</h1>\n<p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M')}</p>\n<p><strong>Data:</strong> {results.N} countries, {df['quarter'].nunique()} quarters ({df['quarter'].min()} to {df['quarter'].max()})</p>\n<p><strong>Model:</strong> Panel VAR({results.p}) with Driscoll-Kraay standard errors</p>\n\n<div class=\"summary-box\">\n<h3>Executive Summary</h3>\n<p>This report analyzes the transmission of monetary policy shocks across {results.N} OECD countries\nusing a Panel VAR framework. Key findings indicate that contractionary monetary policy (interest rate increases)\nsignificantly reduces GDP growth and inflation, with peak effects occurring several quarters after the shock.</p>\n</div>\n\n<h2>1. Model Specification</h2>\n<table>\n<tr><th>Parameter</th><th>Value</th></tr>\n<tr><td>Endogenous variables</td><td>{', '.join(endog_vars)}</td></tr>\n<tr><td>Number of lags (p)</td><td>{results.p}</td></tr>\n<tr><td>Number of countries (N)</td><td>{results.N}</td></tr>\n<tr><td>Observations</td><td>{results.n_obs}</td></tr>\n<tr><td>Covariance type</td><td>Driscoll-Kraay</td></tr>\n<tr><td>Stable</td><td>{results.is_stable()}</td></tr>\n<tr><td>Max eigenvalue modulus</td><td>{results.max_eigenvalue_modulus:.6f}</td></tr>\n<tr><td>AIC</td><td>{results.aic:.4f}</td></tr>\n<tr><td>BIC</td><td>{results.bic:.4f}</td></tr>\n</table>\n\n<h2>2. Granger Causality Results</h2>\n<p>The table below shows p-values for pairwise Granger causality tests. Values below 0.05 indicate\nstatistically significant predictive relationships.</p>\n<table>\n<tr><th>Cause / Effect</th>\"\"\"\n\n# Add Granger causality table\nfor col in gc_matrix.columns:\n    html_content += f\"<th>{html_module.escape(col)}</th>\"\nhtml_content += \"</tr>\"\nfor idx_row, row in gc_matrix.iterrows():\n    html_content += f\"<tr><td><strong>{html_module.escape(str(idx_row))}</strong></td>\"\n    for val in row:\n        if pd.notna(val):\n            color = '#d4edda' if val < 0.05 else '#fff'\n            html_content += f'<td style=\"background:{color}\">{val:.4f}</td>'\n        else:\n            html_content += '<td>--</td>'\n    html_content += \"</tr>\"\nhtml_content += \"</table>\"\n\n# IRF key results\nhtml_content += \"\"\"\n<h2>3. Impulse Response Functions</h2>\n<p>Responses to a one-standard-deviation contractionary monetary shock (Cholesky identification):</p>\n<table>\n<tr><th>Response Variable</th><th>Impact (h=0)</th><th>h=4</th><th>h=8</th><th>Peak Effect</th><th>Peak Horizon</th></tr>\n\"\"\"\nfor var in ['inflation', 'gdp_growth', 'unemployment']:\n    vals = irf[var, 'interest_rate']\n    peak_h = np.argmax(np.abs(vals))\n    html_content += f\"\"\"<tr>\n    <td><strong>{var}</strong></td>\n    <td>{vals[0]:.6f}</td><td>{vals[4]:.6f}</td><td>{vals[8]:.6f}</td>\n    <td>{vals[peak_h]:.6f}</td><td>h={peak_h}</td>\n    </tr>\"\"\"\nhtml_content += \"</table>\"\n\n# FEVD results\nhtml_content += \"\"\"\n<h2>4. Variance Decomposition</h2>\n<p>Share of forecast error variance explained by monetary shocks (interest rate) at selected horizons:</p>\n<table>\n<tr><th>Variable</th><th>h=1</th><th>h=4</th><th>h=8</th><th>h=20</th></tr>\n\"\"\"\nfor i, var in enumerate(fevd.var_names):\n    html_content += f\"<tr><td><strong>{var}</strong></td>\"\n    for horizon in [1, 4, 8, 20]:\n        share = fevd.decomposition[horizon, i, ir_idx_fevd] * 100\n        html_content += f\"<td>{share:.1f}%</td>\"\n    html_content += \"</tr>\"\nhtml_content += \"</table>\"\n\n# Heterogeneity results\nhtml_content += \"\"\"\n<h2>5. Heterogeneity: Advanced vs Emerging Economies</h2>\n<div class=\"finding\">\n<p><strong>Finding:</strong> Advanced and emerging economies show different transmission patterns,\nreflecting differences in financial market development, central bank credibility, and structural rigidities.</p>\n</div>\n<table>\n<tr><th>Variable</th><th>Group</th><th>Peak Effect</th><th>Peak Horizon</th></tr>\n\"\"\"\nfor var in ['gdp_growth', 'inflation']:\n    for group_name, irf_obj in [('Advanced', irf_adv), ('Emerging', irf_emg)]:\n        vals = irf_obj[var, 'interest_rate']\n        peak_h = np.argmax(np.abs(vals))\n        html_content += f\"\"\"<tr><td>{var}</td><td>{group_name}</td>\n        <td>{vals[peak_h]:.6f}</td><td>h={peak_h}</td></tr>\"\"\"\nhtml_content += \"</table>\"\n\n# Conclusions\nhtml_content += \"\"\"\n<h2>6. Policy Implications</h2>\n<div class=\"finding\">\n<p>1. <strong>Monetary policy is effective</strong> but operates with significant lags.</p>\n<p>2. <strong>Forward-looking policy</strong> is essential given the delayed transmission.</p>\n<p>3. <strong>Heterogeneity</strong> across country groups argues against uniform policy rules.</p>\n<p>4. <strong>Communication and credibility</strong> amplify the effectiveness of interest rate adjustments.</p>\n</div>\n\n<div class=\"footer\">\n<p>Generated by PanelBox VAR Analysis | Panel VAR Tutorial Series</p>\n</div>\n</body>\n</html>\"\"\"\n\n# Save the report\nreport_path = '../outputs/monetary_policy_report.html'\nwith open(report_path, 'w', encoding='utf-8') as f:\n    f.write(html_content)\n\nprint(f'HTML report generated: {report_path}')\nprint(f'Report size: {len(html_content):,} characters')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell_68",
   "metadata": {},
   "source": "### Exercise 1: Alternative Specifications (Easy)\n\nInclude an additional variable (e.g., exchange rate or money supply) in the VAR and re-run the full analysis. Also test robustness by excluding the crisis period (2008-2009) and comparing IRFs.\n\n**Tasks:**\n1. Re-estimate the VAR with only 3 variables (drop unemployment): `['interest_rate', 'inflation', 'gdp_growth']`\n2. Exclude the crisis period (2008-2009 quarters) from the data and re-estimate the baseline 4-variable model\n3. Compare the inflation response to a monetary shock across specifications\n\n**Expected output:**\n- IRF comparison plot (full sample vs crisis-excluded) for the inflation response to monetary shock\n- Table of key IRF peak magnitudes across specifications\n- Discussion of whether monetary policy effects are crisis-dependent"
  },
  {
   "cell_type": "code",
   "id": "cell_69",
   "metadata": {},
   "source": "# YOUR CODE HERE",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_70",
   "metadata": {},
   "source": "### Exercise 2: Country-by-Country Heterogeneity (Medium)\n\nEstimate individual VARs for each country (or a subset of 5-6 countries). Compare the inflation IRF to a monetary policy shock across countries. Test for structural breaks using subsample analysis (pre/post 2008).\n\n**Tasks:**\n1. Estimate separate time-series VARs for 5 countries: USA, DEU, JPN, MEX, KOR\n2. Compute the inflation response to a monetary shock for each country\n3. Calculate the half-life of adjustment for each country\n4. Create a bar chart of half-life of inflation adjustment by country\n\n**Expected output:**\n- Multi-panel figure showing country-specific inflation IRFs overlaid on the panel IRF\n- Table of peak response magnitudes and timing by country\n- Bar chart of half-life of inflation adjustment by country"
  },
  {
   "cell_type": "code",
   "id": "cell_71",
   "metadata": {},
   "source": "# YOUR CODE HERE",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_72",
   "metadata": {},
   "source": "### Exercise 3: GMM Extension for Short Panels (Hard)\n\nSubset the data to T=15 quarters (simulating a short panel scenario). Compare OLS-based VAR results with the theoretical Nickell bias. Run full diagnostic analysis.\n\n**Tasks:**\n1. Subset the data to the last 15 quarters only\n2. Estimate the VAR using OLS on the short panel\n3. Compare coefficient estimates from the full sample (T=80) vs the short panel (T=15)\n4. Calculate the theoretical Nickell bias for T=15 and discuss its implications\n5. Discuss when GMM (Arellano-Bond) would be necessary vs when OLS is acceptable\n\n**Expected output:**\n- Side-by-side comparison table: Full sample vs Short panel coefficient estimates and standard errors\n- Diagnostic summary (stability, information criteria comparison)\n- IRF comparison plot across estimation samples\n- Theoretical Nickell bias chart as function of T"
  },
  {
   "cell_type": "code",
   "id": "cell_73",
   "metadata": {},
   "source": "# YOUR CODE HERE",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_74",
   "metadata": {},
   "source": "### Exercise 4: Policy Brief (Medium)\n\nWrite a structured 2-page policy brief summarizing the main findings. Include: (a) key stylized facts from EDA, (b) main IRF results with confidence intervals, (c) FEVD decomposition at policy-relevant horizons, (d) heterogeneity insights, and (e) policy recommendations.\n\n**Tasks:**\n1. Compile key statistics from the analysis\n2. Create at least 2 publication-quality figures\n3. Write an executive summary, methodology section, key findings, and policy implications\n4. Format as a structured markdown cell\n\n**Expected output:**\n- Markdown cell with formatted policy brief\n- At least 2 publication-quality figures embedded\n- Executive summary, methodology section, key findings, and policy implications clearly structured"
  },
  {
   "cell_type": "code",
   "id": "cell_75",
   "metadata": {},
   "source": "# YOUR CODE HERE",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_76",
   "metadata": {},
   "source": "---\n\n## References\n\n- Christiano, L. J., Eichenbaum, M., & Evans, C. L. (1999). Monetary policy shocks: What have we learned and to what end? *Handbook of Macroeconomics*, 1, 65–148.\n- Dumitrescu, E. I., & Hurlin, C. (2012). Testing for Granger non-causality in heterogeneous panels. *Economic Modelling*, 29(4), 1450–1460.\n- Friedman, M. (1961). The lag in effect of monetary policy. *Journal of Political Economy*, 69(5), 447–466.\n- Granger, C. W. J. (1969). Investigating causal relations by econometric models and cross-spectral methods. *Econometrica*, 37(3), 424–438.\n- Holtz-Eakin, D., Newey, W., & Rosen, H. S. (1988). Estimating vector autoregressions with panel data. *Econometrica*, 56(6), 1371–1395.\n- Lutkepohl, H. (2005). *New Introduction to Multiple Time Series Analysis*. Springer.\n- Pesaran, H. H., & Shin, Y. (1998). Generalized impulse response analysis in linear multivariate models. *Economics Letters*, 58(1), 17–29.\n- Romer, C. D., & Romer, D. H. (2004). A new measure of monetary shocks. *American Economic Review*, 94(4), 1055–1084.\n- Sims, C. A. (1980). Macroeconomics and reality. *Econometrica*, 48(1), 1–48.\n- Sims, C. A. (1992). Interpreting the macroeconomic time series facts: The effects of monetary policy. *European Economic Review*, 36(5), 975–1000.\n- Taylor, J. B. (1993). Discretion versus policy rules in practice. *Carnegie-Rochester Conference Series on Public Policy*, 39, 195–214."
  }
 ]
}
