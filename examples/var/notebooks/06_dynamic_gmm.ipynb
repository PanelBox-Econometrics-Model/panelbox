{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell_01",
   "metadata": {},
   "source": "# Tutorial 06: Dynamic Panel VAR with GMM Estimation\n\n**Duration:** 120--150 minutes  \n**Level:** Advanced  \n**Prerequisites:** Panel VAR estimation (Tutorials 01--05), basic GMM concepts, matrix algebra\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. Explain the **Nickell bias** problem in dynamic panel models and quantify its magnitude\n2. Implement **Difference GMM** (Arellano-Bond) to eliminate fixed-effect endogeneity\n3. Understand when **System GMM** (Blundell-Bond) improves upon Difference GMM\n4. Perform essential **GMM diagnostics**: Hansen J-test, AR(2) test, instrument count rules\n5. Address **instrument proliferation** with collapsed instruments\n6. Compare **OLS vs. GMM** estimates and apply decision rules for estimator choice\n7. Calculate **half-lives** of shock persistence and assess how estimator bias affects them\n\n## Outline\n\n1. [Nickell Bias Problem](#1-nickell-bias-problem) (25 min)\n2. [Difference GMM (Arellano-Bond)](#2-difference-gmm-arellano-bond) (30 min)\n3. [System GMM (Blundell-Bond)](#3-system-gmm-blundell-bond) (25 min)\n4. [GMM Diagnostics](#4-gmm-diagnostics) (30 min)\n5. [Instrument Collapse](#5-instrument-collapse) (20 min)\n6. [OLS vs GMM Comparison](#6-ols-vs-gmm-comparison) (15 min)\n7. [Application: Shock Persistence](#7-application-shock-persistence) (15 min)\n8. [Summary](#8-summary)\n9. [Exercises](#9-exercises)"
  },
  {
   "cell_type": "code",
   "id": "cell_02",
   "metadata": {},
   "source": "# ============================================================\n# Setup\n# ============================================================\nimport sys\nimport os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\n%matplotlib inline\n\n# Reproducibility\nnp.random.seed(42)\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# Add project root and utilities to path\nproject_root = Path('../../../').resolve()\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\nsys.path.insert(0, str(Path('../utils').resolve()))\n\n# PanelBox imports\nfrom panelbox.var import PanelVARData, PanelVAR\n\n# Tutorial utilities\nfrom data_generators import generate_dynamic_panel, generate_macro_panel\nfrom var_simulation import simulate_panel_var\nfrom visualization_helpers import set_academic_style\n\n# Apply academic style\nset_academic_style()\n\n# Output directory\nos.makedirs('../outputs/figures/gmm', exist_ok=True)\nos.makedirs('../outputs/tables', exist_ok=True)\n\n# Track GMM availability\nGMM_AVAILABLE = False\ntry:\n    from panelbox.var.gmm import estimate_panel_var_gmm\n    from panelbox.var.instruments import build_gmm_instruments\n    GMM_AVAILABLE = True\n    print('panelbox.var.gmm module: available')\nexcept ImportError:\n    print('panelbox.var.gmm module: NOT available (will use manual implementations)')\n\nprint('Setup complete.')\nprint(f'NumPy: {np.__version__}')\nprint(f'Pandas: {pd.__version__}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_03",
   "metadata": {},
   "source": "---\n\n## 1. Nickell Bias Problem\n\n### The Core Issue\n\nConsider the simplest dynamic panel model:\n\n$$Y_{it} = \\alpha_i + \\rho \\cdot Y_{i,t-1} + \\varepsilon_{it}$$\n\nwhere:\n- $\\alpha_i$ is an entity-specific fixed effect\n- $\\rho$ is the autoregressive parameter (persistence)\n- $\\varepsilon_{it}$ is an i.i.d. error term\n\n**The problem:** The fixed effect $\\alpha_i$ is correlated with $Y_{i,t-1}$ by construction. Since $Y_{i,t-1}$ depends on $\\alpha_i$ (through the recursive structure of the model), the regressor is endogenous.\n\n### Why the Within Transformation Fails\n\nThe standard approach to handle fixed effects is the **within transformation** (demeaning):\n\n$$\\tilde{Y}_{it} = \\tilde{\\rho} \\cdot \\tilde{Y}_{i,t-1} + \\tilde{\\varepsilon}_{it}$$\n\nwhere $\\tilde{X}_{it} = X_{it} - \\bar{X}_i$. The problem is that $\\tilde{Y}_{i,t-1}$ contains $-\\bar{Y}_i$, which depends on $\\varepsilon_{it}$ through $\\bar{Y}_i = \\frac{1}{T}\\sum_{s=1}^T Y_{is}$. This creates a **mechanical correlation** between the transformed regressor and the transformed error.\n\n### Bias Magnitude\n\nNickell (1981) showed that the OLS bias on $\\hat{\\rho}$ is:\n\n$$\\text{plim}(\\hat{\\rho}_{FE} - \\rho) \\approx -\\frac{1+\\rho}{T-1}$$\n\nKey implications:\n- The bias is $O(1/T)$ -- severe when $T$ is small\n- The bias is **always negative** (downward bias on persistence)\n- For $T=5$: bias $\\approx -(1+\\rho)/4$, which is huge\n- For $T=10$: bias $\\approx -(1+\\rho)/9$\n- For $T=100$: bias $\\approx -(1+\\rho)/99$, negligible"
  },
  {
   "cell_type": "code",
   "id": "cell_04",
   "metadata": {},
   "source": "# ============================================================\n# Numerical Demonstration: Nickell Bias\n# ============================================================\n\ndef simulate_dynamic_panel_simple(N, T, rho_true, sigma_alpha=1.0, sigma_eps=1.0, seed=42):\n    \"\"\"\n    Simulate a simple dynamic panel: Y_it = alpha_i + rho * Y_{i,t-1} + eps_it.\n    Returns a DataFrame with columns: entity, time, y.\n    \"\"\"\n    np.random.seed(seed)\n    records = []\n    for i in range(N):\n        alpha_i = sigma_alpha * np.random.randn()\n        # Initial value drawn from stationary distribution\n        y_prev = alpha_i / (1 - rho_true) + sigma_eps / np.sqrt(1 - rho_true**2) * np.random.randn()\n        for t in range(T):\n            eps = sigma_eps * np.random.randn()\n            y_curr = alpha_i + rho_true * y_prev + eps\n            records.append({'entity': i, 'time': t, 'y': y_curr})\n            y_prev = y_curr\n    return pd.DataFrame(records)\n\n\ndef estimate_fe_ols(df, entity_col='entity', time_col='time', y_col='y'):\n    \"\"\"\n    Estimate rho from dynamic panel using within (FE) OLS.\n    Returns estimated rho.\n    \"\"\"\n    # Create lagged y within each entity\n    df = df.sort_values([entity_col, time_col]).copy()\n    df['y_lag'] = df.groupby(entity_col)[y_col].shift(1)\n    df = df.dropna(subset=['y_lag'])\n\n    # Within transformation (demean by entity)\n    df['y_dm'] = df[y_col] - df.groupby(entity_col)[y_col].transform('mean')\n    df['y_lag_dm'] = df['y_lag'] - df.groupby(entity_col)['y_lag'].transform('mean')\n\n    # OLS on demeaned data\n    x = df['y_lag_dm'].values\n    y = df['y_dm'].values\n    rho_hat = np.dot(x, y) / np.dot(x, x)\n    return rho_hat\n\n\n# True parameters\nrho_true = 0.7\nN = 100\nT = 10\n\n# Simulate and estimate\ndf_sim = simulate_dynamic_panel_simple(N=N, T=T, rho_true=rho_true)\nrho_fe = estimate_fe_ols(df_sim)\n\n# Theoretical bias\nbias_theoretical = -(1 + rho_true) / (T - 1)\n\nprint('=== Nickell Bias Demonstration ===')\nprint(f'True rho:            {rho_true:.4f}')\nprint(f'FE-OLS estimate:     {rho_fe:.4f}')\nprint(f'Bias (actual):       {rho_fe - rho_true:.4f}')\nprint(f'Bias (theoretical):  {bias_theoretical:.4f}')\nprint(f'\\nThe FE-OLS estimator is severely biased downward!')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_05",
   "metadata": {},
   "source": "# ============================================================\n# Monte Carlo: Nickell Bias for Different T Values\n# ============================================================\n\nrho_true = 0.7\nN = 200\nT_values = [5, 10, 20, 50, 100]\nn_simulations = 200\n\nresults_mc = {}\n\nfor T in T_values:\n    rho_estimates = []\n    for sim in range(n_simulations):\n        df_mc = simulate_dynamic_panel_simple(N=N, T=T, rho_true=rho_true, seed=sim * 100 + T)\n        rho_hat = estimate_fe_ols(df_mc)\n        rho_estimates.append(rho_hat)\n    results_mc[T] = rho_estimates\n\n# Create summary table\nmc_summary = pd.DataFrame({\n    'T': T_values,\n    'Mean rho_hat': [np.mean(results_mc[T]) for T in T_values],\n    'Std rho_hat': [np.std(results_mc[T]) for T in T_values],\n    'Mean Bias': [np.mean(results_mc[T]) - rho_true for T in T_values],\n    'Theoretical Bias': [-(1 + rho_true) / (T - 1) for T in T_values],\n    'RMSE': [np.sqrt(np.mean([(r - rho_true)**2 for r in results_mc[T]])) for T in T_values],\n})\n\nprint('=== Monte Carlo Results: Nickell Bias ===')\nprint(f'True rho = {rho_true}, N = {N}, Simulations = {n_simulations}')\nprint()\nprint(mc_summary.round(4).to_string(index=False))\n\n# Save table\nmc_summary.round(4).to_csv('../outputs/tables/06_nickell_bias_monte_carlo.csv', index=False)\nprint('\\nTable saved to ../outputs/tables/06_nickell_bias_monte_carlo.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_06",
   "metadata": {},
   "source": "# ============================================================\n# Visualize the Nickell Bias: Bias vs T\n# ============================================================\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left panel: bias vs T\nax = axes[0]\nT_range = np.arange(3, 101)\ntheoretical_bias = -(1 + rho_true) / (T_range - 1)\n\nax.plot(T_range, theoretical_bias, 'b-', linewidth=2, label='Theoretical bias $-(1+\\\\rho)/(T-1)$')\nax.scatter(T_values, [np.mean(results_mc[T]) - rho_true for T in T_values],\n           color='red', s=80, zorder=5, label='Monte Carlo mean bias')\nax.axhline(y=0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\nax.set_xlabel('T (time periods)', fontsize=12)\nax.set_ylabel('Bias', fontsize=12)\nax.set_title('Nickell Bias: O(1/T) Decline', fontsize=13, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\n\n# Right panel: distribution of estimates for different T\nax = axes[1]\ncolors = sns.color_palette('husl', len(T_values))\nfor T_val, color in zip(T_values, colors):\n    ax.hist(results_mc[T_val], bins=30, alpha=0.4, color=color,\n            label=f'T={T_val}', density=True, edgecolor='white', linewidth=0.5)\nax.axvline(x=rho_true, color='black', linewidth=2, linestyle='--', label=f'True $\\\\rho$ = {rho_true}')\nax.set_xlabel('$\\\\hat{\\\\rho}_{FE}$', fontsize=12)\nax.set_ylabel('Density', fontsize=12)\nax.set_title('Distribution of FE-OLS Estimates by T', fontsize=13, fontweight='bold')\nax.legend(fontsize=9)\nax.grid(True, alpha=0.3)\n\nfig.suptitle('Nickell (1981) Bias in Dynamic Panel Models', fontsize=14, fontweight='bold', y=1.02)\nfig.tight_layout()\nfig.savefig('../outputs/figures/gmm/06_nickell_bias.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint('Key insight: As T grows, the bias shrinks toward zero.')\nprint(f'At T=5, the bias is {-(1+rho_true)/4:.3f} -- nearly half the true value!')\nprint(f'At T=100, the bias is {-(1+rho_true)/99:.4f} -- negligible.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_07",
   "metadata": {},
   "source": "### Key Takeaway: When Does Nickell Bias Matter?\n\n| T (time periods) | Bias Magnitude | Practical Implication |\n|---|---|---|\n| T < 10 | Very large | **Must use GMM** |\n| 10 < T < 20 | Moderate | GMM strongly recommended |\n| 20 < T < 30 | Small but non-trivial | Compare OLS and GMM |\n| T > 30 | Small | OLS usually acceptable |\n| T > 100 | Negligible | OLS is fine |\n\nOur dynamic panel dataset has **T = 15**, placing it squarely in the zone where GMM is strongly recommended.\n\n---\n\n## 2. Difference GMM (Arellano-Bond)\n\n### Solution Strategy\n\nArellano and Bond (1991) proposed a two-step solution:\n\n**Step 1:** Take **first differences** to eliminate the fixed effect:\n\n$$\\Delta Y_{it} = \\rho \\cdot \\Delta Y_{i,t-1} + \\Delta \\varepsilon_{it}$$\n\nNow $\\alpha_i$ is gone. But there is a new problem: $\\Delta Y_{i,t-1} = Y_{i,t-1} - Y_{i,t-2}$ is correlated with $\\Delta \\varepsilon_{it} = \\varepsilon_{it} - \\varepsilon_{i,t-1}$ because $Y_{i,t-1}$ depends on $\\varepsilon_{i,t-1}$.\n\n**Step 2:** Use **lagged levels** as instruments for the differenced equation:\n\n$$E[Y_{i,t-s} \\cdot \\Delta\\varepsilon_{it}] = 0 \\quad \\text{for } s \\geq 2$$\n\nThis is valid because $Y_{i,t-2}$ and further lags are predetermined with respect to $\\varepsilon_{it}$ and $\\varepsilon_{i,t-1}$.\n\n### Instrument Matrix\n\nThe instruments form a **block-diagonal** matrix where the number of available instruments grows with $t$:\n- At $t = 3$: only $Y_{i,1}$ is available\n- At $t = 4$: $Y_{i,1}, Y_{i,2}$ are available\n- At $t = T$: $Y_{i,1}, \\ldots, Y_{i,T-2}$ are available"
  },
  {
   "cell_type": "code",
   "id": "cell_08",
   "metadata": {},
   "source": "# ============================================================\n# Load the Dynamic Panel Data\n# ============================================================\n\ndf_dyn = generate_dynamic_panel()\n\nprint('=== Dynamic Panel Data ===')\nprint(f'Shape: {df_dyn.shape}')\nprint(f'Columns: {list(df_dyn.columns)}')\nprint(f'Countries: {df_dyn[\"country\"].nunique()}')\nprint(f'Years per country: {df_dyn[\"year\"].nunique()}')\nprint(f'\\nFirst few rows:')\ndf_dyn.head(10)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_09",
   "metadata": {},
   "source": "# ============================================================\n# Descriptive Statistics\n# ============================================================\n\nprint('=== Descriptive Statistics ===')\nprint(df_dyn[['y1', 'y2', 'y3']].describe().round(4).to_string())\n\n# Correlation matrix\nprint('\\n=== Correlation Matrix ===')\nprint(df_dyn[['y1', 'y2', 'y3']].corr().round(4).to_string())",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_10",
   "metadata": {},
   "source": "# ============================================================\n# Difference GMM Estimation\n# ============================================================\n\n# Create PanelVARData and PanelVAR objects\ndata_gmm = PanelVARData(\n    df_dyn,\n    endog_vars=['y1', 'y2'],\n    entity_col='country',\n    time_col='year',\n    lags=2\n)\n\nmodel_gmm = PanelVAR(data_gmm)\n\nprint('=== PanelVARData Properties ===')\nprint(f'K (endogenous vars): {data_gmm.K}')\nprint(f'p (lags):            {data_gmm.p}')\nprint(f'N (entities):        {data_gmm.N}')\nprint(f'n_obs (total):       {data_gmm.n_obs}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_11",
   "metadata": {},
   "source": "# ============================================================\n# Estimate OLS for comparison (baseline with Nickell bias)\n# ============================================================\n\nresults_ols = model_gmm.fit(method='ols', cov_type='clustered')\n\nprint('=== OLS Estimation (with Nickell Bias) ===')\nprint(results_ols.summary())",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_12",
   "metadata": {},
   "source": "# ============================================================\n# Difference GMM Estimation (try/except for API availability)\n# ============================================================\n# PanelVAR.fit() currently supports method='ols' only. For GMM estimation,\n# we use the lower-level panelbox.var.gmm module with try/except wrapping.\n\ncoef_names = ['y1(t-1)', 'y2(t-1)', 'y1(t-2)', 'y2(t-2)']\nresult_diff_gmm = None\n\n# First: try via PanelVAR.fit(method='gmm')\ntry:\n    result_via_fit = model_gmm.fit(\n        method='gmm',\n        gmm_type='difference',\n        max_lags_instruments=4\n    )\n    print('Difference GMM via PanelVAR.fit() succeeded!')\n    print(result_via_fit.summary())\nexcept Exception as e:\n    print(f'PanelVAR.fit(method=\"gmm\"): {e}')\n    print('  (Expected -- PanelVAR currently supports method=\"ols\" only)')\n\n# Second: try via panelbox.var.gmm module directly\nprint()\ntry:\n    result_diff_gmm = estimate_panel_var_gmm(\n        data=df_dyn,\n        var_lags=2,\n        value_cols=['y1', 'y2'],\n        entity_col='country',\n        time_col='year',\n        transform='fd',\n        gmm_step='two-step',\n        instrument_type='all',\n        max_instruments=4,\n        windmeijer_correction=True\n    )\n\n    print('=== Difference GMM Results ===')\n    print(f'Coefficients shape: {result_diff_gmm.coefficients.shape}')\n    print(f'Number of instruments: {result_diff_gmm.n_instruments}')\n    print(f'Number of observations: {result_diff_gmm.n_obs}')\n    print(f'GMM step: {result_diff_gmm.gmm_step}')\n    print(f'Transform: {result_diff_gmm.transform}')\n    print(f'Instrument type: {result_diff_gmm.instrument_type}')\n    print(f'Windmeijer corrected: {result_diff_gmm.windmeijer_corrected}')\n    print(f'\\nCoefficients:')\n    for eq in range(result_diff_gmm.coefficients.shape[1]):\n        print(f'\\n  Equation y{eq+1}:')\n        for j, name in enumerate(coef_names):\n            if j < result_diff_gmm.coefficients.shape[0]:\n                coef = result_diff_gmm.coefficients[j, eq]\n                se = result_diff_gmm.standard_errors[j, eq] if result_diff_gmm.standard_errors.ndim > 1 else result_diff_gmm.standard_errors[j]\n                print(f'    {name}: {coef:+.4f} (SE: {se:.4f})')\n\nexcept Exception as e:\n    print(f'Difference GMM via panelbox.var.gmm: {e}')\n    print()\n    print('Theoretical Result:')\n    print('Difference GMM removes the Nickell bias by using')\n    print('lagged levels as instruments for first-differenced equations.')\n    print('The estimator is consistent for N -> infinity with T fixed.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_13",
   "metadata": {},
   "source": "# ============================================================\n# Visualize: OLS vs Difference GMM Coefficients\n# ============================================================\n\n# Extract OLS coefficients\nA1_ols = results_ols.A_matrices[0]  # K x K matrix for lag 1\n\n# Extract Diff-GMM coefficients (lag 1 portion) -- with fallback\nK = 2\nif result_diff_gmm is not None:\n    A1_diff = result_diff_gmm.coefficients[:K, :K]\nelse:\n    # Fallback: use theoretical values adjusted for illustration\n    A1_diff = A1_ols * 1.0  # placeholder\n    print('Note: GMM not available; using OLS as placeholder for visualization.')\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Heatmap for OLS\nax = axes[0]\ndf_ols_coef = pd.DataFrame(A1_ols, index=['y1', 'y2'], columns=['y1(t-1)', 'y2(t-1)'])\nsns.heatmap(df_ols_coef, annot=True, fmt='.4f', cmap='RdBu_r', center=0,\n            linewidths=0.5, square=True, ax=ax, vmin=-0.6, vmax=0.6)\nax.set_title('OLS (FE) - A$_1$ Coefficients', fontsize=13, fontweight='bold')\nax.set_xlabel('Regressor (lag 1)', fontsize=11)\nax.set_ylabel('Equation', fontsize=11)\n\n# Heatmap for Diff-GMM\nax = axes[1]\ndf_diff_coef = pd.DataFrame(A1_diff, index=['y1', 'y2'], columns=['y1(t-1)', 'y2(t-1)'])\nsns.heatmap(df_diff_coef, annot=True, fmt='.4f', cmap='RdBu_r', center=0,\n            linewidths=0.5, square=True, ax=ax, vmin=-0.6, vmax=0.6)\ntitle_suffix = '' if result_diff_gmm is not None else ' (placeholder)'\nax.set_title(f'Difference GMM - A$_1$ Coefficients{title_suffix}', fontsize=13, fontweight='bold')\nax.set_xlabel('Regressor (lag 1)', fontsize=11)\nax.set_ylabel('Equation', fontsize=11)\n\nfig.suptitle('OLS vs Difference GMM: Lag 1 Coefficient Comparison',\n             fontsize=14, fontweight='bold', y=1.02)\nfig.tight_layout()\nfig.savefig('../outputs/figures/gmm/06_ols_vs_diff_gmm_coefs.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint('Interpretation:')\nprint('  - OLS (with FE) typically underestimates the diagonal coefficients (persistence)')\nprint('  - Difference GMM corrects the downward Nickell bias')\nprint('  - The true diagonal values of A_1 are 0.50 and 0.40 (from the DGP)')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_14",
   "metadata": {},
   "source": "---\n\n## 3. System GMM (Blundell-Bond)\n\n### Motivation: Weak Instruments in Difference GMM\n\nDifference GMM uses lagged **levels** $Y_{i,t-s}$ as instruments for **differenced** equations. When the autoregressive parameter $\\rho$ is close to 1 (near unit root), lagged levels are **weakly correlated** with first differences.\n\nIntuitively, if $Y_{it}$ is very persistent, then:\n- $\\Delta Y_{it} = Y_{it} - Y_{i,t-1} \\approx \\varepsilon_{it}$ (nearly unpredictable)\n- $Y_{i,t-2}$ is a poor predictor of $\\Delta Y_{i,t-1}$\n\nThis leads to **weak instruments**, resulting in:\n- Large standard errors\n- Biased estimates (toward zero in finite samples)\n- Poor finite-sample performance\n\n### The System GMM Solution\n\nBlundell and Bond (1998) proposed adding a second set of **level equations** to the system:\n\n$$Y_{it} = \\alpha_i + \\rho \\cdot Y_{i,t-1} + \\varepsilon_{it}$$\n\nwith **lagged differences** $\\Delta Y_{i,t-1}$ as instruments for the level equation.\n\nThe additional moment condition is:\n\n$$E[\\Delta Y_{i,t-1} \\cdot (\\alpha_i + \\varepsilon_{it})] = 0$$\n\nThis requires a **stationarity** assumption: the initial deviations $Y_{i,1} - \\alpha_i/(1-\\rho)$ must be uncorrelated with $\\alpha_i$.\n\n### System GMM = Differenced Equations + Level Equations\n\n| Component | Equation | Instruments |\n|---|---|---|\n| Differenced | $\\Delta Y_{it} = \\rho \\cdot \\Delta Y_{i,t-1} + \\Delta \\varepsilon_{it}$ | Lagged levels: $Y_{i,t-2}, Y_{i,t-3}, \\ldots$ |\n| Level | $Y_{it} = \\alpha_i + \\rho \\cdot Y_{i,t-1} + \\varepsilon_{it}$ | Lagged differences: $\\Delta Y_{i,t-1}$ |"
  },
  {
   "cell_type": "code",
   "id": "cell_15",
   "metadata": {},
   "source": "# ============================================================\n# System GMM Estimation (try/except)\n# ============================================================\n\n# Forward Orthogonal Deviations (FOD) transform -- preferred for system GMM\nresult_sys_gmm = None\n\ntry:\n    result_sys_gmm = estimate_panel_var_gmm(\n        data=df_dyn,\n        var_lags=2,\n        value_cols=['y1', 'y2'],\n        entity_col='country',\n        time_col='year',\n        transform='fod',\n        gmm_step='two-step',\n        instrument_type='all',\n        max_instruments=3,\n        windmeijer_correction=True\n    )\n\n    print('=== System GMM (FOD) Results ===')\n    print(f'Coefficients shape: {result_sys_gmm.coefficients.shape}')\n    print(f'Number of instruments: {result_sys_gmm.n_instruments}')\n    print(f'Number of observations: {result_sys_gmm.n_obs}')\n    print(f'Transform: {result_sys_gmm.transform}')\n    print(f'\\nCoefficients:')\n    for eq in range(result_sys_gmm.coefficients.shape[1]):\n        print(f'\\n  Equation y{eq+1}:')\n        for j, name in enumerate(coef_names):\n            if j < result_sys_gmm.coefficients.shape[0]:\n                coef = result_sys_gmm.coefficients[j, eq]\n                se = result_sys_gmm.standard_errors[j, eq] if result_sys_gmm.standard_errors.ndim > 1 else result_sys_gmm.standard_errors[j]\n                print(f'    {name}: {coef:+.4f} (SE: {se:.4f})')\n\nexcept Exception as e:\n    print(f'System GMM via panelbox.var.gmm: {e}')\n    print()\n    print('Theoretical Comparison:')\n    print('-' * 60)\n    print('System GMM adds level equations instrumented by lagged differences.')\n    print('This provides more efficient estimates, especially when rho is high.')\n    print()\n    print('Expected properties:')\n    print('  - Lower standard errors than Difference GMM')\n    print('  - More robust to weak instruments near unit root')\n    print('  - Requires mean-stationarity assumption')\n    print('  - Approximately doubles the number of moment conditions')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_16",
   "metadata": {},
   "source": "# ============================================================\n# Three-Way Comparison: OLS vs Diff-GMM vs Sys-GMM\n# ============================================================\n\n# True DGP values (from data_generators.py)\nA1_true = np.array([\n    [0.50, 0.00],\n    [0.15, 0.40]\n])\n\n# Collect lag-1 coefficient matrices (with fallbacks)\nK = 2\nA1_ols_vals = results_ols.A_matrices[0]\nA1_diff_vals = result_diff_gmm.coefficients[:K, :K] if result_diff_gmm is not None else A1_ols_vals.copy()\nA1_sys_vals = result_sys_gmm.coefficients[:K, :K] if result_sys_gmm is not None else A1_ols_vals.copy()\n\n# Build comparison table\nvar_pairs = [('y1', 'y1(t-1)'), ('y1', 'y2(t-1)'), ('y2', 'y1(t-1)'), ('y2', 'y2(t-1)')]\ncomparison_data = []\nfor idx, (eq, reg) in enumerate(var_pairs):\n    i, j = idx // K, idx % K\n    row = {\n        'Equation': eq,\n        'Regressor': reg,\n        'True': A1_true[i, j],\n        'OLS (FE)': A1_ols_vals[i, j],\n    }\n    if result_diff_gmm is not None:\n        row['Diff-GMM'] = A1_diff_vals[i, j]\n    if result_sys_gmm is not None:\n        row['Sys-GMM'] = A1_sys_vals[i, j]\n    comparison_data.append(row)\n\ndf_comparison = pd.DataFrame(comparison_data)\ndf_comparison['OLS Bias'] = df_comparison['OLS (FE)'] - df_comparison['True']\nif result_diff_gmm is not None:\n    df_comparison['Diff-GMM Bias'] = df_comparison['Diff-GMM'] - df_comparison['True']\nif result_sys_gmm is not None:\n    df_comparison['Sys-GMM Bias'] = df_comparison['Sys-GMM'] - df_comparison['True']\n\nprint('=== Estimator Comparison (Lag 1 Coefficients) ===')\nprint(df_comparison.round(4).to_string(index=False))\n\nif result_diff_gmm is None and result_sys_gmm is None:\n    print('\\nNote: GMM estimates not available. Only OLS shown.')\n    print('The OLS diagonal elements are biased downward (Nickell bias).')\n\n# Save comparison table\ndf_comparison.round(4).to_csv('../outputs/tables/06_estimator_comparison.csv', index=False)\nprint('\\nTable saved to ../outputs/tables/06_estimator_comparison.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_17",
   "metadata": {},
   "source": "# ============================================================\n# Visual Comparison: True vs Estimated Coefficients\n# ============================================================\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\ncoef_labels = [f'{eq} <- {reg}' for eq, reg in var_pairs]\nx_pos = np.arange(len(coef_labels))\n\n# Determine how many estimators to plot\nestimator_data = [\n    ('True DGP', [A1_true[i//K, i%K] for i in range(K*K)], '#2ca02c', 'darkgreen'),\n    ('OLS (FE)', [A1_ols_vals[i//K, i%K] for i in range(K*K)], '#d62728', 'darkred'),\n]\nif result_diff_gmm is not None:\n    estimator_data.append(\n        ('Diff-GMM', [A1_diff_vals[i//K, i%K] for i in range(K*K)], '#1f77b4', 'navy')\n    )\nif result_sys_gmm is not None:\n    estimator_data.append(\n        ('Sys-GMM', [A1_sys_vals[i//K, i%K] for i in range(K*K)], '#ff7f0e', 'darkorange')\n    )\n\nn_est = len(estimator_data)\nwidth = 0.8 / n_est\n\nfor idx, (label, vals, color, edge) in enumerate(estimator_data):\n    offset = (idx - (n_est - 1) / 2) * width\n    ax.bar(x_pos + offset, vals, width, label=label, color=color, alpha=0.8, edgecolor=edge)\n\nax.set_xticks(x_pos)\nax.set_xticklabels(coef_labels, fontsize=11)\nax.set_ylabel('Coefficient Value', fontsize=12)\nax.set_title('Estimator Comparison: Lag 1 Coefficients',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=10, loc='upper right')\nax.axhline(y=0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\nax.grid(True, alpha=0.3, axis='y')\n\nfig.tight_layout()\nfig.savefig('../outputs/figures/gmm/06_three_way_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint('Key observations:')\nprint('  - OLS underestimates persistence (diagonal) due to Nickell bias')\nif result_diff_gmm is not None or result_sys_gmm is not None:\n    print('  - GMM estimators correct the downward bias')\n    print('  - Sys-GMM often has lower variance than Diff-GMM')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_18",
   "metadata": {},
   "source": "---\n\n## 4. GMM Diagnostics\n\nProper GMM estimation requires careful diagnostic checking. Three tests are essential:\n\n### 4.1 Hansen J-Test (Over-identifying Restrictions)\n\nThe Hansen J-test checks whether the instruments are **jointly valid** (i.e., uncorrelated with the error term):\n\n$$H_0: E[Z_i' \\varepsilon_i] = 0 \\quad \\text{(instruments are valid)}$$\n$$J = n \\cdot \\bar{g}' \\hat{W} \\bar{g} \\sim \\chi^2(m - k)$$\n\nwhere $m$ is the number of instruments and $k$ is the number of estimated parameters.\n\n- **Reject** ($p < 0.05$): instruments may be invalid\n- **Do not reject** ($p > 0.05$): instruments appear valid\n- **Caveat:** A very high p-value ($p > 0.99$) with many instruments suggests the test has lost power\n\n### 4.2 Arellano-Bond AR(2) Test\n\nThe validity of GMM instruments depends on the **absence of second-order serial correlation** in the differenced residuals:\n\n- **AR(1)**: Expected to be significant (first differencing induces AR(1) mechanically)\n- **AR(2)**: Should NOT be significant; rejection indicates invalid instruments\n\n### 4.3 Instrument Count Rule\n\nA critical rule of thumb:\n\n$$\\text{Number of instruments} \\leq \\text{Number of entities (N)}$$\n\nToo many instruments:\n- Weaken the Hansen J-test\n- Can bias GMM toward OLS\n- Create numerical instability in the weight matrix"
  },
  {
   "cell_type": "code",
   "id": "cell_19",
   "metadata": {},
   "source": "# ============================================================\n# GMM Diagnostics: Instrument Construction (try/except)\n# ============================================================\n\nZ_instruments = None\ninstrument_meta = None\n\ntry:\n    # Build instruments for diagnostics demonstration\n    Z_instruments, instrument_meta = build_gmm_instruments(\n        data=df_dyn,\n        var_lags=2,\n        n_vars=2,\n        entity_col='country',\n        time_col='year',\n        value_cols=['y1', 'y2'],\n        instrument_type='all',\n        max_instruments=4\n    )\n\n    print('=== Instrument Information ===')\n    print(f'Instrument matrix shape: {Z_instruments.shape}')\n    print(f'Number of instruments:   {Z_instruments.shape[1]}')\n    print(f'Number of observations:  {Z_instruments.shape[0]}')\n    print(f'\\nInstrument metadata:')\n    for key, val in instrument_meta.items():\n        if key != 'observation_metadata':\n            print(f'  {key}: {val}')\n\nexcept Exception as e:\n    print(f'Instrument construction: {e}')\n    print()\n    print('Theoretical instrument structure for VAR(2) with K=2:')\n    print('  Instruments are lagged levels y_{i,t-s} for s >= p+1 = 3')\n    print('  For T=15: up to 12 lag depths per variable')\n    print('  Total standard instruments: K * (T-2)*(T-1)/2 = 2 * 13*14/2 = 182')\n    print('  With max_instruments=4: 2 * 4 = 8 instruments')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_20",
   "metadata": {},
   "source": "# ============================================================\n# Hansen J-Test Framework\n# ============================================================\n\n# The test statistic is: J = n * g_bar' * W_hat * g_bar\n# where g_bar = (1/n) * sum_i Z_i' * epsilon_hat_i\n# Under H0: J ~ chi^2(n_instruments - n_params)\n\nif Z_instruments is not None:\n    n_instruments = Z_instruments.shape[1]\nelse:\n    n_instruments = 8  # theoretical value with max_instruments=4, K=2\n\nn_params = 2 * 2 * 2  # K * K * p parameters\ndf_hansen = n_instruments - n_params\n\nprint('=== Hansen J-Test Framework ===')\nprint(f'Number of instruments (m):      {n_instruments}')\nprint(f'Number of parameters (k):       {n_params}')\nprint(f'Degrees of freedom (m - k):     {df_hansen}')\nprint(f'Number of entities (N):         {df_dyn[\"country\"].nunique()}')\nprint(f'\\nInstrument count rule check:')\nprint(f'  Instruments ({n_instruments}) <= Entities ({df_dyn[\"country\"].nunique()})? '\n      f'{\"PASS\" if n_instruments <= df_dyn[\"country\"].nunique() else \"WARNING: Too many instruments!\"}')\nprint()\nprint('Interpretation guidelines:')\nprint('  - p-value > 0.05: Do not reject H0 -> instruments appear valid')\nprint('  - p-value < 0.05: Reject H0 -> instruments may be invalid')\nprint('  - p-value > 0.99 with many instruments: Test may lack power')\n\n# Try to run the actual Hansen J-test if diagnostics are available\ntry:\n    from panelbox.var.diagnostics import GMMDiagnostics\n    if result_diff_gmm is not None and Z_instruments is not None:\n        resid = result_diff_gmm.residuals\n        n_p = result_diff_gmm.coefficients.shape[0]\n        Z_aligned = Z_instruments[:resid.shape[0], :] if Z_instruments.shape[0] >= resid.shape[0] else Z_instruments\n        if Z_aligned.shape[0] > resid.shape[0]:\n            Z_aligned = Z_aligned[:resid.shape[0], :]\n        elif Z_aligned.shape[0] < resid.shape[0]:\n            resid = resid[:Z_aligned.shape[0], :]\n\n        diagnostics = GMMDiagnostics(\n            residuals=resid,\n            instruments=Z_aligned,\n            n_params=n_p,\n            n_entities=result_diff_gmm.n_entities\n        )\n        hansen = diagnostics.hansen_j_test()\n        print(f'\\n=== Hansen J-Test Result ===')\n        print(f'  Statistic: {hansen[\"statistic\"]:.4f}')\n        print(f'  P-value:   {hansen[\"p_value\"]:.4f}')\n        print(f'  DF:        {hansen[\"df\"]}')\n        print(f'  Result:    {hansen[\"interpretation\"]}')\nexcept Exception as e:\n    print(f'\\nHansen J-test computation: {e}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_21",
   "metadata": {},
   "source": "# ============================================================\n# AR(2) Test for Serial Correlation\n# ============================================================\n\n# The Arellano-Bond AR test checks for serial correlation in the\n# differenced residuals. The key logic:\n#\n# In first differences: Delta_eps_it = eps_it - eps_{i,t-1}\n# AR(1) in differenced residuals is EXPECTED (mechanical)\n# AR(2) in differenced residuals indicates AR(1) in LEVELS -> instruments invalid\n\nprint('=== Arellano-Bond Serial Correlation Tests ===')\nprint()\nprint('Logic of the AR tests:')\nprint('  1. AR(1) in differenced residuals: EXPECTED to be significant')\nprint('     (because Delta_eps_it and Delta_eps_{i,t-1} share eps_{i,t-1})')\nprint()\nprint('  2. AR(2) in differenced residuals: SHOULD NOT be significant')\nprint('     (if significant, suggests eps_it has serial correlation in levels,')\nprint('     which would invalidate the moment conditions)')\nprint()\nprint('Decision rule:')\nprint('  - Reject AR(1)?  Expected -> OK')\nprint('  - Reject AR(2)?  Problem! -> instruments may be invalid')\nprint('  - Do not reject AR(2)?  Good -> model specification is likely correct')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_22",
   "metadata": {},
   "source": "# ============================================================\n# Difference-in-Hansen Test\n# ============================================================\n\n# The Difference-in-Hansen test (also called C-test or incremental Sargan)\n# tests the validity of a SUBSET of instruments.\n# \n# For System GMM, it tests whether the ADDITIONAL level-equation\n# instruments are valid:\n#\n# H0: The additional instruments in the system are valid\n# Test: J_system - J_difference ~ chi^2(df_system - df_difference)\n\nprint('=== Difference-in-Hansen Test ===')\nprint()\nprint('Purpose: Test validity of the additional System GMM instruments')\nprint()\nprint('Procedure:')\nprint('  1. Estimate Difference GMM -> obtain J_diff statistic')\nprint('  2. Estimate System GMM     -> obtain J_sys statistic')\nprint('  3. Compute: C = J_sys - J_diff')\nprint('  4. Under H0: C ~ chi^2(df_sys - df_diff)')\nprint()\nprint('If C is significant (p < 0.05):')\nprint('  -> The stationarity assumption for System GMM may be violated')\nprint('  -> Fall back to Difference GMM')\nprint()\nprint('If C is not significant (p > 0.05):')\nprint('  -> System GMM additional instruments appear valid')\nprint('  -> System GMM is preferred (more efficient)')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_23",
   "metadata": {},
   "source": "# ============================================================\n# Diagnostic Summary Table\n# ============================================================\n\ndiag_summary = pd.DataFrame({\n    'Test': [\n        'Hansen J-test',\n        'AR(1) test',\n        'AR(2) test',\n        'Instrument count rule',\n        'Difference-in-Hansen'\n    ],\n    'Null Hypothesis': [\n        'Instruments are valid',\n        'No AR(1) in diff. residuals',\n        'No AR(2) in diff. residuals',\n        'n_instruments <= N',\n        'Additional instruments valid'\n    ],\n    'Desired Outcome': [\n        'Do NOT reject (p > 0.05)',\n        'Reject (p < 0.05) -- expected',\n        'Do NOT reject (p > 0.05)',\n        'PASS (instruments <= entities)',\n        'Do NOT reject (p > 0.05)'\n    ],\n    'Action if Failed': [\n        'Reconsider instrument set',\n        'No action needed (expected)',\n        'Add more lags, respecify model',\n        'Use collapsed instruments',\n        'Use Diff-GMM instead of Sys-GMM'\n    ]\n})\n\nprint('=== GMM Diagnostic Decision Table ===')\nprint(diag_summary.to_string(index=False))\n\n# Save\ndiag_summary.to_csv('../outputs/tables/06_diagnostic_decision_table.csv', index=False)\n\n# Visualize as a formatted table figure\nfig, ax = plt.subplots(figsize=(14, 4))\nax.axis('off')\ntable = ax.table(\n    cellText=diag_summary.values,\n    colLabels=diag_summary.columns,\n    cellLoc='left',\n    loc='center',\n    colColours=['#f0f0f0'] * len(diag_summary.columns)\n)\ntable.auto_set_font_size(False)\ntable.set_fontsize(9)\ntable.scale(1.0, 1.6)\n\nax.set_title('GMM Diagnostic Decision Table', fontsize=14, fontweight='bold', pad=20)\nfig.tight_layout()\nfig.savefig('../outputs/figures/gmm/06_diagnostic_decision_table.png', dpi=150, bbox_inches='tight')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_24",
   "metadata": {},
   "source": "---\n\n## 5. Instrument Collapse\n\n### The Problem: Instrument Proliferation\n\nIn standard GMM, the number of instruments grows **quadratically** with T:\n\n- At $t=3$: 1 instrument per variable\n- At $t=4$: 2 instruments per variable\n- At $t=T$: $T-2$ instruments per variable\n- **Total per variable:** $\\sum_{s=1}^{T-2} s = \\frac{(T-2)(T-1)}{2}$\n\nFor our panel with $T=15$ and $K=2$ variables, this can quickly exceed the number of entities $N=100$.\n\n### Consequences of Too Many Instruments\n\n1. **Weakens the Hansen J-test:** The test loses power and almost never rejects, even when instruments are invalid\n2. **Biases GMM toward OLS:** With many instruments, GMM approaches the biased OLS estimator\n3. **Overfits endogenous variables:** Creates a spurious perfect fit\n\n### Solution: Collapsed Instruments (Roodman 2009)\n\nInstead of separate columns for each lag at each time period, **collapsed instruments** sum across time periods:\n\n$$z_{it}^{collapsed} = \\sum_{s \\geq 2} y_{i,t-s}$$\n\nThis reduces the instrument count from $O(T^2)$ to $O(T)$, dramatically limiting proliferation."
  },
  {
   "cell_type": "code",
   "id": "cell_25",
   "metadata": {},
   "source": "# ============================================================\n# Instrument Count: Standard vs Collapsed (try/except)\n# ============================================================\n\nZ_all = None\nZ_collapsed = None\nmeta_all = None\nmeta_collapsed = None\n\ntry:\n    # Standard instruments (all available lags)\n    Z_all, meta_all = build_gmm_instruments(\n        data=df_dyn,\n        var_lags=2,\n        n_vars=2,\n        entity_col='country',\n        time_col='year',\n        value_cols=['y1', 'y2'],\n        instrument_type='all',\n        max_instruments=None  # use all available\n    )\n\n    # Collapsed instruments\n    Z_collapsed, meta_collapsed = build_gmm_instruments(\n        data=df_dyn,\n        var_lags=2,\n        n_vars=2,\n        entity_col='country',\n        time_col='year',\n        value_cols=['y1', 'y2'],\n        instrument_type='collapsed',\n        max_instruments=None\n    )\n\n    print('=== Instrument Count Comparison ===')\n    print(f'Standard instruments: {Z_all.shape[1]}')\n    print(f'Collapsed instruments: {Z_collapsed.shape[1]}')\n    print(f'Reduction: {Z_all.shape[1] - Z_collapsed.shape[1]} '\n          f'({(1 - Z_collapsed.shape[1]/Z_all.shape[1])*100:.1f}%)')\n    print(f'\\nNumber of entities (N): {df_dyn[\"country\"].nunique()}')\n    print(f'\\nRule check (instruments <= N):')\n    print(f'  Standard:  {Z_all.shape[1]} <= {df_dyn[\"country\"].nunique()} -> '\n          f'{\"PASS\" if Z_all.shape[1] <= df_dyn[\"country\"].nunique() else \"FAIL\"}')\n    print(f'  Collapsed: {Z_collapsed.shape[1]} <= {df_dyn[\"country\"].nunique()} -> '\n          f'{\"PASS\" if Z_collapsed.shape[1] <= df_dyn[\"country\"].nunique() else \"FAIL\"}')\n\nexcept Exception as e:\n    print(f'Instrument construction: {e}')\n    print()\n    print('Theoretical Instrument Counts (K=2, p=2):')\n    print('-' * 50)\n    print(f'{\"T\":>5} {\"Standard\":>12} {\"Collapsed\":>12}')\n    for T_ex in [5, 10, 15, 20, 30]:\n        standard = 2 * (T_ex - 2) * (T_ex - 1) // 2\n        collapsed = 2 * (T_ex - 2)\n        print(f'{T_ex:>5} {standard:>12} {collapsed:>12}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_26",
   "metadata": {},
   "source": "# ============================================================\n# Estimate with Collapsed Instruments (try/except)\n# ============================================================\n\nresult_collapsed = None\n\ntry:\n    result_collapsed = estimate_panel_var_gmm(\n        data=df_dyn,\n        var_lags=2,\n        value_cols=['y1', 'y2'],\n        entity_col='country',\n        time_col='year',\n        transform='fod',\n        gmm_step='two-step',\n        instrument_type='collapsed',\n        windmeijer_correction=True\n    )\n\n    print('=== Collapsed Instruments GMM Results ===')\n    print(f'Number of instruments: {result_collapsed.n_instruments}')\n    print(f'Instrument type: {result_collapsed.instrument_type}')\n    print(f'\\nCoefficients:')\n    for eq in range(result_collapsed.coefficients.shape[1]):\n        print(f'\\n  Equation y{eq+1}:')\n        for j, name in enumerate(coef_names):\n            if j < result_collapsed.coefficients.shape[0]:\n                coef = result_collapsed.coefficients[j, eq]\n                se = result_collapsed.standard_errors[j, eq] if result_collapsed.standard_errors.ndim > 1 else result_collapsed.standard_errors[j]\n                print(f'    {name}: {coef:+.4f} (SE: {se:.4f})')\n\nexcept Exception as e:\n    print(f'Collapsed GMM estimation: {e}')\n    print()\n    print('Theoretical Result:')\n    print('  Collapsed instruments reduce the instrument count from O(T^2) to O(T)')\n    print('  by summing instruments across time periods within each lag depth.')\n    print('  The coefficient estimates should be similar to standard GMM,')\n    print('  but diagnostic tests retain more power.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_27",
   "metadata": {},
   "source": "# ============================================================\n# Instrument Proliferation Visualization\n# ============================================================\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Instrument count growth with T\nT_range = np.arange(5, 31)\nK_var = 2\nn_instr_standard = [K_var * (t-2)*(t-1)//2 for t in T_range]\nn_instr_collapsed = [K_var * (t-2) for t in T_range]\n\nax = axes[0]\nax.plot(T_range, n_instr_standard, 'o-', color='#d62728', linewidth=2, markersize=5,\n        label='Standard: $O(T^2)$')\nax.plot(T_range, n_instr_collapsed, 's-', color='#2ca02c', linewidth=2, markersize=5,\n        label='Collapsed: $O(T)$')\nax.axhline(y=100, color='blue', linewidth=2, linestyle='--',\n           label='N = 100 (entity count)', alpha=0.7)\nax.fill_between(T_range, 100, max(n_instr_standard), alpha=0.1, color='red')\nax.set_xlabel('T (time periods)', fontsize=12)\nax.set_ylabel('Number of Instruments', fontsize=12)\nax.set_title('Instrument Growth: Standard vs Collapsed', fontsize=13, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\n\n# Right: Coefficient comparison (with fallbacks for unavailable GMM results)\nax = axes[1]\nestimators = ['OLS (FE)']\nrho_y1_estimates = [A1_ols_vals[0, 0]]\ncolors_bar = ['#d62728']\n\nif result_diff_gmm is not None:\n    estimators.append('Diff-GMM\\n(standard)')\n    rho_y1_estimates.append(A1_diff_vals[0, 0])\n    colors_bar.append('#1f77b4')\n\nif result_sys_gmm is not None:\n    estimators.append('Sys-GMM\\n(standard)')\n    rho_y1_estimates.append(A1_sys_vals[0, 0])\n    colors_bar.append('#ff7f0e')\n\nif result_collapsed is not None:\n    estimators.append('Sys-GMM\\n(collapsed)')\n    rho_y1_estimates.append(result_collapsed.coefficients[0, 0])\n    colors_bar.append('#2ca02c')\n\nbars = ax.bar(estimators, rho_y1_estimates, color=colors_bar, alpha=0.8,\n              edgecolor='black', linewidth=0.5)\nax.axhline(y=0.50, color='black', linewidth=2, linestyle='--',\n           label='True $\\\\rho_{y1}$ = 0.50')\nax.set_ylabel('Estimated $\\\\hat{\\\\rho}_{y1}$', fontsize=12)\nax.set_title('Effect of Instrument Choice on $\\\\hat{\\\\rho}$',\n             fontsize=13, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3, axis='y')\n\nif result_collapsed is None and result_diff_gmm is None and result_sys_gmm is None:\n    ax.text(0.5, 0.5, 'GMM not available\\n(OLS only shown)',\n            transform=ax.transAxes, ha='center', va='center',\n            fontsize=11, style='italic', color='gray')\n\nfig.suptitle('Instrument Proliferation and Its Consequences',\n             fontsize=14, fontweight='bold', y=1.02)\nfig.tight_layout()\nfig.savefig('../outputs/figures/gmm/06_instrument_proliferation.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint('Key insights:')\nprint('  - Standard instruments grow quadratically with T')\nprint('  - Collapsed instruments grow linearly, staying well below N')\nif result_collapsed is not None:\n    print('  - Collapsed instruments produce similar coefficient estimates')\nprint('  - Always check: n_instruments <= N')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_28",
   "metadata": {},
   "source": "---\n\n## 6. OLS vs GMM Comparison\n\n### When to Use Which Estimator?\n\nThe choice between OLS (with fixed effects) and GMM depends primarily on the **time dimension T**.\n\nRecall:\n- **OLS bias:** $O(1/T)$ -- decreases as T grows\n- **GMM:** Consistent for fixed T, large N -- but higher variance in finite samples\n\n### Decision Rule\n\n| Condition | Recommendation | Rationale |\n|---|---|---|\n| $T < 20$ | **Use GMM** | Nickell bias is substantial |\n| $T > 30$ | **OLS usually OK** | Bias is small relative to variance |\n| $20 \\leq T \\leq 30$ | **Compare both** | Gray zone -- bias is moderate |\n\nWhen comparing, the GMM estimate should be **above** the OLS estimate (for persistence parameters), since OLS has downward bias. If OLS and GMM are close, this suggests the bias is small and OLS is adequate."
  },
  {
   "cell_type": "code",
   "id": "cell_29",
   "metadata": {},
   "source": "# ============================================================\n# Comprehensive OLS vs GMM Comparison Table\n# ============================================================\n\n# Generate macro panel for a large-T comparison\ndf_macro = generate_macro_panel(n_countries=30, n_quarters=40, seed=42)\n\nprint(f'Dynamic panel: N={df_dyn[\"country\"].nunique()}, T={df_dyn[\"year\"].nunique()} (small T)')\nprint(f'Macro panel:   N={df_macro[\"country\"].nunique()}, T={df_macro[\"quarter\"].nunique()} (large T)')\n\n# Estimate OLS on macro panel for comparison\ntry:\n    data_macro = PanelVARData(\n        df_macro,\n        endog_vars=['gdp_growth', 'inflation'],\n        entity_col='country',\n        time_col='quarter',\n        lags=2\n    )\n    model_macro = PanelVAR(data_macro)\n    results_macro_ols = model_macro.fit(method='ols', cov_type='clustered')\n\n    print('\\n=== Macro Panel OLS Results ===')\n    print(f'A1 diagonal (persistence):')\n    for i, var in enumerate(['gdp_growth', 'inflation']):\n        print(f'  {var}: {results_macro_ols.A_matrices[0][i,i]:.4f}')\nexcept Exception as e:\n    results_macro_ols = None\n    print(f'\\nMacro panel OLS estimation: {e}')\n    print('Continuing with dynamic panel comparison only.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_30",
   "metadata": {},
   "source": "# ============================================================\n# Side-by-Side Comparison Table\n# ============================================================\n\n# Dynamic panel (T=15): OLS vs GMM (with fallbacks for unavailable GMM)\ncomparison_cols = {\n    'Property': [\n        'Dataset',\n        'N (entities)',\n        'T (time periods)',\n        'Method',\n        'rho_y1 (persistence of y1)',\n        'rho_y2 (persistence of y2)',\n        'y1 <- y2(t-1) (cross-effect)',\n        'Number of instruments',\n        'Nickell bias concern',\n    ],\n    'OLS (FE)': [\n        'Dynamic panel',\n        str(df_dyn['country'].nunique()),\n        str(df_dyn['year'].nunique()),\n        'Within OLS',\n        f'{A1_ols_vals[0,0]:.4f}',\n        f'{A1_ols_vals[1,1]:.4f}',\n        f'{A1_ols_vals[1,0]:.4f}',\n        'N/A',\n        'SEVERE (T=15)',\n    ],\n}\n\nif result_diff_gmm is not None:\n    comparison_cols['Diff-GMM'] = [\n        'Dynamic panel',\n        str(df_dyn['country'].nunique()),\n        str(df_dyn['year'].nunique()),\n        'Arellano-Bond (FD)',\n        f'{A1_diff_vals[0,0]:.4f}',\n        f'{A1_diff_vals[1,1]:.4f}',\n        f'{A1_diff_vals[1,0]:.4f}',\n        str(result_diff_gmm.n_instruments),\n        'Corrected',\n    ]\n\nif result_sys_gmm is not None:\n    comparison_cols['Sys-GMM'] = [\n        'Dynamic panel',\n        str(df_dyn['country'].nunique()),\n        str(df_dyn['year'].nunique()),\n        'Blundell-Bond (FOD)',\n        f'{A1_sys_vals[0,0]:.4f}',\n        f'{A1_sys_vals[1,1]:.4f}',\n        f'{A1_sys_vals[1,0]:.4f}',\n        str(result_sys_gmm.n_instruments),\n        'Corrected',\n    ]\n\ncomparison_table = pd.DataFrame(comparison_cols)\n\nprint('=== OLS vs GMM Decision Framework ===')\nprint(comparison_table.to_string(index=False))\nprint()\nprint('Decision rules:')\nprint('  - T < 20:  Use GMM (Nickell bias is substantial)')\nprint('  - T > 30:  OLS usually OK (bias is small)')\nprint('  - 20 <= T <= 30: Compare both estimates; if close, OLS is fine')\n\nif result_diff_gmm is None and result_sys_gmm is None:\n    print()\n    print('Note: GMM estimates are not available. In practice, you would')\n    print('compare OLS against GMM results to assess Nickell bias severity.')\n\n# Save\ncomparison_table.to_csv('../outputs/tables/06_ols_vs_gmm_comparison.csv', index=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_31",
   "metadata": {},
   "source": "---\n\n## 7. Application: Shock Persistence\n\n### Half-Life of Shocks\n\nA key application of dynamic panel models is measuring **how long shocks persist**. The half-life tells us how many periods it takes for a shock to decay to half its initial impact.\n\nFor an AR(1) process with autoregressive coefficient $\\rho$:\n\n$$\\text{Half-life} = \\frac{\\ln(0.5)}{\\ln(\\rho)}$$\n\nThe half-life is increasing in $\\rho$. Since OLS underestimates $\\rho$ (Nickell bias), it also **underestimates the half-life** -- making shocks appear to dissipate faster than they actually do.\n\nThis has important policy implications:\n- If a GDP shock truly persists for 5 years but OLS says 2 years, policymakers may withdraw stimulus too early\n- If an inflation shock persists longer than estimated, central banks may under-react"
  },
  {
   "cell_type": "code",
   "id": "cell_32",
   "metadata": {},
   "source": "# ============================================================\n# Half-Life Calculation: OLS vs GMM\n# ============================================================\n\ndef compute_half_life(rho):\n    \"\"\"Compute half-life from autoregressive coefficient.\"\"\"\n    if rho <= 0 or rho >= 1:\n        return np.inf\n    return np.log(0.5) / np.log(rho)\n\n# Extract persistence parameters (diagonal of A1)\nrho_y1_ols = A1_ols_vals[0, 0]\nrho_y2_ols = A1_ols_vals[1, 1]\nrho_y1_diff = A1_diff_vals[0, 0]\nrho_y2_diff = A1_diff_vals[1, 1]\nrho_y1_sys = A1_sys_vals[0, 0]\nrho_y2_sys = A1_sys_vals[1, 1]\n\n# Build half-life table dynamically based on available results\nhl_rows = []\nfor var_name, rho_true_val, rho_ols_val, rho_diff_val, rho_sys_val in [\n    ('y1', 0.50, rho_y1_ols, rho_y1_diff, rho_y1_sys),\n    ('y2', 0.40, rho_y2_ols, rho_y2_diff, rho_y2_sys),\n]:\n    hl_rows.append({'Variable': var_name, 'Method': 'True DGP', 'rho': rho_true_val})\n    hl_rows.append({'Variable': var_name, 'Method': 'OLS (FE)', 'rho': rho_ols_val})\n    if result_diff_gmm is not None:\n        hl_rows.append({'Variable': var_name, 'Method': 'Diff-GMM', 'rho': rho_diff_val})\n    if result_sys_gmm is not None:\n        hl_rows.append({'Variable': var_name, 'Method': 'Sys-GMM', 'rho': rho_sys_val})\n\nhl_table = pd.DataFrame(hl_rows)\nhl_table['Half-Life (periods)'] = hl_table['rho'].apply(compute_half_life)\n\nprint('=== Half-Life Comparison ===')\nprint(hl_table.round(3).to_string(index=False))\n\nif result_diff_gmm is None and result_sys_gmm is None:\n    print()\n    print('Note: GMM results not available. Only True DGP and OLS shown.')\n    print('In practice, GMM estimates would show longer half-lives (higher rho)')\n    print('because OLS underestimates persistence due to Nickell bias.')\n\n# Save\nhl_table.round(3).to_csv('../outputs/tables/06_half_life_comparison.csv', index=False)\nprint('\\nTable saved to ../outputs/tables/06_half_life_comparison.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell_33",
   "metadata": {},
   "source": "# ============================================================\n# Impulse Decay Visualization\n# ============================================================\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nhorizons = np.arange(0, 21)\n\n# y1 persistence\nax = axes[0]\ny1_lines = [\n    ('True ($\\\\rho$=0.50)', 0.50, '#2ca02c', '-'),\n    ('OLS', rho_y1_ols, '#d62728', '--'),\n]\nif result_diff_gmm is not None:\n    y1_lines.append(('Diff-GMM', rho_y1_diff, '#1f77b4', '-.'))\nif result_sys_gmm is not None:\n    y1_lines.append(('Sys-GMM', rho_y1_sys, '#ff7f0e', ':'))\n\nfor label, rho, color, ls in y1_lines:\n    if 0 < rho < 1:\n        decay = rho ** horizons\n        ax.plot(horizons, decay, color=color, linewidth=2, linestyle=ls, label=label)\n\nax.axhline(y=0.5, color='gray', linewidth=0.8, linestyle='--', alpha=0.5)\nax.set_xlabel('Periods After Shock', fontsize=12)\nax.set_ylabel('Remaining Impact (fraction)', fontsize=12)\nax.set_title('Shock Decay: y1', fontsize=13, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\nax.set_ylim(-0.05, 1.05)\n\n# y2 persistence\nax = axes[1]\ny2_lines = [\n    ('True ($\\\\rho$=0.40)', 0.40, '#2ca02c', '-'),\n    ('OLS', rho_y2_ols, '#d62728', '--'),\n]\nif result_diff_gmm is not None:\n    y2_lines.append(('Diff-GMM', rho_y2_diff, '#1f77b4', '-.'))\nif result_sys_gmm is not None:\n    y2_lines.append(('Sys-GMM', rho_y2_sys, '#ff7f0e', ':'))\n\nfor label, rho, color, ls in y2_lines:\n    if 0 < rho < 1:\n        decay = rho ** horizons\n        ax.plot(horizons, decay, color=color, linewidth=2, linestyle=ls, label=label)\n\nax.axhline(y=0.5, color='gray', linewidth=0.8, linestyle='--', alpha=0.5)\nax.set_xlabel('Periods After Shock', fontsize=12)\nax.set_ylabel('Remaining Impact (fraction)', fontsize=12)\nax.set_title('Shock Decay: y2', fontsize=13, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\nax.set_ylim(-0.05, 1.05)\n\nfig.suptitle('How Estimator Bias Affects Perceived Shock Persistence',\n             fontsize=14, fontweight='bold', y=1.02)\nfig.tight_layout()\nfig.savefig('../outputs/figures/gmm/06_shock_persistence.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint('Key insight:')\nprint('  OLS makes shocks appear to die out FASTER than they actually do.')\nprint('  This is because OLS underestimates persistence (rho) due to Nickell bias.')\nif result_diff_gmm is not None or result_sys_gmm is not None:\n    print('  GMM provides more accurate half-life estimates.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_34",
   "metadata": {},
   "source": "---\n\n## 8. Summary\n\n### Key Takeaways\n\n1. **Nickell Bias** is a fundamental problem in dynamic panel models. The fixed-effects OLS estimator has a downward bias of $O(1/T)$ on the autoregressive parameter. This bias is **severe** when $T < 20$.\n\n2. **Difference GMM (Arellano-Bond)** eliminates the fixed effect through first-differencing and uses lagged levels as instruments. It is consistent for fixed T, large N.\n\n3. **System GMM (Blundell-Bond)** adds level equations with lagged-difference instruments. It is more efficient than Difference GMM when $\\rho$ is close to 1 (persistent processes), but requires a stationarity assumption.\n\n4. **GMM Diagnostics** are essential:\n   - **Hansen J-test**: instruments must be jointly valid ($p > 0.05$)\n   - **AR(2) test**: no second-order serial correlation ($p > 0.05$)\n   - **Instrument count**: must not exceed the number of entities ($m \\leq N$)\n\n5. **Instrument collapse** (Roodman 2009) reduces the instrument count from $O(T^2)$ to $O(T)$, preventing proliferation and maintaining the power of diagnostic tests.\n\n6. **Half-life analysis** demonstrates a practical consequence of estimator bias: OLS underestimates shock persistence, potentially leading to premature policy withdrawal.\n\n### Decision Framework\n\n```\nIs T < 20?\n  YES -> Use GMM (Diff-GMM or Sys-GMM)\n           Is rho near 1 (persistent)?\n             YES -> Prefer System GMM\n             NO  -> Difference GMM is fine\n           Check: n_instruments <= N?\n             NO  -> Use collapsed instruments\n  NO  -> OLS is usually adequate\n           Still compare with GMM as robustness check\n```\n\n### References\n\n- Nickell, S. (1981). Biases in dynamic models with fixed effects. *Econometrica*, 49(6), 1417-1426.\n- Arellano, M., & Bond, S. (1991). Some tests of specification for panel data. *Review of Economic Studies*, 58(2), 277-297.\n- Blundell, R., & Bond, S. (1998). Initial conditions and moment restrictions in dynamic panel data models. *Journal of Econometrics*, 87(1), 115-143.\n- Roodman, D. (2009). How to do xtabond2: An introduction to difference and system GMM in Stata. *The Stata Journal*, 9(1), 86-136.\n- Windmeijer, F. (2005). A finite sample correction for the variance of linear efficient two-step GMM estimators. *Journal of Econometrics*, 126(1), 25-51."
  },
  {
   "cell_type": "markdown",
   "id": "cell_35",
   "metadata": {},
   "source": "---\n\n## 9. Exercises\n\n### Exercise 1: Nickell Bias Monte Carlo (Easy)\n\nReproduce the Monte Carlo experiment from Section 1 with:\n- $\\rho_{true} = 0.9$ (high persistence)\n- $N = 500$\n- $T \\in \\{5, 10, 20, 50, 100\\}$\n- 100 simulations per T\n\nQuestions:\n1. How does the bias compare to $\\rho_{true} = 0.7$?\n2. Is the theoretical formula $-(1+\\rho)/(T-1)$ still accurate?\n3. At what T does the bias become less than 5% of the true value?"
  },
  {
   "cell_type": "code",
   "id": "cell_36",
   "metadata": {},
   "source": "# YOUR CODE HERE",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_37",
   "metadata": {},
   "source": "### Exercise 2: Difference GMM vs System GMM Comparison (Medium)\n\nUsing the dynamic panel data (`generate_dynamic_panel()`), estimate both Difference GMM and System GMM with:\n- `var_lags=1` (simpler model)\n- `max_instruments=3`\n- Both one-step and two-step variants\n\nCompare:\n1. Coefficient estimates across all four variants\n2. Standard errors (are two-step SEs smaller?)\n3. Number of instruments\n4. Which is closest to the true DGP values?"
  },
  {
   "cell_type": "code",
   "id": "cell_38",
   "metadata": {},
   "source": "# YOUR CODE HERE",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_39",
   "metadata": {},
   "source": "### Exercise 3: Instrument Proliferation Analysis (Medium)\n\nSystematically analyze how the number of instruments affects GMM estimates:\n\n1. Estimate System GMM with `max_instruments` ranging from 2 to 10\n2. For each, record: number of instruments, coefficient estimates, and standard errors\n3. Plot how the persistence estimate $\\hat{\\rho}_{y1}$ changes with instrument count\n4. Identify the point where adding more instruments starts pushing estimates toward OLS\n\nThis exercise demonstrates why controlling instrument count matters."
  },
  {
   "cell_type": "code",
   "id": "cell_40",
   "metadata": {},
   "source": "# YOUR CODE HERE",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell_41",
   "metadata": {},
   "source": "### Exercise 4: Forward Orthogonal Deviations (Hard)\n\nThe Forward Orthogonal Deviations (FOD) transformation is an alternative to first-differencing:\n\n$$\\tilde{Y}_{it} = \\sqrt{\\frac{T-t}{T-t+1}} \\left( Y_{it} - \\frac{1}{T-t} \\sum_{s=t+1}^{T} Y_{is} \\right)$$\n\nFOD has several advantages:\n- Preserves orthogonality of the errors (if original errors are i.i.d.)\n- Works better with unbalanced panels\n- Uses all available instruments efficiently\n\nTasks:\n1. Implement the FOD transformation manually for a single entity\n2. Compare first-difference and FOD transformed data visually\n3. Estimate GMM using both `transform='fd'` and `transform='fod'`\n4. Discuss: when does the choice of transformation matter most?"
  },
  {
   "cell_type": "code",
   "id": "cell_42",
   "metadata": {},
   "source": "# YOUR CODE HERE",
   "outputs": [],
   "execution_count": null
  }
 ]
}
