{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": "# Tutorial 02: IRF Analysis -- Solutions\n\nThis notebook contains **complete solutions** for all exercises in Tutorial 02 (Impulse Response Function Analysis).\n\nEach exercise is presented with its original description followed by a fully worked solution including code, output interpretation, and discussion.\n\n---\n\n**Exercises covered:**\n\n| Exercise | Topic | Difficulty |\n|----------|-------|------------|\n| 1 | Ordering Sensitivity | Medium |\n| 2 | Bootstrap CI Analysis | Easy |\n| 3 | Peak Effect Identification | Medium |\n| 4 | Multiplier Calculation | Hard |"
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": "---\n\n## Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n%matplotlib inline\nnp.random.seed(42)\nwarnings.filterwarnings('ignore')\n\nproject_root = Path('../../../').resolve()\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\nsys.path.insert(0, '../utils')\n\nfrom panelbox.var import PanelVARData, PanelVAR\n\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('husl')\nplt.rcParams.update({'figure.figsize': (10, 6), 'figure.dpi': 100, 'font.size': 11})"
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": "### Load Data and Estimate Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": "df = pd.read_csv('../data/macro_panel.csv')\n\n# Convert quarter strings to pandas PeriodIndex for proper temporal handling\ndf['quarter'] = pd.PeriodIndex(df['quarter'], freq='Q')\n\nendog_vars = ['gdp_growth', 'inflation', 'interest_rate']\ndata = PanelVARData(df, endog_vars=endog_vars, entity_col='country', time_col='quarter', lags=2)\nmodel = PanelVAR(data)\nresults = model.fit(method='ols', cov_type='clustered')\nprint(f\"Model estimated: {data.endog_vars}, lags={data.p}\")\nprint(f\"Entities (N): {data.N}, Variables (K): {data.K}\")\nprint(f\"Observations: {results.n_obs}\")\nprint(f\"Stability: {'STABLE' if results.is_stable() else 'UNSTABLE'}\")"
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": "---\n\n## Exercise 1: Ordering Sensitivity (Medium)\n\nUsing the macro panel data:\n\n1. Compute Cholesky IRFs with at least **3 different orderings** of the variables `['interest_rate', 'inflation', 'gdp_growth']`\n2. Plot the **GDP response to an interest rate shock** for all 3 orderings on the same chart\n3. Compare the results and discuss: How sensitive is this particular IRF to the ordering?\n4. Create a table of peak effects and horizons\n5. Discuss which ordering is most plausible economically"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 1 Solution: Define 3 different Cholesky orderings\n# Each ordering reflects a different assumption about contemporaneous causality\n\norderings = {\n    'Policy First':    ['interest_rate', 'inflation', 'gdp_growth'],\n    'Real Sector First': ['gdp_growth', 'inflation', 'interest_rate'],\n    'Prices First':    ['inflation', 'gdp_growth', 'interest_rate'],\n}\n\n# Compute Cholesky IRFs for each ordering\nirf_results = {}\nfor label, order in orderings.items():\n    irf_results[label] = results.irf(\n        periods=20,\n        method='cholesky',\n        order=order,\n    )\n    print(f\"Computed IRF with ordering '{label}': {order}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": "# Plot GDP response to interest_rate shock for all 3 orderings\nfig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n\ncolors = ['#2166ac', '#b2182b', '#4dac26']\nstyles = ['-', '--', '-.']\nhorizons = np.arange(21)\n\nfor idx, (label, irf_obj) in enumerate(irf_results.items()):\n    ax = axes[idx]\n    gdp_response = irf_obj['gdp_growth', 'interest_rate']\n    \n    ax.plot(horizons, gdp_response, color=colors[idx], linewidth=2.2,\n            marker='o', markersize=3, label=label)\n    ax.axhline(y=0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\n    ax.fill_between(horizons, 0, gdp_response, alpha=0.15, color=colors[idx])\n    \n    # Mark peak\n    peak_h = np.argmax(np.abs(gdp_response))\n    peak_val = gdp_response[peak_h]\n    ax.annotate(\n        f'Peak: {peak_val:.4f}\\nh={peak_h}',\n        xy=(peak_h, peak_val),\n        xytext=(peak_h + 3, peak_val + 0.01 * np.sign(peak_val)),\n        fontsize=9,\n        arrowprops=dict(arrowstyle='->', color='gray'),\n        bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', edgecolor='gray'),\n    )\n    \n    order = orderings[label]\n    ax.set_title(f'{label}\\n{\" -> \".join(order)}', fontsize=11, fontweight='bold')\n    ax.set_xlabel('Horizon (quarters)', fontsize=11)\n    if idx == 0:\n        ax.set_ylabel('Response of GDP Growth', fontsize=11)\n    ax.grid(True, alpha=0.3)\n\nfig.suptitle('GDP Growth Response to Interest Rate Shock\\nacross Cholesky Orderings',\n             fontsize=14, fontweight='bold', y=1.05)\nfig.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": "# Overlay all 3 orderings on a single chart for direct comparison\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor idx, (label, irf_obj) in enumerate(irf_results.items()):\n    gdp_response = irf_obj['gdp_growth', 'interest_rate']\n    ax.plot(horizons, gdp_response, color=colors[idx], linewidth=2.2,\n            linestyle=styles[idx], label=label, marker='o', markersize=3)\n\nax.axhline(y=0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\nax.set_xlabel('Horizon (quarters)', fontsize=12)\nax.set_ylabel('Response of GDP Growth', fontsize=12)\nax.set_title('Ordering Sensitivity: GDP Growth Response to Interest Rate Shock',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=10, loc='best')\nax.grid(True, alpha=0.3)\nfig.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": "# Create summary table of peak effects and horizons\nrows = []\nfor label, irf_obj in irf_results.items():\n    gdp_response = irf_obj['gdp_growth', 'interest_rate']\n    peak_h = int(np.argmax(np.abs(gdp_response)))\n    peak_val = gdp_response[peak_h]\n    impact = gdp_response[0]\n    long_run = gdp_response[-1]\n    sign = 'Positive' if peak_val > 0 else 'Negative'\n    \n    rows.append({\n        'Ordering': label,\n        'Impact (h=0)': f'{impact:.6f}',\n        'Peak Horizon': peak_h,\n        'Peak Magnitude': f'{peak_val:.6f}',\n        'Sign': sign,\n        'Long-run (h=20)': f'{long_run:.6f}',\n    })\n\ndf_summary = pd.DataFrame(rows)\nprint('\\n=== Peak Effect Summary Across Orderings ===')\nprint('=' * 80)\nprint(df_summary.to_string(index=False))\n\n# Compute max difference across orderings at each horizon\nall_responses = np.column_stack([\n    irf_results[label]['gdp_growth', 'interest_rate']\n    for label in irf_results\n])\nmax_diff_by_h = np.max(all_responses, axis=1) - np.min(all_responses, axis=1)\n\nprint(f'\\nMax difference across orderings at each horizon:')\nprint(f'  Max absolute difference: {np.max(max_diff_by_h):.6f} (at h={np.argmax(max_diff_by_h)})')\nprint(f'  Average difference:      {np.mean(max_diff_by_h):.6f}')"
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": "### Discussion: Ordering Sensitivity\n\n**Key findings:**\n\n1. **Policy First** ordering (`interest_rate -> inflation -> gdp_growth`) is the most standard in the monetary policy VAR literature (Christiano, Eichenbaum, Evans, 1999). It assumes the central bank sets rates based on last period's information, so interest rates are predetermined within the quarter.\n\n2. **Real Sector First** ordering (`gdp_growth -> inflation -> interest_rate`) assumes real output is most exogenous and that monetary policy reacts to both GDP and inflation contemporaneously. This reflects a Taylor rule perspective.\n\n3. **Prices First** ordering (`inflation -> gdp_growth -> interest_rate`) assumes price signals are the fastest to propagate.\n\n**Which is most plausible?** The \"Policy First\" ordering has the strongest theoretical justification:\n- Central banks typically make policy decisions at discrete intervals (meetings)\n- Interest rate decisions are based on information available at the time of the meeting (lagged data)\n- This makes the interest rate predetermined relative to within-quarter movements in GDP and inflation\n\n**Sensitivity:** If the responses are qualitatively similar across orderings (same sign, similar timing), the results are robust. If they differ substantially, the identification is fragile, and Generalized IRFs may be preferred."
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5",
   "metadata": {},
   "source": "---\n\n## Exercise 2: Bootstrap CI Analysis (Easy)\n\nUsing the macro panel VAR:\n\n1. Compute bootstrap IRFs with `n_bootstrap=200` and `ci_level=0.90` (instead of 0.95)\n2. Also compute with `ci_level=0.95`\n3. Plot both on the same axes\n4. Table of CI widths at selected horizons"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 2 Solution: Compute IRFs with 90% and 95% bootstrap CIs\n\n# 90% confidence interval\nirf_90 = results.irf(\n    periods=20,\n    method='cholesky',\n    order=['interest_rate', 'inflation', 'gdp_growth'],\n    ci_method='bootstrap',\n    n_bootstrap=200,\n    ci_level=0.90,\n    seed=42,\n    verbose=False,\n)\n\n# 95% confidence interval\nirf_95 = results.irf(\n    periods=20,\n    method='cholesky',\n    order=['interest_rate', 'inflation', 'gdp_growth'],\n    ci_method='bootstrap',\n    n_bootstrap=200,\n    ci_level=0.95,\n    seed=42,\n    verbose=False,\n)\n\nprint(f'90% CI computed: ci_lower shape = {irf_90.ci_lower.shape}')\nprint(f'95% CI computed: ci_upper shape = {irf_95.ci_upper.shape}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": "# Plot GDP response to interest rate shock with both 90% and 95% CIs\nresponse_var = 'gdp_growth'\nimpulse_var = 'interest_rate'\n\n# Point estimate (same for both since same method/ordering)\nirf_values = irf_95[response_var, impulse_var]\nhorizons = np.arange(len(irf_values))\n\n# Extract CI bounds using index positions\nresp_idx = irf_95.var_names.index(response_var)\nimp_idx = irf_95.var_names.index(impulse_var)\n\nlower_95 = irf_95.ci_lower[:, resp_idx, imp_idx]\nupper_95 = irf_95.ci_upper[:, resp_idx, imp_idx]\nlower_90 = irf_90.ci_lower[:, resp_idx, imp_idx]\nupper_90 = irf_90.ci_upper[:, resp_idx, imp_idx]\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# 95% CI (wider, lighter)\nax.fill_between(horizons, lower_95, upper_95,\n                alpha=0.15, color='#2166ac', label='95% Bootstrap CI')\nax.plot(horizons, lower_95, color='#2166ac', linewidth=0.7, linestyle=':', alpha=0.5)\nax.plot(horizons, upper_95, color='#2166ac', linewidth=0.7, linestyle=':', alpha=0.5)\n\n# 90% CI (narrower, darker)\nax.fill_between(horizons, lower_90, upper_90,\n                alpha=0.25, color='#b2182b', label='90% Bootstrap CI')\nax.plot(horizons, lower_90, color='#b2182b', linewidth=0.7, linestyle='--', alpha=0.5)\nax.plot(horizons, upper_90, color='#b2182b', linewidth=0.7, linestyle='--', alpha=0.5)\n\n# Point estimate\nax.plot(horizons, irf_values, color='black', linewidth=2.2,\n        label='Point Estimate', marker='o', markersize=3)\n\n# Zero line\nax.axhline(y=0, color='gray', linewidth=0.8, linestyle='--', alpha=0.5)\n\nax.set_xlabel('Horizon (quarters)', fontsize=12)\nax.set_ylabel(f'Response of {response_var}', fontsize=12)\nax.set_title(f'{response_var} Response to {impulse_var} Shock\\n90% vs 95% Bootstrap Confidence Intervals (B=200)',\n             fontsize=14, fontweight='bold')\nax.legend(fontsize=10, loc='best')\nax.grid(True, alpha=0.3)\nfig.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": "# Table of CI widths at selected horizons\nselected_horizons = [0, 1, 2, 4, 8, 12, 16, 20]\n\nrows = []\nfor h in selected_horizons:\n    width_90 = upper_90[h] - lower_90[h]\n    width_95 = upper_95[h] - lower_95[h]\n    \n    # Check significance\n    sig_90 = (upper_90[h] < 0) or (lower_90[h] > 0)\n    sig_95 = (upper_95[h] < 0) or (lower_95[h] > 0)\n    \n    rows.append({\n        'Horizon': h,\n        'Point Est.': f'{irf_values[h]:.4f}',\n        '90% CI Width': f'{width_90:.4f}',\n        '95% CI Width': f'{width_95:.4f}',\n        'Ratio (95/90)': f'{width_95 / width_90:.3f}' if width_90 > 0 else 'N/A',\n        'Sig. 90%': 'YES' if sig_90 else 'no',\n        'Sig. 95%': 'YES' if sig_95 else 'no',\n    })\n\ndf_ci = pd.DataFrame(rows)\nprint('=== CI Width Comparison: 90% vs 95% ===')\nprint('=' * 80)\nprint(df_ci.to_string(index=False))\n\nprint('\\n--- Interpretation ---')\nprint('The 95% CI is wider than the 90% CI at every horizon.')\nprint('If a response is significant at 95%, it is automatically significant at 90%.')\nprint('The 90% CI may reveal additional horizons where the response is significant.')"
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9",
   "metadata": {},
   "source": "### Discussion: Bootstrap CI Analysis\n\n**Key findings:**\n\n1. The 95% CI is always wider than the 90% CI (as expected -- higher confidence requires a wider interval).\n2. The ratio of widths (95% / 90%) is approximately constant across horizons, reflecting the distributional properties of the bootstrap.\n3. Changing from 95% to 90% confidence may reveal additional horizons where the response is statistically significant (the CI excludes zero).\n4. CI widths generally increase with the horizon, reflecting the accumulation of estimation uncertainty over time.\n\n**Practical guidance:** Use 95% CIs for standard inference and 90% CIs for a less conservative assessment. Always report the confidence level used."
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": "---\n\n## Exercise 3: Peak Effect Identification (Medium)\n\nFor all 9 impulse-response pairs (3 shocks x 3 responses) in the macro VAR:\n\n1. Compute Cholesky IRFs with the standard ordering `['interest_rate', 'inflation', 'gdp_growth']`\n2. For each pair, identify the horizon of the peak (absolute) effect, the magnitude, and the sign\n3. Present the results in a summary table (pandas DataFrame)\n4. Which shock has the largest absolute peak effect on GDP growth? At what horizon?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 3 Solution: Peak effect identification for all 9 pairs\n\n# Compute Cholesky IRF with standard ordering\nirf_standard = results.irf(\n    periods=20,\n    method='cholesky',\n    order=['interest_rate', 'inflation', 'gdp_growth'],\n)\n\nvar_names = irf_standard.var_names\nprint(f'Variables: {var_names}')\nprint(f'Number of IRF pairs: {len(var_names) ** 2}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": "# Loop over all impulse-response pairs and identify peak effects\nrows = []\n\nfor impulse_var in var_names:\n    for response_var in var_names:\n        irf_vals = irf_standard[response_var, impulse_var]\n        \n        # Find peak absolute effect\n        peak_h = int(np.argmax(np.abs(irf_vals)))\n        peak_val = irf_vals[peak_h]\n        sign = 'Positive' if peak_val > 0 else 'Negative'\n        \n        # Also compute impact and long-run\n        impact = irf_vals[0]\n        long_run = irf_vals[-1]\n        \n        rows.append({\n            'Impulse': impulse_var,\n            'Response': response_var,\n            'Peak Horizon': peak_h,\n            'Peak Magnitude': peak_val,\n            'Abs. Peak': abs(peak_val),\n            'Sign': sign,\n            'Impact (h=0)': impact,\n            'Long-run (h=20)': long_run,\n        })\n\ndf_peaks = pd.DataFrame(rows)\n\n# Display formatted table\nprint('=== Peak Effect Summary for All Impulse-Response Pairs ===')\nprint('=' * 95)\ndf_display = df_peaks[['Impulse', 'Response', 'Peak Horizon', 'Peak Magnitude', 'Sign', 'Impact (h=0)', 'Long-run (h=20)']].copy()\ndf_display['Peak Magnitude'] = df_display['Peak Magnitude'].apply(lambda x: f'{x:.6f}')\ndf_display['Impact (h=0)'] = df_display['Impact (h=0)'].apply(lambda x: f'{x:.6f}')\ndf_display['Long-run (h=20)'] = df_display['Long-run (h=20)'].apply(lambda x: f'{x:.6f}')\nprint(df_display.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": "# Identify the largest absolute peak effect on GDP growth\ngdp_rows = df_peaks[df_peaks['Response'] == 'gdp_growth'].copy()\ngdp_rows_sorted = gdp_rows.sort_values('Abs. Peak', ascending=False)\n\nprint('\\n=== Shocks Ranked by Absolute Peak Effect on GDP Growth ===')\nprint('=' * 70)\nfor _, row in gdp_rows_sorted.iterrows():\n    print(f\"  {row['Impulse']:>15s} shock -> GDP Growth: \"\n          f\"peak = {row['Peak Magnitude']:.6f} at h={row['Peak Horizon']} ({row['Sign']})\")\n\ntop = gdp_rows_sorted.iloc[0]\nprint(f\"\\n>>> Largest absolute peak on GDP growth: \"\n      f\"{top['Impulse']} shock with magnitude {top['Peak Magnitude']:.6f} at h={top['Peak Horizon']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": "# Heatmap visualization of peak magnitudes\npivot_peak = df_peaks.pivot(index='Response', columns='Impulse', values='Peak Magnitude')\npivot_horizon = df_peaks.pivot(index='Response', columns='Impulse', values='Peak Horizon')\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Peak magnitude heatmap\nsns.heatmap(pivot_peak, annot=True, fmt='.4f', cmap='RdBu_r', center=0,\n            linewidths=0.5, ax=axes[0], cbar_kws={'label': 'Magnitude'})\naxes[0].set_title('Peak IRF Magnitude', fontsize=13, fontweight='bold')\naxes[0].set_xlabel('Impulse Variable', fontsize=11)\naxes[0].set_ylabel('Response Variable', fontsize=11)\n\n# Peak horizon heatmap\nsns.heatmap(pivot_horizon, annot=True, fmt='.0f', cmap='YlOrRd',\n            linewidths=0.5, ax=axes[1], cbar_kws={'label': 'Horizon'})\naxes[1].set_title('Horizon of Peak Effect', fontsize=13, fontweight='bold')\naxes[1].set_xlabel('Impulse Variable', fontsize=11)\naxes[1].set_ylabel('Response Variable', fontsize=11)\n\nfig.suptitle('Peak Effect Analysis: All Impulse-Response Pairs',\n             fontsize=14, fontweight='bold', y=1.03)\nfig.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "d2e3f4a5",
   "metadata": {},
   "source": "### Discussion: Peak Effect Identification\n\n**Summary of findings:**\n\n- Own shocks (diagonal elements) typically have the largest peak effects and occur at h=0 (impact).\n- Cross-variable effects tend to peak at later horizons (h > 0), reflecting transmission lags.\n- The peak horizon for GDP growth's response to interest rate shocks reflects the monetary policy transmission lag.\n- Negative cross-effects (e.g., interest rate shock reducing GDP growth) are consistent with standard macroeconomic theory."
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6",
   "metadata": {},
   "source": "---\n\n## Exercise 4: Multiplier Calculation (Hard)\n\nUsing both level and cumulative IRFs from the macro VAR:\n\n1. Compute the short-run multiplier (h=4), medium-run (h=8), and long-run (h=20) multipliers\n2. Verify the cumulative relationship: `cumulative_irf[h] = sum(level_irf[0:h+1])`\n3. Create a formatted multiplier table\n4. Plot a \"multiplier timeline\" showing the evolution of the cumulative multiplier"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 4 Solution: Compute level and cumulative IRFs\n\ncholesky_order = ['interest_rate', 'inflation', 'gdp_growth']\n\n# Level IRF\nirf_level = results.irf(\n    periods=20,\n    method='cholesky',\n    order=cholesky_order,\n    cumulative=False,\n)\n\n# Cumulative IRF\nirf_cumulative = results.irf(\n    periods=20,\n    method='cholesky',\n    order=cholesky_order,\n    cumulative=True,\n)\n\nprint(f'Level IRF computed: cumulative={irf_level.cumulative}')\nprint(f'Cumulative IRF computed: cumulative={irf_cumulative.cumulative}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6c7d8",
   "metadata": {},
   "outputs": [],
   "source": "# Extract multipliers for all variables in response to interest rate shock\nimpulse = 'interest_rate'\nresponse_vars = irf_level.var_names\nmultiplier_horizons = {'Short-run (h=4)': 4, 'Medium-run (h=8)': 8, 'Long-run (h=20)': 20}\n\nprint('=== Dynamic Multipliers: Response to Interest Rate Shock ===')\nprint('=' * 75)\n\nrows = []\nfor resp_var in response_vars:\n    cum_vals = irf_cumulative[resp_var, impulse]\n    level_vals = irf_level[resp_var, impulse]\n    \n    row = {'Response Variable': resp_var}\n    for label, h in multiplier_horizons.items():\n        row[label] = cum_vals[h]\n    rows.append(row)\n\ndf_multipliers = pd.DataFrame(rows)\nprint(df_multipliers.to_string(index=False, float_format='%.6f'))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7d8e9",
   "metadata": {},
   "outputs": [],
   "source": "# Verify: cumulative_irf[h] == sum(level_irf[0:h+1]) for all horizons\nprint('=== Verification: Cumulative IRF = Sum of Level IRFs ===')\nprint('=' * 70)\n\nmax_errors = []\nfor resp_var in response_vars:\n    level_vals = irf_level[resp_var, impulse]\n    cum_vals = irf_cumulative[resp_var, impulse]\n    \n    # Compute manual cumulative sum\n    manual_cumsum = np.cumsum(level_vals)\n    \n    # Check difference\n    max_err = np.max(np.abs(cum_vals - manual_cumsum))\n    max_errors.append(max_err)\n    \n    print(f'\\n  {resp_var} <- {impulse}:')\n    print(f'    Max absolute error: {max_err:.2e}')\n    \n    # Show a few horizons for detailed verification\n    for h in [4, 8, 20]:\n        print(f'    h={h:2d}: cumulative_irf={cum_vals[h]:.8f}, '\n              f'sum(level[0:{h+1}])={manual_cumsum[h]:.8f}, '\n              f'diff={abs(cum_vals[h] - manual_cumsum[h]):.2e}')\n\nprint(f'\\n>>> All verifications passed: max error = {max(max_errors):.2e}')\nprint('    The cumulative IRF is exactly the running sum of level IRFs.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8e9f0",
   "metadata": {},
   "outputs": [],
   "source": "# Detailed multiplier table with all horizons\nprint('\\n=== Complete Multiplier Table: Interest Rate Shock ===')\nprint('=' * 80)\n\nprint(f'{\"Horizon\":>8} {\"Type\":>14}', end='')\nfor var in response_vars:\n    print(f' {var:>16}', end='')\nprint()\nprint('-' * 80)\n\nfor h in [0, 1, 2, 4, 8, 12, 16, 20]:\n    # Level (period-by-period)\n    print(f'{h:>8} {\"Level\":>14}', end='')\n    for var in response_vars:\n        val = irf_level[var, impulse][h]\n        print(f' {val:>16.6f}', end='')\n    print()\n    \n    # Cumulative (multiplier)\n    print(f'{\"\":>8} {\"Cumulative\":>14}', end='')\n    for var in response_vars:\n        val = irf_cumulative[var, impulse][h]\n        print(f' {val:>16.6f}', end='')\n    print()\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9f0a1",
   "metadata": {},
   "outputs": [],
   "source": "# Plot: Multiplier timeline -- cumulative IRF evolution from h=0 to h=20\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nhorizons = np.arange(21)\ncolors_resp = ['#2166ac', '#d6604d', '#4dac26']\n\nfor idx, resp_var in enumerate(response_vars):\n    ax = axes[idx]\n    \n    # Level IRF (bars)\n    level_vals = irf_level[resp_var, impulse]\n    ax.bar(horizons, level_vals, alpha=0.3, color=colors_resp[idx],\n           label='Level IRF (period effect)', width=0.8)\n    \n    # Cumulative IRF (line)\n    cum_vals = irf_cumulative[resp_var, impulse]\n    ax.plot(horizons, cum_vals, color=colors_resp[idx], linewidth=2.5,\n            marker='s', markersize=4, label='Cumulative IRF (multiplier)')\n    \n    # Annotate key multiplier horizons\n    for h_label, h_val in [(4, 4), (8, 8), (20, 20)]:\n        ax.annotate(\n            f'h={h_val}: {cum_vals[h_val]:.4f}',\n            xy=(h_val, cum_vals[h_val]),\n            xytext=(h_val + 1.5, cum_vals[h_val] + 0.02 * np.sign(cum_vals[h_val]) if cum_vals[h_val] != 0 else 0.02),\n            fontsize=8,\n            arrowprops=dict(arrowstyle='->', color='gray', lw=0.8),\n            bbox=dict(boxstyle='round,pad=0.2', facecolor='lightyellow', edgecolor='gray', alpha=0.8),\n        )\n    \n    ax.axhline(y=0, color='black', linewidth=0.8, linestyle='--', alpha=0.5)\n    ax.set_title(f'Response: {resp_var}', fontsize=12, fontweight='bold')\n    ax.set_xlabel('Horizon (quarters)', fontsize=11)\n    if idx == 0:\n        ax.set_ylabel('IRF Value', fontsize=11)\n    ax.legend(fontsize=8, loc='best')\n    ax.grid(True, alpha=0.3)\n\nfig.suptitle('Multiplier Timeline: Cumulative Response to Interest Rate Shock',\n             fontsize=14, fontweight='bold', y=1.03)\nfig.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0a1b2",
   "metadata": {},
   "outputs": [],
   "source": "# Summary: Short-run, medium-run, and long-run multipliers as a clean table\nprint('\\n=== Final Multiplier Summary ===')\nprint('=' * 65)\nprint(f'{\"Response\":>18} {\"Short-run (h=4)\":>16} {\"Medium-run (h=8)\":>17} {\"Long-run (h=20)\":>16}')\nprint('-' * 65)\n\nfor resp_var in response_vars:\n    cum_vals = irf_cumulative[resp_var, impulse]\n    print(f'{resp_var:>18} {cum_vals[4]:>16.6f} {cum_vals[8]:>17.6f} {cum_vals[20]:>16.6f}')\n\nprint('\\n--- Economic Interpretation ---')\nprint('Since variables are in growth rates (percentage points per quarter):')\nprint('  - The LEVEL IRF at horizon h gives the effect on the growth rate at quarter h.')\nprint('  - The CUMULATIVE IRF at horizon h gives the total accumulated effect on the')\nprint('    level of the variable from period 0 through h.')\nprint('  - For GDP growth: a cumulative multiplier of X at h=8 means that GDP is')\nprint('    X percentage points higher (or lower) after 2 years compared to the')\nprint('    no-shock scenario.')\nprint('  - The long-run multiplier (h=20) approximates the permanent level shift.')"
  },
  {
   "cell_type": "markdown",
   "id": "f0a1b2c3",
   "metadata": {},
   "source": "### Discussion: Multiplier Calculation\n\n**Key findings:**\n\n1. **Cumulative verification**: The cumulative IRF at each horizon exactly equals the sum of level IRFs from 0 to h, confirming the mathematical relationship.\n\n2. **Short-run multiplier (h=4)**: Captures the first-year accumulated effect. For GDP growth, this shows how much output is affected in the first year after a monetary policy shock.\n\n3. **Medium-run multiplier (h=8)**: The two-year accumulated effect. This is often the most policy-relevant horizon for monetary policy analysis.\n\n4. **Long-run multiplier (h=20)**: Approximates the total permanent effect. In a stationary VAR, this converges as h increases.\n\n5. **Economic significance**: If the cumulative GDP response to a 1 s.d. interest rate shock is, say, -0.15 at h=8, this means that the **level** of GDP is 0.15 percentage points lower after 2 years. For a country with 2% trend growth, this represents roughly a 7.5% reduction relative to the growth rate, which is economically significant.\n\n---\n\n## End of Solutions\n\nThese solutions demonstrate the key techniques for IRF analysis:\n- Testing robustness to Cholesky ordering\n- Comparing bootstrap CI widths across confidence levels\n- Systematically identifying peak effects across all variable pairs\n- Computing and verifying dynamic multipliers from cumulative IRFs"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
