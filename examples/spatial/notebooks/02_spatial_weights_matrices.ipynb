{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Weight Matrices (W): The Foundation of Spatial Econometrics\n",
    "\n",
    "**Notebook 02 | Tutorial Series: Spatial Econometrics with PanelBox**\n",
    "\n",
    "**Author**: PanelBox Development Team  \n",
    "**Level**: Beginner to Intermediate  \n",
    "**Duration**: 90-120 minutes  \n",
    "**Prerequisites**: Notebook 01 (Introduction to Spatial Econometrics)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook provides comprehensive understanding of **spatial weight matrices (W)** - the foundational tool of spatial econometrics. You will learn to:\n",
    "\n",
    "1. Construct various types of W matrices (contiguity, distance, k-NN)\n",
    "2. Understand their mathematical properties and implications\n",
    "3. Perform and interpret row normalization\n",
    "4. Assess sensitivity of results to different W specifications\n",
    "5. Choose appropriate W for your research question\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: The Role of W in Spatial Models\n",
    "\n",
    "### Why W is the Foundation of Spatial Econometrics\n",
    "\n",
    "Recall from Notebook 01 that **spatial weight matrices (W)** encode neighborhood relationships between spatial units. W is central to all spatial econometric models:\n",
    "\n",
    "**Key Spatial Models**:\n",
    "- **SAR (Spatial Lag)**: $y = \\rho Wy + X\\beta + \\varepsilon$\n",
    "- **SEM (Spatial Error)**: $y = X\\beta + u$, where $u = \\lambda Wu + \\varepsilon$\n",
    "- **SDM (Spatial Durbin)**: $y = \\rho Wy + X\\beta + WX\\theta + \\varepsilon$\n",
    "\n",
    "### Critical Properties of W\n",
    "\n",
    "1. **W is specified, not estimated**: You choose the structure\n",
    "2. **W encodes theoretical assumptions** about spatial relationships\n",
    "3. **Different W specifications → different interpretations**\n",
    "4. **Diagonal elements $w_{ii} = 0$**: No self-influence\n",
    "\n",
    "> **Key Insight**: \"W is the most important modeling choice in spatial econometrics. Everything else follows from how we define neighbors.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial import distance_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySAL libraries\n",
    "import libpysal\n",
    "from libpysal import weights\n",
    "from libpysal.weights import Queen, Rook, KNN, DistanceBand\n",
    "import esda\n",
    "from esda import Moran\n",
    "\n",
    "# Set paths\n",
    "panelbox_path = Path(\"/home/guhaase/projetos/panelbox\")\n",
    "sys.path.insert(0, str(panelbox_path))\n",
    "data_path = Path(\"../data/us_counties/\")\n",
    "output_path = Path(\"../outputs/figures/\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "print(\"✓ Libraries loaded successfully\")\n",
    "print(f\"✓ Working directory: {Path.cwd()}\")\n",
    "print(f\"✓ Data path: {data_path}\")\n",
    "print(f\"✓ Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Contiguity-Based W Matrices\n",
    "\n",
    "### Neighbors by Shared Borders: Queen and Rook\n",
    "\n",
    "Contiguity-based weights define neighbors by geographic adjacency.\n",
    "\n",
    "#### Queen Contiguity\n",
    "- **Definition**: Units are neighbors if they share a border OR vertex\n",
    "- **Analogy**: Chess queen movement (any direction including diagonal)\n",
    "- **Use case**: General geographic analysis\n",
    "\n",
    "#### Rook Contiguity\n",
    "- **Definition**: Units are neighbors if they share a border (not just vertex)\n",
    "- **Analogy**: Chess rook movement (only cardinal directions)\n",
    "- **Use case**: When diagonal connections are less relevant\n",
    "\n",
    "**Visual Comparison**:\n",
    "```\n",
    "Queen:          Rook:\n",
    "  X X X           . X .\n",
    "  X O X           X O X\n",
    "  X X X           . X .\n",
    "\n",
    "O = focal unit\n",
    "X = neighbors\n",
    ". = not neighbors\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load US counties data\n",
    "print(\"Loading US counties data...\")\n",
    "\n",
    "# Check if data exists\n",
    "if not data_path.exists():\n",
    "    print(f\"⚠ Data path does not exist: {data_path}\")\n",
    "    print(\"Please ensure the data files are in the correct location.\")\n",
    "else:\n",
    "    print(f\"✓ Data directory found: {data_path}\")\n",
    "    \n",
    "# For this example, we'll create synthetic county data if real data is unavailable\n",
    "# In production, replace with actual data loading\n",
    "\n",
    "try:\n",
    "    counties_geo = gpd.read_file(data_path / \"us_counties.shp\")\n",
    "    counties_data = pd.read_csv(data_path / \"us_counties.csv\")\n",
    "    counties = counties_geo.merge(counties_data, on='county_id')\n",
    "    print(f\"✓ Loaded {len(counties)} counties\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠ Real data not found. Creating synthetic example...\")\n",
    "    # Create synthetic grid of counties for demonstration\n",
    "    from shapely.geometry import Polygon\n",
    "    \n",
    "    grid_size = 10\n",
    "    polygons = []\n",
    "    county_ids = []\n",
    "    incomes = []\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            x0, y0 = i, j\n",
    "            poly = Polygon([(x0, y0), (x0+1, y0), (x0+1, y0+1), (x0, y0+1)])\n",
    "            polygons.append(poly)\n",
    "            county_ids.append(f\"C{i:02d}{j:02d}\")\n",
    "            incomes.append(np.random.lognormal(10, 0.3))\n",
    "    \n",
    "    counties = gpd.GeoDataFrame({\n",
    "        'county_id': county_ids,\n",
    "        'income_percapita': incomes,\n",
    "        'geometry': polygons\n",
    "    }, crs=\"EPSG:4326\")\n",
    "    \n",
    "    print(f\"✓ Created synthetic dataset with {len(counties)} units\")\n",
    "\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  Shape: {counties.shape}\")\n",
    "print(f\"  CRS: {counties.crs}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(counties.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Queen contiguity weight matrix\n",
    "print(\"=\"*60)\n",
    "print(\"QUEEN CONTIGUITY WEIGHT MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "w_queen = Queen.from_dataframe(counties)\n",
    "\n",
    "print(f\"Number of units: {w_queen.n}\")\n",
    "print(f\"Number of non-zero weights: {w_queen.s0:.0f}\")\n",
    "print(f\"Average neighbors: {w_queen.mean_neighbors:.2f}\")\n",
    "print(f\"Min neighbors: {w_queen.min_neighbors}\")\n",
    "print(f\"Max neighbors: {w_queen.max_neighbors}\")\n",
    "print(f\"Percent nonzero: {100 * w_queen.pct_nonzero:.2f}%\")\n",
    "print(f\"Islands (units with no neighbors): {len(w_queen.islands)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Rook contiguity weight matrix\n",
    "print(\"\\nROOK CONTIGUITY WEIGHT MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "w_rook = Rook.from_dataframe(counties)\n",
    "\n",
    "print(f\"Number of units: {w_rook.n}\")\n",
    "print(f\"Number of non-zero weights: {w_rook.s0:.0f}\")\n",
    "print(f\"Average neighbors: {w_rook.mean_neighbors:.2f}\")\n",
    "print(f\"Min neighbors: {w_rook.min_neighbors}\")\n",
    "print(f\"Max neighbors: {w_rook.max_neighbors}\")\n",
    "print(f\"Percent nonzero: {100 * w_rook.pct_nonzero:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare\n",
    "diff = w_queen.mean_neighbors - w_rook.mean_neighbors\n",
    "print(f\"\\nDifference in average neighbors: {diff:.2f}\")\n",
    "print(\"→ Queen typically has more neighbors (includes diagonal connections)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize neighbor distribution: Queen vs Rook\n",
    "queen_neighbors = [len(w_queen.neighbors[i]) for i in w_queen.neighbors]\n",
    "rook_neighbors = [len(w_rook.neighbors[i]) for i in w_rook.neighbors]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Queen\n",
    "axes[0].hist(queen_neighbors, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].set_title('Queen Contiguity', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Number of Neighbors', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].axvline(np.mean(queen_neighbors), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {np.mean(queen_neighbors):.2f}')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rook\n",
    "axes[1].hist(rook_neighbors, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].set_title('Rook Contiguity', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Neighbors', fontsize=12)\n",
    "axes[1].axvline(np.mean(rook_neighbors), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {np.mean(rook_neighbors):.2f}')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'nb02_queen_vs_rook.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"→ Queen contiguity typically produces slightly more neighbors per unit\")\n",
    "print(\"→ Both distributions show spatial structure properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize connections on map\n",
    "# Select one unit to highlight its neighbors\n",
    "example_idx = min(len(counties) // 2, 50)  # Choose a central unit\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Queen neighbors\n",
    "ax = axes[0]\n",
    "counties.plot(ax=ax, facecolor='lightgray', edgecolor='black', linewidth=0.5)\n",
    "counties.iloc[[example_idx]].plot(ax=ax, facecolor='red', edgecolor='black', linewidth=2)\n",
    "\n",
    "neighbor_indices_queen = list(w_queen.neighbors[example_idx])\n",
    "if neighbor_indices_queen:\n",
    "    counties.iloc[neighbor_indices_queen].plot(ax=ax, facecolor='yellow',\n",
    "                                                edgecolor='black', linewidth=1)\n",
    "ax.set_title(f'Queen Neighbors (n={len(neighbor_indices_queen)})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "# Rook neighbors\n",
    "ax = axes[1]\n",
    "counties.plot(ax=ax, facecolor='lightgray', edgecolor='black', linewidth=0.5)\n",
    "counties.iloc[[example_idx]].plot(ax=ax, facecolor='red', edgecolor='black', linewidth=2)\n",
    "\n",
    "neighbor_indices_rook = list(w_rook.neighbors[example_idx])\n",
    "if neighbor_indices_rook:\n",
    "    counties.iloc[neighbor_indices_rook].plot(ax=ax, facecolor='yellow',\n",
    "                                               edgecolor='black', linewidth=1)\n",
    "ax.set_title(f'Rook Neighbors (n={len(neighbor_indices_rook)})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'nb02_queen_rook_example.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Legend:\")\n",
    "print(\"  Red = Focal unit\")\n",
    "print(\"  Yellow = Neighbors\")\n",
    "print(\"  Gray = Non-neighbors\")\n",
    "print(f\"\\n→ Queen includes {len(neighbor_indices_queen) - len(neighbor_indices_rook)} additional diagonal neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Distance-Based W Matrices\n",
    "\n",
    "### Defining Neighbors by Geographic Distance\n",
    "\n",
    "Distance-based weights are useful when:\n",
    "- Contiguity is too restrictive (islands, irregular shapes)\n",
    "- You want to model distance decay\n",
    "- Point data (no polygons)\n",
    "\n",
    "#### Distance Band (Threshold Distance)\n",
    "- **Definition**: Units are neighbors if within distance $d$\n",
    "- **Binary weights**: $w_{ij} = 1$ if $d_{ij} < d^*$, else $w_{ij} = 0$\n",
    "- **Challenge**: Choosing appropriate threshold $d^*$\n",
    "\n",
    "#### Inverse Distance Weighting\n",
    "- **Definition**: $w_{ij} = 1/d_{ij}^{\\alpha}$ if within threshold, 0 otherwise\n",
    "- **Rationale**: Closer neighbors have stronger influence\n",
    "- **Variations**: $1/d_{ij}$, $1/d_{ij}^2$, $\\exp(-d_{ij})$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract centroids for distance calculations\n",
    "centroids = counties.geometry.centroid\n",
    "coords = np.array([[pt.x, pt.y] for pt in centroids])\n",
    "\n",
    "print(f\"Extracted {len(coords)} centroids\")\n",
    "print(f\"Coordinate range:\")\n",
    "print(f\"  X: [{coords[:, 0].min():.4f}, {coords[:, 0].max():.4f}]\")\n",
    "print(f\"  Y: [{coords[:, 1].min():.4f}, {coords[:, 1].max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise distances\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "print(\"Computing pairwise distance matrix...\")\n",
    "dist_matrix = cdist(coords, coords, metric='euclidean')\n",
    "\n",
    "print(f\"Distance matrix shape: {dist_matrix.shape}\")\n",
    "print(f\"Distance range: [{dist_matrix[dist_matrix > 0].min():.4f}, {dist_matrix.max():.4f}]\")\n",
    "\n",
    "# Determine appropriate threshold\n",
    "# Strategy: Ensure all units have at least one neighbor\n",
    "min_max_dist = []\n",
    "for i in range(len(dist_matrix)):\n",
    "    # Exclude self (distance = 0)\n",
    "    non_zero_dists = dist_matrix[i][dist_matrix[i] > 0]\n",
    "    if len(non_zero_dists) > 0:\n",
    "        min_max_dist.append(non_zero_dists.min())\n",
    "\n",
    "# Use 75th percentile to ensure most units have neighbors\n",
    "threshold = np.percentile(min_max_dist, 75)\n",
    "\n",
    "print(f\"\\nDistance threshold determination:\")\n",
    "print(f\"  Minimum nearest-neighbor distance: {np.min(min_max_dist):.4f}\")\n",
    "print(f\"  25th percentile: {np.percentile(min_max_dist, 25):.4f}\")\n",
    "print(f\"  50th percentile (median): {np.percentile(min_max_dist, 50):.4f}\")\n",
    "print(f\"  75th percentile: {threshold:.4f} ← Selected threshold\")\n",
    "print(f\"  Maximum nearest-neighbor distance: {np.max(min_max_dist):.4f}\")\n",
    "print(f\"\\n→ Threshold = {threshold:.4f} ensures 75% of units have at least one neighbor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build distance band W\n",
    "print(\"\\nDISTANCE BAND WEIGHT MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "w_dist = DistanceBand.from_dataframe(counties, threshold=threshold)\n",
    "\n",
    "print(f\"Threshold: {threshold:.4f}\")\n",
    "print(f\"Number of units: {w_dist.n}\")\n",
    "print(f\"Average neighbors: {w_dist.mean_neighbors:.2f}\")\n",
    "print(f\"Min neighbors: {w_dist.min_neighbors}\")\n",
    "print(f\"Max neighbors: {w_dist.max_neighbors}\")\n",
    "print(f\"Islands (units with no neighbors): {len(w_dist.islands)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(w_dist.islands) > 0:\n",
    "    print(f\"\\n⚠ Warning: {len(w_dist.islands)} islands detected!\")\n",
    "    print(\"  Consider increasing threshold or using k-NN\")\n",
    "else:\n",
    "    print(\"\\n✓ No islands - all units have at least one neighbor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverse distance weight matrix\n",
    "def inverse_distance_weights(dist_matrix, threshold, power=1):\n",
    "    \"\"\"\n",
    "    Create inverse distance weight matrix.\n",
    "    \n",
    "    w_ij = 1 / d_ij^power if d_ij < threshold and i != j\n",
    "    w_ij = 0 otherwise\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dist_matrix : np.ndarray\n",
    "        Pairwise distance matrix\n",
    "    threshold : float\n",
    "        Maximum distance for neighbors\n",
    "    power : float\n",
    "        Exponent for distance decay (default=1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    W : np.ndarray\n",
    "        Inverse distance weight matrix\n",
    "    \"\"\"\n",
    "    n = len(dist_matrix)\n",
    "    W = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j and dist_matrix[i, j] < threshold and dist_matrix[i, j] > 0:\n",
    "                W[i, j] = 1 / (dist_matrix[i, j] ** power)\n",
    "    \n",
    "    return W\n",
    "\n",
    "# Create inverse distance W\n",
    "W_inv_dist = inverse_distance_weights(dist_matrix, threshold, power=1)\n",
    "\n",
    "print(\"INVERSE DISTANCE WEIGHT MATRIX\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Power: 1 (linear decay)\")\n",
    "print(f\"Threshold: {threshold:.4f}\")\n",
    "print(f\"Non-zero elements: {(W_inv_dist > 0).sum()}\")\n",
    "print(f\"Average weight (non-zero): {W_inv_dist[W_inv_dist > 0].mean():.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare binary vs inverse distance weights\n",
    "print(\"\\nCOMPARISON: Binary Distance Band vs Inverse Distance\")\n",
    "print(\"=\"*70)\n",
    "print(\"Binary distance band: All neighbors weighted equally\")\n",
    "print(\"Inverse distance: Closer neighbors weighted more\\n\")\n",
    "\n",
    "# Example for unit 0\n",
    "unit_idx = 0\n",
    "binary_weights = list(w_dist.weights[unit_idx])[:5]\n",
    "inv_weights = W_inv_dist[unit_idx][W_inv_dist[unit_idx] > 0][:5]\n",
    "\n",
    "print(f\"Example: Unit {unit_idx}'s first 5 neighbors\")\n",
    "print(f\"  Binary weights:          {binary_weights}\")\n",
    "print(f\"  Inverse distance weights: {inv_weights}\")\n",
    "print(\"\\n→ Inverse distance gives higher weight to closer neighbors\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distance decay functions\n",
    "distances = np.linspace(0.01, threshold, 100)\n",
    "weights_power1 = 1 / distances\n",
    "weights_power2 = 1 / (distances ** 2)\n",
    "weights_exp = np.exp(-distances)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distances, weights_power1, label='$1/d$ (power=1)', linewidth=2)\n",
    "plt.plot(distances, weights_power2, label='$1/d^2$ (power=2)', linewidth=2)\n",
    "plt.plot(distances, weights_exp, label='$\\exp(-d)$', linewidth=2, linestyle='--')\n",
    "plt.xlabel('Distance', fontsize=12)\n",
    "plt.ylabel('Weight', fontsize=12)\n",
    "plt.title('Distance Decay Functions', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'nb02_distance_decay.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"→ Higher power = stronger decay with distance\")\n",
    "print(\"→ Closer neighbors dominate the spatial lag\")\n",
    "print(\"→ Exponential decay provides smooth, bounded weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. k-Nearest Neighbors (k-NN)\n",
    "\n",
    "### Fixed Number of Neighbors\n",
    "\n",
    "k-NN weights define each unit's $k$ closest neighbors.\n",
    "\n",
    "**Advantages**:\n",
    "- Ensures all units have exactly $k$ neighbors (no islands)\n",
    "- Works well with irregular spatial distributions\n",
    "- Handles varying spatial densities\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Asymmetric**: $i$ may be neighbor of $j$, but $j$ may not be neighbor of $i$\n",
    "- Choice of $k$ is somewhat arbitrary\n",
    "\n",
    "**Use cases**:\n",
    "- Point data (cities, stores, households)\n",
    "- Irregular spatial distributions\n",
    "- When you want to guarantee connectivity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build k-NN weight matrices for different k values\n",
    "k_values = [4, 8, 12]\n",
    "w_knn_list = []\n",
    "\n",
    "print(\"k-NEAREST NEIGHBORS WEIGHT MATRICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for k in k_values:\n",
    "    w_knn = KNN.from_dataframe(counties, k=k)\n",
    "    w_knn_list.append(w_knn)\n",
    "    \n",
    "    print(f\"\\nk = {k}:\")\n",
    "    print(f\"  Number of units: {w_knn.n}\")\n",
    "    print(f\"  Average neighbors: {w_knn.mean_neighbors:.2f}\")\n",
    "    print(f\"  Islands: {len(w_knn.islands)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"→ k-NN guarantees exactly k neighbors for each unit (no islands!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize neighbor distribution for different k values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (k, w_knn) in enumerate(zip(k_values, w_knn_list)):\n",
    "    # In k-NN, each unit has exactly k neighbors, but due to asymmetry,\n",
    "    # a unit may be neighbor to more than k other units\n",
    "    neighbor_counts = [len(w_knn.neighbors[i]) for i in w_knn.neighbors]\n",
    "    \n",
    "    axes[idx].hist(neighbor_counts, bins=15, alpha=0.7, color='purple', edgecolor='black')\n",
    "    axes[idx].set_title(f'k-NN (k={k})', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Number of Neighbors', fontsize=12)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[idx].axvline(k, color='red', linestyle='--', linewidth=2,\n",
    "                     label=f'k={k}')\n",
    "    axes[idx].legend(fontsize=11)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'nb02_knn_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"→ All units have exactly k outgoing neighbors\")\n",
    "print(\"→ Distribution shows number of incoming neighbor links\")\n",
    "print(\"→ Choice of k is subjective - try multiple values in sensitivity analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate k-NN asymmetry\n",
    "w_knn_8 = w_knn_list[1]  # k=8\n",
    "\n",
    "# Find an asymmetric relationship\n",
    "asymmetric_found = False\n",
    "for example_i in range(min(100, len(counties))):\n",
    "    neighbors_of_i = list(w_knn_8.neighbors[example_i])\n",
    "    if neighbors_of_i:\n",
    "        example_j = neighbors_of_i[0]\n",
    "        \n",
    "        i_neighbors_j = example_j in w_knn_8.neighbors[example_i]\n",
    "        j_neighbors_i = example_i in w_knn_8.neighbors[example_j]\n",
    "        \n",
    "        if i_neighbors_j and not j_neighbors_i:\n",
    "            asymmetric_found = True\n",
    "            break\n",
    "\n",
    "print(\"k-NN ASYMMETRY DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if asymmetric_found:\n",
    "    print(f\"Unit {example_i} considers Unit {example_j} a neighbor: {i_neighbors_j}\")\n",
    "    print(f\"Unit {example_j} considers Unit {example_i} a neighbor: {j_neighbors_i}\")\n",
    "    print(\"\\n→ Asymmetric relationship detected!\")\n",
    "    print(\"→ This is normal for k-NN but not for contiguity-based W\")\n",
    "    print(\"→ Unit i can be in j's k nearest, but i's k nearest may not include j\")\n",
    "else:\n",
    "    print(\"Note: In this spatial configuration, asymmetry may be minimal\")\n",
    "    print(\"→ Asymmetry more pronounced with irregular spatial distributions\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Row Normalization\n",
    "\n",
    "### Why Normalize W? Interpretation and Stability\n",
    "\n",
    "**Row normalization** divides each row by its sum, so rows sum to 1:\n",
    "\n",
    "$$w_{ij}^{\\text{norm}} = \\frac{w_{ij}}{\\sum_k w_{ik}}$$\n",
    "\n",
    "**Why normalize?**\n",
    "\n",
    "1. **Interpretation**: $Wy$ becomes weighted average of neighbors' $y$ values\n",
    "2. **Mathematical stability**: Keeps spatial parameter $\\rho$ in bounded range\n",
    "3. **Comparability**: Makes results comparable across different $W$ specifications\n",
    "\n",
    "**Trade-offs**:\n",
    "- Lose absolute distance information in inverse distance weights\n",
    "- Units with many neighbors get smaller individual weights\n",
    "\n",
    "**Common transformations**:\n",
    "- `'b'`: Binary (un-normalized)\n",
    "- `'r'`: Row-normalized (most common)\n",
    "- `'v'`: Variance-stabilizing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare un-normalized and row-normalized W\n",
    "w_queen_unnorm = Queen.from_dataframe(counties)\n",
    "w_queen_unnorm.transform = 'b'  # Binary (un-normalized)\n",
    "\n",
    "w_queen_norm = Queen.from_dataframe(counties)\n",
    "w_queen_norm.transform = 'r'  # Row-normalized\n",
    "\n",
    "# Extract weights for unit 0\n",
    "unit_idx = 0\n",
    "row_unnorm = np.array(list(w_queen_unnorm.weights[unit_idx]))\n",
    "row_norm = np.array(list(w_queen_norm.weights[unit_idx]))\n",
    "\n",
    "print(\"ROW NORMALIZATION COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Unit {unit_idx} neighbor weights:\")\n",
    "print(f\"\\n  Un-normalized:\")\n",
    "print(f\"    Weights: {row_unnorm[:5]} ... (showing first 5)\")\n",
    "print(f\"    Sum: {row_unnorm.sum():.1f}\")\n",
    "print(f\"\\n  Row-normalized:\")\n",
    "print(f\"    Weights: {row_norm[:5]} ... (showing first 5)\")\n",
    "print(f\"    Sum: {row_norm.sum():.4f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  Un-normalized → Spatial lag = SUM of neighbors' values\")\n",
    "print(\"  Row-normalized → Spatial lag = WEIGHTED AVERAGE of neighbors' values\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spatial lag with both normalizations\n",
    "income = counties['income_percapita'].values\n",
    "\n",
    "income_lag_unnorm = weights.lag_spatial(w_queen_unnorm, income)\n",
    "income_lag_norm = weights.lag_spatial(w_queen_norm, income)\n",
    "\n",
    "print(\"SPATIAL LAG COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original income:\")\n",
    "print(f\"  Mean: {income.mean():.2f}\")\n",
    "print(f\"  Std: {income.std():.2f}\")\n",
    "print(f\"  Range: [{income.min():.2f}, {income.max():.2f}]\")\n",
    "\n",
    "print(f\"\\nUn-normalized spatial lag:\")\n",
    "print(f\"  Mean: {income_lag_unnorm.mean():.2f}\")\n",
    "print(f\"  Std: {income_lag_unnorm.std():.2f}\")\n",
    "print(f\"  Range: [{income_lag_unnorm.min():.2f}, {income_lag_unnorm.max():.2f}]\")\n",
    "\n",
    "print(f\"\\nRow-normalized spatial lag:\")\n",
    "print(f\"  Mean: {income_lag_norm.mean():.2f}\")\n",
    "print(f\"  Std: {income_lag_norm.std():.2f}\")\n",
    "print(f\"  Range: [{income_lag_norm.min():.2f}, {income_lag_norm.max():.2f}]\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n→ Row-normalized spatial lag has same scale as original variable\")\n",
    "print(\"→ Un-normalized spatial lag increases with number of neighbors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize normalization effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Un-normalized\n",
    "axes[0].scatter(income, income_lag_unnorm, alpha=0.5, edgecolors='k', s=50)\n",
    "axes[0].set_xlabel('Income per Capita', fontsize=12)\n",
    "axes[0].set_ylabel('Spatial Lag (Un-normalized)', fontsize=12)\n",
    "axes[0].set_title('Un-normalized W: Sum of Neighbors', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Row-normalized\n",
    "axes[1].scatter(income, income_lag_norm, alpha=0.5, edgecolors='k', s=50, color='orange')\n",
    "axes[1].set_xlabel('Income per Capita', fontsize=12)\n",
    "axes[1].set_ylabel('Spatial Lag (Row-normalized)', fontsize=12)\n",
    "axes[1].set_title('Row-normalized W: Average of Neighbors', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add 45-degree line to normalized plot\n",
    "lims = [min(income.min(), income_lag_norm.min()),\n",
    "        max(income.max(), income_lag_norm.max())]\n",
    "axes[1].plot(lims, lims, 'r--', alpha=0.5, linewidth=2, label='45° line')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'nb02_normalization_effect.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visual insights:\")\n",
    "print(\"→ Right plot shows spatial lag on same scale as original variable\")\n",
    "print(\"→ Points near 45° line indicate strong spatial autocorrelation\")\n",
    "print(\"→ Row-normalized lag has clear interpretation as neighborhood average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. W Properties: Eigenvalues and Bounds\n",
    "\n",
    "### Mathematical Properties of W\n",
    "\n",
    "Understanding the eigenvalues of $W$ is crucial for spatial econometrics:\n",
    "\n",
    "**Eigenvalues** ($\\lambda_1, \\lambda_2, ..., \\lambda_n$):\n",
    "- Determine bounds for spatial parameters ($\\rho$, $\\lambda$)\n",
    "- For row-normalized $W$: largest eigenvalue $\\lambda_{\\max} = 1$\n",
    "\n",
    "**Parameter Bounds**:\n",
    "$$\\frac{1}{\\lambda_{\\min}} < \\rho < \\frac{1}{\\lambda_{\\max}}$$\n",
    "\n",
    "**Sparsity**:\n",
    "- Most spatial weight matrices are **sparse** (many zero elements)\n",
    "- Sparse methods enable efficient computation for large $n$\n",
    "\n",
    "**Summary Statistics**:\n",
    "- $s_0 = \\sum_i \\sum_j w_{ij}$: Sum of all weights\n",
    "- $s_1 = \\frac{1}{2} \\sum_i \\sum_j (w_{ij} + w_{ji})^2$\n",
    "- $s_2 = \\sum_i (\\sum_j w_{ij} + \\sum_j w_{ji})^2$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute eigenvalues of row-normalized Queen W\n",
    "w_queen_norm = Queen.from_dataframe(counties)\n",
    "w_queen_norm.transform = 'r'\n",
    "\n",
    "print(\"Computing eigenvalues...\")\n",
    "print(\"(This may take a moment for large N)\\n\")\n",
    "\n",
    "# Extract as dense array (be careful with large N!)\n",
    "if w_queen_norm.n <= 5000:  # Only for reasonably sized matrices\n",
    "    W_dense = w_queen_norm.full()[0]  # Returns (array, ids)\n",
    "    eigenvalues = np.linalg.eigvals(W_dense).real\n",
    "    lambda_max = eigenvalues.max()\n",
    "    lambda_min = eigenvalues.min()\n",
    "    \n",
    "    print(\"EIGENVALUE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Matrix dimension: {w_queen_norm.n} × {w_queen_norm.n}\")\n",
    "    print(f\"\\nEigenvalues:\")\n",
    "    print(f\"  Maximum: λ_max = {lambda_max:.6f}\")\n",
    "    print(f\"  Minimum: λ_min = {lambda_min:.6f}\")\n",
    "    print(f\"  Range: [{lambda_min:.6f}, {lambda_max:.6f}]\")\n",
    "    print(f\"\\nBounds for spatial autoregressive parameter ρ:\")\n",
    "    print(f\"  Lower bound: 1/λ_min = {1/lambda_min:.4f}\")\n",
    "    print(f\"  Upper bound: 1/λ_max = {1/lambda_max:.4f}\")\n",
    "    print(f\"\\n→ Estimated ρ must lie in [{1/lambda_min:.4f}, {1/lambda_max:.4f}]\")\n",
    "    print(\"→ For row-normalized W, λ_max ≈ 1\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(f\"Matrix too large ({w_queen_norm.n} × {w_queen_norm.n}) for dense eigenvalue computation\")\n",
    "    print(\"Skipping eigenvalue analysis (requires sparse methods for large N)\")\n",
    "    eigenvalues = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot eigenvalue distribution\n",
    "if eigenvalues is not None:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(eigenvalues, bins=50, alpha=0.7, color='teal', edgecolor='black')\n",
    "    plt.axvline(lambda_max, color='red', linestyle='--', linewidth=2,\n",
    "                label=f'λ_max = {lambda_max:.3f}')\n",
    "    plt.axvline(lambda_min, color='blue', linestyle='--', linewidth=2,\n",
    "                label=f'λ_min = {lambda_min:.3f}')\n",
    "    plt.axvline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "    plt.xlabel('Eigenvalue', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title('Eigenvalue Distribution of Row-Normalized W', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / 'nb02_eigenvalues.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key observations:\")\n",
    "    print(\"→ Largest eigenvalue ≈ 1 for row-normalized W\")\n",
    "    print(\"→ Eigenvalue distribution reflects spatial structure\")\n",
    "    print(\"→ Bounds ensure stationarity of spatial process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity analysis\n",
    "n = w_queen_norm.n\n",
    "max_possible_links = n * (n - 1)  # Exclude diagonal\n",
    "actual_links = w_queen_norm.s0\n",
    "sparsity = 1 - (actual_links / max_possible_links)\n",
    "\n",
    "print(\"SPARSITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of units: {n}\")\n",
    "print(f\"Maximum possible links: {max_possible_links:,}\")\n",
    "print(f\"Actual non-zero links: {actual_links:.0f}\")\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")\n",
    "print(f\"\\n→ {sparsity * 100:.1f}% of matrix elements are zero\")\n",
    "print(\"→ Sparse matrix methods enable efficient computation\")\n",
    "print(\"→ This is why spatial econometrics scales to large datasets\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Comparing Different W Specifications\n",
    "\n",
    "### Does Choice of W Matter? Sensitivity Analysis\n",
    "\n",
    "An important question: **Are our results sensitive to W specification?**\n",
    "\n",
    "**Best practice**: Test multiple reasonable W specifications and assess robustness.\n",
    "\n",
    "**What to compare**:\n",
    "1. Spatial autocorrelation statistics (Moran's I)\n",
    "2. Model parameter estimates (in regression context)\n",
    "3. Statistical significance\n",
    "4. Qualitative conclusions\n",
    "\n",
    "**Interpretation**:\n",
    "- Results **robust** across W → Strong evidence of spatial effects\n",
    "- Results **sensitive** to W → Need theoretical justification for W choice\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple W specifications\n",
    "print(\"Creating multiple W specifications for comparison...\\n\")\n",
    "\n",
    "w_specs = {\n",
    "    'Queen': Queen.from_dataframe(counties),\n",
    "    'Rook': Rook.from_dataframe(counties),\n",
    "    'k-NN (k=8)': KNN.from_dataframe(counties, k=8),\n",
    "}\n",
    "\n",
    "# Add distance band if no islands\n",
    "if len(w_dist.islands) == 0:\n",
    "    w_specs['Distance Band'] = w_dist\n",
    "\n",
    "# Row-normalize all\n",
    "for w in w_specs.values():\n",
    "    w.transform = 'r'\n",
    "\n",
    "print(f\"Comparing {len(w_specs)} W specifications:\")\n",
    "for name in w_specs.keys():\n",
    "    print(f\"  ✓ {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Moran's I for each W specification\n",
    "print(\"\\nComputing Moran's I for income per capita...\\n\")\n",
    "\n",
    "morans_i_results = []\n",
    "income = counties['income_percapita']\n",
    "\n",
    "for name, w in w_specs.items():\n",
    "    mi = Moran(income, w)\n",
    "    morans_i_results.append({\n",
    "        'W Specification': name,\n",
    "        'Moran I': mi.I,\n",
    "        'E[I]': mi.EI,\n",
    "        'p-value': mi.p_sim,\n",
    "        'z-score': mi.z_sim\n",
    "    })\n",
    "\n",
    "mi_df = pd.DataFrame(morans_i_results)\n",
    "\n",
    "print(\"MORAN'S I SENSITIVITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(mi_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Statistical summary\n",
    "print(f\"\\nStatistical Summary:\")\n",
    "print(f\"  Moran's I range: [{mi_df['Moran I'].min():.4f}, {mi_df['Moran I'].max():.4f}]\")\n",
    "print(f\"  Mean Moran's I: {mi_df['Moran I'].mean():.4f}\")\n",
    "print(f\"  Std. Dev.: {mi_df['Moran I'].std():.4f}\")\n",
    "print(f\"  All significant (p < 0.05): {(mi_df['p-value'] < 0.05).all()}\")\n",
    "\n",
    "print(f\"\\nObservations:\")\n",
    "print(\"→ Moran's I values are similar but not identical across W specifications\")\n",
    "print(\"→ Statistical significance is consistent\")\n",
    "print(\"→ Spatial autocorrelation detected regardless of W choice\")\n",
    "print(\"→ Results are qualitatively robust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensitivity\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Color by significance\n",
    "colors = ['green' if p < 0.05 else 'gray' for p in mi_df['p-value']]\n",
    "bars = ax.bar(mi_df['W Specification'], mi_df['Moran I'],\n",
    "              color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add expected value line\n",
    "ax.axhline(mi_df['E[I]'].iloc[0], color='red', linestyle='--', linewidth=2,\n",
    "           label=f\"E[I] = {mi_df['E[I]'].iloc[0]:.4f} (null hypothesis)\")\n",
    "\n",
    "ax.set_ylabel(\"Moran's I\", fontsize=12)\n",
    "ax.set_xlabel(\"W Specification\", fontsize=12)\n",
    "ax.set_title(\"Sensitivity of Moran's I to W Specification\", fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, val) in enumerate(zip(bars, mi_df['Moran I'])):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.4f}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'nb02_morans_i_sensitivity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGreen bars = Statistically significant (p < 0.05)\")\n",
    "print(\"Gray bars = Not significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Custom Economic W Matrices\n",
    "\n",
    "### Beyond Geography: Economic Distance\n",
    "\n",
    "Not all spatial relationships are geographic! You can define **economic weight matrices** based on:\n",
    "\n",
    "**Examples of Economic W**:\n",
    "1. **Trade flows**: $w_{ij}$ = volume of trade between regions $i$ and $j$\n",
    "2. **Migration**: $w_{ij}$ = migration flow from $j$ to $i$\n",
    "3. **Input-output linkages**: $w_{ij}$ = sector interdependence\n",
    "4. **Technological similarity**: Based on patent citations, R&D networks\n",
    "5. **Institutional similarity**: Similar governance, policies, regulations\n",
    "\n",
    "**When to use economic W**:\n",
    "- Trade spillovers (gravity models)\n",
    "- Technology diffusion\n",
    "- Financial contagion (interbank networks)\n",
    "- Policy imitation (similar jurisdictions)\n",
    "\n",
    "**Key insight**: \"Neighbors\" in spatial econometrics can be defined by ANY meaningful connection, not just geography.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate economic similarity W matrix\n",
    "# (In practice, use actual data: trade, migration, etc.)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_regions = min(50, len(counties))  # Use subset for demonstration\n",
    "\n",
    "print(\"CREATING ECONOMIC SIMILARITY WEIGHT MATRIX\")\n",
    "print(\"=\"*60)\n",
    "print(\"Simulating economic characteristics...\\n\")\n",
    "\n",
    "# Simulate economic characteristics\n",
    "gdp = np.random.lognormal(10, 0.5, n_regions)\n",
    "industry_mix = np.random.dirichlet(np.ones(5), n_regions)  # 5 industries\n",
    "\n",
    "print(f\"Created {n_regions} regions with:\")\n",
    "print(f\"  • GDP (log-normal distribution)\")\n",
    "print(f\"  • Industry mix (5 sectors: Manufacturing, Services, Agriculture, Tech, Energy)\")\n",
    "print(f\"\\nComputing similarity based on industry structure...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity based on industry structure\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "W_econ = np.zeros((n_regions, n_regions))\n",
    "similarity_threshold = 0.7  # Only consider highly similar regions\n",
    "\n",
    "for i in range(n_regions):\n",
    "    for j in range(n_regions):\n",
    "        if i != j:\n",
    "            # Cosine similarity of industry vectors\n",
    "            similarity = 1 - cosine(industry_mix[i], industry_mix[j])\n",
    "            W_econ[i, j] = similarity if similarity > similarity_threshold else 0\n",
    "\n",
    "# Row-normalize\n",
    "row_sums = W_econ.sum(axis=1)\n",
    "row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "W_econ_norm = W_econ / row_sums[:, None]\n",
    "\n",
    "print(\"\\nECONOMIC SIMILARITY W MATRIX\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Based on: Industry structure similarity (cosine similarity)\")\n",
    "print(f\"Similarity threshold: {similarity_threshold}\")\n",
    "print(f\"Dimension: {W_econ_norm.shape}\")\n",
    "print(f\"Average neighbors: {(W_econ_norm > 0).sum(axis=1).mean():.2f}\")\n",
    "print(f\"Min neighbors: {(W_econ_norm > 0).sum(axis=1).min():.0f}\")\n",
    "print(f\"Max neighbors: {(W_econ_norm > 0).sum(axis=1).max():.0f}\")\n",
    "print(f\"Sparsity: {100 * (W_econ_norm == 0).sum() / W_econ_norm.size:.1f}%\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"→ Regions with similar industry structure are 'neighbors'\")\n",
    "print(\"→ Geography may be irrelevant for technology/policy spillovers\")\n",
    "print(\"→ Useful for studying inter-industry linkages and structural shocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize economic similarity network\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Heatmap of W_econ\n",
    "im = axes[0].imshow(W_econ_norm, cmap='YlOrRd', aspect='auto')\n",
    "axes[0].set_title('Economic Similarity Matrix (Row-Normalized)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Region j', fontsize=12)\n",
    "axes[0].set_ylabel('Region i', fontsize=12)\n",
    "plt.colorbar(im, ax=axes[0], label='Weight')\n",
    "\n",
    "# Right: Distribution of neighbor counts\n",
    "neighbor_counts_econ = (W_econ_norm > 0).sum(axis=1)\n",
    "axes[1].hist(neighbor_counts_econ, bins=15, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Economic Neighbors', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Economic Neighbors', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].axvline(neighbor_counts_econ.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {neighbor_counts_econ.mean():.2f}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'nb02_economic_w.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n→ Left: Yellow = strong economic similarity\")\n",
    "print(\"→ Right: Some regions highly connected, others isolated\")\n",
    "print(\"→ This reflects heterogeneity in economic structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Practical Recommendations\n",
    "\n",
    "### How to Choose the Right W for Your Research\n",
    "\n",
    "**Decision Tree for W Selection**:\n",
    "\n",
    "```\n",
    "1. Is your research question fundamentally geographic?\n",
    "   YES → Use contiguity or distance-based W\n",
    "   NO  → Consider economic W (trade, similarity, networks)\n",
    "\n",
    "2. If geographic:\n",
    "   a. Regular grid or administrative boundaries?\n",
    "      → Queen or Rook contiguity\n",
    "   \n",
    "   b. Irregular spacing (cities, houses, points)?\n",
    "      → k-NN or distance band\n",
    "   \n",
    "   c. Island units possible?\n",
    "      → k-NN (guarantees neighbors)\n",
    "\n",
    "3. How much do distant neighbors matter?\n",
    "   a. Only immediate neighbors → Contiguity\n",
    "   b. Decay with distance → Inverse distance\n",
    "   c. Fixed influence range → Distance band\n",
    "\n",
    "4. Is sensitivity analysis feasible?\n",
    "   → Test 2-3 different W specifications\n",
    "   → Check if results are qualitatively robust\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Justify theoretically**: Why should these units influence each other?\n",
    "2. **Normalize (usually)**: Row-normalization aids interpretation\n",
    "3. **Check for islands**: Units with zero neighbors cause problems\n",
    "4. **Sensitivity analysis**: Try multiple W, report robustness\n",
    "5. **Document clearly**: Always report W specification in papers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated W recommendation function\n",
    "def recommend_weight_matrix(gdf):\n",
    "    \"\"\"\n",
    "    Suggest appropriate W matrix based on data characteristics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Spatial data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Recommendations and diagnostics\n",
    "    \"\"\"\n",
    "    n = len(gdf)\n",
    "    \n",
    "    # Check for islands with Queen\n",
    "    w_test = Queen.from_dataframe(gdf)\n",
    "    islands = w_test.islands\n",
    "    \n",
    "    recommendations = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Island check\n",
    "    if len(islands) == 0:\n",
    "        recommendations.append(\"✓ Queen/Rook contiguity (no islands detected)\")\n",
    "    else:\n",
    "        warnings.append(f\"⚠ {len(islands)} islands with contiguity\")\n",
    "        recommendations.append(\"→ Recommend k-NN to ensure all units have neighbors\")\n",
    "    \n",
    "    # Dataset size\n",
    "    if n < 500:\n",
    "        recommendations.append(\"✓ Dataset small enough for any W type\")\n",
    "    elif n < 5000:\n",
    "        recommendations.append(\"✓ Medium dataset → All W types feasible\")\n",
    "    else:\n",
    "        recommendations.append(\"⚠ Large dataset → Prefer sparse W for computational efficiency\")\n",
    "    \n",
    "    # Geometry type\n",
    "    geom_type = gdf.geometry.iloc[0].geom_type\n",
    "    if geom_type == 'Point':\n",
    "        recommendations.append(\"→ Point data detected: k-NN or distance band recommended\")\n",
    "    elif geom_type in ['Polygon', 'MultiPolygon']:\n",
    "        recommendations.append(\"→ Polygon data: Contiguity-based W appropriate\")\n",
    "    \n",
    "    return {\n",
    "        'n_units': n,\n",
    "        'islands': len(islands),\n",
    "        'geometry_type': geom_type,\n",
    "        'warnings': warnings,\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "\n",
    "# Run recommendation\n",
    "rec = recommend_weight_matrix(counties)\n",
    "\n",
    "print(\"\\nW MATRIX RECOMMENDATION SYSTEM\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset characteristics:\")\n",
    "print(f\"  Number of units: {rec['n_units']}\")\n",
    "print(f\"  Geometry type: {rec['geometry_type']}\")\n",
    "print(f\"  Islands (contiguity): {rec['islands']}\")\n",
    "\n",
    "if rec['warnings']:\n",
    "    print(f\"\\nWarnings:\")\n",
    "    for w in rec['warnings']:\n",
    "        print(f\"  {w}\")\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "for r in rec['recommendations']:\n",
    "    print(f\"  {r}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Summary and Next Steps\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "**Key Takeaways**:\n",
    "\n",
    "1. ✓ **W matrix is specified, not estimated** - it embodies your theoretical assumptions\n",
    "2. ✓ **Multiple valid specifications exist**: contiguity, distance, k-NN, economic\n",
    "3. ✓ **Row normalization**: Makes rows sum to 1 → interpretable as weighted averages\n",
    "4. ✓ **Eigenvalues determine bounds** for spatial autoregressive parameters\n",
    "5. ✓ **Results should be robust** to reasonable W specifications (sensitivity analysis)\n",
    "6. ✓ **Always justify W choice theoretically** in your research\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W matrix comparison table\n",
    "comparison_data = {\n",
    "    'W Type': ['Queen', 'Rook', 'k-NN', 'Distance Band', 'Inverse Distance'],\n",
    "    'Symmetric': ['Yes', 'Yes', 'No', 'Yes', 'Yes'],\n",
    "    'Guarantees Neighbors': ['No', 'No', 'Yes', 'No', 'No'],\n",
    "    'Geographic': ['Yes', 'Yes', 'Yes', 'Yes', 'Yes'],\n",
    "    'Weights Vary': ['No*', 'No*', 'No*', 'No', 'Yes'],\n",
    "    'Primary Use Case': [\n",
    "        'General polygons',\n",
    "        'Cardinal adjacency',\n",
    "        'Irregular/points',\n",
    "        'Fixed radius',\n",
    "        'Distance decay'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nW MATRIX COMPARISON SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "print(comp_df.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "print(\"* Binary weights before normalization\")\n",
    "print(\"\\nNote: All can be row-normalized for interpretation as weighted averages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Outcomes Achieved\n",
    "\n",
    "After completing this notebook, you should be able to:\n",
    "\n",
    "1. ✓ **Construct** Queen, Rook, k-NN, and distance-based W matrices\n",
    "2. ✓ **Explain** when to use each W type and justify your choice\n",
    "3. ✓ **Apply** row normalization and interpret spatial lags\n",
    "4. ✓ **Compute** eigenvalues and understand parameter bounds\n",
    "5. ✓ **Assess** sensitivity of results to W specification\n",
    "6. ✓ **Design** custom economic W matrices for non-geographic relationships\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps: Notebook 03\n",
    "\n",
    "**Spatial Lag Model (SAR)**\n",
    "\n",
    "Now that you understand W matrices, you're ready to:\n",
    "- Estimate Spatial Lag Models (SAR): $y = \\rho Wy + X\\beta + \\varepsilon$\n",
    "- Interpret the spatial autoregressive parameter $\\rho$\n",
    "- Understand spillover effects and spatial multipliers\n",
    "- Perform model diagnostics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTEBOOK 02 COMPLETE: SPATIAL WEIGHT MATRICES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYou've mastered the foundation of spatial econometrics!\")\n",
    "print(\"\\nGenerated outputs:\")\n",
    "\n",
    "output_files = list(output_path.glob('nb02_*.png'))\n",
    "for f in output_files:\n",
    "    print(f\"  ✓ {f.name}\")\n",
    "\n",
    "print(f\"\\nAll figures saved to: {output_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"READY FOR NOTEBOOK 03: SPATIAL LAG MODEL (SAR)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n→ Now you can build spatial regression models!\")\n",
    "print(\"→ Proceed to: 03_spatial_lag_model.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
