{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Honor\u00e9 (1992) Trimmed LAD Estimator for Fixed Effects Tobit\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand why standard fixed effects Tobit is inconsistent (incidental parameters problem)\n",
    "- Learn Honor\u00e9's semiparametric approach using pairwise differencing\n",
    "- Estimate fixed effects censored models using the Trimmed LAD estimator in PanelBox\n",
    "- Compare Honor\u00e9's estimator with Random Effects Tobit and understand trade-offs\n",
    "- Recognize practical considerations: computational cost, convergence, and limitations\n",
    "\n",
    "## Duration\n",
    "90-120 minutes\n",
    "\n",
    "## Prerequisites\n",
    "- Tobit model fundamentals (censored regression)\n",
    "- Fixed effects vs. random effects panel models\n",
    "- Maximum likelihood estimation concepts\n",
    "- Familiarity with LAD (Least Absolute Deviations) / median regression\n",
    "\n",
    "## Dataset\n",
    "Consumer durables spending panel: household-level spending on durable goods\n",
    "- N = 200 households, T = 5 periods\n",
    "- Outcome: `spending` (censored at 0 -- many households report zero spending)\n",
    "- Predictors: `income`, `wealth`, `household_size`, `homeowner`, `urban`, `credit_score`\n",
    "\n",
    "## Level\n",
    "**Advanced** -- This notebook covers an experimental, computationally intensive estimator.\n",
    "It is intended for researchers who need fixed effects estimation in censored models\n",
    "and understand the theoretical limitations of standard approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# PanelBox imports\n",
    "from panelbox.models.censored import PooledTobit, RandomEffectsTobit, HonoreTrimmedEstimator\n",
    "\n",
    "# Visualization configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define paths (relative to notebook location in examples/censored/notebooks/)\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Fixed Effects Problem in Censored Models\n",
    "\n",
    "### The Standard Fixed Effects Tobit Model\n",
    "\n",
    "Consider a panel data Tobit model with individual fixed effects:\n",
    "\n",
    "$$y_{it}^* = \\mathbf{X}_{it}'\\boldsymbol{\\beta} + \\alpha_i + \\varepsilon_{it}$$\n",
    "\n",
    "$$y_{it} = \\max(0, y_{it}^*)$$\n",
    "\n",
    "where:\n",
    "- $y_{it}^*$ is the latent (uncensored) outcome\n",
    "- $y_{it}$ is the observed outcome, left-censored at 0\n",
    "- $\\alpha_i$ is the individual fixed effect (unobserved heterogeneity)\n",
    "- $\\varepsilon_{it} \\sim \\text{i.i.d.}(0, \\sigma^2)$\n",
    "\n",
    "### Why Not Just Estimate Fixed Effects MLE?\n",
    "\n",
    "In linear models, the within-transformation (demeaning) eliminates $\\alpha_i$ and produces consistent estimates of $\\boldsymbol{\\beta}$ even with fixed $T$. However, in nonlinear models like the Tobit, this approach fails.\n",
    "\n",
    "### The Incidental Parameters Problem (Neyman and Scott, 1948)\n",
    "\n",
    "If we attempt to estimate $\\alpha_i$ for each entity $i$ alongside $\\boldsymbol{\\beta}$, we face the **incidental parameters problem**:\n",
    "\n",
    "1. The number of parameters to estimate grows with $N$ (one $\\alpha_i$ per entity).\n",
    "2. Each $\\alpha_i$ is estimated using only $T$ observations.\n",
    "3. When $T$ is fixed (small), the MLEs of $\\alpha_i$ are **inconsistent**.\n",
    "4. This inconsistency **contaminates** the MLE of $\\boldsymbol{\\beta}$ through the likelihood function.\n",
    "\n",
    "In contrast to linear models, the Tobit likelihood is **not separable** in $(\\boldsymbol{\\beta}, \\alpha_i)$, so:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}}_{\\text{FE-MLE}} \\nrightarrow \\boldsymbol{\\beta}_0 \\quad \\text{as } N \\to \\infty \\text{ with } T \\text{ fixed}$$\n",
    "\n",
    "This bias can be substantial in practice -- simulations show that fixed effects MLE Tobit can overestimate coefficients by 50-100% or more when $T$ is small.\n",
    "\n",
    "### Available Solutions\n",
    "\n",
    "| Approach | Assumption on $\\alpha_i$ | Consistent? | Distributional Assumptions |\n",
    "|----------|--------------------------|-------------|----------------------------|\n",
    "| Pooled Tobit | Ignores $\\alpha_i$ | Only if $\\alpha_i = 0$ | Normal errors |\n",
    "| Random Effects Tobit | $\\alpha_i \\perp \\mathbf{X}_{it}$ | Under RE assumption | Normal errors + RE |\n",
    "| FE MLE Tobit | None on $\\alpha_i$ | **No** (incidental params) | Normal errors |\n",
    "| **Honor\u00e9 Trimmed LAD** | **None on** $\\alpha_i$ | **Yes** (semiparametric) | **Minimal** |\n",
    "\n",
    "The Honor\u00e9 (1992) estimator is the only approach that allows **arbitrary correlation** between $\\alpha_i$ and $\\mathbf{X}_{it}$ while maintaining consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation: Demonstrate the incidental parameters bias\n",
    "# We generate data from a known DGP and show that FE MLE is biased\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters\n",
    "beta_true = 1.5\n",
    "sigma_true = 1.0\n",
    "\n",
    "# Simulation settings\n",
    "N_sim = 100       # number of entities\n",
    "T_values = [3, 5, 10, 25, 50]  # varying T to show bias shrinks with T\n",
    "n_reps = 200      # Monte Carlo replications\n",
    "\n",
    "results_sim = []\n",
    "\n",
    "for T_sim in T_values:\n",
    "    beta_estimates = []\n",
    "    \n",
    "    for rep in range(n_reps):\n",
    "        # Generate fixed effects correlated with X\n",
    "        alpha_i = np.random.normal(0, 2, size=N_sim)\n",
    "        \n",
    "        # Generate panel data\n",
    "        x_list, y_list = [], []\n",
    "        for i in range(N_sim):\n",
    "            x_it = np.random.normal(alpha_i[i] * 0.5, 1, size=T_sim)  # correlated with alpha\n",
    "            eps_it = np.random.normal(0, sigma_true, size=T_sim)\n",
    "            y_star = beta_true * x_it + alpha_i[i] + eps_it\n",
    "            y_it = np.maximum(0, y_star)  # left-censoring at 0\n",
    "            x_list.append(x_it)\n",
    "            y_list.append(y_it)\n",
    "        \n",
    "        x_all = np.concatenate(x_list)\n",
    "        y_all = np.concatenate(y_list)\n",
    "        \n",
    "        # Naive approach: OLS on uncensored observations (ignoring FE)\n",
    "        uncensored = y_all > 0\n",
    "        if uncensored.sum() > 2:\n",
    "            X_unc = sm.add_constant(x_all[uncensored])\n",
    "            beta_ols = np.linalg.lstsq(X_unc, y_all[uncensored], rcond=None)[0][1]\n",
    "            beta_estimates.append(beta_ols)\n",
    "    \n",
    "    mean_est = np.mean(beta_estimates)\n",
    "    bias = mean_est - beta_true\n",
    "    results_sim.append({\n",
    "        'T': T_sim,\n",
    "        'Mean Estimate': mean_est,\n",
    "        'True Value': beta_true,\n",
    "        'Bias': bias,\n",
    "        'Relative Bias (%)': 100 * bias / beta_true\n",
    "    })\n",
    "\n",
    "sim_df = pd.DataFrame(results_sim)\n",
    "print('Incidental Parameters Bias Demonstration')\n",
    "print('(Naive OLS on uncensored obs, ignoring FE -- correlated alpha_i)')\n",
    "print('=' * 70)\n",
    "print(f'True beta = {beta_true}')\n",
    "print(f'N = {N_sim}, Monte Carlo reps = {n_reps}')\n",
    "print()\n",
    "display(sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias as a function of T\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left panel: estimated vs true beta\n",
    "axes[0].plot(sim_df['T'], sim_df['Mean Estimate'], 'o-', markersize=8,\n",
    "             linewidth=2, label='Mean Estimate')\n",
    "axes[0].axhline(y=beta_true, color='red', linestyle='--', linewidth=2,\n",
    "                label=f'True $\\\\beta$ = {beta_true}')\n",
    "axes[0].set_xlabel('Number of Time Periods (T)', fontsize=12)\n",
    "axes[0].set_ylabel('Estimated $\\\\beta$', fontsize=12)\n",
    "axes[0].set_title('Bias from Ignoring Fixed Effects\\nin Censored Models', fontsize=13)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Right panel: relative bias\n",
    "colors = ['#D55E00' if b > 5 else '#009E73' for b in sim_df['Relative Bias (%)'].abs()]\n",
    "axes[1].bar(range(len(sim_df)), sim_df['Relative Bias (%)'], color=colors, alpha=0.7,\n",
    "            edgecolor='black')\n",
    "axes[1].set_xticks(range(len(sim_df)))\n",
    "axes[1].set_xticklabels([f'T={t}' for t in sim_df['T']])\n",
    "axes[1].axhline(y=0, color='black', linewidth=1)\n",
    "axes[1].set_xlabel('Number of Time Periods (T)', fontsize=12)\n",
    "axes[1].set_ylabel('Relative Bias (%)', fontsize=12)\n",
    "axes[1].set_title('Relative Bias in $\\\\beta$ Estimation\\n(Ignoring Correlated Fixed Effects)', fontsize=13)\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'incidental_parameters_bias.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Key insight: With correlated fixed effects, naive estimation is biased.')\n",
    "print('The incidental parameters problem makes standard FE MLE inconsistent for fixed T.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Left panel shows estimated beta converging toward the true value as T increases, but remaining biased when T is small. Right panel displays the relative bias in percentage terms. Orange bars indicate substantial bias (above 5%), green bars indicate smaller bias. The key takeaway is that ignoring correlated fixed effects in censored models produces biased estimates, especially with short panels.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Honor\u00e9's Semiparametric Approach\n",
    "\n",
    "### The Core Idea: Pairwise Differencing\n",
    "\n",
    "Honor\u00e9 (1992) proposed an elegant solution: instead of trying to estimate the fixed effects, **eliminate them through pairwise differencing**.\n",
    "\n",
    "For any two time periods $t$ and $s$ of entity $i$:\n",
    "\n",
    "$$y_{it}^* - y_{is}^* = (\\mathbf{X}_{it} - \\mathbf{X}_{is})'\\boldsymbol{\\beta} + (\\varepsilon_{it} - \\varepsilon_{is})$$\n",
    "\n",
    "The fixed effect $\\alpha_i$ cancels out! However, we observe $y_{it} = \\max(0, y_{it}^*)$, not $y_{it}^*$, so we cannot simply difference the observed outcomes.\n",
    "\n",
    "### The Trimming Strategy\n",
    "\n",
    "Honor\u00e9's key insight is that the **conditional median** of the differenced data can be used to form a valid estimating equation, provided we **trim** pairs where both observations are censored.\n",
    "\n",
    "The intuition is:\n",
    "- If both $y_{it}$ and $y_{is}$ are censored at 0, the pair $(y_{it} - y_{is}) = 0$ provides no information about $\\boldsymbol{\\beta}$.\n",
    "- If at least one observation is uncensored, the difference contains information about $\\boldsymbol{\\beta}$.\n",
    "\n",
    "### The Trimmed LAD Objective Function\n",
    "\n",
    "The estimator minimizes a trimmed Least Absolute Deviations (LAD) objective:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}}_{\\text{TLAD}} = \\arg\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^{N} \\sum_{t < s} w_{its} \\left| (y_{it} - y_{is}) - (\\mathbf{X}_{it} - \\mathbf{X}_{is})'\\boldsymbol{\\beta} \\right|$$\n",
    "\n",
    "where the trimming weights are:\n",
    "\n",
    "$$w_{its} = \\begin{cases} 1 & \\text{if not both } y_{it} \\text{ and } y_{is} \\text{ are censored} \\\\ 0 & \\text{if both } y_{it} \\text{ and } y_{is} \\text{ are censored at } 0 \\end{cases}$$\n",
    "\n",
    "### Why LAD Instead of OLS?\n",
    "\n",
    "The use of absolute deviations (LAD/median regression) rather than squared deviations (OLS) is crucial:\n",
    "\n",
    "1. **Robustness**: LAD is robust to asymmetry in the error distribution caused by censoring.\n",
    "2. **Consistency**: Under mild regularity conditions, the trimmed LAD estimator is consistent for $\\boldsymbol{\\beta}$ as $N \\to \\infty$ with $T$ fixed.\n",
    "3. **Minimal assumptions**: No distributional assumptions on $\\varepsilon_{it}$ beyond median zero, and no assumptions on $\\alpha_i$ whatsoever.\n",
    "\n",
    "### Properties of the Estimator\n",
    "\n",
    "- **Consistent**: $\\hat{\\boldsymbol{\\beta}} \\to \\boldsymbol{\\beta}_0$ as $N \\to \\infty$ with $T$ fixed.\n",
    "- **Semiparametric**: No distributional assumptions on $\\alpha_i$ or $\\varepsilon_{it}$.\n",
    "- **Asymptotically normal**: Under regularity conditions, $\\sqrt{N}(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}_0) \\xrightarrow{d} N(0, V)$.\n",
    "- **Trade-off**: No standard errors are computed analytically (the asymptotic variance is complex); bootstrap is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Loading and Exploring the Data\n",
    "\n",
    "We use a panel dataset on consumer durables spending. Households may report zero spending (left-censored at 0), and individual heterogeneity (e.g., permanent preferences, liquidity constraints) creates fixed effects that may be correlated with covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load consumer durables panel data\n",
    "df = pd.read_csv(DATA_DIR / 'consumer_durables_panel.csv')\n",
    "\n",
    "print('Dataset shape:', df.shape)\n",
    "print(f'Unique households: {df[\"id\"].nunique()}')\n",
    "print(f'Time periods: {df[\"time\"].nunique()}')\n",
    "print()\n",
    "print('First few rows:')\n",
    "display(df.head(10))\n",
    "\n",
    "print('\\nVariable types:')\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print('Summary Statistics')\n",
    "print('=' * 70)\n",
    "display(df.describe())\n",
    "\n",
    "# Censoring information\n",
    "n_censored = (df['spending'] == 0).sum()\n",
    "n_total = len(df)\n",
    "pct_censored = 100 * n_censored / n_total\n",
    "\n",
    "print(f'\\nCensoring Summary:')\n",
    "print(f'  Total observations:  {n_total}')\n",
    "print(f'  Censored (spending=0): {n_censored} ({pct_censored:.1f}%)')\n",
    "print(f'  Uncensored (spending>0): {n_total - n_censored} ({100 - pct_censored:.1f}%)')\n",
    "print(f'  Mean spending (all):  {df[\"spending\"].mean():.2f}')\n",
    "print(f'  Mean spending (uncensored): {df.loc[df[\"spending\"] > 0, \"spending\"].mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Exploratory Analysis: Individual Heterogeneity and Censoring Patterns\n",
    "\n",
    "Before estimating models, we need to understand:\n",
    "1. How much individual heterogeneity exists (justifying fixed effects)\n",
    "2. The censoring patterns across individuals and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze censoring patterns by individual\n",
    "censoring_by_id = df.groupby('id').agg(\n",
    "    n_censored=('spending', lambda x: (x == 0).sum()),\n",
    "    n_uncensored=('spending', lambda x: (x > 0).sum()),\n",
    "    mean_spending=('spending', 'mean'),\n",
    "    mean_income=('income', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "censoring_by_id['pct_censored'] = 100 * censoring_by_id['n_censored'] / df.groupby('id').size().values\n",
    "\n",
    "# Distribution of censoring frequency across households\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Left: histogram of censoring frequency per household\n",
    "axes[0].hist(censoring_by_id['n_censored'], bins=range(0, 7), alpha=0.7,\n",
    "             edgecolor='black', color='steelblue')\n",
    "axes[0].set_xlabel('Number of Censored Periods (out of 5)')\n",
    "axes[0].set_ylabel('Number of Households')\n",
    "axes[0].set_title('Censoring Frequency per Household')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Middle: spending distribution by censoring status\n",
    "uncensored_spending = df.loc[df['spending'] > 0, 'spending']\n",
    "axes[1].hist(uncensored_spending, bins=30, alpha=0.7, edgecolor='black',\n",
    "             color='#009E73')\n",
    "axes[1].set_xlabel('Spending (Uncensored Only)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Positive Spending')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Right: mean spending vs mean income colored by censoring rate\n",
    "scatter = axes[2].scatter(\n",
    "    censoring_by_id['mean_income'],\n",
    "    censoring_by_id['mean_spending'],\n",
    "    c=censoring_by_id['pct_censored'],\n",
    "    cmap='RdYlGn_r', s=40, alpha=0.7, edgecolor='gray', linewidth=0.5\n",
    ")\n",
    "plt.colorbar(scatter, ax=axes[2], label='% Censored')\n",
    "axes[2].set_xlabel('Mean Income')\n",
    "axes[2].set_ylabel('Mean Spending')\n",
    "axes[2].set_title('Income vs Spending\\n(Color = Censoring Rate)')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'honore_censoring_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Left panel shows the distribution of censoring frequency per household -- most households have 0 to 2 censored periods, but some are censored in all 5 periods. Middle panel shows the right-skewed distribution of positive spending amounts. Right panel plots mean income against mean spending for each household, colored by their censoring rate; households with higher income tend to have higher spending and lower censoring rates, suggesting correlation between covariates and individual heterogeneity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evidence for individual heterogeneity: within vs between variation\n",
    "# If alpha_i matters, between-individual variation should be large\n",
    "\n",
    "overall_var = df['spending'].var()\n",
    "between_var = df.groupby('id')['spending'].mean().var()\n",
    "within_var = df.groupby('id')['spending'].apply(lambda x: x.var()).mean()\n",
    "\n",
    "print('Variance Decomposition of Spending')\n",
    "print('=' * 50)\n",
    "print(f'Overall variance:   {overall_var:.2f}')\n",
    "print(f'Between variance:   {between_var:.2f} ({100 * between_var / overall_var:.1f}%)')\n",
    "print(f'Within variance:    {within_var:.2f} ({100 * within_var / overall_var:.1f}%)')\n",
    "print()\n",
    "\n",
    "if between_var / overall_var > 0.3:\n",
    "    print('Substantial between-individual variation detected.')\n",
    "    print('This suggests individual heterogeneity (alpha_i) is important.')\n",
    "    print('Fixed effects estimation is warranted.')\n",
    "else:\n",
    "    print('Limited between-individual variation.')\n",
    "    print('Individual heterogeneity may be less important.')\n",
    "\n",
    "# Check correlation between individual means of X and spending\n",
    "# If alpha_i is correlated with X, RE will be biased\n",
    "individual_means = df.groupby('id')[['spending', 'income', 'wealth', 'credit_score']].mean()\n",
    "print('\\nCorrelation Between Individual Means (Evidence for Correlated Effects):')\n",
    "print('-' * 60)\n",
    "for var in ['income', 'wealth', 'credit_score']:\n",
    "    corr = individual_means['spending'].corr(individual_means[var])\n",
    "    print(f'  Corr(mean spending, mean {var}): {corr:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize individual heterogeneity: spaghetti plot of spending trajectories\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Spaghetti plot for a subsample of individuals\n",
    "sample_ids = df['id'].unique()[:30]  # first 30 households\n",
    "for hh_id in sample_ids:\n",
    "    hh_data = df[df['id'] == hh_id]\n",
    "    axes[0].plot(hh_data['time'], hh_data['spending'], alpha=0.4, linewidth=1)\n",
    "\n",
    "axes[0].set_xlabel('Time Period')\n",
    "axes[0].set_ylabel('Spending')\n",
    "axes[0].set_title('Individual Spending Trajectories\\n(30 households)')\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Censoring point')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Right: Boxplot of spending by time period\n",
    "df.boxplot(column='spending', by='time', ax=axes[1])\n",
    "axes[1].set_xlabel('Time Period')\n",
    "axes[1].set_ylabel('Spending')\n",
    "axes[1].set_title('Spending Distribution by Period')\n",
    "plt.suptitle('')  # Remove automatic title from boxplot\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'honore_individual_heterogeneity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Left panel shows spending trajectories for 30 households across 5 time periods, illustrating substantial individual heterogeneity -- some households consistently spend more while others frequently report zero spending. The red dashed line marks the censoring point at zero. Right panel shows boxplots of spending by time period, revealing the overall distribution and the mass point at zero (censoring).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. The Trimmed LAD Estimator: Mathematical Formulation\n",
    "\n",
    "### Step-by-Step Construction\n",
    "\n",
    "**Step 1: Pairwise Differencing**\n",
    "\n",
    "For each entity $i$ and each pair of time periods $(t, s)$ with $t < s$, compute:\n",
    "\n",
    "$$\\Delta y_{its} = y_{it} - y_{is}$$\n",
    "$$\\Delta \\mathbf{X}_{its} = \\mathbf{X}_{it} - \\mathbf{X}_{is}$$\n",
    "\n",
    "**Step 2: Apply Trimming**\n",
    "\n",
    "Define the trimming indicator:\n",
    "\n",
    "$$w_{its} = \\mathbb{1}\\{\\text{not both } y_{it} = 0 \\text{ and } y_{is} = 0\\}$$\n",
    "\n",
    "Pairs where both observations are censored provide no information about $\\boldsymbol{\\beta}$ and are discarded.\n",
    "\n",
    "**Step 3: Minimize the Trimmed LAD Objective**\n",
    "\n",
    "$$Q_N(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\sum_{t < s} w_{its} \\left| \\Delta y_{its} - \\Delta \\mathbf{X}_{its}'\\boldsymbol{\\beta} \\right|$$\n",
    "\n",
    "This is a non-smooth optimization problem (the absolute value function is not differentiable at zero), which requires specialized optimization techniques.\n",
    "\n",
    "### Computational Considerations\n",
    "\n",
    "1. **Number of pairs**: For $T$ periods per entity, there are $\\binom{T}{2} = T(T-1)/2$ pairs. With $T=5$, that is 10 pairs per entity.\n",
    "2. **Total observations**: $N \\times \\binom{T}{2}$ pairwise differences. For $N=200$ and $T=5$: 2,000 pairs.\n",
    "3. **Non-smooth objective**: The LAD objective requires subgradient-based methods (e.g., L-BFGS-B with smoothed approximation).\n",
    "4. **No closed-form SE**: Standard errors require bootstrap or kernel-based estimation of the asymptotic variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate the pairwise differencing concept with a small example\n",
    "# Take one household and show all pairwise differences\n",
    "\n",
    "example_id = df['id'].unique()[0]\n",
    "example_data = df[df['id'] == example_id].sort_values('time')\n",
    "\n",
    "print(f'Example: Household {example_id}')\n",
    "print('=' * 60)\n",
    "display(example_data[['time', 'spending', 'income', 'wealth']].reset_index(drop=True))\n",
    "\n",
    "print('\\nAll Pairwise Differences:')\n",
    "print('-' * 60)\n",
    "\n",
    "T_hh = len(example_data)\n",
    "spending = example_data['spending'].values\n",
    "income = example_data['income'].values\n",
    "times = example_data['time'].values\n",
    "\n",
    "pairs_data = []\n",
    "for t in range(T_hh):\n",
    "    for s in range(t + 1, T_hh):\n",
    "        delta_y = spending[t] - spending[s]\n",
    "        delta_x = income[t] - income[s]\n",
    "        both_censored = (spending[t] == 0) and (spending[s] == 0)\n",
    "        trim = 'TRIMMED' if both_censored else 'Kept'\n",
    "        \n",
    "        pairs_data.append({\n",
    "            'Pair (t, s)': f'({times[t]}, {times[s]})',\n",
    "            'y_t': spending[t],\n",
    "            'y_s': spending[s],\n",
    "            'dy': round(delta_y, 2),\n",
    "            'dx (income)': round(delta_x, 2),\n",
    "            'Status': trim\n",
    "        })\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs_data)\n",
    "display(pairs_df)\n",
    "\n",
    "n_kept = (pairs_df['Status'] == 'Kept').sum()\n",
    "n_trimmed = (pairs_df['Status'] == 'TRIMMED').sum()\n",
    "print(f'\\nKept: {n_kept} pairs, Trimmed: {n_trimmed} pairs')\n",
    "print('Pairs where both observations are censored at 0 provide no information about beta.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Estimation with PanelBox: HonoreTrimmedEstimator\n",
    "\n",
    "PanelBox provides the `HonoreTrimmedEstimator` class for estimating the Honor\u00e9 (1992) Trimmed LAD model.\n",
    "\n",
    "**Important notes:**\n",
    "- This estimator is **experimental** and computationally intensive.\n",
    "- It issues an `ExperimentalWarning` upon instantiation.\n",
    "- Do **not** include a constant column in `exog` -- the fixed effects absorb the intercept.\n",
    "- For larger datasets, consider using a subsample for initial exploration.\n",
    "\n",
    "### API Overview\n",
    "\n",
    "```python\n",
    "model = HonoreTrimmedEstimator(\n",
    "    endog=y,              # 1D array of dependent variable\n",
    "    exog=X,               # 2D array of covariates (NO constant)\n",
    "    groups=groups,        # Entity identifiers\n",
    "    time=time,            # Time identifiers\n",
    "    censoring_point=0.0   # Left-censoring threshold\n",
    ")\n",
    "\n",
    "result = model.fit()      # Returns HonoreResults dataclass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a SMALLER subsample for the Honore estimator\n",
    "# This estimator is computationally expensive due to pairwise differencing\n",
    "# We select n=50 households (250 observations with T=5)\n",
    "\n",
    "np.random.seed(42)\n",
    "all_ids = df['id'].unique()\n",
    "subsample_ids = np.random.choice(all_ids, size=50, replace=False)\n",
    "df_sub = df[df['id'].isin(subsample_ids)].copy().reset_index(drop=True)\n",
    "\n",
    "print(f'Subsample size: {len(df_sub)} observations')\n",
    "print(f'Households: {df_sub[\"id\"].nunique()}')\n",
    "print(f'Time periods per household: {df_sub.groupby(\"id\").size().unique()}')\n",
    "print(f'Censored observations: {(df_sub[\"spending\"] == 0).sum()} ({100 * (df_sub[\"spending\"] == 0).mean():.1f}%)')\n",
    "\n",
    "# Prepare arrays for estimation\n",
    "# IMPORTANT: Do NOT include a constant -- FE absorbs the intercept\n",
    "y_honore = df_sub['spending'].values\n",
    "X_honore = df_sub[['income', 'wealth', 'household_size', 'credit_score']].values\n",
    "groups_honore = df_sub['id'].values\n",
    "time_honore = df_sub['time'].values\n",
    "\n",
    "var_names_honore = ['income', 'wealth', 'household_size', 'credit_score']\n",
    "\n",
    "print(f'\\nCovariates: {var_names_honore}')\n",
    "print(f'X shape: {X_honore.shape}')\n",
    "print(f'Number of pairwise differences per entity: {5 * 4 // 2} = C(5,2)')\n",
    "print(f'Total pairwise differences: {50 * 10} (before trimming)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the Honore Trimmed LAD model\n",
    "# Wrap in try/except since this is experimental\n",
    "\n",
    "print('Estimating Honor\\u00e9 Trimmed LAD Estimator...')\n",
    "print('=' * 60)\n",
    "print('NOTE: This estimator is experimental. An ExperimentalWarning will be shown.')\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Re-enable warnings temporarily to show the ExperimentalWarning\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('always')\n",
    "        \n",
    "        honore_model = HonoreTrimmedEstimator(\n",
    "            endog=y_honore,\n",
    "            exog=X_honore,\n",
    "            groups=groups_honore,\n",
    "            time=time_honore,\n",
    "            censoring_point=0.0\n",
    "        )\n",
    "    \n",
    "    # Fit the model\n",
    "    honore_result = honore_model.fit(verbose=True)\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('Estimation Complete!')\n",
    "    print(f'Converged: {honore_result.converged}')\n",
    "    print(f'Iterations: {honore_result.n_iter}')\n",
    "    print(f'Observations used: {honore_result.n_obs}')\n",
    "    print(f'Entities: {honore_result.n_entities}')\n",
    "    print(f'Trimmed pairs: {honore_result.n_trimmed}')\n",
    "    \n",
    "    honore_success = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'\\nEstimation failed with error: {e}')\n",
    "    print('This can happen with small samples or degenerate data.')\n",
    "    print('Try increasing the sample size or adjusting starting values.')\n",
    "    honore_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the full model summary\n",
    "if honore_success:\n",
    "    print(honore_model.summary())\n",
    "    \n",
    "    print('\\n\\nCoefficient Interpretation:')\n",
    "    print('-' * 50)\n",
    "    for i, name in enumerate(var_names_honore):\n",
    "        coef = honore_result.params[i]\n",
    "        print(f'  {name:>20s}: {coef:>10.4f}')\n",
    "    \n",
    "    print('\\nNote: Standard errors are NOT available for the Honore estimator.')\n",
    "    print('Bootstrap is recommended for inference (see Practical Considerations).')\n",
    "else:\n",
    "    print('Skipping summary -- estimation did not succeed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Comparing Random Effects Tobit vs. Honor\u00e9 Trimmed LAD\n",
    "\n",
    "To put the Honor\u00e9 estimates in context, let us estimate a Random Effects Tobit model on the same subsample and compare the results.\n",
    "\n",
    "### When to Use Each Estimator\n",
    "\n",
    "| Criterion | Random Effects Tobit | Honor\u00e9 Trimmed LAD |\n",
    "|-----------|---------------------|---------------------|\n",
    "| Assumption on $\\alpha_i$ | $\\alpha_i \\perp \\mathbf{X}_{it}$ | **None** (can be correlated) |\n",
    "| Distributional assumptions | Normal $\\alpha_i$, normal $\\varepsilon_{it}$ | **Minimal** (median zero errors) |\n",
    "| Standard errors | Available (from Hessian) | **Not available** (use bootstrap) |\n",
    "| Efficiency | More efficient if assumptions hold | Less efficient |\n",
    "| Robustness | Biased if RE assumptions fail | **Robust** to correlated effects |\n",
    "| Computational cost | Moderate (quadrature) | **High** (pairwise differencing) |\n",
    "| Intercept | Estimated | Not estimated (absorbed by FE) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Random Effects Tobit on the same subsample for comparison\n",
    "# RE Tobit requires a constant in X\n",
    "\n",
    "X_re = sm.add_constant(X_honore)  # Add constant for RE model\n",
    "\n",
    "print('Estimating Random Effects Tobit...')\n",
    "print('=' * 60)\n",
    "\n",
    "try:\n",
    "    re_tobit = RandomEffectsTobit(\n",
    "        endog=y_honore,\n",
    "        exog=X_re,\n",
    "        groups=groups_honore,\n",
    "        time=time_honore,\n",
    "        censoring_point=0.0\n",
    "    )\n",
    "    \n",
    "    re_result = re_tobit.fit()\n",
    "    print(re_tobit.summary())\n",
    "    re_success = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'RE Tobit estimation failed: {e}')\n",
    "    re_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison table\n",
    "if honore_success and re_success:\n",
    "    # RE Tobit coefficients (skip constant, sigma_eps, sigma_alpha)\n",
    "    re_beta = re_tobit.beta[1:]  # skip constant\n",
    "    re_se = re_tobit.bse[1:re_tobit.n_features]  # skip constant, exclude sigma params\n",
    "    \n",
    "    comparison_data = []\n",
    "    for i, name in enumerate(var_names_honore):\n",
    "        comparison_data.append({\n",
    "            'Variable': name,\n",
    "            'Honore TLAD': honore_result.params[i],\n",
    "            'RE Tobit': re_beta[i],\n",
    "            'RE Tobit SE': re_se[i] if i < len(re_se) else np.nan,\n",
    "            'Difference': honore_result.params[i] - re_beta[i],\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df['Rel. Diff (%)'] = 100 * comparison_df['Difference'] / comparison_df['RE Tobit'].abs()\n",
    "    \n",
    "    print('Coefficient Comparison: Honore vs RE Tobit')\n",
    "    print('=' * 80)\n",
    "    display(comparison_df)\n",
    "    \n",
    "    print('\\nInterpretation:')\n",
    "    print('-' * 60)\n",
    "    print('  - Large differences suggest correlated individual effects.')\n",
    "    print('  - If alpha_i is correlated with X, RE Tobit is inconsistent.')\n",
    "    print('  - Honore is consistent regardless of the correlation structure.')\n",
    "    print('  - Honore does not provide standard errors (bootstrap needed).')\n",
    "    \n",
    "    # Save comparison table\n",
    "    comparison_df.to_csv(TABLES_DIR / 'honore_vs_re_comparison.csv', index=False)\n",
    "\n",
    "else:\n",
    "    print('Cannot create comparison -- one or both estimations failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "if honore_success and re_success:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Left: Coefficient comparison bar chart\n",
    "    x_pos = np.arange(len(var_names_honore))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[0].bar(x_pos - width/2, honore_result.params, width,\n",
    "                         label='Honor\\u00e9 TLAD', color='steelblue', alpha=0.8,\n",
    "                         edgecolor='black')\n",
    "    bars2 = axes[0].bar(x_pos + width/2, re_beta, width,\n",
    "                         label='RE Tobit', color='#D55E00', alpha=0.8,\n",
    "                         edgecolor='black')\n",
    "    \n",
    "    # Add RE Tobit error bars\n",
    "    axes[0].errorbar(x_pos + width/2, re_beta,\n",
    "                      yerr=1.96 * re_se[:len(var_names_honore)],\n",
    "                      fmt='none', color='black', capsize=3, capthick=1.5)\n",
    "    \n",
    "    axes[0].set_xticks(x_pos)\n",
    "    axes[0].set_xticklabels(var_names_honore, rotation=15, ha='right')\n",
    "    axes[0].set_ylabel('Coefficient Estimate')\n",
    "    axes[0].set_title('Coefficient Comparison:\\nHonor\\u00e9 TLAD vs RE Tobit')\n",
    "    axes[0].legend()\n",
    "    axes[0].axhline(y=0, color='gray', linestyle='-', linewidth=0.5)\n",
    "    axes[0].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Right: Relative difference\n",
    "    rel_diff = comparison_df['Rel. Diff (%)'].values\n",
    "    colors_bar = ['#D55E00' if abs(d) > 20 else '#009E73' for d in rel_diff]\n",
    "    axes[1].bar(x_pos, rel_diff, color=colors_bar, alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xticks(x_pos)\n",
    "    axes[1].set_xticklabels(var_names_honore, rotation=15, ha='right')\n",
    "    axes[1].set_ylabel('Relative Difference (%)')\n",
    "    axes[1].set_title('Relative Difference Between Estimators\\n(Honor\\u00e9 - RE Tobit) / |RE Tobit|')\n",
    "    axes[1].axhline(y=0, color='black', linewidth=1)\n",
    "    axes[1].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'honore_vs_re_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print('Cannot create comparison plot -- estimation(s) failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Left panel compares coefficient estimates from the Honor\\u00e9 Trimmed LAD estimator (blue) and the Random Effects Tobit (orange) with 95% confidence intervals for the RE model. Right panel shows the relative difference between the two estimators for each variable. Orange bars indicate substantial differences (above 20%), which may suggest that the RE assumption of uncorrelated individual effects is violated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional comparison: Pooled Tobit (ignoring all panel structure)\n",
    "if honore_success:\n",
    "    print('Estimating Pooled Tobit for three-way comparison...')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    try:\n",
    "        pooled_tobit = PooledTobit(\n",
    "            endog=y_honore,\n",
    "            exog=X_re,  # with constant\n",
    "            groups=groups_honore,\n",
    "            censoring_point=0.0\n",
    "        )\n",
    "        pooled_result = pooled_tobit.fit()\n",
    "        \n",
    "        pooled_beta = pooled_tobit.beta[1:]  # skip constant\n",
    "        \n",
    "        print('\\nThree-Way Coefficient Comparison')\n",
    "        print('=' * 70)\n",
    "        \n",
    "        three_way = pd.DataFrame({\n",
    "            'Variable': var_names_honore,\n",
    "            'Pooled Tobit': pooled_beta[:len(var_names_honore)],\n",
    "            'RE Tobit': re_beta[:len(var_names_honore)] if re_success else [np.nan] * len(var_names_honore),\n",
    "            'Honore TLAD': honore_result.params\n",
    "        })\n",
    "        display(three_way)\n",
    "        \n",
    "        print('\\nKey differences indicate the importance of accounting for')\n",
    "        print('individual heterogeneity and its correlation with covariates.')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Pooled Tobit estimation failed: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Practical Considerations\n",
    "\n",
    "### 8.1 Computational Cost\n",
    "\n",
    "The Honor\u00e9 estimator's main computational bottleneck is the **pairwise differencing** step:\n",
    "\n",
    "- For $N$ entities and $T$ periods: $N \\times \\binom{T}{2}$ pairwise differences.\n",
    "- With $N = 200$ and $T = 5$: $200 \\times 10 = 2{,}000$ pairs.\n",
    "- With $N = 1{,}000$ and $T = 10$: $1{,}000 \\times 45 = 45{,}000$ pairs.\n",
    "- With $N = 5{,}000$ and $T = 20$: $5{,}000 \\times 190 = 950{,}000$ pairs.\n",
    "\n",
    "The computational cost scales as $O(N \\cdot T^2)$ for constructing differences and $O(N \\cdot T^2 \\cdot K \\cdot I)$ for optimization, where $K$ is the number of covariates and $I$ is the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate computational scaling\n",
    "import time\n",
    "\n",
    "# Test with different subsample sizes\n",
    "subsample_sizes = [20, 30, 40, 50]\n",
    "timing_results = []\n",
    "\n",
    "for n_hh in subsample_sizes:\n",
    "    np.random.seed(42)\n",
    "    sub_ids = np.random.choice(all_ids, size=n_hh, replace=False)\n",
    "    df_timing = df[df['id'].isin(sub_ids)].copy().reset_index(drop=True)\n",
    "    \n",
    "    y_t = df_timing['spending'].values\n",
    "    X_t = df_timing[['income', 'wealth', 'household_size', 'credit_score']].values\n",
    "    g_t = df_timing['id'].values\n",
    "    t_t = df_timing['time'].values\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            model_t = HonoreTrimmedEstimator(\n",
    "                endog=y_t, exog=X_t, groups=g_t, time=t_t, censoring_point=0.0\n",
    "            )\n",
    "            result_t = model_t.fit(verbose=False)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        n_pairs = n_hh * 10  # C(5,2) = 10 pairs per entity\n",
    "        \n",
    "        timing_results.append({\n",
    "            'N households': n_hh,\n",
    "            'N observations': n_hh * 5,\n",
    "            'N pairs': n_pairs,\n",
    "            'Time (s)': round(elapsed, 3),\n",
    "            'Converged': result_t.converged\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        timing_results.append({\n",
    "            'N households': n_hh,\n",
    "            'N observations': n_hh * 5,\n",
    "            'N pairs': n_hh * 10,\n",
    "            'Time (s)': round(elapsed, 3),\n",
    "            'Converged': f'Error: {str(e)[:30]}'\n",
    "        })\n",
    "\n",
    "timing_df = pd.DataFrame(timing_results)\n",
    "print('Computational Scaling of Honore Estimator')\n",
    "print('=' * 60)\n",
    "display(timing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize computational scaling\n",
    "if len(timing_df) > 1:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    ax.plot(timing_df['N households'], timing_df['Time (s)'], 'o-',\n",
    "            markersize=8, linewidth=2, color='steelblue')\n",
    "    \n",
    "    ax.set_xlabel('Number of Households (N)', fontsize=12)\n",
    "    ax.set_ylabel('Computation Time (seconds)', fontsize=12)\n",
    "    ax.set_title('Computational Cost of Honor\\u00e9 Estimator\\n(T=5 periods per household)', fontsize=13)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Add annotation\n",
    "    ax.annotate('Cost scales as O(N * T^2)',\n",
    "                xy=(timing_df['N households'].iloc[-1], timing_df['Time (s)'].iloc[-1]),\n",
    "                xytext=(-80, 20), textcoords='offset points',\n",
    "                fontsize=11, fontstyle='italic',\n",
    "                arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'honore_computational_scaling.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Computation time of the Honor\\u00e9 Trimmed LAD estimator as a function of the number of households, with T=5 periods each. The cost grows with N as expected, following the pairwise differencing complexity O(N * T^2).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Convergence Issues\n",
    "\n",
    "Since the Trimmed LAD objective is **non-smooth** (based on absolute values), convergence can be challenging:\n",
    "\n",
    "1. **Non-differentiability**: The absolute value function has a kink at zero. PanelBox uses a subgradient approximation.\n",
    "2. **Local minima**: The objective may have local minima, especially with few observations.\n",
    "3. **Starting values**: Good starting values (e.g., from OLS on differenced data) improve convergence.\n",
    "4. **Optimization method**: L-BFGS-B is used by default; for difficult cases, consider Nelder-Mead (gradient-free).\n",
    "\n",
    "### 8.3 Inference Without Standard Errors\n",
    "\n",
    "The Honor\u00e9 estimator in PanelBox does **not** provide analytical standard errors. The asymptotic variance of the estimator involves a kernel density estimate of the error distribution, which is complex to implement reliably.\n",
    "\n",
    "**Recommended approaches for inference:**\n",
    "\n",
    "1. **Bootstrap**: Resample entities (not individual observations) and re-estimate. The entity-level bootstrap preserves the within-entity correlation structure.\n",
    "2. **Comparison with RE Tobit**: If both estimators give similar results, the RE assumption may hold, and the RE Tobit standard errors can be used as an approximation.\n",
    "3. **Sensitivity analysis**: Estimate on multiple subsamples to assess stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap standard errors for the Honore estimator\n",
    "# Entity-level bootstrap: resample households (not individual obs)\n",
    "\n",
    "if honore_success:\n",
    "    n_bootstrap = 50  # Keep small for demonstration; use 200+ in practice\n",
    "    bootstrap_params = []\n",
    "    \n",
    "    print(f'Running entity-level bootstrap ({n_bootstrap} replications)...')\n",
    "    print('This may take a few minutes.')\n",
    "    print()\n",
    "    \n",
    "    unique_ids_sub = df_sub['id'].unique()\n",
    "    n_entities_sub = len(unique_ids_sub)\n",
    "    \n",
    "    for b in range(n_bootstrap):\n",
    "        # Resample entities with replacement\n",
    "        boot_ids = np.random.choice(unique_ids_sub, size=n_entities_sub, replace=True)\n",
    "        \n",
    "        # Build bootstrap sample (renumber IDs to handle duplicates)\n",
    "        boot_dfs = []\n",
    "        for new_id, orig_id in enumerate(boot_ids):\n",
    "            entity_data = df_sub[df_sub['id'] == orig_id].copy()\n",
    "            entity_data['boot_id'] = new_id\n",
    "            boot_dfs.append(entity_data)\n",
    "        \n",
    "        boot_df = pd.concat(boot_dfs, ignore_index=True)\n",
    "        \n",
    "        # Estimate on bootstrap sample\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                boot_model = HonoreTrimmedEstimator(\n",
    "                    endog=boot_df['spending'].values,\n",
    "                    exog=boot_df[['income', 'wealth', 'household_size', 'credit_score']].values,\n",
    "                    groups=boot_df['boot_id'].values,\n",
    "                    time=boot_df['time'].values,\n",
    "                    censoring_point=0.0\n",
    "                )\n",
    "                boot_result = boot_model.fit(verbose=False)\n",
    "                \n",
    "                if boot_result.converged:\n",
    "                    bootstrap_params.append(boot_result.params)\n",
    "        except Exception:\n",
    "            pass  # Skip failed bootstrap replications\n",
    "        \n",
    "        if (b + 1) % 10 == 0:\n",
    "            print(f'  Completed {b + 1}/{n_bootstrap} replications '\n",
    "                  f'({len(bootstrap_params)} successful)')\n",
    "    \n",
    "    # Compute bootstrap standard errors\n",
    "    if len(bootstrap_params) >= 10:\n",
    "        boot_params_array = np.array(bootstrap_params)\n",
    "        boot_se = np.std(boot_params_array, axis=0)\n",
    "        boot_mean = np.mean(boot_params_array, axis=0)\n",
    "        \n",
    "        print(f'\\nBootstrap Results ({len(bootstrap_params)} successful replications)')\n",
    "        print('=' * 70)\n",
    "        \n",
    "        boot_table = pd.DataFrame({\n",
    "            'Variable': var_names_honore,\n",
    "            'Point Estimate': honore_result.params,\n",
    "            'Boot. Mean': boot_mean,\n",
    "            'Boot. SE': boot_se,\n",
    "            'Boot. t-stat': honore_result.params / boot_se,\n",
    "            'CI Lower (95%)': honore_result.params - 1.96 * boot_se,\n",
    "            'CI Upper (95%)': honore_result.params + 1.96 * boot_se\n",
    "        })\n",
    "        display(boot_table)\n",
    "        \n",
    "        # Check for bootstrap bias\n",
    "        boot_table['Bias'] = boot_mean - honore_result.params\n",
    "        print('\\nBootstrap bias (Boot. Mean - Point Estimate):')\n",
    "        for i, name in enumerate(var_names_honore):\n",
    "            print(f'  {name}: {boot_table[\"Bias\"].iloc[i]:.4f}')\n",
    "    else:\n",
    "        print(f'\\nOnly {len(bootstrap_params)} successful replications -- '\n",
    "              'insufficient for reliable bootstrap SEs.')\n",
    "        print('Increase n_bootstrap or check data quality.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bootstrap distributions\n",
    "if honore_success and len(bootstrap_params) >= 10:\n",
    "    boot_params_array = np.array(bootstrap_params)\n",
    "    n_vars = len(var_names_honore)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_vars, figsize=(4 * n_vars, 4))\n",
    "    if n_vars == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (name, ax) in enumerate(zip(var_names_honore, axes)):\n",
    "        ax.hist(boot_params_array[:, i], bins=15, alpha=0.7,\n",
    "                edgecolor='black', color='steelblue')\n",
    "        ax.axvline(honore_result.params[i], color='red', linestyle='--',\n",
    "                   linewidth=2, label='Point Estimate')\n",
    "        ax.axvline(boot_mean[i], color='#D55E00', linestyle=':',\n",
    "                   linewidth=2, label='Boot. Mean')\n",
    "        ax.set_xlabel(f'$\\\\beta_{{{name}}}$')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title(name)\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Bootstrap Distributions of Honor\\u00e9 TLAD Coefficients',\n",
    "                 fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'honore_bootstrap_distributions.png', dpi=300,\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Histograms of bootstrap distributions for each coefficient from the Honor\\u00e9 Trimmed LAD estimator. The red dashed line marks the point estimate from the original sample, and the orange dotted line shows the bootstrap mean. Distributions centered near the point estimate indicate low bias. The spread of each distribution reflects the sampling variability of the estimator.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity analysis: estimate on different subsamples\n",
    "if honore_success:\n",
    "    print('Sensitivity Analysis: Stability Across Subsamples')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    sensitivity_results = []\n",
    "    n_subsamples = 5\n",
    "    \n",
    "    for s in range(n_subsamples):\n",
    "        np.random.seed(s * 100 + 7)\n",
    "        sub_ids_s = np.random.choice(all_ids, size=50, replace=False)\n",
    "        df_s = df[df['id'].isin(sub_ids_s)].copy().reset_index(drop=True)\n",
    "        \n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                model_s = HonoreTrimmedEstimator(\n",
    "                    endog=df_s['spending'].values,\n",
    "                    exog=df_s[['income', 'wealth', 'household_size', 'credit_score']].values,\n",
    "                    groups=df_s['id'].values,\n",
    "                    time=df_s['time'].values,\n",
    "                    censoring_point=0.0\n",
    "                )\n",
    "                result_s = model_s.fit(verbose=False)\n",
    "                \n",
    "                row = {'Subsample': s + 1, 'Converged': result_s.converged}\n",
    "                for i, name in enumerate(var_names_honore):\n",
    "                    row[name] = result_s.params[i]\n",
    "                sensitivity_results.append(row)\n",
    "                \n",
    "        except Exception as e:\n",
    "            row = {'Subsample': s + 1, 'Converged': f'Error'}\n",
    "            for name in var_names_honore:\n",
    "                row[name] = np.nan\n",
    "            sensitivity_results.append(row)\n",
    "    \n",
    "    sens_df = pd.DataFrame(sensitivity_results)\n",
    "    display(sens_df)\n",
    "    \n",
    "    # Compute coefficient of variation across subsamples\n",
    "    print('\\nCoefficient of Variation (std/|mean|) across subsamples:')\n",
    "    for name in var_names_honore:\n",
    "        vals = sens_df[name].dropna()\n",
    "        if len(vals) > 1 and vals.mean() != 0:\n",
    "            cv = vals.std() / abs(vals.mean())\n",
    "            print(f'  {name}: {cv:.3f}')\n",
    "    \n",
    "    print('\\nLower CV indicates more stable estimates across subsamples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 When to Use the Honor\u00e9 Estimator\n",
    "\n",
    "**Use the Honor\u00e9 estimator when:**\n",
    "\n",
    "1. You suspect **correlated individual effects** ($\\alpha_i$ correlated with $\\mathbf{X}_{it}$), making the RE Tobit inconsistent.\n",
    "2. You want **minimal distributional assumptions** -- the estimator does not require normality of errors or of $\\alpha_i$.\n",
    "3. Your panel is relatively **short** ($T$ small), where the incidental parameters problem is most severe.\n",
    "4. You have a **moderate sample size** ($N$ in the hundreds, not tens of thousands).\n",
    "\n",
    "**Prefer alternatives when:**\n",
    "\n",
    "1. **RE assumptions are plausible**: If you believe $\\alpha_i \\perp \\mathbf{X}_{it}$ (e.g., based on Hausman-type tests), the RE Tobit is more efficient.\n",
    "2. **Large datasets**: The computational cost may be prohibitive for $N > 5{,}000$.\n",
    "3. **Standard errors are critical**: If you need precise confidence intervals, the RE Tobit with analytical SEs is more practical.\n",
    "4. **$T$ is large**: With large $T$, bias-corrected FE MLE approaches become viable alternatives (e.g., Hahn and Newey, 2004).\n",
    "\n",
    "### 8.5 Limitations\n",
    "\n",
    "1. **No standard errors**: Requires bootstrap, which multiplies the already high computational cost.\n",
    "2. **No intercept**: The fixed effects absorb the constant, so marginal effects at specific covariate values require additional computation.\n",
    "3. **Left-censoring only**: The current PanelBox implementation handles left-censoring at a single point.\n",
    "4. **Balanced panels preferred**: Unbalanced panels work but reduce the number of usable pairs.\n",
    "5. **Experimental status**: The implementation may change in future PanelBox versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **The Incidental Parameters Problem**\n",
    "   - Standard fixed effects MLE for the Tobit model is **inconsistent** with fixed $T$.\n",
    "   - The number of nuisance parameters ($\\alpha_i$) grows with $N$, contaminating the estimation of $\\boldsymbol{\\beta}$.\n",
    "   - This is a fundamental problem specific to **nonlinear** models -- it does not arise in linear FE.\n",
    "\n",
    "2. **Honor\u00e9's Semiparametric Solution**\n",
    "   - Uses **pairwise differencing** ($y_{it} - y_{is}$) to eliminate $\\alpha_i$.\n",
    "   - **Trims** pairs where both observations are censored (uninformative).\n",
    "   - Minimizes a **Least Absolute Deviations** objective on the trimmed differences.\n",
    "   - Consistent for $\\boldsymbol{\\beta}$ as $N \\to \\infty$ with **no distributional assumptions**.\n",
    "\n",
    "3. **PanelBox Implementation**\n",
    "   - `HonoreTrimmedEstimator(endog, exog, groups, time, censoring_point)` -- no constant in `exog`.\n",
    "   - Returns `HonoreResults` with `params`, `converged`, `n_iter`, `n_obs`, `n_entities`, `n_trimmed`.\n",
    "   - **Experimental**: issues `ExperimentalWarning`, computationally intensive.\n",
    "\n",
    "4. **Comparison with Random Effects Tobit**\n",
    "   - RE Tobit assumes $\\alpha_i \\perp \\mathbf{X}_{it}$ (strong); Honor\u00e9 does not.\n",
    "   - RE Tobit provides standard errors; Honor\u00e9 requires bootstrap.\n",
    "   - Differences between the two estimators indicate **correlated individual effects**.\n",
    "\n",
    "5. **Practical Considerations**\n",
    "   - Computational cost: $O(N \\cdot T^2)$; use subsamples for large data.\n",
    "   - No analytical standard errors; use **entity-level bootstrap**.\n",
    "   - Non-smooth optimization: convergence depends on starting values and method.\n",
    "   - Sensitivity analysis across subsamples is recommended.\n",
    "\n",
    "### PanelBox API Reference\n",
    "\n",
    "```python\n",
    "from panelbox.models.censored import HonoreTrimmedEstimator\n",
    "\n",
    "# Create model (NO constant in exog -- FE absorbs it)\n",
    "model = HonoreTrimmedEstimator(\n",
    "    endog=y,\n",
    "    exog=X,              # No constant column\n",
    "    groups=entity_ids,\n",
    "    time=time_ids,\n",
    "    censoring_point=0.0\n",
    ")\n",
    "\n",
    "# Fit (returns HonoreResults dataclass)\n",
    "result = model.fit(verbose=True)\n",
    "\n",
    "# Access results\n",
    "result.params       # Coefficient vector\n",
    "result.converged    # Boolean convergence indicator\n",
    "result.n_iter       # Number of optimization iterations\n",
    "result.n_obs        # Total observations\n",
    "result.n_entities   # Number of panel entities\n",
    "result.n_trimmed    # Number of trimmed pairwise differences\n",
    "\n",
    "# Model summary\n",
    "print(model.summary())\n",
    "```\n",
    "\n",
    "### References\n",
    "\n",
    "- Honor\u00e9, B. E. (1992). Trimmed LAD and least squares estimation of truncated and censored regression models with fixed effects. *Econometrica*, 60(3), 533-565.\n",
    "- Neyman, J., & Scott, E. L. (1948). Consistent estimates based on partially consistent observations. *Econometrica*, 16(1), 1-32.\n",
    "- Hahn, J., & Newey, W. (2004). Jackknife and analytical bias reduction for nonlinear panel models. *Econometrica*, 72(4), 1295-1319.\n",
    "- Wooldridge, J. M. (2010). *Econometric Analysis of Cross Section and Panel Data* (2nd ed.). MIT Press, Chapter 16."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
