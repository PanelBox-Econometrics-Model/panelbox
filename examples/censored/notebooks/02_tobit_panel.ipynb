{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Random Effects Tobit for Panel Data\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Explain** why panel data structure matters for censored models\n",
    "2. **Formulate** the Random Effects Tobit model and its assumptions\n",
    "3. **Estimate** Random Effects Tobit models using PanelBox\n",
    "4. **Decompose** variance into between- and within-individual components\n",
    "5. **Compare** Pooled Tobit vs Random Effects Tobit estimates\n",
    "6. **Generate** latent and censored predictions from panel Tobit models\n",
    "7. **Interpret** results with attention to the intra-class correlation\n",
    "\n",
    "### Prerequisites\n",
    "- Completed Notebook 01 (Tobit Introduction)\n",
    "- Understanding of censored regression and the Tobit model\n",
    "- Familiarity with random effects concepts in linear panel models\n",
    "\n",
    "### Duration\n",
    "- **Estimated time**: 75-90 minutes\n",
    "- **Sections**: 10 main sections\n",
    "\n",
    "### Dataset\n",
    "- **File**: `health_expenditure_panel.csv` -- Health expenditure data for 500 individuals over 4 time periods (2,000 obs)\n",
    "- **Key feature**: ~27% of observations censored at zero; repeated measurements per individual\n",
    "\n",
    "### References\n",
    "- Wooldridge, J. M. (2010). *Econometric Analysis of Cross Section and Panel Data*. MIT Press, Ch. 16.\n",
    "- Tobin, J. (1958). Estimation of relationships for limited dependent variables. *Econometrica*, 26(1), 24-36.\n",
    "- Greene, W. H. (2012). *Econometric Analysis*. 7th ed. Pearson, Ch. 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# PanelBox imports\n",
    "from panelbox.models.censored import PooledTobit, RandomEffectsTobit\n",
    "\n",
    "# Visualization configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures' / '02_tobit_panel'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables' / '02_tobit_panel'\n",
    "\n",
    "# Create output directories\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Setup complete!')\n",
    "print(f'Figures will be saved to: {FIGURES_DIR}')\n",
    "print(f'Tables will be saved to: {TABLES_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: From Cross-Section to Panel\n",
    "\n",
    "### Why Panel Data Matters for Censored Models\n",
    "\n",
    "In Notebook 01 we estimated a **Pooled Tobit** model that treats every observation as independent. But when we observe the **same individuals over multiple time periods**, ignoring this panel structure leads to several problems:\n",
    "\n",
    "1. **Unobserved heterogeneity**: Individuals differ in unobserved ways (e.g., health consciousness, genetic predisposition) that affect their expenditure in every period. Ignoring this inflates the error variance.\n",
    "\n",
    "2. **Biased standard errors**: Observations from the same individual are correlated. Treating them as independent understates standard errors, leading to spurious significance.\n",
    "\n",
    "3. **Inefficient estimation**: By not exploiting the within-individual information structure, pooled estimates are less precise than panel estimates.\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "Consider a person who consistently has high health expenditure. Part of this is explained by observables (income, age, chronic conditions), but part is due to **unobserved individual-specific factors** $\\alpha_i$ -- perhaps their risk aversion, access to specialists, or family health history.\n",
    "\n",
    "The Random Effects Tobit model explicitly accounts for this by decomposing the error into:\n",
    "- $\\alpha_i$ -- individual-specific component (constant over time)\n",
    "- $\\varepsilon_{it}$ -- idiosyncratic component (varies over time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: The Random Effects Tobit Model\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "The Random Effects Tobit model for left-censored panel data is:\n",
    "\n",
    "$$y_{it}^* = \\mathbf{X}_{it}'\\boldsymbol{\\beta} + \\alpha_i + \\varepsilon_{it}$$\n",
    "\n",
    "$$y_{it} = \\max(0, \\, y_{it}^*)$$\n",
    "\n",
    "where:\n",
    "- $y_{it}^*$ is the **latent** (uncensored) outcome for individual $i$ at time $t$\n",
    "- $y_{it}$ is the **observed** (censored) outcome\n",
    "- $\\mathbf{X}_{it}$ is the vector of explanatory variables\n",
    "- $\\boldsymbol{\\beta}$ is the coefficient vector\n",
    "- $\\alpha_i \\sim N(0, \\sigma^2_\\alpha)$ is the individual random effect\n",
    "- $\\varepsilon_{it} \\sim N(0, \\sigma^2_\\varepsilon)$ is the idiosyncratic error\n",
    "- $\\alpha_i \\perp \\varepsilon_{it}$ (independence between components)\n",
    "\n",
    "### Key Assumptions\n",
    "\n",
    "1. **Normality**: Both $\\alpha_i$ and $\\varepsilon_{it}$ are normally distributed\n",
    "2. **Independence**: $\\alpha_i$ is independent of $\\varepsilon_{it}$ and of $\\mathbf{X}_{it}$ (the RE assumption)\n",
    "3. **Strict exogeneity**: $E[\\varepsilon_{it} | \\mathbf{X}_{i1}, \\ldots, \\mathbf{X}_{iT}, \\alpha_i] = 0$\n",
    "\n",
    "### Variance Decomposition\n",
    "\n",
    "The total error variance is:\n",
    "\n",
    "$$\\text{Var}(\\alpha_i + \\varepsilon_{it}) = \\sigma^2_\\alpha + \\sigma^2_\\varepsilon$$\n",
    "\n",
    "The **intra-class correlation** (ICC) measures the fraction of total variance due to individual effects:\n",
    "\n",
    "$$\\rho = \\frac{\\sigma^2_\\alpha}{\\sigma^2_\\alpha + \\sigma^2_\\varepsilon}$$\n",
    "\n",
    "- $\\rho \\approx 0$: Little individual heterogeneity (pooled model may suffice)\n",
    "- $\\rho \\approx 1$: Most variation is between individuals (strong individual effects)\n",
    "\n",
    "### Estimation via Gauss-Hermite Quadrature\n",
    "\n",
    "The likelihood for individual $i$ requires integrating over the unobserved $\\alpha_i$:\n",
    "\n",
    "$$L_i = \\int_{-\\infty}^{\\infty} \\prod_{t=1}^{T_i} f(y_{it} | \\mathbf{X}_{it}, \\alpha_i) \\, \\phi(\\alpha_i / \\sigma_\\alpha) \\, d\\alpha_i$$\n",
    "\n",
    "This integral has no closed-form solution. PanelBox uses **Gauss-Hermite quadrature** to approximate it numerically:\n",
    "\n",
    "$$L_i \\approx \\sum_{q=1}^{Q} w_q \\prod_{t=1}^{T_i} f(y_{it} | \\mathbf{X}_{it}, \\sqrt{2}\\sigma_\\alpha \\cdot n_q)$$\n",
    "\n",
    "where $n_q$ and $w_q$ are the quadrature nodes and weights, and $Q$ is the number of quadrature points (default: 12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Loading Panel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(DATA_DIR / 'health_expenditure_panel.csv')\n",
    "\n",
    "print('Dataset shape:', df.shape)\n",
    "print(f'N individuals: {df[\"id\"].nunique()}')\n",
    "print(f'T periods: {df[\"time\"].nunique()} ({df[\"time\"].min()}-{df[\"time\"].max()})')\n",
    "print(f'Panel structure: balanced = {len(df) == df[\"id\"].nunique() * df[\"time\"].nunique()}')\n",
    "print()\n",
    "print('First rows:')\n",
    "display(df.head(12))\n",
    "\n",
    "print('\\nVariable descriptions:')\n",
    "var_desc = {\n",
    "    'id': 'Individual identifier',\n",
    "    'time': 'Time period (1-4)',\n",
    "    'expenditure': 'Health expenditure (censored at 0)',\n",
    "    'income': 'Annual income (thousands)',\n",
    "    'age': 'Age in years',\n",
    "    'chronic': 'Number of chronic conditions',\n",
    "    'insurance': 'Has health insurance (0/1)',\n",
    "    'female': 'Female indicator (0/1)',\n",
    "    'bmi': 'Body Mass Index'\n",
    "}\n",
    "for var, desc in var_desc.items():\n",
    "    print(f'  {var:15s} {desc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print('Summary Statistics')\n",
    "print('=' * 60)\n",
    "display(df.describe().round(3))\n",
    "\n",
    "# Censoring analysis\n",
    "n_censored = (df['expenditure'] == 0).sum()\n",
    "n_total = len(df)\n",
    "pct_censored = n_censored / n_total * 100\n",
    "\n",
    "print(f'\\nCensoring Summary:')\n",
    "print(f'  Total observations:    {n_total}')\n",
    "print(f'  Censored (y = 0):      {n_censored} ({pct_censored:.1f}%)')\n",
    "print(f'  Uncensored (y > 0):    {n_total - n_censored} ({100 - pct_censored:.1f}%)')\n",
    "print(f'  Mean (all):            {df[\"expenditure\"].mean():.2f}')\n",
    "print(f'  Mean (uncensored):     {df.loc[df[\"expenditure\"] > 0, \"expenditure\"].mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Exploratory Data Analysis\n",
    "\n",
    "Before estimation, we explore the panel structure focusing on **within-individual variation** and **censoring patterns over time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Distribution of expenditure: overall and by censoring\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "# 1. Overall distribution\n",
    "ax = axes[0]\n",
    "ax.hist(df['expenditure'], bins=40, color='steelblue', edgecolor='white', alpha=0.8)\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Censoring point')\n",
    "ax.set_xlabel('Health Expenditure')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Health Expenditure')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "textstr = f'N = {n_total}\\nCensored = {pct_censored:.1f}%\\nMean = {df[\"expenditure\"].mean():.2f}'\n",
    "ax.text(0.95, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', horizontalalignment='right',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# 2. Censoring pattern by time period\n",
    "ax = axes[1]\n",
    "censoring_by_time = df.groupby('time').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'pct_censored': (x['expenditure'] == 0).mean() * 100,\n",
    "        'mean_exp': x['expenditure'].mean()\n",
    "    })\n",
    ")\n",
    "bars = ax.bar(censoring_by_time.index, censoring_by_time['pct_censored'],\n",
    "              color='coral', edgecolor='black', alpha=0.8)\n",
    "ax.set_xlabel('Time Period')\n",
    "ax.set_ylabel('Percent Censored (%)')\n",
    "ax.set_title('Censoring Rate by Time Period')\n",
    "ax.set_xticks(censoring_by_time.index)\n",
    "for bar, val in zip(bars, censoring_by_time['pct_censored']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "            f'{val:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 3. Distribution of uncensored expenditure by time\n",
    "ax = axes[2]\n",
    "for t in sorted(df['time'].unique()):\n",
    "    sub = df[(df['time'] == t) & (df['expenditure'] > 0)]\n",
    "    ax.hist(sub['expenditure'], bins=25, alpha=0.4, label=f't = {t}',\n",
    "            density=True, edgecolor='white')\n",
    "ax.set_xlabel('Health Expenditure (uncensored only)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Uncensored Distribution by Period')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'expenditure_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Censoring rate by period:')\n",
    "display(censoring_by_time.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Left -- overall distribution of health expenditure showing the mass point at zero (censoring) and the continuous positive part. Center -- censoring rate is relatively stable across time periods (~27%). Right -- the distribution of positive expenditure values is similar across periods.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Within-individual variation in expenditure\n",
    "# ============================================================\n",
    "\n",
    "# Compute individual-level statistics\n",
    "individual_stats = df.groupby('id').agg(\n",
    "    mean_exp=('expenditure', 'mean'),\n",
    "    std_exp=('expenditure', 'std'),\n",
    "    min_exp=('expenditure', 'min'),\n",
    "    max_exp=('expenditure', 'max'),\n",
    "    n_censored=('expenditure', lambda x: (x == 0).sum()),\n",
    "    mean_income=('income', 'mean'),\n",
    "    mean_age=('age', 'mean'),\n",
    "    mean_chronic=('chronic', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "individual_stats['range_exp'] = individual_stats['max_exp'] - individual_stats['min_exp']\n",
    "individual_stats['cv_exp'] = individual_stats['std_exp'] / (individual_stats['mean_exp'] + 0.01)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Distribution of individual means\n",
    "ax = axes[0, 0]\n",
    "ax.hist(individual_stats['mean_exp'], bins=30, color='steelblue',\n",
    "        edgecolor='white', alpha=0.8)\n",
    "ax.axvline(individual_stats['mean_exp'].mean(), color='red', linestyle='--',\n",
    "           linewidth=2, label=f'Grand mean = {individual_stats[\"mean_exp\"].mean():.2f}')\n",
    "ax.set_xlabel('Individual Mean Expenditure')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Between-Individual Variation')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# 2. Number of censored periods per individual\n",
    "ax = axes[0, 1]\n",
    "censored_counts = individual_stats['n_censored'].value_counts().sort_index()\n",
    "bars = ax.bar(censored_counts.index, censored_counts.values,\n",
    "              color='coral', edgecolor='black', alpha=0.8)\n",
    "ax.set_xlabel('Number of Censored Periods (out of 4)')\n",
    "ax.set_ylabel('Number of Individuals')\n",
    "ax.set_title('Censoring Frequency Distribution')\n",
    "ax.set_xticks(range(5))\n",
    "for bar, val in zip(bars, censored_counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 2,\n",
    "            f'{val}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "# 3. Within-individual range vs mean\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(individual_stats['mean_exp'], individual_stats['range_exp'],\n",
    "           alpha=0.4, s=20, color='seagreen')\n",
    "ax.set_xlabel('Individual Mean Expenditure')\n",
    "ax.set_ylabel('Within-Individual Range')\n",
    "ax.set_title('Within-Individual Variation')\n",
    "\n",
    "# 4. Individual trajectories (sample of 20 individuals)\n",
    "ax = axes[1, 1]\n",
    "sample_ids = np.random.choice(df['id'].unique(), size=20, replace=False)\n",
    "for pid in sample_ids:\n",
    "    sub = df[df['id'] == pid].sort_values('time')\n",
    "    ax.plot(sub['time'], sub['expenditure'], marker='o', alpha=0.5,\n",
    "            linewidth=1.5, markersize=4)\n",
    "ax.set_xlabel('Time Period')\n",
    "ax.set_ylabel('Health Expenditure')\n",
    "ax.set_title('Individual Trajectories (sample of 20)')\n",
    "ax.set_xticks([1, 2, 3, 4])\n",
    "ax.axhline(0, color='red', linestyle=':', linewidth=1, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'within_individual_variation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print('Within-individual variation summary:')\n",
    "print(f'  Mean of individual means:   {individual_stats[\"mean_exp\"].mean():.2f}')\n",
    "print(f'  SD of individual means:     {individual_stats[\"mean_exp\"].std():.2f} (between variation)')\n",
    "print(f'  Mean within-individual SD:  {individual_stats[\"std_exp\"].mean():.2f} (within variation)')\n",
    "print(f'  Mean within-individual range: {individual_stats[\"range_exp\"].mean():.2f}')\n",
    "print(f'\\nCensoring patterns:')\n",
    "for nc in range(5):\n",
    "    n = (individual_stats['n_censored'] == nc).sum()\n",
    "    pct = n / len(individual_stats) * 100\n",
    "    print(f'  {nc} censored periods: {n} individuals ({pct:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Top-left -- substantial between-individual variation in mean expenditure motivates the random effect. Top-right -- censoring frequency shows many individuals have mixed censored/uncensored observations. Bottom-left -- within-individual range increases with mean expenditure. Bottom-right -- individual trajectories show considerable within-person variation over time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Between vs Within Variation (ANOVA decomposition)\n",
    "# ============================================================\n",
    "\n",
    "# Grand mean\n",
    "grand_mean = df['expenditure'].mean()\n",
    "\n",
    "# Between variation: variation in individual means\n",
    "individual_means = df.groupby('id')['expenditure'].transform('mean')\n",
    "ss_between = ((individual_means - grand_mean) ** 2).sum()\n",
    "\n",
    "# Within variation: deviation from individual means\n",
    "ss_within = ((df['expenditure'] - individual_means) ** 2).sum()\n",
    "\n",
    "# Total variation\n",
    "ss_total = ((df['expenditure'] - grand_mean) ** 2).sum()\n",
    "\n",
    "print('ANOVA-style Variance Decomposition')\n",
    "print('=' * 50)\n",
    "print(f'Total SS:      {ss_total:12.2f}')\n",
    "print(f'Between SS:    {ss_between:12.2f} ({ss_between/ss_total*100:.1f}%)')\n",
    "print(f'Within SS:     {ss_within:12.2f} ({ss_within/ss_total*100:.1f}%)')\n",
    "print(f'Check (B+W):   {ss_between + ss_within:12.2f}')\n",
    "print()\n",
    "print(f'Between std:   {individual_stats[\"mean_exp\"].std():.3f}')\n",
    "print(f'Within std:    {(df[\"expenditure\"] - individual_means).std():.3f}')\n",
    "print()\n",
    "print('Interpretation:')\n",
    "print(f'  {ss_between/ss_total*100:.0f}% of the variation in expenditure is BETWEEN individuals.')\n",
    "print(f'  {ss_within/ss_total*100:.0f}% is WITHIN individuals over time.')\n",
    "print(f'  => Substantial individual heterogeneity justifies the RE model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Correlations between expenditure and explanatory variables\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(17, 10))\n",
    "\n",
    "# 1. Income vs expenditure\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(df['income'], df['expenditure'], alpha=0.15, s=10, color='steelblue')\n",
    "# Add smoothed trend for positive values\n",
    "pos_mask = df['expenditure'] > 0\n",
    "z = np.polyfit(df.loc[pos_mask, 'income'], df.loc[pos_mask, 'expenditure'], 1)\n",
    "x_line = np.linspace(df['income'].min(), df['income'].max(), 100)\n",
    "ax.plot(x_line, np.polyval(z, x_line), 'r-', linewidth=2, label='Linear fit (y>0)')\n",
    "ax.set_xlabel('Income')\n",
    "ax.set_ylabel('Expenditure')\n",
    "ax.set_title('Income vs Expenditure')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# 2. Age vs expenditure\n",
    "ax = axes[0, 1]\n",
    "age_bins = pd.cut(df['age'], bins=8)\n",
    "age_means = df.groupby(age_bins)['expenditure'].mean()\n",
    "age_centers = [(interval.left + interval.right) / 2 for interval in age_means.index]\n",
    "ax.bar(range(len(age_centers)), age_means.values, color='seagreen', edgecolor='black', alpha=0.8)\n",
    "ax.set_xticks(range(len(age_centers)))\n",
    "ax.set_xticklabels([f'{c:.0f}' for c in age_centers], rotation=45)\n",
    "ax.set_xlabel('Age Group (midpoint)')\n",
    "ax.set_ylabel('Mean Expenditure')\n",
    "ax.set_title('Expenditure by Age Group')\n",
    "\n",
    "# 3. Chronic conditions vs expenditure\n",
    "ax = axes[0, 2]\n",
    "chronic_stats = df.groupby('chronic')['expenditure'].agg(['mean', 'std'])\n",
    "ax.bar(chronic_stats.index, chronic_stats['mean'], color='coral',\n",
    "       edgecolor='black', alpha=0.8)\n",
    "ax.errorbar(chronic_stats.index, chronic_stats['mean'],\n",
    "            yerr=chronic_stats['std'] / np.sqrt(df.groupby('chronic').size()),\n",
    "            fmt='none', color='black', capsize=4)\n",
    "ax.set_xlabel('Number of Chronic Conditions')\n",
    "ax.set_ylabel('Mean Expenditure')\n",
    "ax.set_title('Expenditure by Chronic Conditions')\n",
    "\n",
    "# 4. Insurance effect\n",
    "ax = axes[1, 0]\n",
    "ins_data = [df.loc[df['insurance'] == 0, 'expenditure'],\n",
    "            df.loc[df['insurance'] == 1, 'expenditure']]\n",
    "bp = ax.boxplot(ins_data, labels=['No Insurance', 'Insured'],\n",
    "                patch_artist=True, flierprops=dict(markersize=2, alpha=0.3))\n",
    "bp['boxes'][0].set_facecolor('lightcoral')\n",
    "bp['boxes'][1].set_facecolor('lightblue')\n",
    "ax.set_ylabel('Expenditure')\n",
    "ax.set_title('Expenditure by Insurance Status')\n",
    "\n",
    "# 5. Gender effect\n",
    "ax = axes[1, 1]\n",
    "gender_data = [df.loc[df['female'] == 0, 'expenditure'],\n",
    "               df.loc[df['female'] == 1, 'expenditure']]\n",
    "bp = ax.boxplot(gender_data, labels=['Male', 'Female'],\n",
    "                patch_artist=True, flierprops=dict(markersize=2, alpha=0.3))\n",
    "bp['boxes'][0].set_facecolor('lightblue')\n",
    "bp['boxes'][1].set_facecolor('lightcoral')\n",
    "ax.set_ylabel('Expenditure')\n",
    "ax.set_title('Expenditure by Gender')\n",
    "\n",
    "# 6. BMI vs expenditure\n",
    "ax = axes[1, 2]\n",
    "ax.scatter(df['bmi'], df['expenditure'], alpha=0.15, s=10, color='purple')\n",
    "ax.set_xlabel('BMI')\n",
    "ax.set_ylabel('Expenditure')\n",
    "ax.set_title('BMI vs Expenditure')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'eda_covariates.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "print('\\nCorrelation with expenditure:')\n",
    "corrs = df[['expenditure', 'income', 'age', 'chronic', 'insurance', 'female', 'bmi']].corr()['expenditure']\n",
    "for var, corr in corrs.items():\n",
    "    if var != 'expenditure':\n",
    "        print(f'  {var:15s}: {corr:+.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Exploratory plots showing the relationship between health expenditure and key covariates. Chronic conditions and age show the strongest positive associations. Insurance status and income also appear related to expenditure levels.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Estimation -- Random Effects Tobit\n",
    "\n",
    "We now estimate the Random Effects Tobit model using PanelBox. The model accounts for both:\n",
    "- **Censoring** at zero (mass point in expenditure distribution)\n",
    "- **Individual heterogeneity** via random effects $\\alpha_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Prepare data for estimation\n",
    "# ============================================================\n",
    "\n",
    "# Dependent variable\n",
    "y = df['expenditure'].values\n",
    "\n",
    "# Explanatory variables (with constant)\n",
    "X_vars = df[['income', 'age', 'chronic', 'insurance', 'female', 'bmi']].values\n",
    "X = sm.add_constant(X_vars)\n",
    "\n",
    "var_names = ['const', 'income', 'age', 'chronic', 'insurance', 'female', 'bmi']\n",
    "\n",
    "# Panel identifiers\n",
    "groups = df['id'].values\n",
    "time = df['time'].values\n",
    "\n",
    "print('Data preparation:')\n",
    "print(f'  y shape:       {y.shape}')\n",
    "print(f'  X shape:       {X.shape}')\n",
    "print(f'  groups shape:  {groups.shape}')\n",
    "print(f'  N individuals: {len(np.unique(groups))}')\n",
    "print(f'  T periods:     {len(np.unique(time))}')\n",
    "print(f'  Variables:     {var_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Estimate Random Effects Tobit\n",
    "# ============================================================\n",
    "\n",
    "print('Fitting Random Effects Tobit model...')\n",
    "print('(This may take a minute due to quadrature integration)')\n",
    "print('=' * 60)\n",
    "\n",
    "re_tobit = RandomEffectsTobit(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    groups=groups,\n",
    "    time=time,\n",
    "    censoring_point=0.0,\n",
    "    censoring_type='left',\n",
    "    quadrature_points=12\n",
    ")\n",
    "\n",
    "re_tobit.fit(method='BFGS', maxiter=1000)\n",
    "\n",
    "print()\n",
    "print(re_tobit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Detailed results extraction\n",
    "# ============================================================\n",
    "\n",
    "K = len(var_names)\n",
    "\n",
    "# Extract coefficients and standard errors\n",
    "re_beta = re_tobit.beta\n",
    "re_bse = re_tobit.bse[:K]\n",
    "re_t = re_beta / re_bse\n",
    "re_p = 2 * (1 - stats.norm.cdf(np.abs(re_t)))\n",
    "\n",
    "def add_stars(p):\n",
    "    if p < 0.001: return '***'\n",
    "    elif p < 0.01: return '**'\n",
    "    elif p < 0.05: return '*'\n",
    "    else: return ''\n",
    "\n",
    "re_table = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Coefficient': re_beta,\n",
    "    'Std. Error': re_bse,\n",
    "    'z-statistic': re_t,\n",
    "    'p-value': re_p\n",
    "})\n",
    "re_table['Sig'] = re_table['p-value'].apply(add_stars)\n",
    "\n",
    "print('Random Effects Tobit -- Coefficient Estimates')\n",
    "print('=' * 70)\n",
    "display(re_table.round(4))\n",
    "print('Significance: *** p<0.001, ** p<0.01, * p<0.05')\n",
    "\n",
    "# Save table\n",
    "re_table.to_csv(TABLES_DIR / 'table_01_re_tobit_estimates.csv', index=False)\n",
    "print(f'\\nSaved to {TABLES_DIR / \"table_01_re_tobit_estimates.csv\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Interpreting Results -- Variance Decomposition\n",
    "\n",
    "A key output of the Random Effects Tobit is the **variance decomposition** between individual-level and idiosyncratic components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Variance decomposition and intra-class correlation\n",
    "# ============================================================\n",
    "\n",
    "sigma_eps = re_tobit.sigma_eps\n",
    "sigma_alpha = re_tobit.sigma_alpha\n",
    "\n",
    "sigma2_eps = sigma_eps ** 2\n",
    "sigma2_alpha = sigma_alpha ** 2\n",
    "sigma2_total = sigma2_eps + sigma2_alpha\n",
    "\n",
    "rho = sigma2_alpha / sigma2_total  # Intra-class correlation\n",
    "\n",
    "print('Variance Decomposition')\n",
    "print('=' * 50)\n",
    "print(f'sigma_eps (idiosyncratic):  {sigma_eps:.4f}')\n",
    "print(f'sigma_alpha (individual):   {sigma_alpha:.4f}')\n",
    "print()\n",
    "print(f'sigma^2_eps:               {sigma2_eps:.4f}')\n",
    "print(f'sigma^2_alpha:             {sigma2_alpha:.4f}')\n",
    "print(f'sigma^2_total:             {sigma2_total:.4f}')\n",
    "print()\n",
    "print(f'Intra-class correlation (rho):')\n",
    "print(f'  rho = sigma^2_alpha / (sigma^2_alpha + sigma^2_eps)')\n",
    "print(f'  rho = {sigma2_alpha:.4f} / ({sigma2_alpha:.4f} + {sigma2_eps:.4f})')\n",
    "print(f'  rho = {rho:.4f}')\n",
    "print()\n",
    "print('Interpretation:')\n",
    "print(f'  {rho*100:.1f}% of the total error variance is due to')\n",
    "print(f'  individual-specific unobserved heterogeneity.')\n",
    "if rho > 0.3:\n",
    "    print(f'  => Substantial individual effects. The RE model is important.')\n",
    "elif rho > 0.1:\n",
    "    print(f'  => Moderate individual effects. RE model provides some improvement.')\n",
    "else:\n",
    "    print(f'  => Modest individual effects. Pooled model may be adequate.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize variance decomposition\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Pie chart of variance components\n",
    "ax = axes[0]\n",
    "sizes = [sigma2_alpha, sigma2_eps]\n",
    "labels = [f'Between\\n($\\\\sigma^2_\\\\alpha$ = {sigma2_alpha:.3f})',\n",
    "          f'Within\\n($\\\\sigma^2_\\\\varepsilon$ = {sigma2_eps:.3f})']\n",
    "colors_pie = ['#ff9999', '#66b3ff']\n",
    "explode = (0.05, 0)\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    sizes, labels=labels, colors=colors_pie, explode=explode,\n",
    "    autopct='%1.1f%%', startangle=90, textprops={'fontsize': 11}\n",
    ")\n",
    "for autotext in autotexts:\n",
    "    autotext.set_fontsize(12)\n",
    "    autotext.set_fontweight('bold')\n",
    "ax.set_title(f'Error Variance Decomposition\\n$\\\\rho$ = {rho:.3f}', fontsize=14)\n",
    "\n",
    "# 2. Bar chart with confidence-like display\n",
    "ax = axes[1]\n",
    "components = ['$\\\\sigma_\\\\alpha$\\n(individual)', '$\\\\sigma_\\\\varepsilon$\\n(idiosyncratic)',\n",
    "              '$\\\\sigma_{total}$\\n(total)']\n",
    "values = [sigma_alpha, sigma_eps, np.sqrt(sigma2_total)]\n",
    "colors_bar = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "\n",
    "bars = ax.bar(components, values, color=colors_bar, edgecolor='black', alpha=0.8)\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Standard Deviation')\n",
    "ax.set_title('Error Components (Standard Deviations)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'variance_decomposition.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'The intra-class correlation rho = {rho:.3f} indicates that {rho*100:.1f}%')\n",
    "print(f'of the unobserved variation is time-invariant individual heterogeneity.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Left -- pie chart showing the decomposition of error variance into between-individual and within-individual components. Right -- bar chart comparing the standard deviations of the random effect, the idiosyncratic error, and the total error.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Substantive interpretation of coefficients\n",
    "# ============================================================\n",
    "\n",
    "print('Substantive Interpretation of RE Tobit Coefficients')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print('IMPORTANT: In a Tobit model, coefficients represent the effect')\n",
    "print('on the LATENT variable y*. The effect on the observed (censored)')\n",
    "print('outcome is attenuated by the probability of being uncensored.')\n",
    "print()\n",
    "\n",
    "for i, var in enumerate(var_names[1:], 1):  # Skip constant\n",
    "    coef = re_beta[i]\n",
    "    se = re_bse[i]\n",
    "    p = re_p[i]\n",
    "    sig = add_stars(p)\n",
    "    print(f'{i}. {var} (beta = {coef:.4f}, p = {p:.4f}) {sig}')\n",
    "    \n",
    "    if var == 'income':\n",
    "        print(f'   A 1-unit increase in income (thousand $) is associated with')\n",
    "        print(f'   a {coef:.4f} change in latent health expenditure.')\n",
    "    elif var == 'age':\n",
    "        print(f'   Each additional year of age changes latent expenditure by {coef:.4f}.')\n",
    "    elif var == 'chronic':\n",
    "        print(f'   Each additional chronic condition changes latent expenditure by {coef:.4f}.')\n",
    "    elif var == 'insurance':\n",
    "        print(f'   Having health insurance changes latent expenditure by {coef:.4f}.')\n",
    "    elif var == 'female':\n",
    "        print(f'   Being female changes latent expenditure by {coef:.4f}.')\n",
    "    elif var == 'bmi':\n",
    "        print(f'   Each unit increase in BMI changes latent expenditure by {coef:.4f}.')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Pooled vs Random Effects Comparison\n",
    "\n",
    "To understand the value of accounting for panel structure, we estimate a **Pooled Tobit** model and compare it with the Random Effects specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Estimate Pooled Tobit\n",
    "# ============================================================\n",
    "\n",
    "print('Fitting Pooled Tobit model...')\n",
    "print('=' * 60)\n",
    "\n",
    "pooled_tobit = PooledTobit(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    groups=groups,\n",
    "    censoring_point=0.0,\n",
    "    censoring_type='left'\n",
    ")\n",
    "\n",
    "pooled_tobit.fit(method='BFGS', maxiter=1000)\n",
    "\n",
    "print()\n",
    "print(pooled_tobit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Side-by-side comparison\n",
    "# ============================================================\n",
    "\n",
    "pooled_beta = pooled_tobit.beta\n",
    "pooled_bse = pooled_tobit.bse[:K]\n",
    "pooled_t = pooled_beta / pooled_bse\n",
    "pooled_p = 2 * (1 - stats.norm.cdf(np.abs(pooled_t)))\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Pooled_Coef': pooled_beta,\n",
    "    'Pooled_SE': pooled_bse,\n",
    "    'RE_Coef': re_beta,\n",
    "    'RE_SE': re_bse,\n",
    "    'Coef_Diff': re_beta - pooled_beta,\n",
    "    'SE_Ratio': re_bse / pooled_bse\n",
    "})\n",
    "\n",
    "print('Coefficient Comparison: Pooled Tobit vs Random Effects Tobit')\n",
    "print('=' * 85)\n",
    "display(comparison.round(4))\n",
    "\n",
    "print(f'\\nModel fit comparison:')\n",
    "print(f'  Pooled Tobit log-likelihood:   {pooled_tobit.llf:.2f}')\n",
    "print(f'  RE Tobit log-likelihood:       {re_tobit.llf:.2f}')\n",
    "print(f'  Difference:                    {re_tobit.llf - pooled_tobit.llf:.2f}')\n",
    "print(f'  Pooled sigma:                  {pooled_tobit.sigma:.4f}')\n",
    "print(f'  RE sigma_eps:                  {re_tobit.sigma_eps:.4f}')\n",
    "print(f'  RE sigma_alpha:                {re_tobit.sigma_alpha:.4f}')\n",
    "\n",
    "# Save comparison table\n",
    "comparison.to_csv(TABLES_DIR / 'table_02_pooled_vs_re.csv', index=False)\n",
    "print(f'\\nSaved to {TABLES_DIR / \"table_02_pooled_vs_re.csv\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visual comparison: Forest plot\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "n_vars = len(var_names) - 1  # Exclude constant\n",
    "y_pos = np.arange(n_vars)\n",
    "var_labels = var_names[1:]\n",
    "\n",
    "# Plot Pooled\n",
    "ax.errorbar(pooled_beta[1:], y_pos + 0.12,\n",
    "            xerr=1.96 * pooled_bse[1:],\n",
    "            fmt='o', color='coral', markersize=9, capsize=5,\n",
    "            label='Pooled Tobit', linewidth=2)\n",
    "\n",
    "# Plot RE\n",
    "ax.errorbar(re_beta[1:], y_pos - 0.12,\n",
    "            xerr=1.96 * re_bse[1:],\n",
    "            fmt='s', color='steelblue', markersize=9, capsize=5,\n",
    "            label='RE Tobit', linewidth=2)\n",
    "\n",
    "ax.axvline(x=0, color='gray', linestyle='--', linewidth=1)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(var_labels, fontsize=12)\n",
    "ax.set_xlabel('Coefficient (95% CI)', fontsize=12)\n",
    "ax.set_title('Pooled Tobit vs Random Effects Tobit\\nCoefficient Comparison', fontsize=14)\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'coefficient_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Key observations:')\n",
    "print(f'  - Average SE ratio (RE/Pooled): {comparison[\"SE_Ratio\"].iloc[1:].mean():.2f}')\n",
    "print(f'  - RE model generally has different SEs due to variance decomposition')\n",
    "print(f'  - Coefficient magnitudes may shift when accounting for individual effects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Forest plot comparing coefficient estimates and 95% confidence intervals between Pooled Tobit and Random Effects Tobit. Differences in both point estimates and interval widths reflect the impact of accounting for individual-level unobserved heterogeneity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Likelihood ratio test: Pooled vs RE\n",
    "# ============================================================\n",
    "\n",
    "# Under H0: sigma_alpha = 0 (pooled model is adequate)\n",
    "# LR = 2 * (LL_RE - LL_Pooled)\n",
    "# Note: sigma_alpha >= 0 is on the boundary, so the test uses a\n",
    "# mixture of chi2(0) and chi2(1) => halve the p-value.\n",
    "\n",
    "LR = 2 * (re_tobit.llf - pooled_tobit.llf)\n",
    "p_value_lr = 0.5 * (1 - stats.chi2.cdf(LR, df=1))  # One-sided boundary test\n",
    "\n",
    "print('Likelihood Ratio Test: Pooled vs Random Effects Tobit')\n",
    "print('=' * 60)\n",
    "print(f'H0: sigma_alpha = 0 (no individual effects)')\n",
    "print(f'H1: sigma_alpha > 0 (individual effects present)')\n",
    "print()\n",
    "print(f'Log-likelihood (Pooled):   {pooled_tobit.llf:.2f}')\n",
    "print(f'Log-likelihood (RE):       {re_tobit.llf:.2f}')\n",
    "print(f'LR statistic:              {LR:.2f}')\n",
    "print(f'p-value (boundary test):   {p_value_lr:.2e}')\n",
    "print()\n",
    "\n",
    "if p_value_lr < 0.001:\n",
    "    print('Conclusion: STRONGLY reject pooled model (p < 0.001).')\n",
    "    print('The Random Effects Tobit is significantly preferred.')\n",
    "elif p_value_lr < 0.05:\n",
    "    print('Conclusion: Reject pooled model at 5% level.')\n",
    "    print('The Random Effects Tobit provides a better fit.')\n",
    "else:\n",
    "    print('Conclusion: Fail to reject pooled model.')\n",
    "    print('Individual effects may not be important.')\n",
    "\n",
    "print()\n",
    "print('Note: The p-value is halved because sigma_alpha is tested on the')\n",
    "print('boundary of the parameter space (sigma_alpha >= 0). This follows')\n",
    "print('the standard approach for testing variance components.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Comprehensive model comparison table\n",
    "# ============================================================\n",
    "\n",
    "# AIC/BIC\n",
    "k_pooled = K + 1  # betas + sigma\n",
    "k_re = K + 2       # betas + sigma_eps + sigma_alpha\n",
    "\n",
    "aic_pooled = -2 * pooled_tobit.llf + 2 * k_pooled\n",
    "bic_pooled = -2 * pooled_tobit.llf + np.log(len(y)) * k_pooled\n",
    "\n",
    "aic_re = -2 * re_tobit.llf + 2 * k_re\n",
    "bic_re = -2 * re_tobit.llf + np.log(len(y)) * k_re\n",
    "\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Metric': ['Log-Likelihood', 'Parameters', 'AIC', 'BIC',\n",
    "               'sigma (total)', 'sigma_eps', 'sigma_alpha', 'rho (ICC)',\n",
    "               'LR test stat', 'LR p-value', 'Converged'],\n",
    "    'Pooled Tobit': [\n",
    "        f'{pooled_tobit.llf:.2f}', k_pooled, f'{aic_pooled:.2f}', f'{bic_pooled:.2f}',\n",
    "        f'{pooled_tobit.sigma:.4f}', '--', '--', '--',\n",
    "        '--', '--', str(pooled_tobit.converged)\n",
    "    ],\n",
    "    'RE Tobit': [\n",
    "        f'{re_tobit.llf:.2f}', k_re, f'{aic_re:.2f}', f'{bic_re:.2f}',\n",
    "        f'{np.sqrt(sigma2_total):.4f}', f'{sigma_eps:.4f}',\n",
    "        f'{sigma_alpha:.4f}', f'{rho:.4f}',\n",
    "        f'{LR:.2f}', f'{p_value_lr:.2e}', str(re_tobit.converged)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print('Comprehensive Model Comparison')\n",
    "print('=' * 70)\n",
    "display(model_comparison)\n",
    "\n",
    "print(f'\\nAIC difference (Pooled - RE): {aic_pooled - aic_re:.1f} (lower is better)')\n",
    "print(f'BIC difference (Pooled - RE): {bic_pooled - bic_re:.1f} (lower is better)')\n",
    "\n",
    "model_comparison.to_csv(TABLES_DIR / 'table_03_model_comparison.csv', index=False)\n",
    "print(f'\\nSaved to {TABLES_DIR / \"table_03_model_comparison.csv\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Predictions -- Latent vs Censored\n",
    "\n",
    "The Random Effects Tobit model provides two types of predictions:\n",
    "\n",
    "1. **Latent predictions**: $E[y^* | \\mathbf{X}] = \\mathbf{X}'\\hat{\\boldsymbol{\\beta}}$ -- the expected value of the underlying latent variable (can be negative)\n",
    "\n",
    "2. **Censored predictions**: $E[y | \\mathbf{X}]$ -- the expected value accounting for the censoring mechanism (always $\\geq 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate predictions\n",
    "# ============================================================\n",
    "\n",
    "# RE Tobit predictions\n",
    "y_pred_latent = re_tobit.predict(pred_type='latent')\n",
    "y_pred_censored = re_tobit.predict(pred_type='censored')\n",
    "\n",
    "# Pooled Tobit predictions for comparison\n",
    "y_pooled_latent = pooled_tobit.predict(pred_type='latent')\n",
    "y_pooled_censored = pooled_tobit.predict(pred_type='censored')\n",
    "\n",
    "print('Prediction Summary')\n",
    "print('=' * 60)\n",
    "print(f'{\"\":20s} {\"Latent\":>12s} {\"Censored\":>12s}')\n",
    "print('-' * 44)\n",
    "print(f'{\"RE Tobit mean\":20s} {y_pred_latent.mean():>12.3f} {y_pred_censored.mean():>12.3f}')\n",
    "print(f'{\"RE Tobit min\":20s} {y_pred_latent.min():>12.3f} {y_pred_censored.min():>12.3f}')\n",
    "print(f'{\"RE Tobit max\":20s} {y_pred_latent.max():>12.3f} {y_pred_censored.max():>12.3f}')\n",
    "print(f'{\"Pooled Tobit mean\":20s} {y_pooled_latent.mean():>12.3f} {y_pooled_censored.mean():>12.3f}')\n",
    "print(f'{\"Observed mean\":20s} {\"\":>12s} {y.mean():>12.3f}')\n",
    "print()\n",
    "print('Note: Latent predictions can be negative (they predict y*,')\n",
    "print('the uncensored latent variable). Censored predictions account')\n",
    "print('for the probability of censoring and are always non-negative.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization: Latent vs Censored predictions\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 11))\n",
    "\n",
    "# 1. Latent predictions vs observed\n",
    "ax = axes[0, 0]\n",
    "jitter = np.random.uniform(-0.2, 0.2, len(y))\n",
    "ax.scatter(y_pred_latent, y + jitter, alpha=0.1, s=8, color='steelblue')\n",
    "lims = [min(y_pred_latent.min(), y.min()) - 1, max(y_pred_latent.max(), y.max()) + 1]\n",
    "ax.plot(lims, lims, 'r--', linewidth=2, label='45-degree line')\n",
    "ax.axvline(0, color='orange', linestyle=':', linewidth=1.5, alpha=0.7, label='Censoring point')\n",
    "ax.axhline(0, color='orange', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "ax.set_xlabel('Latent Prediction (y*)', fontsize=12)\n",
    "ax.set_ylabel('Observed Expenditure', fontsize=12)\n",
    "ax.set_title('RE Tobit: Latent Predictions vs Observed', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "corr_latent = np.corrcoef(y_pred_latent, y)[0, 1]\n",
    "ax.text(0.05, 0.95, f'Corr = {corr_latent:.3f}', transform=ax.transAxes,\n",
    "        fontsize=11, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# 2. Censored predictions vs observed\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(y_pred_censored, y + jitter, alpha=0.1, s=8, color='seagreen')\n",
    "max_val = max(y_pred_censored.max(), y.max())\n",
    "ax.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='45-degree line')\n",
    "ax.set_xlabel('Censored Prediction E[y|X]', fontsize=12)\n",
    "ax.set_ylabel('Observed Expenditure', fontsize=12)\n",
    "ax.set_title('RE Tobit: Censored Predictions vs Observed', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "corr_cens = np.corrcoef(y_pred_censored, y)[0, 1]\n",
    "ax.text(0.05, 0.95, f'Corr = {corr_cens:.3f}', transform=ax.transAxes,\n",
    "        fontsize=11, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# 3. Distribution of predictions\n",
    "ax = axes[1, 0]\n",
    "ax.hist(y_pred_latent, bins=40, alpha=0.6, color='steelblue',\n",
    "        edgecolor='white', label='Latent (y*)', density=True)\n",
    "ax.hist(y_pred_censored, bins=40, alpha=0.6, color='seagreen',\n",
    "        edgecolor='white', label='Censored E[y|X]', density=True)\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Censoring point')\n",
    "ax.set_xlabel('Predicted Value')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Distribution of Predictions')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# 4. Pooled vs RE censored predictions\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(y_pooled_censored, y_pred_censored, alpha=0.15, s=8, color='purple')\n",
    "max_pred = max(y_pooled_censored.max(), y_pred_censored.max())\n",
    "ax.plot([0, max_pred], [0, max_pred], 'r--', linewidth=2, label='45-degree line')\n",
    "ax.set_xlabel('Pooled Tobit Prediction', fontsize=12)\n",
    "ax.set_ylabel('RE Tobit Prediction', fontsize=12)\n",
    "ax.set_title('Pooled vs RE Predictions', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "corr_models = np.corrcoef(y_pooled_censored, y_pred_censored)[0, 1]\n",
    "ax.text(0.05, 0.95, f'Corr = {corr_models:.3f}', transform=ax.transAxes,\n",
    "        fontsize=11, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'predictions_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Prediction accuracy:')\n",
    "print(f'  Correlation (latent, observed):            {corr_latent:.4f}')\n",
    "print(f'  Correlation (censored, observed):          {corr_cens:.4f}')\n",
    "print(f'  Correlation (pooled pred, RE pred):        {corr_models:.4f}')\n",
    "print(f'  RMSE (RE censored):  {np.sqrt(np.mean((y - y_pred_censored)**2)):.3f}')\n",
    "print(f'  RMSE (Pooled cens.): {np.sqrt(np.mean((y - y_pooled_censored)**2)):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Top-left -- latent predictions versus observed values; some latent predictions are negative for individuals at the censoring boundary. Top-right -- censored predictions versus observed values showing the non-negative predictions. Bottom-left -- distribution comparison of latent (can be negative) versus censored predictions. Bottom-right -- Pooled and RE censored predictions are correlated but differ, reflecting the impact of the random effect.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Residual analysis\n",
    "# ============================================================\n",
    "\n",
    "# Residuals from censored predictions\n",
    "residuals_re = y - y_pred_censored\n",
    "residuals_pooled = y - y_pooled_censored\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "# 1. Residuals vs fitted (RE)\n",
    "ax = axes[0]\n",
    "ax.scatter(y_pred_censored, residuals_re, alpha=0.1, s=8, color='steelblue')\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=1.5)\n",
    "ax.set_xlabel('Fitted Values (censored)', fontsize=12)\n",
    "ax.set_ylabel('Residuals', fontsize=12)\n",
    "ax.set_title('RE Tobit: Residuals vs Fitted', fontsize=13)\n",
    "\n",
    "# 2. Residual distribution\n",
    "ax = axes[1]\n",
    "ax.hist(residuals_re, bins=40, color='steelblue', edgecolor='white',\n",
    "        alpha=0.8, density=True, label='RE residuals')\n",
    "ax.hist(residuals_pooled, bins=40, color='coral', edgecolor='white',\n",
    "        alpha=0.5, density=True, label='Pooled residuals')\n",
    "ax.set_xlabel('Residual')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Residual Distributions')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# 3. Mean residual by time period\n",
    "ax = axes[2]\n",
    "resid_by_time = pd.DataFrame({\n",
    "    'time': time,\n",
    "    'resid_RE': residuals_re,\n",
    "    'resid_Pooled': residuals_pooled\n",
    "}).groupby('time')[['resid_RE', 'resid_Pooled']].mean()\n",
    "\n",
    "x_pos = np.arange(len(resid_by_time))\n",
    "width = 0.35\n",
    "ax.bar(x_pos - width/2, resid_by_time['resid_RE'], width,\n",
    "       label='RE Tobit', color='steelblue', alpha=0.8)\n",
    "ax.bar(x_pos + width/2, resid_by_time['resid_Pooled'], width,\n",
    "       label='Pooled Tobit', color='coral', alpha=0.8)\n",
    "ax.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xlabel('Time Period')\n",
    "ax.set_ylabel('Mean Residual')\n",
    "ax.set_title('Mean Residual by Period')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(resid_by_time.index)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'residual_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Residual summary:')\n",
    "print(f'  RE Tobit  -- Mean: {residuals_re.mean():.4f}, SD: {residuals_re.std():.4f}')\n",
    "print(f'  Pooled    -- Mean: {residuals_pooled.mean():.4f}, SD: {residuals_pooled.std():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Left -- residuals versus fitted values for the RE Tobit model. Center -- residual distribution comparison between RE and Pooled Tobit. Right -- mean residual by time period; smaller residuals indicate better fit.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Panel structure matters for censored models**: Ignoring repeated measurements from the same individual leads to incorrect standard errors and potentially biased estimates.\n",
    "\n",
    "2. **The Random Effects Tobit model** decomposes the error into:\n",
    "   - $\\alpha_i \\sim N(0, \\sigma^2_\\alpha)$ -- time-invariant individual effect\n",
    "   - $\\varepsilon_{it} \\sim N(0, \\sigma^2_\\varepsilon)$ -- idiosyncratic error\n",
    "\n",
    "3. **Intra-class correlation** $\\rho = \\sigma^2_\\alpha / (\\sigma^2_\\alpha + \\sigma^2_\\varepsilon)$ quantifies the importance of individual heterogeneity.\n",
    "\n",
    "4. **Gauss-Hermite quadrature** is the numerical method used to integrate out the random effect in the likelihood.\n",
    "\n",
    "5. **The likelihood ratio test** provides a formal comparison between pooled and RE specifications.\n",
    "\n",
    "6. **Two types of predictions** are available:\n",
    "   - Latent: $E[y^*|X]$ -- can be negative\n",
    "   - Censored: $E[y|X]$ -- accounts for censoring, always $\\geq 0$\n",
    "\n",
    "### PanelBox Workflow Summary\n",
    "\n",
    "```python\n",
    "# Step 1: Prepare data\n",
    "X = sm.add_constant(df[['income', 'age', 'chronic', 'insurance', 'female', 'bmi']].values)\n",
    "y = df['expenditure'].values\n",
    "\n",
    "# Step 2: Estimate RE Tobit\n",
    "model = RandomEffectsTobit(\n",
    "    endog=y, exog=X, groups=df['id'].values, time=df['time'].values,\n",
    "    censoring_point=0.0, censoring_type='left', quadrature_points=12\n",
    ")\n",
    "model.fit(method='BFGS', maxiter=1000)\n",
    "\n",
    "# Step 3: View results\n",
    "print(model.summary())\n",
    "\n",
    "# Step 4: Variance decomposition\n",
    "rho = model.sigma_alpha**2 / (model.sigma_alpha**2 + model.sigma_eps**2)\n",
    "\n",
    "# Step 5: Predictions\n",
    "y_latent = model.predict(pred_type='latent')\n",
    "y_censored = model.predict(pred_type='censored')\n",
    "```\n",
    "\n",
    "### Assumptions and Limitations\n",
    "\n",
    "- **RE assumption**: $\\alpha_i$ is uncorrelated with $\\mathbf{X}_{it}$. If violated, consider the Honore semiparametric estimator (see Notebook 03).\n",
    "- **Normality**: Both error components are assumed normal. This is stronger than in linear RE models.\n",
    "- **Strict exogeneity**: Covariates cannot be affected by past values of $y_{it}$.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- **Notebook 03**: Semiparametric alternatives -- the Honore trimmed estimator for fixed effects Tobit\n",
    "- **Notebook 04**: Marginal effects in censored models -- converting latent-variable coefficients to interpretable effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: Exercises\n",
    "\n",
    "### Exercise 1: Quadrature Sensitivity\n",
    "\n",
    "Re-estimate the Random Effects Tobit model using different numbers of quadrature points: 6, 12, and 24. Compare the estimated coefficients, $\\sigma_\\alpha$, $\\sigma_\\varepsilon$, and log-likelihood. At what point do the results stabilize?\n",
    "\n",
    "### Exercise 2: Subsample Analysis by Gender\n",
    "\n",
    "Estimate separate RE Tobit models for males and females. Compare:\n",
    "- Do the key coefficients (income, chronic conditions) differ across genders?\n",
    "- Is the intra-class correlation $\\rho$ different for males vs females?\n",
    "- What does this suggest about gender-specific health expenditure dynamics?\n",
    "\n",
    "### Exercise 3: Marginal Effects\n",
    "\n",
    "Using the fitted RE Tobit model, compute the marginal effect of adding one chronic condition on:\n",
    "- The latent expenditure $y^*$\n",
    "- The expected observed expenditure $E[y|X]$\n",
    "- The probability of positive expenditure $P(y > 0|X)$\n",
    "\n",
    "Hint: Use `model.marginal_effects(at='overall', which='unconditional')` and compare with `which='conditional'`.\n",
    "\n",
    "### Exercise 4: Prediction Performance\n",
    "\n",
    "Split the data into training (periods 1-3) and test (period 4) sets. Estimate both Pooled and RE Tobit on the training data. Compare prediction accuracy (RMSE, MAE) on the test set. Does the RE model generalize better to out-of-sample data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise Solutions (template)\n",
    "# ============================================================\n",
    "\n",
    "# Exercise 1: Quadrature sensitivity\n",
    "# Hint:\n",
    "# for nq in [6, 12, 24]:\n",
    "#     model_q = RandomEffectsTobit(\n",
    "#         endog=y, exog=X, groups=groups, time=time,\n",
    "#         censoring_point=0.0, quadrature_points=nq\n",
    "#     )\n",
    "#     model_q.fit(method='BFGS', maxiter=1000)\n",
    "#     print(f'Q={nq}: LL={model_q.llf:.2f}, sigma_alpha={model_q.sigma_alpha:.4f}')\n",
    "\n",
    "# Exercise 2: Subsample by gender\n",
    "# Hint:\n",
    "# mask_female = df['female'].values == 1\n",
    "# model_f = RandomEffectsTobit(\n",
    "#     endog=y[mask_female], exog=X[mask_female],\n",
    "#     groups=groups[mask_female], time=time[mask_female],\n",
    "#     censoring_point=0.0\n",
    "# )\n",
    "# model_f.fit(method='BFGS', maxiter=1000)\n",
    "# rho_f = model_f.sigma_alpha**2 / (model_f.sigma_alpha**2 + model_f.sigma_eps**2)\n",
    "\n",
    "# Exercise 3: Marginal effects\n",
    "# Hint:\n",
    "# me_uncond = re_tobit.marginal_effects(at='overall', which='unconditional')\n",
    "# me_cond = re_tobit.marginal_effects(at='overall', which='conditional')\n",
    "# print(me_uncond.summary())\n",
    "# print(me_cond.summary())\n",
    "\n",
    "# Exercise 4: Train/test split\n",
    "# Hint:\n",
    "# train_mask = df['time'].values <= 3\n",
    "# test_mask = df['time'].values == 4\n",
    "# model_train = RandomEffectsTobit(\n",
    "#     endog=y[train_mask], exog=X[train_mask],\n",
    "#     groups=groups[train_mask], time=time[train_mask],\n",
    "#     censoring_point=0.0\n",
    "# )\n",
    "# model_train.fit(method='BFGS', maxiter=1000)\n",
    "# y_test_pred = model_train.predict(exog=X[test_mask], pred_type='censored')\n",
    "# rmse_test = np.sqrt(np.mean((y[test_mask] - y_test_pred)**2))\n",
    "\n",
    "print('Complete the exercises above to deepen your understanding!')\n",
    "print('Solutions are available in the solutions directory.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
