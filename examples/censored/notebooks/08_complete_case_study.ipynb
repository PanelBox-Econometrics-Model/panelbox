{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Case Study: Health Expenditure and Labor Supply\n",
    "## Integrating Tobit and Heckman Models with PanelBox\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. Apply a complete censored data modeling workflow to a real research question\n",
    "2. Systematically compare OLS, Pooled Tobit, and Random Effects Tobit\n",
    "3. Compute and interpret McDonald-Moffitt marginal effects decomposition\n",
    "4. Detect and correct sample selection bias with the Heckman two-step estimator\n",
    "5. Conduct robustness and sensitivity analyses across multiple specifications\n",
    "6. Produce publication-quality tables and figures summarizing results\n",
    "\n",
    "### Duration\n",
    "\n",
    "~90-120 minutes (advanced level)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Notebooks 01-07 of this tutorial series (Tobit fundamentals through Heckman selection)\n",
    "- Familiarity with maximum likelihood estimation and panel data concepts\n",
    "- Working knowledge of NumPy, Pandas, and Matplotlib\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "| Part | Topic | Duration |\n",
    "|------|-------|----------|\n",
    "| I | Case Study Overview & Data Loading | 10 min |\n",
    "| II | Descriptive Analysis | 10 min |\n",
    "| III | Naive OLS Analysis | 10 min |\n",
    "| IV | Pooled Tobit | 10 min |\n",
    "| V | Random Effects Tobit | 10 min |\n",
    "| VI | Model Comparison | 10 min |\n",
    "| VII | Marginal Effects (McDonald-Moffitt) | 15 min |\n",
    "| VIII | Heckman Selection Analysis | 15 min |\n",
    "| IX | Sensitivity Analysis | 10 min |\n",
    "| X | Results Summary & Policy Implications | 10 min |\n",
    "| -- | Exercises | 15 min |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Setup\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from panelbox.models.censored import PooledTobit, RandomEffectsTobit\n",
    "from panelbox.models.selection import PanelHeckman\n",
    "from panelbox.marginal_effects.censored_me import compute_tobit_ame, compute_tobit_mem\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, str(BASE_DIR / 'utils'))\n",
    "from comparison_tools import compare_tobit_ols\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part I: Case Study Overview and Data Loading (10 min)\n",
    "\n",
    "### Research Context\n",
    "\n",
    "Health expenditure is a classic example of **censored data**: many individuals report zero\n",
    "spending in a given period, either because they did not need medical care or because\n",
    "they could not afford it. Simply discarding zeros or ignoring the censoring mechanism\n",
    "biases regression estimates toward zero (**attenuation bias**).\n",
    "\n",
    "This case study walks through the full analysis pipeline:\n",
    "\n",
    "1. **OLS baseline** -- demonstrates the problem with ignoring censoring\n",
    "2. **Pooled Tobit** -- corrects for censoring, but ignores individual heterogeneity\n",
    "3. **Random Effects Tobit** -- accounts for both censoring *and* unobserved individual effects\n",
    "4. **Marginal effects** -- translates Tobit coefficients into interpretable quantities\n",
    "5. **Heckman selection** -- investigates whether *selecting into positive spending* itself introduces bias\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "1. What are the key determinants of health expenditure?\n",
    "2. How large is the attenuation bias from ignoring censoring?\n",
    "3. Does accounting for individual heterogeneity change substantive conclusions?\n",
    "4. Is there evidence of sample selection bias in observed spending?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load the health expenditure panel\n",
    "# ============================================================\n",
    "\n",
    "df = pd.read_csv(DATA_DIR / 'health_expenditure_panel.csv')\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Individuals: {df[\"id\"].nunique()}')\n",
    "print(f'Time periods: {df[\"time\"].nunique()} (t = {df[\"time\"].min()} .. {df[\"time\"].max()})')\n",
    "print(f'\\nColumns: {list(df.columns)}')\n",
    "print(f'\\nFirst 5 rows:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Quick variable overview\n",
    "# ============================================================\n",
    "\n",
    "variable_descriptions = {\n",
    "    'id': 'Individual identifier',\n",
    "    'time': 'Time period',\n",
    "    'expenditure': 'Health expenditure (censored at 0)',\n",
    "    'income': 'Household income (thousands)',\n",
    "    'age': 'Age of individual (years)',\n",
    "    'chronic': 'Number of chronic conditions',\n",
    "    'insurance': 'Health insurance indicator (1 = insured)',\n",
    "    'female': 'Female indicator (1 = female)',\n",
    "    'bmi': 'Body mass index',\n",
    "}\n",
    "\n",
    "for var, desc in variable_descriptions.items():\n",
    "    print(f'  {var:15s}  {desc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part II: Descriptive Analysis (10 min)\n",
    "\n",
    "Before fitting any model, we need to understand the data thoroughly:\n",
    "\n",
    "- What fraction of observations are censored at zero?\n",
    "- How does spending vary across individuals and time?\n",
    "- Are there clear patterns by insurance status or chronic conditions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Table 01: Summary statistics\n",
    "# ============================================================\n",
    "\n",
    "summary_stats = df.describe().T\n",
    "summary_stats['zeros'] = (df == 0).sum()\n",
    "summary_stats['pct_zeros'] = (df == 0).mean() * 100\n",
    "summary_stats['skewness'] = df.skew()\n",
    "\n",
    "print('Table 01: Summary Statistics')\n",
    "print('=' * 80)\n",
    "display(summary_stats.round(3))\n",
    "\n",
    "# Save\n",
    "summary_stats.round(3).to_csv(TABLES_DIR / 'table01_summary_statistics.csv')\n",
    "\n",
    "# Key censoring statistic\n",
    "n_censored = (df['expenditure'] == 0).sum()\n",
    "pct_censored = n_censored / len(df) * 100\n",
    "print(f'\\nCensored observations (expenditure = 0): {n_censored} ({pct_censored:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Figure 01: Distribution of health expenditure\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Panel A: Full histogram including zeros\n",
    "axes[0].hist(df['expenditure'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', linewidth=1.5, label='Censoring point')\n",
    "axes[0].set_xlabel('Health Expenditure')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('A. Full Distribution (with zero pile-up)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Annotate the zero pile\n",
    "axes[0].annotate(\n",
    "    f'{pct_censored:.1f}% at zero',\n",
    "    xy=(0, n_censored * 0.6), xytext=(df['expenditure'].max() * 0.4, n_censored * 0.8),\n",
    "    arrowprops=dict(arrowstyle='->', color='red'),\n",
    "    fontsize=11, color='red', fontweight='bold'\n",
    ")\n",
    "\n",
    "# Panel B: Positive expenditure only\n",
    "positive = df.loc[df['expenditure'] > 0, 'expenditure']\n",
    "axes[1].hist(positive, bins=40, edgecolor='black', alpha=0.7, color='darkorange')\n",
    "axes[1].set_xlabel('Health Expenditure')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('B. Positive Expenditure Only')\n",
    "\n",
    "# Panel C: Log of positive expenditure\n",
    "axes[2].hist(np.log1p(positive), bins=40, edgecolor='black', alpha=0.7, color='seagreen')\n",
    "axes[2].set_xlabel('log(1 + Expenditure)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('C. Log-Transformed (positive only)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig01_expenditure_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('*Figure 01: Distribution of health expenditure showing the characteristic '\n",
    "      'zero pile-up (Panel A), the continuous positive mass (Panel B), '\n",
    "      'and the approximately log-normal shape of positive expenditures (Panel C).*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Figure 02: Panel structure -- spaghetti plot\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Select a random subset of individuals for readability\n",
    "sample_ids = np.random.choice(df['id'].unique(), size=min(20, df['id'].nunique()), replace=False)\n",
    "df_sample = df[df['id'].isin(sample_ids)]\n",
    "\n",
    "# Panel A: Spaghetti plot of expenditure trajectories\n",
    "for pid in sample_ids:\n",
    "    person = df_sample[df_sample['id'] == pid]\n",
    "    axes[0].plot(person['time'], person['expenditure'], marker='o', alpha=0.6, markersize=4)\n",
    "\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.7)\n",
    "axes[0].set_xlabel('Time Period')\n",
    "axes[0].set_ylabel('Health Expenditure')\n",
    "axes[0].set_title('A. Individual Expenditure Trajectories (n=20 sample)')\n",
    "\n",
    "# Panel B: Fraction censored per period\n",
    "censor_by_time = df.groupby('time').apply(lambda g: (g['expenditure'] == 0).mean())\n",
    "axes[1].bar(censor_by_time.index, censor_by_time.values, color='salmon', edgecolor='black')\n",
    "axes[1].set_xlabel('Time Period')\n",
    "axes[1].set_ylabel('Fraction Censored')\n",
    "axes[1].set_title('B. Censoring Rate by Period')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig02_panel_structure.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('*Figure 02: Panel structure of the health expenditure data. Panel A shows individual '\n",
    "      'expenditure trajectories over time with substantial within-individual variation and '\n",
    "      'frequent censoring at zero. Panel B displays the censoring rate by time period.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Censoring patterns by covariates\n",
    "# ============================================================\n",
    "\n",
    "print('Censoring rate by subgroup:')\n",
    "print('-' * 50)\n",
    "\n",
    "# By insurance status\n",
    "for ins_val in [0, 1]:\n",
    "    mask = df['insurance'] == ins_val\n",
    "    rate = (df.loc[mask, 'expenditure'] == 0).mean()\n",
    "    label = 'Insured' if ins_val == 1 else 'Uninsured'\n",
    "    print(f'  {label:20s}: {rate:.1%}')\n",
    "\n",
    "print()\n",
    "\n",
    "# By gender\n",
    "for fem_val in [0, 1]:\n",
    "    mask = df['female'] == fem_val\n",
    "    rate = (df.loc[mask, 'expenditure'] == 0).mean()\n",
    "    label = 'Female' if fem_val == 1 else 'Male'\n",
    "    print(f'  {label:20s}: {rate:.1%}')\n",
    "\n",
    "print()\n",
    "\n",
    "# By chronic condition count (binned)\n",
    "for lo, hi, label in [(0, 0, '0 chronic'), (1, 2, '1-2 chronic'), (3, 99, '3+ chronic')]:\n",
    "    mask = (df['chronic'] >= lo) & (df['chronic'] <= hi)\n",
    "    if mask.sum() > 0:\n",
    "        rate = (df.loc[mask, 'expenditure'] == 0).mean()\n",
    "        print(f'  {label:20s}: {rate:.1%}  (n={mask.sum()})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part III: Naive OLS Analysis (10 min)\n",
    "\n",
    "We begin with ordinary least squares as a **baseline**. This intentionally ignores the\n",
    "censoring at zero and will produce **attenuated** (biased toward zero) coefficient estimates.\n",
    "The purpose is to establish what happens when censoring is neglected.\n",
    "\n",
    "**Key insight**: OLS treats zero-expenditure observations as informative about the\n",
    "relationship between covariates and spending, when in fact those observations are\n",
    "constrained by the lower bound, not by the covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Prepare model matrices\n",
    "# ============================================================\n",
    "\n",
    "# Variable names for our models\n",
    "depvar = 'expenditure'\n",
    "covariates = ['income', 'age', 'chronic', 'insurance', 'female', 'bmi']\n",
    "\n",
    "y = df[depvar].values\n",
    "X_raw = df[covariates].values\n",
    "\n",
    "# Add constant for OLS and Tobit\n",
    "X = sm.add_constant(X_raw)\n",
    "var_names = ['const'] + covariates\n",
    "\n",
    "groups = df['id'].values\n",
    "\n",
    "print(f'Dependent variable: {depvar}')\n",
    "print(f'Covariates: {covariates}')\n",
    "print(f'Design matrix shape: {X.shape}')\n",
    "print(f'Number of groups: {len(np.unique(groups))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OLS estimation\n",
    "# ============================================================\n",
    "\n",
    "ols_model = sm.OLS(y, X)\n",
    "ols_result = ols_model.fit(cov_type='cluster', cov_kwds={'groups': groups})\n",
    "\n",
    "print('OLS Results (cluster-robust standard errors)')\n",
    "print('=' * 60)\n",
    "\n",
    "# Custom summary table\n",
    "ols_table = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Coefficient': ols_result.params,\n",
    "    'Std. Error': ols_result.bse,\n",
    "    't-stat': ols_result.tvalues,\n",
    "    'p-value': ols_result.pvalues,\n",
    "}).set_index('Variable')\n",
    "\n",
    "display(ols_table.round(4))\n",
    "\n",
    "print(f'\\nR-squared:     {ols_result.rsquared:.4f}')\n",
    "print(f'Observations:  {int(ols_result.nobs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation (OLS)**:\n",
    "\n",
    "The OLS coefficients give us a rough sense of the determinants of expenditure,\n",
    "but these estimates are biased downward because OLS treats zero observations as if\n",
    "individuals genuinely chose to spend nothing, rather than recognizing that spending\n",
    "is *censored* at the lower bound.\n",
    "\n",
    "We will now compare these to Tobit estimates to quantify the attenuation bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part IV: Pooled Tobit (10 min)\n",
    "\n",
    "The Pooled Tobit model explicitly accounts for left-censoring at zero. It models a\n",
    "**latent** variable $y^*_{it} = X_{it}'\\beta + \\varepsilon_{it}$ and assumes we\n",
    "observe $y_{it} = \\max(0, y^*_{it})$.\n",
    "\n",
    "This corrects the attenuation bias but still ignores the panel structure (treats\n",
    "all observations as if they came from different individuals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Pooled Tobit estimation\n",
    "# ============================================================\n",
    "\n",
    "tobit_pooled = PooledTobit(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    groups=groups,\n",
    "    censoring_point=0.0,\n",
    ")\n",
    "tobit_pooled.fit()\n",
    "\n",
    "print(tobit_pooled.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Store Pooled Tobit results in a DataFrame\n",
    "# ============================================================\n",
    "\n",
    "n_beta = len(var_names)\n",
    "\n",
    "tobit_pooled_table = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Coefficient': tobit_pooled.beta,\n",
    "    'Std. Error': tobit_pooled.bse[:n_beta],\n",
    "    't-stat': tobit_pooled.beta / tobit_pooled.bse[:n_beta],\n",
    "    'p-value': 2 * (1 - stats.norm.cdf(np.abs(tobit_pooled.beta / tobit_pooled.bse[:n_beta]))),\n",
    "}).set_index('Variable')\n",
    "\n",
    "display(tobit_pooled_table.round(4))\n",
    "\n",
    "print(f'\\nsigma:          {tobit_pooled.sigma:.4f}')\n",
    "print(f'Log-likelihood: {tobit_pooled.llf:.2f}')\n",
    "print(f'Observations:   {tobit_pooled.n_obs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation (Pooled Tobit)**:\n",
    "\n",
    "Notice that the Tobit coefficients are **larger in absolute value** than OLS.\n",
    "This is the classic pattern: OLS attenuates coefficients when censoring is present,\n",
    "and the Tobit model corrects for this.\n",
    "\n",
    "However, the Pooled Tobit still ignores individual-specific unobserved heterogeneity,\n",
    "which can bias results if correlated with the regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part V: Random Effects Tobit (10 min)\n",
    "\n",
    "The Random Effects Tobit adds an individual-specific random effect $\\alpha_i \\sim N(0, \\sigma^2_\\alpha)$\n",
    "to the latent equation:\n",
    "\n",
    "$$y^*_{it} = X_{it}'\\beta + \\alpha_i + \\varepsilon_{it}$$\n",
    "\n",
    "The likelihood is integrated over the distribution of $\\alpha_i$ using Gauss-Hermite\n",
    "quadrature. This accounts for unobserved individual heterogeneity under the assumption\n",
    "that $\\alpha_i$ is uncorrelated with $X_{it}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Random Effects Tobit estimation\n",
    "# ============================================================\n",
    "\n",
    "tobit_re = RandomEffectsTobit(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    groups=groups,\n",
    "    censoring_point=0.0,\n",
    "    quadrature_points=12,\n",
    ")\n",
    "tobit_re.fit(method='BFGS', maxiter=2000, options={'disp': False})\n",
    "\n",
    "print(tobit_re.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Store RE Tobit results in a DataFrame\n",
    "# ============================================================\n",
    "\n",
    "re_tobit_table = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Coefficient': tobit_re.beta,\n",
    "    'Std. Error': tobit_re.bse[:n_beta],\n",
    "    't-stat': tobit_re.beta / tobit_re.bse[:n_beta],\n",
    "    'p-value': 2 * (1 - stats.norm.cdf(np.abs(tobit_re.beta / tobit_re.bse[:n_beta]))),\n",
    "}).set_index('Variable')\n",
    "\n",
    "display(re_tobit_table.round(4))\n",
    "\n",
    "print(f'\\nsigma_eps:   {tobit_re.sigma_eps:.4f}')\n",
    "print(f'sigma_alpha: {tobit_re.sigma_alpha:.4f}')\n",
    "print(f'rho (ICC):   {tobit_re.sigma_alpha**2 / (tobit_re.sigma_alpha**2 + tobit_re.sigma_eps**2):.4f}')\n",
    "print(f'Log-lik:     {tobit_re.llf:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation (RE Tobit)**:\n",
    "\n",
    "The **intra-class correlation (ICC)** $\\rho = \\sigma^2_\\alpha / (\\sigma^2_\\alpha + \\sigma^2_\\varepsilon)$\n",
    "measures the fraction of total latent variance due to individual heterogeneity.\n",
    "A large ICC indicates that unobserved individual characteristics matter, validating\n",
    "the use of a panel model rather than the pooled specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part VI: Model Comparison (10 min)\n",
    "\n",
    "We now compare the three models side by side to understand how each estimation\n",
    "choice affects the substantive conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Table 02: Full model comparison\n",
    "# ============================================================\n",
    "\n",
    "comparison_rows = []\n",
    "\n",
    "for i, var in enumerate(var_names):\n",
    "    row = {\n",
    "        'Variable': var,\n",
    "        'OLS_Coef': ols_result.params[i],\n",
    "        'OLS_SE': ols_result.bse[i],\n",
    "        'PooledTobit_Coef': tobit_pooled.beta[i],\n",
    "        'PooledTobit_SE': tobit_pooled.bse[i],\n",
    "        'RE_Tobit_Coef': tobit_re.beta[i],\n",
    "        'RE_Tobit_SE': tobit_re.bse[i],\n",
    "    }\n",
    "    comparison_rows.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows).set_index('Variable')\n",
    "\n",
    "# Add model-level statistics\n",
    "model_stats = pd.DataFrame({\n",
    "    'OLS_Coef': [ols_result.rsquared, np.nan, np.nan, ols_result.nobs],\n",
    "    'OLS_SE': [np.nan] * 4,\n",
    "    'PooledTobit_Coef': [np.nan, tobit_pooled.sigma, tobit_pooled.llf, tobit_pooled.n_obs],\n",
    "    'PooledTobit_SE': [np.nan] * 4,\n",
    "    'RE_Tobit_Coef': [np.nan, tobit_re.sigma_eps, tobit_re.llf, tobit_re.n_obs],\n",
    "    'RE_Tobit_SE': [np.nan] * 4,\n",
    "}, index=['R2 / sigma_alpha', 'sigma', 'Log-Likelihood', 'N'])\n",
    "\n",
    "# Add sigma_alpha for RE model\n",
    "model_stats.loc['R2 / sigma_alpha', 'RE_Tobit_Coef'] = tobit_re.sigma_alpha\n",
    "\n",
    "full_comparison = pd.concat([comparison_df, model_stats])\n",
    "\n",
    "print('Table 02: Model Comparison -- OLS vs Pooled Tobit vs RE Tobit')\n",
    "print('=' * 90)\n",
    "display(full_comparison.round(4))\n",
    "\n",
    "# Save\n",
    "full_comparison.round(4).to_csv(TABLES_DIR / 'table02_model_comparison.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Figure 03: Coefficient comparison forest plot\n",
    "# ============================================================\n",
    "\n",
    "# Exclude the constant for visual clarity\n",
    "plot_vars = covariates\n",
    "idx = np.arange(len(plot_vars))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "ols_coefs = [ols_result.params[var_names.index(v)] for v in plot_vars]\n",
    "ols_ses = [1.96 * ols_result.bse[var_names.index(v)] for v in plot_vars]\n",
    "\n",
    "pt_coefs = [tobit_pooled.beta[var_names.index(v)] for v in plot_vars]\n",
    "pt_ses = [1.96 * tobit_pooled.bse[var_names.index(v)] for v in plot_vars]\n",
    "\n",
    "re_coefs = [tobit_re.beta[var_names.index(v)] for v in plot_vars]\n",
    "re_ses = [1.96 * tobit_re.bse[var_names.index(v)] for v in plot_vars]\n",
    "\n",
    "ax.barh(idx + width, ols_coefs, width, xerr=ols_ses, label='OLS',\n",
    "        color='steelblue', alpha=0.8, capsize=3)\n",
    "ax.barh(idx, pt_coefs, width, xerr=pt_ses, label='Pooled Tobit',\n",
    "        color='darkorange', alpha=0.8, capsize=3)\n",
    "ax.barh(idx - width, re_coefs, width, xerr=re_ses, label='RE Tobit',\n",
    "        color='seagreen', alpha=0.8, capsize=3)\n",
    "\n",
    "ax.set_yticks(idx)\n",
    "ax.set_yticklabels(plot_vars, fontsize=12)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.set_xlabel('Coefficient Estimate', fontsize=12)\n",
    "ax.set_title('Coefficient Comparison: OLS vs Pooled Tobit vs RE Tobit', fontsize=14)\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig03_coefficient_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('*Figure 03: Forest plot comparing coefficient estimates across OLS, Pooled Tobit, '\n",
    "      'and Random Effects Tobit. Error bars represent 95% confidence intervals. '\n",
    "      'Notice the systematic attenuation of OLS coefficients relative to the Tobit models.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Attenuation ratios: Tobit / OLS\n",
    "# ============================================================\n",
    "\n",
    "print('Attenuation Ratios (Tobit / OLS):')\n",
    "print('-' * 50)\n",
    "print(f'{\"Variable\":15s} {\"Pooled Tobit/OLS\":>18s} {\"RE Tobit/OLS\":>15s}')\n",
    "print('-' * 50)\n",
    "\n",
    "for var in covariates:\n",
    "    i = var_names.index(var)\n",
    "    ols_c = ols_result.params[i]\n",
    "    pt_c = tobit_pooled.beta[i]\n",
    "    re_c = tobit_re.beta[i]\n",
    "    ratio_pt = pt_c / ols_c if abs(ols_c) > 1e-6 else np.nan\n",
    "    ratio_re = re_c / ols_c if abs(ols_c) > 1e-6 else np.nan\n",
    "    print(f'{var:15s} {ratio_pt:18.3f} {ratio_re:15.3f}')\n",
    "\n",
    "print('\\nRatios > 1 indicate OLS was attenuated (biased toward zero).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part VII: Marginal Effects -- McDonald-Moffitt Decomposition (15 min)\n",
    "\n",
    "In the Tobit model, the raw coefficients $\\beta$ are the marginal effects on the\n",
    "**latent** variable $y^*$. To understand the effect on the **observed** variable $y$,\n",
    "we decompose the marginal effect into three components (McDonald and Moffitt, 1980):\n",
    "\n",
    "1. **Unconditional**: $\\frac{\\partial E[y|X]}{\\partial x_k} = \\beta_k \\cdot \\Phi(z)$\n",
    "   -- Effect on overall expected value, accounting for censoring probability\n",
    "\n",
    "2. **Conditional**: $\\frac{\\partial E[y|y>0, X]}{\\partial x_k} = \\beta_k \\cdot [1 - \\lambda(z)(z + \\lambda(z))]$\n",
    "   -- Effect among those with positive expenditure\n",
    "\n",
    "3. **Probability**: $\\frac{\\partial P(y>0|X)}{\\partial x_k} = \\frac{\\beta_k}{\\sigma} \\cdot \\phi(z)$\n",
    "   -- Effect on the probability of having positive expenditure\n",
    "\n",
    "where $z = (X'\\beta - c)/\\sigma$ and $\\lambda(z) = \\phi(z)/\\Phi(z)$ is the inverse Mills ratio.\n",
    "\n",
    "We compute both **Average Marginal Effects (AME)** and **Marginal Effects at Means (MEM)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Assign variable names to the model for cleaner output\n",
    "# ============================================================\n",
    "\n",
    "tobit_pooled.exog_names = var_names\n",
    "\n",
    "# ============================================================\n",
    "# Compute AME for all three types\n",
    "# ============================================================\n",
    "\n",
    "ame_unconditional = tobit_pooled.marginal_effects(at='overall', which='unconditional')\n",
    "ame_conditional = tobit_pooled.marginal_effects(at='overall', which='conditional')\n",
    "ame_probability = tobit_pooled.marginal_effects(at='overall', which='probability')\n",
    "\n",
    "print('Average Marginal Effects -- Unconditional E[y|X]:')\n",
    "display(ame_unconditional.summary().round(4))\n",
    "\n",
    "print('\\nAverage Marginal Effects -- Conditional E[y|y>0, X]:')\n",
    "display(ame_conditional.summary().round(4))\n",
    "\n",
    "print('\\nAverage Marginal Effects -- Probability P(y>0|X):')\n",
    "display(ame_probability.summary().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Compute MEM for comparison\n",
    "# ============================================================\n",
    "\n",
    "mem_unconditional = tobit_pooled.marginal_effects(at='mean', which='unconditional')\n",
    "mem_conditional = tobit_pooled.marginal_effects(at='mean', which='conditional')\n",
    "mem_probability = tobit_pooled.marginal_effects(at='mean', which='probability')\n",
    "\n",
    "print('Marginal Effects at Means -- Unconditional E[y|X_bar]:')\n",
    "display(mem_unconditional.summary().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Table 03: Combined marginal effects table\n",
    "# ============================================================\n",
    "\n",
    "me_table = pd.DataFrame({\n",
    "    'beta (latent)': pd.Series({v: tobit_pooled.beta[var_names.index(v)] for v in covariates}),\n",
    "    'AME unconditional': ame_unconditional.marginal_effects[covariates],\n",
    "    'AME conditional': ame_conditional.marginal_effects[covariates],\n",
    "    'AME probability': ame_probability.marginal_effects[covariates],\n",
    "    'MEM unconditional': mem_unconditional.marginal_effects[covariates],\n",
    "})\n",
    "\n",
    "print('Table 03: Marginal Effects Decomposition (Pooled Tobit)')\n",
    "print('=' * 90)\n",
    "display(me_table.round(4))\n",
    "\n",
    "# Save\n",
    "me_table.round(4).to_csv(TABLES_DIR / 'table03_marginal_effects.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Figure 04: Marginal effects comparison bar chart\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "idx = np.arange(len(covariates))\n",
    "width = 0.2\n",
    "\n",
    "beta_vals = [tobit_pooled.beta[var_names.index(v)] for v in covariates]\n",
    "ame_uncond_vals = [ame_unconditional.marginal_effects[v] for v in covariates]\n",
    "ame_cond_vals = [ame_conditional.marginal_effects[v] for v in covariates]\n",
    "ame_prob_vals = [ame_probability.marginal_effects[v] for v in covariates]\n",
    "\n",
    "bars1 = ax.bar(idx - 1.5*width, beta_vals, width, label=r'$\\beta$ (latent)', color='navy', alpha=0.8)\n",
    "bars2 = ax.bar(idx - 0.5*width, ame_uncond_vals, width, label='AME (unconditional)', color='steelblue', alpha=0.8)\n",
    "bars3 = ax.bar(idx + 0.5*width, ame_cond_vals, width, label='AME (conditional)', color='darkorange', alpha=0.8)\n",
    "bars4 = ax.bar(idx + 1.5*width, ame_prob_vals, width, label='AME (probability)', color='seagreen', alpha=0.8)\n",
    "\n",
    "ax.set_xticks(idx)\n",
    "ax.set_xticklabels(covariates, fontsize=12, rotation=15)\n",
    "ax.axhline(y=0, color='black', linewidth=0.8)\n",
    "ax.set_ylabel('Effect Size', fontsize=12)\n",
    "ax.set_title('McDonald-Moffitt Decomposition of Marginal Effects', fontsize=14)\n",
    "ax.legend(fontsize=10, loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig04_marginal_effects.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('*Figure 04: McDonald-Moffitt decomposition comparing raw Tobit coefficients (latent) '\n",
    "      'with unconditional, conditional, and probability marginal effects. The unconditional '\n",
    "      'AME is always smaller than the raw coefficient because it accounts for the probability '\n",
    "      'of censoring.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Scaling factor decomposition\n",
    "# ============================================================\n",
    "\n",
    "print('Scaling Factors (AME / beta):')\n",
    "print('-' * 60)\n",
    "print(f'{\"Variable\":15s} {\"Unconditional\":>15s} {\"Conditional\":>15s} {\"Probability\":>15s}')\n",
    "print('-' * 60)\n",
    "\n",
    "for var in covariates:\n",
    "    i = var_names.index(var)\n",
    "    b = tobit_pooled.beta[i]\n",
    "    if abs(b) > 1e-8:\n",
    "        s_unc = ame_unconditional.marginal_effects[var] / b\n",
    "        s_con = ame_conditional.marginal_effects[var] / b\n",
    "        s_prb = ame_probability.marginal_effects[var] / b\n",
    "        print(f'{var:15s} {s_unc:15.4f} {s_con:15.4f} {s_prb:15.4f}')\n",
    "\n",
    "print('\\nNote: The unconditional scaling factor equals Phi(z_bar), the average '\n",
    "      'probability of being uncensored.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part VIII: Heckman Selection Analysis (15 min)\n",
    "\n",
    "The Tobit model assumes that the same process determines **whether** spending occurs\n",
    "and **how much** is spent. The Heckman selection model relaxes this by specifying\n",
    "**two separate equations**:\n",
    "\n",
    "1. **Selection equation**: $s_{it} = \\mathbf{1}[Z_{it}'\\gamma + u_{it} > 0]$\n",
    "   -- Determines participation (e.g., labor force participation)\n",
    "\n",
    "2. **Outcome equation**: $y_{it} = X_{it}'\\beta + \\varepsilon_{it}$ if $s_{it} = 1$\n",
    "   -- Determines the level (e.g., wages, observed only for workers)\n",
    "\n",
    "If $\\text{Corr}(u, \\varepsilon) = \\rho \\neq 0$, OLS on the selected sample is biased.\n",
    "\n",
    "We use the **Mroz (1987)** dataset -- a classic sample of married women's labor supply -- to\n",
    "demonstrate this selection correction workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load Mroz (1987) data for Heckman analysis\n",
    "# ============================================================\n",
    "\n",
    "mroz = pd.read_csv(DATA_DIR / 'mroz_1987.csv')\n",
    "\n",
    "print(f'Mroz dataset shape: {mroz.shape}')\n",
    "print(f'\\nVariables: {list(mroz.columns)}')\n",
    "print(f'\\nLabor force participation rate: {mroz[\"lfp\"].mean():.1%}')\n",
    "print(f'Observations with wages:         {mroz[\"lfp\"].sum()} / {len(mroz)}')\n",
    "\n",
    "mroz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Prepare Heckman model matrices\n",
    "# ============================================================\n",
    "\n",
    "# Selection indicator\n",
    "selection = mroz['lfp'].values.astype(int)\n",
    "\n",
    "# Outcome variable: log wage (observed only for workers)\n",
    "# Fill missing wages with 0 (PanelHeckman uses the selection indicator to handle this)\n",
    "wage = mroz['wage'].fillna(0).values\n",
    "\n",
    "# Outcome equation regressors: education, experience, experience^2\n",
    "outcome_vars = ['education', 'experience', 'experience_sq']\n",
    "X_outcome = sm.add_constant(mroz[outcome_vars].values)\n",
    "outcome_names = ['const'] + outcome_vars\n",
    "\n",
    "# Selection equation regressors: same as outcome + exclusion restrictions\n",
    "# Exclusion restrictions: children_lt6, children_6_18, husband_income\n",
    "# These affect LFP but not wages directly (conditional on working)\n",
    "selection_vars = ['education', 'experience', 'experience_sq', 'age',\n",
    "                  'children_lt6', 'children_6_18', 'husband_income']\n",
    "Z_selection = sm.add_constant(mroz[selection_vars].values)\n",
    "selection_names = ['const'] + selection_vars\n",
    "\n",
    "print(f'Outcome equation variables:   {outcome_names}')\n",
    "print(f'Selection equation variables:  {selection_names}')\n",
    "print(f'Exclusion restrictions:        age, children_lt6, children_6_18, husband_income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Heckman two-step estimation\n",
    "# ============================================================\n",
    "\n",
    "heckman_model = PanelHeckman(\n",
    "    endog=wage,\n",
    "    exog=X_outcome,\n",
    "    selection=selection,\n",
    "    exog_selection=Z_selection,\n",
    "    method='two_step',\n",
    ")\n",
    "\n",
    "heckman_result = heckman_model.fit()\n",
    "\n",
    "print(heckman_result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Selection bias test\n",
    "# ============================================================\n",
    "\n",
    "sel_test = heckman_result.selection_test()\n",
    "\n",
    "print('Test for Selection Bias (H0: rho = 0):')\n",
    "print('-' * 50)\n",
    "for key, val in sel_test.items():\n",
    "    if isinstance(val, float):\n",
    "        print(f'  {key:20s}: {val:.4f}')\n",
    "    else:\n",
    "        print(f'  {key:20s}: {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Compare OLS on selected sample vs Heckman-corrected estimates\n",
    "# ============================================================\n",
    "\n",
    "comparison = heckman_result.compare_ols_heckman()\n",
    "\n",
    "heckman_comparison_df = pd.DataFrame({\n",
    "    'Variable': outcome_names,\n",
    "    'OLS (selected)': comparison['beta_ols'],\n",
    "    'Heckman': comparison['beta_heckman'],\n",
    "    'Difference': comparison['difference'],\n",
    "    'Pct Diff (%)': comparison['pct_difference'],\n",
    "}).set_index('Variable')\n",
    "\n",
    "print('OLS (on selected sample) vs Heckman Two-Step:')\n",
    "print('=' * 70)\n",
    "display(heckman_comparison_df.round(4))\n",
    "\n",
    "print(f'\\n{comparison[\"interpretation\"]}')\n",
    "print(f'\\nrho (selection correlation): {heckman_result.rho:.4f}')\n",
    "print(f'sigma:                       {heckman_result.sigma:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Figure 05: IMR diagnostics for Heckman model\n",
    "# ============================================================\n",
    "\n",
    "fig = heckman_result.plot_imr(figsize=(14, 5))\n",
    "\n",
    "plt.savefig(FIGURES_DIR / 'fig05_imr_diagnostics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('*Figure 05: Inverse Mills Ratio (IMR) diagnostics for the Heckman two-step '\n",
    "      'estimator. The left panel shows IMR versus the predicted selection probability, '\n",
    "      'while the right panel shows the distribution of IMR values among the selected '\n",
    "      'sample. Observations with very high IMR (above the red threshold) experience '\n",
    "      'strong selection effects.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMR summary diagnostics\n",
    "# ============================================================\n",
    "\n",
    "imr_diag = heckman_result.imr_diagnostics()\n",
    "\n",
    "print('IMR Diagnostics:')\n",
    "print('-' * 40)\n",
    "for key, val in imr_diag.items():\n",
    "    if isinstance(val, float):\n",
    "        print(f'  {key:25s}: {val:.4f}')\n",
    "    else:\n",
    "        print(f'  {key:25s}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part IX: Sensitivity Analysis (10 min)\n",
    "\n",
    "Robust conclusions require checking that our results are not sensitive to particular\n",
    "modeling choices. We examine:\n",
    "\n",
    "1. **Subsample stability**: Do results change when restricting to particular groups?\n",
    "2. **Covariate sensitivity**: Are key results robust to adding or dropping variables?\n",
    "3. **Heckman vs Tobit framing**: Does the Heckman selection model on the health data\n",
    "   yield different conclusions from the Tobit model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Sensitivity 1: Pooled Tobit on subsamples\n",
    "# ============================================================\n",
    "\n",
    "subsamples = {\n",
    "    'Full sample': df,\n",
    "    'Males only': df[df['female'] == 0],\n",
    "    'Females only': df[df['female'] == 1],\n",
    "    'Insured only': df[df['insurance'] == 1],\n",
    "    'Uninsured only': df[df['insurance'] == 0],\n",
    "}\n",
    "\n",
    "subsample_results = {}\n",
    "\n",
    "for label, sub_df in subsamples.items():\n",
    "    y_sub = sub_df[depvar].values\n",
    "    X_sub = sm.add_constant(sub_df[covariates].values)\n",
    "    g_sub = sub_df['id'].values\n",
    "    \n",
    "    try:\n",
    "        model_sub = PooledTobit(endog=y_sub, exog=X_sub, groups=g_sub, censoring_point=0.0)\n",
    "        model_sub.fit()\n",
    "        subsample_results[label] = {\n",
    "            'n': len(y_sub),\n",
    "            'pct_censored': (y_sub == 0).mean() * 100,\n",
    "            'beta': model_sub.beta.copy(),\n",
    "            'sigma': model_sub.sigma,\n",
    "            'llf': model_sub.llf,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        subsample_results[label] = {'error': str(e)}\n",
    "\n",
    "# Display results\n",
    "print('Subsample Sensitivity Analysis (Pooled Tobit):')\n",
    "print('=' * 100)\n",
    "\n",
    "header = f'{\"Subsample\":20s} {\"N\":>6s} {\"% Cens\":>7s}'\n",
    "for var in covariates:\n",
    "    header += f' {var:>10s}'\n",
    "header += f' {\"sigma\":>8s}'\n",
    "print(header)\n",
    "print('-' * 100)\n",
    "\n",
    "for label, res in subsample_results.items():\n",
    "    if 'error' in res:\n",
    "        print(f'{label:20s}  ERROR: {res[\"error\"]}')\n",
    "    else:\n",
    "        line = f'{label:20s} {res[\"n\"]:>6d} {res[\"pct_censored\"]:>6.1f}%'\n",
    "        for i, var in enumerate(covariates):\n",
    "            # beta index is i+1 because of the constant at index 0\n",
    "            line += f' {res[\"beta\"][i+1]:>10.4f}'\n",
    "        line += f' {res[\"sigma\"]:>8.4f}'\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Sensitivity 2: Alternative covariate specifications\n",
    "# ============================================================\n",
    "\n",
    "specifications = [\n",
    "    ('Baseline', covariates),\n",
    "    ('Parsimonious', ['income', 'chronic', 'insurance']),\n",
    "    ('Demographics only', ['income', 'age', 'female', 'bmi']),\n",
    "    ('With interaction', covariates),  # We will manually add an interaction\n",
    "]\n",
    "\n",
    "spec_results = {}\n",
    "\n",
    "for label, vars_spec in specifications:\n",
    "    if label == 'With interaction':\n",
    "        # Add income * insurance interaction\n",
    "        X_spec_raw = df[vars_spec].values.copy()\n",
    "        interaction = (df['income'] * df['insurance']).values.reshape(-1, 1)\n",
    "        X_spec = sm.add_constant(np.column_stack([X_spec_raw, interaction]))\n",
    "        spec_varnames = ['const'] + vars_spec + ['income_x_insurance']\n",
    "    else:\n",
    "        X_spec = sm.add_constant(df[vars_spec].values)\n",
    "        spec_varnames = ['const'] + vars_spec\n",
    "    \n",
    "    try:\n",
    "        model_spec = PooledTobit(endog=y, exog=X_spec, groups=groups, censoring_point=0.0)\n",
    "        model_spec.fit()\n",
    "        spec_results[label] = {\n",
    "            'vars': spec_varnames,\n",
    "            'beta': model_spec.beta,\n",
    "            'se': model_spec.bse[:len(spec_varnames)],\n",
    "            'sigma': model_spec.sigma,\n",
    "            'llf': model_spec.llf,\n",
    "            'n_params': len(spec_varnames),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        spec_results[label] = {'error': str(e)}\n",
    "\n",
    "# Display specification comparison\n",
    "print('Specification Sensitivity Analysis (Pooled Tobit):')\n",
    "print('=' * 80)\n",
    "\n",
    "for label, res in spec_results.items():\n",
    "    if 'error' in res:\n",
    "        print(f'\\n{label}: ERROR - {res[\"error\"]}')\n",
    "    else:\n",
    "        print(f'\\n{label} (Log-Lik: {res[\"llf\"]:.2f}, sigma: {res[\"sigma\"]:.4f})')\n",
    "        print('-' * 60)\n",
    "        for i, var in enumerate(res['vars']):\n",
    "            t_stat = res['beta'][i] / res['se'][i] if res['se'][i] > 0 else np.nan\n",
    "            sig = '***' if abs(t_stat) > 3.29 else '**' if abs(t_stat) > 2.58 else '*' if abs(t_stat) > 1.96 else ''\n",
    "            print(f'  {var:22s} {res[\"beta\"][i]:>10.4f} ({res[\"se\"][i]:.4f}) {sig}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Sensitivity 3: Heckman framing on health expenditure data\n",
    "# ============================================================\n",
    "\n",
    "# Treat the health data as a selection problem:\n",
    "# Selection equation: whether expenditure > 0\n",
    "# Outcome equation: expenditure level given expenditure > 0\n",
    "\n",
    "health_sel = (df['expenditure'] > 0).astype(int).values\n",
    "health_y = df['expenditure'].values\n",
    "\n",
    "# Outcome equation: income, chronic, insurance\n",
    "health_X_outcome = sm.add_constant(df[['income', 'chronic', 'insurance']].values)\n",
    "\n",
    "# Selection equation: add age, female, bmi as exclusion restrictions\n",
    "health_Z_sel = sm.add_constant(\n",
    "    df[['income', 'chronic', 'insurance', 'age', 'female', 'bmi']].values\n",
    ")\n",
    "\n",
    "try:\n",
    "    heckman_health = PanelHeckman(\n",
    "        endog=health_y,\n",
    "        exog=health_X_outcome,\n",
    "        selection=health_sel,\n",
    "        exog_selection=health_Z_sel,\n",
    "        method='two_step',\n",
    "    )\n",
    "    heckman_health_result = heckman_health.fit()\n",
    "\n",
    "    print('Heckman Two-Step on Health Expenditure Data:')\n",
    "    print(heckman_health_result.summary())\n",
    "\n",
    "    print(f'\\nrho: {heckman_health_result.rho:.4f}')\n",
    "    if abs(heckman_health_result.rho) > 0.1:\n",
    "        print('=> Evidence of selection bias: the Tobit single-equation assumption may not hold.')\n",
    "    else:\n",
    "        print('=> Little evidence of selection bias: Tobit assumption is reasonable.')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Heckman estimation on health data failed: {e}')\n",
    "    print('This may occur if the selection rate is extreme or the model is not identified.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part X: Results Summary and Policy Implications (10 min)\n",
    "\n",
    "We now consolidate all findings into a comprehensive summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Table 04: Final comprehensive results\n",
    "# ============================================================\n",
    "\n",
    "def stars(coef, se):\n",
    "    \"\"\"Return significance stars.\"\"\"\n",
    "    if se == 0 or np.isnan(se):\n",
    "        return ''\n",
    "    t = abs(coef / se)\n",
    "    if t > 3.29:\n",
    "        return '***'\n",
    "    elif t > 2.58:\n",
    "        return '**'\n",
    "    elif t > 1.96:\n",
    "        return '*'\n",
    "    return ''\n",
    "\n",
    "print('Table 04: Final Results Summary')\n",
    "print('=' * 95)\n",
    "print(f'{\"\":20s} {\"OLS\":>14s} {\"Pooled Tobit\":>14s} {\"RE Tobit\":>14s} {\"AME (uncond.)\":>14s}')\n",
    "print('-' * 95)\n",
    "\n",
    "for var in covariates:\n",
    "    i = var_names.index(var)\n",
    "    \n",
    "    ols_c = ols_result.params[i]\n",
    "    ols_s = ols_result.bse[i]\n",
    "    \n",
    "    pt_c = tobit_pooled.beta[i]\n",
    "    pt_s = tobit_pooled.bse[i]\n",
    "    \n",
    "    re_c = tobit_re.beta[i]\n",
    "    re_s = tobit_re.bse[i]\n",
    "    \n",
    "    ame_val = ame_unconditional.marginal_effects[var]\n",
    "    \n",
    "    line = f'{var:20s}'\n",
    "    line += f' {ols_c:>10.4f}{stars(ols_c, ols_s):3s}'\n",
    "    line += f' {pt_c:>10.4f}{stars(pt_c, pt_s):3s}'\n",
    "    line += f' {re_c:>10.4f}{stars(re_c, re_s):3s}'\n",
    "    line += f' {ame_val:>14.4f}'\n",
    "    print(line)\n",
    "    \n",
    "    # Standard errors on second line\n",
    "    se_line = f'{\"\":20s}'\n",
    "    se_line += f' ({ols_s:>9.4f})  '\n",
    "    se_line += f' ({pt_s:>9.4f})  '\n",
    "    se_line += f' ({re_s:>9.4f})  '\n",
    "    se_line += f' {\"\":>14s}'\n",
    "    print(se_line)\n",
    "\n",
    "print('-' * 95)\n",
    "print(f'{\"sigma\":20s} {\"\":>14s} {tobit_pooled.sigma:>14.4f} {tobit_re.sigma_eps:>14.4f}')\n",
    "print(f'{\"sigma_alpha\":20s} {\"\":>14s} {\"\":>14s} {tobit_re.sigma_alpha:>14.4f}')\n",
    "print(f'{\"R2\":20s} {ols_result.rsquared:>14.4f}')\n",
    "print(f'{\"Log-Likelihood\":20s} {\"\":>14s} {tobit_pooled.llf:>14.2f} {tobit_re.llf:>14.2f}')\n",
    "print(f'{\"N\":20s} {int(ols_result.nobs):>14d} {tobit_pooled.n_obs:>14d} {tobit_re.n_obs:>14d}')\n",
    "print('=' * 95)\n",
    "print('\\nSignificance: *** p<0.001, ** p<0.01, * p<0.05')\n",
    "print('Standard errors in parentheses (cluster-robust for OLS, MLE-based for Tobit).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Summary of key findings\n",
    "# ============================================================\n",
    "\n",
    "print('KEY FINDINGS')\n",
    "print('=' * 70)\n",
    "\n",
    "print('\\n1. ATTENUATION BIAS')\n",
    "print('-' * 70)\n",
    "print('   OLS systematically underestimates the effect of covariates on')\n",
    "print('   health expenditure due to ignoring censoring at zero.')\n",
    "for var in covariates:\n",
    "    i = var_names.index(var)\n",
    "    ols_c = ols_result.params[i]\n",
    "    pt_c = tobit_pooled.beta[i]\n",
    "    if abs(ols_c) > 1e-6:\n",
    "        ratio = pt_c / ols_c\n",
    "        print(f'   {var:15s}: Tobit/OLS ratio = {ratio:.2f}')\n",
    "\n",
    "print('\\n2. INDIVIDUAL HETEROGENEITY')\n",
    "print('-' * 70)\n",
    "rho_icc = tobit_re.sigma_alpha**2 / (tobit_re.sigma_alpha**2 + tobit_re.sigma_eps**2)\n",
    "print(f'   ICC (intra-class correlation): {rho_icc:.4f}')\n",
    "print(f'   => {rho_icc*100:.1f}% of latent variance is due to individual effects.')\n",
    "if rho_icc > 0.1:\n",
    "    print('   The panel structure is important; RE Tobit is preferred over Pooled Tobit.')\n",
    "\n",
    "print('\\n3. MARGINAL EFFECTS DECOMPOSITION')\n",
    "print('-' * 70)\n",
    "print('   The unconditional AME is the policy-relevant quantity. For key variables:')\n",
    "for var in ['income', 'chronic', 'insurance']:\n",
    "    i = var_names.index(var)\n",
    "    beta_val = tobit_pooled.beta[i]\n",
    "    ame_val = ame_unconditional.marginal_effects[var]\n",
    "    scale = ame_val / beta_val if abs(beta_val) > 1e-8 else np.nan\n",
    "    print(f'   {var:15s}: beta={beta_val:.4f}, AME={ame_val:.4f} (scaling={scale:.3f})')\n",
    "\n",
    "print('\\n4. SELECTION BIAS (Heckman on Mroz data)')\n",
    "print('-' * 70)\n",
    "print(f'   rho = {heckman_result.rho:.4f}')\n",
    "if abs(heckman_result.rho) > 0.1:\n",
    "    print('   Selection bias is present. Heckman correction is needed.')\n",
    "else:\n",
    "    print('   Minimal evidence of selection bias. OLS on the selected sample is adequate.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Policy implications\n",
    "# ============================================================\n",
    "\n",
    "print('POLICY IMPLICATIONS')\n",
    "print('=' * 70)\n",
    "\n",
    "print('''\n",
    "Based on the Random Effects Tobit (our preferred model):\n",
    "\n",
    "1. INCOME: Higher income is associated with higher health spending.\n",
    "   The unconditional AME represents the total effect, including the\n",
    "   increased probability of any spending.\n",
    "\n",
    "2. CHRONIC CONDITIONS: Having more chronic conditions strongly increases\n",
    "   both the probability of positive spending and the amount spent.\n",
    "   This is the most important predictor in the model.\n",
    "\n",
    "3. INSURANCE: Insurance coverage significantly increases health\n",
    "   expenditure, likely through reduced out-of-pocket costs. This\n",
    "   operates through both the intensive and extensive margins.\n",
    "\n",
    "4. METHODOLOGICAL: Ignoring censoring (OLS) leads to underestimating\n",
    "   the true effects by a substantial margin. The panel structure\n",
    "   matters: the ICC indicates meaningful individual heterogeneity.\n",
    "   Future work should consider fixed effects Tobit or Honore (1992)\n",
    "   estimators if the random effects assumption is questionable.\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **OLS is biased for censored data**: Coefficients are attenuated toward zero when the\n",
    "   dependent variable has a mass point at the censoring bound.\n",
    "\n",
    "2. **Tobit corrects for censoring**: Both Pooled and RE Tobit produce larger (in absolute\n",
    "   value) coefficients than OLS, reflecting the true latent-variable relationship.\n",
    "\n",
    "3. **Panel structure matters**: The Random Effects Tobit captures individual heterogeneity\n",
    "   that the Pooled Tobit ignores, as indicated by a non-trivial ICC.\n",
    "\n",
    "4. **Raw Tobit coefficients are not marginal effects**: The McDonald-Moffitt decomposition\n",
    "   is essential for correct interpretation. The unconditional AME is the policy-relevant\n",
    "   quantity for most applications.\n",
    "\n",
    "5. **Selection vs. censoring**: The Heckman model relaxes the Tobit assumption that the same\n",
    "   process governs participation and level. When there are good exclusion restrictions,\n",
    "   the Heckman model can detect and correct sample selection bias.\n",
    "\n",
    "6. **Robustness matters**: Sensitivity analysis across subsamples and specifications gives\n",
    "   confidence that the key conclusions are not artifacts of a particular modeling choice.\n",
    "\n",
    "### Modeling Decision Flowchart\n",
    "\n",
    "```\n",
    "Is the dependent variable censored?\n",
    "    No  --> Standard panel models (FE, RE, GMM)\n",
    "    Yes --> Is the censoring mechanism the same as the outcome process?\n",
    "              Yes --> Tobit model\n",
    "                        Is there panel structure?\n",
    "                          No  --> Pooled Tobit\n",
    "                          Yes --> Random Effects Tobit\n",
    "                                    Concern about RE assumption?\n",
    "                                      Yes --> Honore Fixed Effects Tobit\n",
    "              No  --> Heckman selection model\n",
    "                        Have exclusion restrictions?\n",
    "                          Yes --> Two-step or MLE\n",
    "                          No  --> Identification is weak; proceed with caution\n",
    "```\n",
    "\n",
    "### Connection to Other Notebooks\n",
    "\n",
    "| Notebook | Topic | Builds On |\n",
    "|----------|-------|-----------|\n",
    "| 01 | Tobit Fundamentals | -- |\n",
    "| 02 | Pooled vs Panel Tobit | 01 |\n",
    "| 03 | Random Effects Tobit | 01, 02 |\n",
    "| 04 | Marginal Effects | 01-03 |\n",
    "| 05 | Heckman Selection | 01 |\n",
    "| 06 | Model Diagnostics | 01-05 |\n",
    "| 07 | Advanced Topics | 01-06 |\n",
    "| **08** | **Complete Case Study** | **01-07 (this notebook)** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Extended Model Comparison (20 min)\n",
    "\n",
    "Extend the analysis by:\n",
    "\n",
    "a) Fitting a **log-transformed OLS** model (i.e., `log(1 + expenditure)` as dependent variable)\n",
    "   and comparing the predicted values to the Tobit model.\n",
    "\n",
    "b) Computing the **AIC and BIC** for the Pooled Tobit and RE Tobit models.\n",
    "   Recall: $\\text{AIC} = -2 \\ln L + 2k$ and $\\text{BIC} = -2 \\ln L + k \\ln n$.\n",
    "\n",
    "c) Which model does each criterion prefer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# --------------------------\n",
    "\n",
    "# (a) Log-transformed OLS\n",
    "# y_log = np.log1p(y)\n",
    "# ols_log = sm.OLS(y_log, X).fit(cov_type='cluster', cov_kwds={'groups': groups})\n",
    "# ...\n",
    "\n",
    "# (b) Information criteria\n",
    "# k_pooled = len(tobit_pooled.params)\n",
    "# k_re = len(tobit_re.params)\n",
    "# n = tobit_pooled.n_obs\n",
    "# aic_pooled = -2 * tobit_pooled.llf + 2 * k_pooled\n",
    "# bic_pooled = -2 * tobit_pooled.llf + k_pooled * np.log(n)\n",
    "# ...\n",
    "\n",
    "# (c) Interpretation\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Marginal Effects at Representative Values (15 min)\n",
    "\n",
    "Compute the marginal effect of `insurance` for two profiles:\n",
    "\n",
    "- **Profile A**: age=35, income=30, chronic=0, female=1, bmi=25 (young healthy woman)\n",
    "- **Profile B**: age=65, income=50, chronic=3, female=0, bmi=30 (older man with chronic conditions)\n",
    "\n",
    "Compute both the unconditional and probability marginal effects at each profile\n",
    "using the Pooled Tobit coefficients.\n",
    "\n",
    "*Hint*: For a specific observation, the unconditional ME of variable $k$ is\n",
    "$\\beta_k \\cdot \\Phi((X'\\beta - c)/\\sigma)$.\n",
    "Evaluate at each profile's covariate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# --------------------------\n",
    "\n",
    "# Profile A\n",
    "# x_a = np.array([1, 30, 35, 0, 0, 1, 25])  # [const, income, age, chronic, insurance, female, bmi]\n",
    "# z_a = (x_a @ tobit_pooled.beta - 0) / tobit_pooled.sigma\n",
    "# ...\n",
    "\n",
    "# Profile B\n",
    "# x_b = np.array([1, 50, 65, 3, 0, 0, 30])\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Heckman MLE Estimation (20 min)\n",
    "\n",
    "a) Re-estimate the Heckman model on the Mroz data using **MLE** instead of two-step.\n",
    "\n",
    "b) Compare the two-step and MLE estimates. How different are the outcome coefficients?\n",
    "   How different are the estimated $\\rho$ and $\\sigma$?\n",
    "\n",
    "c) Use the `compare_heckman_methods` utility (from `comparison_tools`) to generate\n",
    "   a formatted comparison table.\n",
    "\n",
    "*Hint*: Use `PanelHeckman(..., method='mle')` and note the performance warning\n",
    "for larger samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# --------------------------\n",
    "\n",
    "# (a) Heckman MLE\n",
    "# heckman_mle = PanelHeckman(\n",
    "#     endog=wage,\n",
    "#     exog=X_outcome,\n",
    "#     selection=selection,\n",
    "#     exog_selection=Z_selection,\n",
    "#     method='mle',\n",
    "# )\n",
    "# heckman_mle_result = heckman_mle.fit()\n",
    "\n",
    "# (b) Compare estimates\n",
    "# print(heckman_mle_result.summary())\n",
    "\n",
    "# (c) Use comparison utility\n",
    "# from comparison_tools import compare_heckman_methods\n",
    "# comparison = compare_heckman_methods(heckman_result, heckman_mle_result, outcome_names)\n",
    "# display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Prediction and Model Validation (15 min)\n",
    "\n",
    "a) Generate **in-sample predictions** from the Pooled Tobit (censored predictions)\n",
    "   and compare them to OLS fitted values. Plot predicted vs. actual values for both.\n",
    "\n",
    "b) Compute the **RMSE** for both models (on the observed, not latent, scale).\n",
    "\n",
    "c) Which model produces predictions that are more consistent with the observed\n",
    "   distribution of expenditures? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n",
    "# --------------------------\n",
    "\n",
    "# (a) In-sample predictions\n",
    "# y_pred_tobit = tobit_pooled.predict(pred_type='censored')\n",
    "# y_pred_ols = ols_result.fittedvalues\n",
    "\n",
    "# (b) RMSE\n",
    "# rmse_tobit = np.sqrt(np.mean((y - y_pred_tobit)**2))\n",
    "# rmse_ols = np.sqrt(np.mean((y - y_pred_ols)**2))\n",
    "# ...\n",
    "\n",
    "# (c) Plot\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook is part of the PanelBox Censored Models Tutorial Series.*\n",
    "*For questions or feedback, consult the PanelBox documentation.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
