{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heckman Two-Step Selection Correction\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the sample selection problem and when OLS fails\n",
    "- Formulate selection and outcome equations with proper exclusion restrictions\n",
    "- Estimate the Heckman two-step model using PanelBox\n",
    "- Interpret the Inverse Mills Ratio, rho, and sigma parameters\n",
    "- Test for selection bias and compare OLS vs Heckman estimates\n",
    "- Diagnose model fit using IMR visualizations\n",
    "\n",
    "## Duration\n",
    "75-90 minutes\n",
    "\n",
    "## Prerequisites\n",
    "- OLS regression (linear models)\n",
    "- Probit/logit models (binary choice)\n",
    "- Basic probability theory (normal distribution, conditional expectation)\n",
    "\n",
    "## Dataset\n",
    "Mroz (1987): Married women's labor force participation and wages\n",
    "- N = 753 married women\n",
    "- Outcome: hourly wage (observed only for participants)\n",
    "- Selection: labor force participation (0/1)\n",
    "- Rich set of demographic and household variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# PanelBox imports - Heckman selection model and IMR utilities\n",
    "from panelbox.models.selection import PanelHeckman, compute_imr, imr_diagnostics, test_selection_effect\n",
    "\n",
    "# Visualization configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define paths (relative to notebook location in examples/censored/notebooks/)\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Sample Selection Problem\n",
    "\n",
    "### When OLS Fails\n",
    "\n",
    "Suppose we want to estimate the returns to education on wages for married women. A natural approach would be:\n",
    "\n",
    "$$\\text{wage}_i = \\beta_0 + \\beta_1 \\text{education}_i + \\beta_2 \\text{experience}_i + \\beta_3 \\text{experience}_i^2 + \\varepsilon_i$$\n",
    "\n",
    "**The problem**: We only observe wages for women who are *working*. Women who are not in the labor force have missing wages.\n",
    "\n",
    "### Why is This a Problem?\n",
    "\n",
    "If we simply run OLS on the observed (working) sample, we get **biased** estimates because:\n",
    "\n",
    "1. **Non-random sample**: Women who choose to work are not a random sample of all women\n",
    "2. **Systematic selection**: Unobserved factors (ability, motivation) that influence participation *also* affect wages\n",
    "3. **Omitted variable bias**: The error term in the wage equation is correlated with the selection decision\n",
    "\n",
    "### An Analogy\n",
    "\n",
    "Imagine estimating the effect of training on performance, but only observing performance for employees who *chose* to participate in training. If more motivated employees self-select into training, naive OLS overstates the training effect.\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "The working sample is **censored by choice**, not at random. High-ability women may be more likely to enter the labor force, creating a systematic upward bias in wage estimates.\n",
    "\n",
    "$$E[\\text{wage}_i | \\text{working}] \\neq E[\\text{wage}_i]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Heckman's Insight\n",
    "\n",
    "James Heckman (Nobel Prize, 2000) showed that sample selection bias can be framed as an **omitted variable problem**, and proposed an elegant two-step correction.\n",
    "\n",
    "### The Two-Equation Framework\n",
    "\n",
    "**Selection equation** (who participates?):\n",
    "\n",
    "$$s_i^* = \\mathbf{Z}_i'\\boldsymbol{\\gamma} + u_i, \\qquad s_i = \\mathbf{1}[s_i^* > 0]$$\n",
    "\n",
    "- $s_i^*$ is a latent (unobserved) propensity to participate\n",
    "- $s_i$ is the observed binary participation indicator\n",
    "- $\\mathbf{Z}_i$ includes variables that affect the participation decision\n",
    "- $u_i$ is the selection error\n",
    "\n",
    "**Outcome equation** (what is the wage?):\n",
    "\n",
    "$$y_i = \\mathbf{X}_i'\\boldsymbol{\\beta} + \\varepsilon_i \\qquad \\text{(observed only if } s_i = 1\\text{)}$$\n",
    "\n",
    "- $y_i$ is the outcome of interest (wage)\n",
    "- $\\mathbf{X}_i$ includes variables that affect the outcome\n",
    "- $\\varepsilon_i$ is the outcome error\n",
    "\n",
    "### The Critical Assumption\n",
    "\n",
    "The errors $(u_i, \\varepsilon_i)$ are **jointly bivariate normal**:\n",
    "\n",
    "$$\\begin{pmatrix} u_i \\\\ \\varepsilon_i \\end{pmatrix} \\sim N\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 & \\rho\\sigma_\\varepsilon \\\\ \\rho\\sigma_\\varepsilon & \\sigma_\\varepsilon^2 \\end{pmatrix}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\sigma_\\varepsilon$ = standard deviation of the outcome error\n",
    "- $\\rho$ = correlation between $u_i$ and $\\varepsilon_i$\n",
    "- If $\\rho \\neq 0$, OLS on the selected sample is **biased**\n",
    "- If $\\rho = 0$, there is no selection bias and OLS is fine\n",
    "\n",
    "### The Omitted Variable\n",
    "\n",
    "Heckman showed that the conditional expectation for the selected sample is:\n",
    "\n",
    "$$E[y_i | s_i = 1] = \\mathbf{X}_i'\\boldsymbol{\\beta} + \\rho\\sigma_\\varepsilon \\cdot \\underbrace{\\frac{\\phi(\\mathbf{Z}_i'\\hat{\\boldsymbol{\\gamma}})}{\\Phi(\\mathbf{Z}_i'\\hat{\\boldsymbol{\\gamma}})}}_{\\lambda_i \\text{ (Inverse Mills Ratio)}}$$\n",
    "\n",
    "The **Inverse Mills Ratio** $\\lambda_i$ captures the selection effect. Omitting it from the regression is what causes the bias.\n",
    "\n",
    "### Exclusion Restrictions\n",
    "\n",
    "For the model to be well-identified, $\\mathbf{Z}_i$ should include at least one variable that:\n",
    "- Affects the **selection** decision (participation)\n",
    "- Does **NOT** directly affect the **outcome** (wages)\n",
    "\n",
    "These are called **exclusion restrictions**. In the Mroz data:\n",
    "- `children_lt6`, `children_6_18`: Young children affect whether a woman works, but not her hourly wage rate\n",
    "- `husband_income`: Husband's income affects need to work, but not the woman's wage rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Loading and Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Mroz (1987) dataset: married women's labor force participation\n",
    "df = pd.read_csv(DATA_DIR / 'mroz_1987.csv')\n",
    "\n",
    "# Preview dataset structure\n",
    "print('Dataset shape:', df.shape)\n",
    "print('\\nFirst 10 rows:')\n",
    "display(df.head(10))\n",
    "\n",
    "print('\\nVariable types:')\n",
    "print(df.dtypes)\n",
    "\n",
    "print('\\nBasic summary statistics:')\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the selection structure: who participates in the labor force?\n",
    "n_total = len(df)\n",
    "n_working = df['lfp'].sum()\n",
    "n_not_working = n_total - n_working\n",
    "\n",
    "print('Sample Selection Structure')\n",
    "print('=' * 50)\n",
    "print(f'Total women:          {n_total}')\n",
    "print(f'Working (lfp=1):      {n_working} ({n_working/n_total:.1%})')\n",
    "print(f'Not working (lfp=0):  {n_not_working} ({n_not_working/n_total:.1%})')\n",
    "print(f'\\nWages observed:       {df[\"wage\"].notna().sum()}')\n",
    "print(f'Wages missing:        {df[\"wage\"].isna().sum()}')\n",
    "\n",
    "print('\\n--- Working women ---')\n",
    "display(df[df['lfp'] == 1][['wage', 'education', 'experience', 'age']].describe())\n",
    "\n",
    "print('\\n--- Non-working women ---')\n",
    "display(df[df['lfp'] == 0][['education', 'experience', 'age',\n",
    "                             'children_lt6', 'children_6_18', 'husband_income']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Exploratory Analysis: Who Participates? Who Doesn't?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare characteristics of working vs non-working women\n",
    "comparison_vars = ['education', 'experience', 'age', 'children_lt6',\n",
    "                   'children_6_18', 'husband_income']\n",
    "\n",
    "comparison_table = pd.DataFrame({\n",
    "    'Working (lfp=1)': df[df['lfp'] == 1][comparison_vars].mean(),\n",
    "    'Not Working (lfp=0)': df[df['lfp'] == 0][comparison_vars].mean(),\n",
    "})\n",
    "comparison_table['Difference'] = (\n",
    "    comparison_table['Working (lfp=1)'] - comparison_table['Not Working (lfp=0)']\n",
    ")\n",
    "\n",
    "print('Mean Characteristics by Labor Force Status')\n",
    "print('=' * 70)\n",
    "display(comparison_table.round(3))\n",
    "\n",
    "print('\\nKey patterns to notice:')\n",
    "print('- Working women tend to have more education (human capital incentive)')\n",
    "print('- Working women have fewer young children (childcare constraint)')\n",
    "print('- Working women have lower husband income (financial need)')\n",
    "print('- These patterns drive selection and create potential bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize participation patterns across key variables\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Education distribution by LFP status\n",
    "for lfp_val, label, color in [(1, 'Working', 'steelblue'), (0, 'Not Working', '#D55E00')]:\n",
    "    subset = df[df['lfp'] == lfp_val]\n",
    "    axes[0, 0].hist(subset['education'], bins=12, alpha=0.6, label=label,\n",
    "                    color=color, edgecolor='black', density=True)\n",
    "axes[0, 0].set_xlabel('Years of Education')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title('Education by LFP Status')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Age distribution by LFP status\n",
    "for lfp_val, label, color in [(1, 'Working', 'steelblue'), (0, 'Not Working', '#D55E00')]:\n",
    "    subset = df[df['lfp'] == lfp_val]\n",
    "    axes[0, 1].hist(subset['age'], bins=15, alpha=0.6, label=label,\n",
    "                    color=color, edgecolor='black', density=True)\n",
    "axes[0, 1].set_xlabel('Age')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('Age by LFP Status')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Husband income by LFP status\n",
    "for lfp_val, label, color in [(1, 'Working', 'steelblue'), (0, 'Not Working', '#D55E00')]:\n",
    "    subset = df[df['lfp'] == lfp_val]\n",
    "    axes[0, 2].hist(subset['husband_income'], bins=15, alpha=0.6, label=label,\n",
    "                    color=color, edgecolor='black', density=True)\n",
    "axes[0, 2].set_xlabel('Husband Income ($1000s)')\n",
    "axes[0, 2].set_ylabel('Density')\n",
    "axes[0, 2].set_title('Husband Income by LFP Status')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Participation rate by number of young children\n",
    "lfp_by_children = df.groupby('children_lt6')['lfp'].mean()\n",
    "axes[1, 0].bar(lfp_by_children.index, lfp_by_children.values,\n",
    "               color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Number of Children < 6')\n",
    "axes[1, 0].set_ylabel('Participation Rate')\n",
    "axes[1, 0].set_title('LFP Rate by Young Children')\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "\n",
    "# Participation rate by education level\n",
    "edu_bins = pd.cut(df['education'], bins=[0, 10, 12, 14, 20],\n",
    "                  labels=['<= 10', '11-12', '13-14', '15+'])\n",
    "lfp_by_edu = df.groupby(edu_bins, observed=True)['lfp'].mean()\n",
    "axes[1, 1].bar(range(len(lfp_by_edu)), lfp_by_edu.values,\n",
    "               color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xticks(range(len(lfp_by_edu)))\n",
    "axes[1, 1].set_xticklabels(lfp_by_edu.index)\n",
    "axes[1, 1].set_xlabel('Education Level')\n",
    "axes[1, 1].set_ylabel('Participation Rate')\n",
    "axes[1, 1].set_title('LFP Rate by Education')\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "# Wage distribution (working women only)\n",
    "working_wages = df[df['lfp'] == 1]['wage'].dropna()\n",
    "axes[1, 2].hist(working_wages, bins=25, color='steelblue',\n",
    "                edgecolor='black', alpha=0.7)\n",
    "axes[1, 2].axvline(working_wages.mean(), color='red', linestyle='--',\n",
    "                    linewidth=2, label=f'Mean = ${working_wages.mean():.2f}')\n",
    "axes[1, 2].set_xlabel('Hourly Wage ($)')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].set_title('Wage Distribution (Working Women Only)')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.suptitle('Exploratory Analysis: Labor Force Participation Patterns',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'eda_participation_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Six-panel exploratory analysis. Top row: density distributions of education, age, and husband income by labor force status (blue = working, orange = not working). Bottom row: participation rate by number of young children (strong negative effect), participation rate by education level (strong positive effect), and the wage distribution for working women (right-skewed, observed only for participants). These patterns reveal systematic differences between participants and non-participants, motivating the need for selection correction.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. The Two-Step Procedure\n",
    "\n",
    "### Step 1: Probit Model for Selection\n",
    "\n",
    "Estimate the probability of labor force participation:\n",
    "\n",
    "$$P(s_i = 1 | \\mathbf{Z}_i) = \\Phi(\\mathbf{Z}_i'\\boldsymbol{\\gamma})$$\n",
    "\n",
    "Where $\\mathbf{Z}_i$ includes:\n",
    "- `education`, `experience`, `age` (also in outcome equation)\n",
    "- `children_lt6`, `children_6_18`, `husband_income` (exclusion restrictions)\n",
    "\n",
    "### Step 2: Augmented OLS with IMR\n",
    "\n",
    "From the Probit estimates $\\hat{\\boldsymbol{\\gamma}}$, compute the **Inverse Mills Ratio** for each selected observation:\n",
    "\n",
    "$$\\hat{\\lambda}_i = \\frac{\\phi(\\mathbf{Z}_i'\\hat{\\boldsymbol{\\gamma}})}{\\Phi(\\mathbf{Z}_i'\\hat{\\boldsymbol{\\gamma}})}$$\n",
    "\n",
    "Then estimate the outcome equation by OLS, augmented with $\\hat{\\lambda}_i$:\n",
    "\n",
    "$$y_i = \\mathbf{X}_i'\\boldsymbol{\\beta} + \\hat{\\theta} \\cdot \\hat{\\lambda}_i + \\eta_i \\qquad \\text{(selected sample only)}$$\n",
    "\n",
    "Where $\\hat{\\theta} = \\hat{\\rho} \\cdot \\hat{\\sigma}_\\varepsilon$. If $\\hat{\\theta}$ is statistically significant, selection bias is present.\n",
    "\n",
    "### Intuition for the IMR\n",
    "\n",
    "The Inverse Mills Ratio $\\lambda_i$ is the **hazard rate** of the standard normal distribution. It captures:\n",
    "\n",
    "- **For observations with low selection probability**: $\\lambda_i$ is large. These women are \"unlikely\" participants, so their wages contain more information about unobserved ability.\n",
    "- **For observations with high selection probability**: $\\lambda_i$ is small. These women would work regardless of unobserved factors, so little correction is needed.\n",
    "\n",
    "Think of $\\lambda_i$ as measuring the \"surprise\" of being selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step demonstration of the Heckman procedure\n",
    "# (Before using PanelBox, let's see the mechanics)\n",
    "\n",
    "# ---- STEP 1: Probit for selection equation ----\n",
    "print('STEP 1: Probit Model for Labor Force Participation')\n",
    "print('=' * 60)\n",
    "\n",
    "# Selection equation variables (Z): broader set including exclusion restrictions\n",
    "Z_vars = ['education', 'experience', 'age',\n",
    "          'children_lt6', 'children_6_18', 'husband_income']\n",
    "\n",
    "Z_raw = df[Z_vars].values\n",
    "Z = sm.add_constant(Z_raw)  # Add intercept\n",
    "selection = df['lfp'].values\n",
    "\n",
    "# Fit Probit using statsmodels for illustration\n",
    "probit_model = sm.Probit(selection, Z)\n",
    "probit_result = probit_model.fit(disp=0)\n",
    "\n",
    "# Display Probit results\n",
    "probit_table = pd.DataFrame({\n",
    "    'Variable': ['const'] + Z_vars,\n",
    "    'Coefficient': probit_result.params,\n",
    "    'Std. Error': probit_result.bse,\n",
    "    'z-stat': probit_result.tvalues,\n",
    "    'p-value': probit_result.pvalues,\n",
    "})\n",
    "\n",
    "display(probit_table.round(4))\n",
    "\n",
    "print('\\nExclusion restrictions (in selection but NOT in outcome):')\n",
    "print('  - children_lt6:   strong negative effect on participation')\n",
    "print('  - children_6_18:  moderate negative effect on participation')\n",
    "print('  - husband_income: higher husband income reduces need to work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 2: Compute the Inverse Mills Ratio ----\n",
    "print('STEP 2: Compute the Inverse Mills Ratio')\n",
    "print('=' * 60)\n",
    "\n",
    "# Linear prediction from Probit: Z * gamma_hat\n",
    "linear_pred = Z @ probit_result.params\n",
    "\n",
    "# Compute IMR = phi(Z*gamma) / Phi(Z*gamma)\n",
    "phi_vals = stats.norm.pdf(linear_pred)  # Standard normal PDF\n",
    "Phi_vals = stats.norm.cdf(linear_pred)  # Standard normal CDF\n",
    "\n",
    "# Clip CDF to avoid division by zero\n",
    "Phi_clipped = np.clip(Phi_vals, 1e-10, 1 - 1e-10)\n",
    "imr_manual = phi_vals / Phi_clipped\n",
    "\n",
    "# Compare: PanelBox compute_imr utility\n",
    "imr_panelbox = compute_imr(linear_pred)\n",
    "\n",
    "# Show IMR statistics for selected (working) women\n",
    "selected_mask = df['lfp'] == 1\n",
    "imr_selected = imr_manual[selected_mask]\n",
    "\n",
    "print(f'IMR statistics (working women only):')\n",
    "print(f'  Mean:   {imr_selected.mean():.4f}')\n",
    "print(f'  Std:    {imr_selected.std():.4f}')\n",
    "print(f'  Min:    {imr_selected.min():.4f}')\n",
    "print(f'  Max:    {imr_selected.max():.4f}')\n",
    "print(f'  High IMR (>2): {(imr_selected > 2).sum()} observations')\n",
    "print(f'\\nManual vs PanelBox IMR match: {np.allclose(imr_manual, imr_panelbox)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 2 continued: Augmented OLS on selected sample ----\n",
    "print('STEP 2 (continued): Augmented OLS with IMR')\n",
    "print('=' * 60)\n",
    "\n",
    "# Outcome equation variables (X): regressors that affect wages\n",
    "X_vars = ['education', 'experience', 'experience_sq']\n",
    "\n",
    "# Extract selected sample (working women only)\n",
    "df_working = df[df['lfp'] == 1].copy()\n",
    "y_working = df_working['wage'].values\n",
    "X_working_raw = df_working[X_vars].values\n",
    "X_working = sm.add_constant(X_working_raw)\n",
    "\n",
    "# IMR for working women\n",
    "lambda_working = imr_selected.reshape(-1, 1)\n",
    "\n",
    "# Augmented design matrix: [X, lambda]\n",
    "X_augmented = np.column_stack([X_working, lambda_working])\n",
    "\n",
    "# OLS on augmented model\n",
    "ols_augmented = sm.OLS(y_working, X_augmented).fit()\n",
    "\n",
    "aug_table = pd.DataFrame({\n",
    "    'Variable': ['const'] + X_vars + ['lambda (IMR)'],\n",
    "    'Coefficient': ols_augmented.params,\n",
    "    'Std. Error': ols_augmented.bse,\n",
    "    't-stat': ols_augmented.tvalues,\n",
    "    'p-value': ols_augmented.pvalues,\n",
    "})\n",
    "\n",
    "display(aug_table.round(4))\n",
    "\n",
    "# Extract selection parameters\n",
    "theta_hat = ols_augmented.params[-1]  # IMR coefficient = rho * sigma\n",
    "residuals = ols_augmented.resid\n",
    "sigma_hat = np.sqrt(np.mean(residuals**2))\n",
    "rho_hat = np.clip(theta_hat / sigma_hat, -0.99, 0.99)\n",
    "\n",
    "print(f'\\nSelection parameters:')\n",
    "print(f'  theta (rho*sigma) = {theta_hat:.4f}')\n",
    "print(f'  sigma_hat         = {sigma_hat:.4f}')\n",
    "print(f'  rho_hat           = {rho_hat:.4f}')\n",
    "print(f'\\nThe IMR coefficient is the key test for selection bias.')\n",
    "print(f'If lambda is significant, selection bias is present.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Estimation with PanelBox\n",
    "\n",
    "Now let us use PanelBox's `PanelHeckman` class to estimate the model in a single, clean call. This handles all the steps internally: Probit estimation, IMR computation, and augmented OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PanelHeckman\n",
    "# IMPORTANT: PanelHeckman requires ALL observations (selected and not selected)\n",
    "\n",
    "# Outcome variable: wage (set to 0 for non-participants since PanelHeckman\n",
    "# needs a complete vector; these values are ignored in estimation)\n",
    "y_all = df['wage'].fillna(0).values\n",
    "\n",
    "# Selection indicator: labor force participation (0/1)\n",
    "selection = df['lfp'].values.astype(float)\n",
    "\n",
    "# Outcome equation regressors (X): variables that affect wages\n",
    "X_outcome_raw = df[['education', 'experience', 'experience_sq']].values\n",
    "X_outcome = sm.add_constant(X_outcome_raw)  # Add intercept\n",
    "\n",
    "# Selection equation regressors (Z): broader set with exclusion restrictions\n",
    "Z_selection_raw = df[['education', 'experience', 'age',\n",
    "                       'children_lt6', 'children_6_18', 'husband_income']].values\n",
    "Z_selection = sm.add_constant(Z_selection_raw)  # Add intercept\n",
    "\n",
    "print('Data dimensions for PanelHeckman:')\n",
    "print(f'  y (outcome):         {y_all.shape}')\n",
    "print(f'  X (outcome regs):    {X_outcome.shape}  (const + {X_outcome.shape[1]-1} vars)')\n",
    "print(f'  selection:           {selection.shape}  ({int(selection.sum())} selected)')\n",
    "print(f'  Z (selection regs):  {Z_selection.shape}  (const + {Z_selection.shape[1]-1} vars)')\n",
    "print(f'\\nExclusion restrictions: children_lt6, children_6_18, husband_income')\n",
    "print(f'  (in Z but NOT in X -- required for identification)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the Heckman two-step model\n",
    "print('Estimating Heckman Two-Step Model...')\n",
    "print('=' * 60)\n",
    "\n",
    "heckman_model = PanelHeckman(\n",
    "    endog=y_all,               # outcome variable (full sample)\n",
    "    exog=X_outcome,            # outcome regressors with constant\n",
    "    selection=selection,        # binary selection indicator\n",
    "    exog_selection=Z_selection, # selection regressors with constant\n",
    "    method='two_step'          # Heckman two-step procedure\n",
    ")\n",
    "\n",
    "heckman_result = heckman_model.fit()\n",
    "\n",
    "# Display full summary\n",
    "print(heckman_result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Interpreting the Results\n",
    "\n",
    "### Outcome Equation Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed outcome equation results\n",
    "outcome_var_names = ['const', 'education', 'experience', 'experience_sq']\n",
    "\n",
    "outcome_table = pd.DataFrame({\n",
    "    'Variable': outcome_var_names,\n",
    "    'Coefficient': heckman_result.outcome_params,\n",
    "})\n",
    "\n",
    "print('Outcome Equation: wage = X * beta')\n",
    "print('=' * 60)\n",
    "display(outcome_table.round(4))\n",
    "\n",
    "print('\\nInterpretation:')\n",
    "print(f'  Education:    Each additional year of education increases')\n",
    "print(f'                hourly wage by ${outcome_table.iloc[1][\"Coefficient\"]:.2f}')\n",
    "print(f'  Experience:   Returns to experience (with diminishing returns')\n",
    "print(f'                captured by the squared term)')\n",
    "print(f'\\nThese are the CORRECTED estimates, accounting for selection bias.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed selection equation results\n",
    "selection_var_names = ['const', 'education', 'experience', 'age',\n",
    "                       'children_lt6', 'children_6_18', 'husband_income']\n",
    "\n",
    "selection_table = pd.DataFrame({\n",
    "    'Variable': selection_var_names,\n",
    "    'Coefficient': heckman_result.probit_params,\n",
    "})\n",
    "\n",
    "print('Selection Equation: P(lfp=1) = Phi(Z * gamma)')\n",
    "print('=' * 60)\n",
    "display(selection_table.round(4))\n",
    "\n",
    "print('\\nInterpretation of exclusion restrictions:')\n",
    "print(f'  children_lt6:   Young children strongly reduce participation')\n",
    "print(f'  children_6_18:  School-age children have a smaller effect')\n",
    "print(f'  husband_income: Higher husband income reduces need to work')\n",
    "print(f'\\nThese variables affect SELECTION but not WAGES directly.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection parameters: rho and sigma\n",
    "print('Selection Parameters')\n",
    "print('=' * 60)\n",
    "print(f'  sigma (error std dev):  {heckman_result.sigma:.4f}')\n",
    "print(f'  rho (error correlation): {heckman_result.rho:.4f}')\n",
    "print(f'  lambda = rho * sigma:    {heckman_result.rho * heckman_result.sigma:.4f}')\n",
    "\n",
    "print(f'\\nInterpretation of rho = {heckman_result.rho:.4f}:')\n",
    "if heckman_result.rho > 0:\n",
    "    print('  POSITIVE selection: unobserved factors that increase')\n",
    "    print('  participation also increase wages.')\n",
    "    print('  --> Women who choose to work have higher-than-average')\n",
    "    print('      unobserved ability, so OLS on the working sample')\n",
    "    print('      OVERSTATES average wage effects.')\n",
    "elif heckman_result.rho < 0:\n",
    "    print('  NEGATIVE selection: unobserved factors that increase')\n",
    "    print('  participation decrease wages.')\n",
    "    print('  --> Women who choose to work have lower-than-average')\n",
    "    print('      unobserved wage potential (e.g., financial need).')\n",
    "else:\n",
    "    print('  No selection bias: participation and wages are independent.')\n",
    "\n",
    "print(f'\\n  sigma = {heckman_result.sigma:.4f} represents the standard deviation')\n",
    "print(f'  of the outcome equation errors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. The Inverse Mills Ratio: Visualization and Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the theoretical IMR function\n",
    "# lambda(z) = phi(z) / Phi(z)\n",
    "z_grid = np.linspace(-3, 3, 500)\n",
    "imr_grid = stats.norm.pdf(z_grid) / stats.norm.cdf(z_grid)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: IMR as a function of z\n",
    "axes[0].plot(z_grid, imr_grid, color='steelblue', linewidth=2.5)\n",
    "axes[0].set_xlabel(r\"$z = \\mathbf{Z}'\\hat{\\gamma}$ (selection index)\", fontsize=12)\n",
    "axes[0].set_ylabel(r'$\\lambda(z) = \\phi(z) / \\Phi(z)$', fontsize=12)\n",
    "axes[0].set_title('Inverse Mills Ratio Function')\n",
    "axes[0].axhline(y=0, color='gray', linestyle=':', alpha=0.5)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Annotate key regions\n",
    "axes[0].annotate('Low selection prob\\n(large correction)',\n",
    "                 xy=(-2, stats.norm.pdf(-2)/stats.norm.cdf(-2)),\n",
    "                 xytext=(-1.5, 6), fontsize=10,\n",
    "                 arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                 color='red')\n",
    "axes[0].annotate('High selection prob\\n(small correction)',\n",
    "                 xy=(2, stats.norm.pdf(2)/stats.norm.cdf(2)),\n",
    "                 xytext=(0.5, 3), fontsize=10,\n",
    "                 arrowprops=dict(arrowstyle='->', color='green'),\n",
    "                 color='green')\n",
    "\n",
    "# Right: Selection probability vs IMR\n",
    "prob_grid = stats.norm.cdf(z_grid)\n",
    "axes[1].plot(prob_grid, imr_grid, color='steelblue', linewidth=2.5)\n",
    "axes[1].set_xlabel('Selection Probability P(s=1)', fontsize=12)\n",
    "axes[1].set_ylabel(r'$\\lambda$ (Inverse Mills Ratio)', fontsize=12)\n",
    "axes[1].set_title('IMR vs Selection Probability')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'imr_theoretical.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Key insight: The IMR is a decreasing function of the selection probability.')\n",
    "print('Observations with low P(selected) receive the largest corrections.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Left panel shows the Inverse Mills Ratio as a function of the selection index z. The IMR is large when z is negative (low selection probability), meaning observations with low participation likelihood receive the greatest correction. As z increases (high participation probability), the IMR approaches zero (minimal correction needed). Right panel shows the same relationship plotted against the selection probability directly, confirming the monotonically decreasing relationship.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PanelHeckman's built-in IMR diagnostics and plotting\n",
    "\n",
    "# IMR diagnostics\n",
    "diag = heckman_result.imr_diagnostics()\n",
    "\n",
    "print('IMR Diagnostics')\n",
    "print('=' * 50)\n",
    "for key, value in diag.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f'  {key:20s}: {value:.4f}')\n",
    "    else:\n",
    "        print(f'  {key:20s}: {value}')\n",
    "\n",
    "print(f'\\nInterpretation:')\n",
    "print(f'  - Selection rate: {diag[\"selection_rate\"]:.1%} of women participate')\n",
    "if diag['high_imr_count'] > 0:\n",
    "    print(f'  - {diag[\"high_imr_count\"]} observations have high IMR (> 2),')\n",
    "    print(f'    indicating strong selection effects for those individuals')\n",
    "else:\n",
    "    print(f'  - No observations have extremely high IMR values')\n",
    "    print(f'    Selection correction is moderate across the sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PanelHeckman's built-in plot_imr method\n",
    "fig = heckman_result.plot_imr(figsize=(14, 5))\n",
    "plt.savefig(FIGURES_DIR / 'imr_diagnostics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Two-panel IMR diagnostic plot from PanelHeckman. Left panel: scatter plot of IMR values against predicted selection probability for working women. The inverse relationship is clear -- women with lower selection probability receive larger corrections. The red dashed line at IMR = 2 marks the threshold for strong selection effects. Right panel: histogram of IMR values for the selected sample, showing the distribution of correction magnitudes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional IMR visualization: how the correction varies with key variables\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Get IMR values for working women\n",
    "imr_values = heckman_result.lambda_imr[selection == 1]\n",
    "df_working_viz = df[df['lfp'] == 1].copy()\n",
    "df_working_viz['imr'] = imr_values\n",
    "\n",
    "# IMR vs Education\n",
    "axes[0].scatter(df_working_viz['education'], df_working_viz['imr'],\n",
    "                alpha=0.4, s=20, color='steelblue')\n",
    "axes[0].set_xlabel('Years of Education')\n",
    "axes[0].set_ylabel('Inverse Mills Ratio')\n",
    "axes[0].set_title('IMR vs Education')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# IMR vs Husband Income\n",
    "axes[1].scatter(df_working_viz['husband_income'], df_working_viz['imr'],\n",
    "                alpha=0.4, s=20, color='steelblue')\n",
    "axes[1].set_xlabel('Husband Income ($1000s)')\n",
    "axes[1].set_ylabel('Inverse Mills Ratio')\n",
    "axes[1].set_title('IMR vs Husband Income')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# IMR vs Wage\n",
    "axes[2].scatter(df_working_viz['wage'], df_working_viz['imr'],\n",
    "                alpha=0.4, s=20, color='steelblue')\n",
    "axes[2].set_xlabel('Hourly Wage ($)')\n",
    "axes[2].set_ylabel('Inverse Mills Ratio')\n",
    "axes[2].set_title('IMR vs Observed Wage')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Selection Correction Across Key Variables',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'imr_by_variable.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('The IMR captures how much selection correction each observation needs.')\n",
    "print('Women with high husband income (who work despite not needing to)')\n",
    "print('may have particularly high unobserved ability, reflected in higher IMR.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Three scatter plots showing how the Inverse Mills Ratio relates to education, husband income, and observed wages. The IMR tends to be higher for women whose observable characteristics make participation less likely (e.g., higher husband income), indicating that these women receive a larger selection correction. The relationship between IMR and wage helps visualize the selection mechanism.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Testing for Selection Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Using PanelHeckman's built-in selection_test()\n",
    "print('Test for Selection Bias')\n",
    "print('=' * 60)\n",
    "print('H0: rho = 0 (no selection bias, OLS is consistent)')\n",
    "print('H1: rho != 0 (selection bias present, OLS is biased)')\n",
    "print()\n",
    "\n",
    "test_result = heckman_result.selection_test()\n",
    "\n",
    "print(f'Results:')\n",
    "print(f'  rho           = {test_result[\"rho\"]:.4f}')\n",
    "print(f'  z-statistic   = {test_result[\"z_statistic\"]:.4f}')\n",
    "print(f'  p-value       = {test_result[\"p_value\"]:.4f}')\n",
    "print(f'  Significant?  = {test_result[\"significant\"]}')\n",
    "\n",
    "if test_result['significant']:\n",
    "    print(f'\\nConclusion: REJECT H0 at 5% level.')\n",
    "    print(f'Selection bias is statistically significant.')\n",
    "    print(f'OLS on the working sample would produce biased estimates.')\n",
    "    print(f'The Heckman correction is warranted.')\n",
    "else:\n",
    "    print(f'\\nConclusion: FAIL TO REJECT H0 at 5% level.')\n",
    "    print(f'No strong evidence of selection bias.')\n",
    "    print(f'OLS and Heckman estimates should be similar.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Using the selection_effect() method (more detailed)\n",
    "print('Detailed Selection Effect Test')\n",
    "print('=' * 60)\n",
    "\n",
    "effect_result = heckman_result.selection_effect(alpha=0.05)\n",
    "\n",
    "print(f'  Test statistic:  {effect_result[\"statistic\"]:.4f}')\n",
    "print(f'  p-value:         {effect_result[\"pvalue\"]:.4f}')\n",
    "print(f'  Reject H0:       {effect_result[\"reject\"]}')\n",
    "print(f'\\n  {effect_result[\"interpretation\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Direct test using IMR coefficient from augmented OLS\n",
    "# This is the most transparent approach\n",
    "print('Direct Test: Is the IMR Coefficient Significant?')\n",
    "print('=' * 60)\n",
    "\n",
    "# Use the test_selection_effect utility with the augmented OLS results\n",
    "theta = ols_augmented.params[-1]      # IMR coefficient\n",
    "theta_se = ols_augmented.bse[-1]      # IMR standard error\n",
    "\n",
    "direct_test = test_selection_effect(\n",
    "    imr_coefficient=theta,\n",
    "    imr_se=theta_se,\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(f'  IMR coefficient (theta = rho*sigma): {direct_test[\"imr_coefficient\"]:.4f}')\n",
    "print(f'  Standard error:                      {direct_test[\"imr_se\"]:.4f}')\n",
    "print(f'  t-statistic:                         {direct_test[\"statistic\"]:.4f}')\n",
    "print(f'  p-value:                             {direct_test[\"pvalue\"]:.4f}')\n",
    "print(f'  Reject H0 (alpha=0.05):              {direct_test[\"reject\"]}')\n",
    "print(f'\\n  {direct_test[\"interpretation\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. OLS vs Heckman: Demonstrating the Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLS (biased) vs Heckman (corrected) using built-in method\n",
    "print('OLS vs Heckman Comparison')\n",
    "print('=' * 60)\n",
    "\n",
    "comparison = heckman_result.compare_ols_heckman()\n",
    "\n",
    "comp_table = pd.DataFrame({\n",
    "    'Variable': outcome_var_names,\n",
    "    'OLS (biased)': comparison['beta_ols'],\n",
    "    'Heckman (corrected)': comparison['beta_heckman'],\n",
    "    'Difference': comparison['difference'],\n",
    "    '% Difference': comparison['pct_difference'],\n",
    "})\n",
    "\n",
    "display(comp_table.round(4))\n",
    "\n",
    "print(f'\\nMaximum absolute difference: {comparison[\"max_abs_difference\"]:.4f}')\n",
    "print(f'\\n{comparison[\"interpretation\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: side-by-side coefficient comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left panel: bar chart of coefficients\n",
    "x_pos = np.arange(len(outcome_var_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x_pos - width/2, comparison['beta_ols'],\n",
    "                     width, label='OLS (biased)', color='#D55E00',\n",
    "                     alpha=0.8, edgecolor='black')\n",
    "bars2 = axes[0].bar(x_pos + width/2, comparison['beta_heckman'],\n",
    "                     width, label='Heckman (corrected)', color='steelblue',\n",
    "                     alpha=0.8, edgecolor='black')\n",
    "\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(outcome_var_names, rotation=15)\n",
    "axes[0].set_ylabel('Coefficient Value')\n",
    "axes[0].set_title('OLS vs Heckman: Coefficient Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "# Right panel: percentage difference\n",
    "# Exclude constant for cleaner visualization\n",
    "pct_diff = comparison['pct_difference'][1:]  # Skip constant\n",
    "var_names_no_const = outcome_var_names[1:]\n",
    "\n",
    "colors = ['#D55E00' if d > 0 else 'steelblue' for d in pct_diff]\n",
    "axes[1].barh(var_names_no_const, pct_diff, color=colors,\n",
    "              alpha=0.8, edgecolor='black')\n",
    "axes[1].axvline(x=0, color='black', linewidth=1)\n",
    "axes[1].set_xlabel('Percentage Difference (OLS - Heckman) / Heckman x 100')\n",
    "axes[1].set_title('Selection Bias: % Difference in Coefficients')\n",
    "axes[1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('Quantifying Selection Bias: OLS vs Heckman',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ols_vs_heckman.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Positive % difference: OLS overestimates relative to Heckman')\n",
    "print('Negative % difference: OLS underestimates relative to Heckman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Left panel shows a side-by-side bar chart comparing OLS (orange) and Heckman (blue) coefficients for each variable in the outcome equation. Right panel shows the percentage difference between the two estimators for the slope coefficients (excluding the constant). Bars to the right of zero indicate OLS overestimation; bars to the left indicate underestimation. The magnitude of the bars quantifies the selection bias in each coefficient.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted wage comparison: OLS vs Heckman\n",
    "# Show how predictions differ for the working sample\n",
    "\n",
    "# OLS predictions (on working sample)\n",
    "y_pred_ols = X_working @ comparison['beta_ols']\n",
    "\n",
    "# Heckman predictions (conditional on selection)\n",
    "y_pred_heckman = heckman_result.predict(type='conditional')\n",
    "y_pred_heckman_selected = y_pred_heckman[selection == 1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: OLS predicted vs actual\n",
    "axes[0].scatter(y_working, y_pred_ols, alpha=0.3, s=15, color='#D55E00')\n",
    "axes[0].plot([0, y_working.max()], [0, y_working.max()],\n",
    "             'k--', linewidth=1.5, alpha=0.7, label='45-degree line')\n",
    "axes[0].set_xlabel('Actual Wage ($)')\n",
    "axes[0].set_ylabel('Predicted Wage ($)')\n",
    "axes[0].set_title('OLS Predictions (Biased)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Right: Heckman predicted vs actual\n",
    "axes[1].scatter(y_working, y_pred_heckman_selected, alpha=0.3, s=15,\n",
    "                color='steelblue')\n",
    "axes[1].plot([0, y_working.max()], [0, y_working.max()],\n",
    "             'k--', linewidth=1.5, alpha=0.7, label='45-degree line')\n",
    "axes[1].set_xlabel('Actual Wage ($)')\n",
    "axes[1].set_ylabel('Predicted Wage ($)')\n",
    "axes[1].set_title('Heckman Predictions (Corrected)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Predicted vs Actual Wages', fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'predictions_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Fit statistics\n",
    "rmse_ols = np.sqrt(np.mean((y_working - y_pred_ols)**2))\n",
    "rmse_heckman = np.sqrt(np.mean((y_working - y_pred_heckman_selected)**2))\n",
    "corr_ols = np.corrcoef(y_working, y_pred_ols)[0, 1]\n",
    "corr_heckman = np.corrcoef(y_working, y_pred_heckman_selected)[0, 1]\n",
    "\n",
    "print(f'Prediction Performance:')\n",
    "print(f'  OLS:     RMSE = {rmse_ols:.3f}, Corr = {corr_ols:.3f}')\n",
    "print(f'  Heckman: RMSE = {rmse_heckman:.3f}, Corr = {corr_heckman:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Left panel shows OLS predicted wages vs actual wages for the working sample. Right panel shows Heckman predicted wages vs actual wages. The 45-degree line represents perfect prediction. Differences between the two panels reflect the impact of the selection correction on predicted values.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **The Selection Problem**\n",
    "   - When outcomes are observed only for a non-random subsample, OLS is biased\n",
    "   - The bias arises because unobservables that drive selection are correlated with the outcome\n",
    "   - This is a form of omitted variable bias\n",
    "\n",
    "2. **Heckman's Two-Step Correction**\n",
    "   - Step 1: Estimate a Probit model for the selection equation\n",
    "   - Step 2: Compute the Inverse Mills Ratio and include it as an additional regressor in the outcome equation\n",
    "   - The IMR absorbs the selection bias, yielding consistent estimates\n",
    "\n",
    "3. **Key Parameters**\n",
    "   - $\\rho$ (rho): correlation between selection and outcome errors\n",
    "     - $\\rho > 0$: positive selection (participants have higher unobserved ability)\n",
    "     - $\\rho < 0$: negative selection (participants have lower unobserved ability)\n",
    "     - $\\rho = 0$: no selection bias\n",
    "   - $\\sigma$: standard deviation of outcome errors\n",
    "   - $\\lambda = \\rho \\sigma$: the IMR coefficient, directly testing for selection bias\n",
    "\n",
    "4. **Exclusion Restrictions Are Essential**\n",
    "   - At least one variable must affect selection but NOT the outcome\n",
    "   - Without exclusion restrictions, the model is identified only through functional form (fragile)\n",
    "   - In the Mroz data: `children_lt6`, `children_6_18`, `husband_income`\n",
    "\n",
    "5. **Testing for Selection Bias**\n",
    "   - Test $H_0: \\rho = 0$ using the significance of the IMR coefficient\n",
    "   - If we fail to reject, OLS is acceptable\n",
    "   - If we reject, the Heckman correction is needed\n",
    "\n",
    "### PanelBox Implementation Summary\n",
    "\n",
    "```python\n",
    "from panelbox.models.selection import PanelHeckman\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Prepare data (ALL observations, not just selected)\n",
    "y = df['wage'].fillna(0).values\n",
    "selection = df['lfp'].values\n",
    "X = sm.add_constant(df[['education', 'experience', 'experience_sq']].values)\n",
    "Z = sm.add_constant(df[['education', 'experience', 'age',\n",
    "                         'children_lt6', 'children_6_18', 'husband_income']].values)\n",
    "\n",
    "# Estimate\n",
    "model = PanelHeckman(endog=y, exog=X, selection=selection,\n",
    "                     exog_selection=Z, method='two_step')\n",
    "result = model.fit()\n",
    "\n",
    "# Interpret\n",
    "print(result.summary())\n",
    "print(f'rho = {result.rho:.4f}, sigma = {result.sigma:.4f}')\n",
    "\n",
    "# Test for selection bias\n",
    "test = result.selection_test()\n",
    "\n",
    "# Compare with OLS\n",
    "comparison = result.compare_ols_heckman()\n",
    "\n",
    "# Diagnostics\n",
    "diag = result.imr_diagnostics()\n",
    "fig = result.plot_imr()\n",
    "```\n",
    "\n",
    "### Mathematical Summary\n",
    "\n",
    "| Component | Formula |\n",
    "|---|---|\n",
    "| Selection | $s_i^* = \\mathbf{Z}_i'\\boldsymbol{\\gamma} + u_i$, $s_i = \\mathbf{1}[s_i^* > 0]$ |\n",
    "| Outcome | $y_i = \\mathbf{X}_i'\\boldsymbol{\\beta} + \\varepsilon_i$ |\n",
    "| IMR | $\\lambda_i = \\phi(\\mathbf{Z}_i'\\hat{\\boldsymbol{\\gamma}}) / \\Phi(\\mathbf{Z}_i'\\hat{\\boldsymbol{\\gamma}})$ |\n",
    "| Corrected | $y_i = \\mathbf{X}_i'\\boldsymbol{\\beta} + \\rho\\sigma_\\varepsilon\\lambda_i + \\eta_i$ |\n",
    "| Bias test | $H_0: \\rho = 0$ via t-test on $\\hat{\\theta} = \\hat{\\rho}\\hat{\\sigma}_\\varepsilon$ |\n",
    "\n",
    "### References\n",
    "\n",
    "- Heckman, J.J. (1979). \"Sample Selection Bias as a Specification Error.\" *Econometrica*, 47(1), 153-161.\n",
    "- Mroz, T.A. (1987). \"The Sensitivity of an Empirical Model of Married Women's Hours of Work to Economic and Statistical Assumptions.\" *Econometrica*, 55(4), 765-799.\n",
    "- Wooldridge, J.M. (1995). \"Selection Corrections for Panel Data Models Under Conditional Mean Independence Assumptions.\" *Journal of Econometrics*, 68(1), 115-132.\n",
    "- Wooldridge, J.M. (2010). *Econometric Analysis of Cross Section and Panel Data* (2nd ed.). MIT Press, Chapter 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Exercises\n",
    "\n",
    "Try these exercises to reinforce your understanding:\n",
    "\n",
    "### Exercise 1: Alternative Exclusion Restrictions\n",
    "Re-estimate the Heckman model using only `children_lt6` and `husband_income` as exclusion restrictions (drop `children_6_18` from Z). Compare the results with the full specification. Are the estimates sensitive to the choice of exclusion restrictions?\n",
    "\n",
    "### Exercise 2: Log-Wage Specification\n",
    "Estimate the Heckman model with `log(wage)` as the outcome variable instead of `wage` in levels. This is more standard in labor economics. Compare the education coefficient with the level specification. Which specification is more appropriate, and why?\n",
    "\n",
    "### Exercise 3: Unconditional vs Conditional Predictions\n",
    "Use `result.predict(type='unconditional')` and `result.predict(type='conditional')` to generate both types of predictions. Plot them against each other and explain the difference. For which women is the gap largest?\n",
    "\n",
    "### Exercise 4: Monte Carlo Selection Bias\n",
    "Generate synthetic data where you know the true parameters:\n",
    "- Create a DGP with known rho = 0.5\n",
    "- Estimate OLS (ignoring selection) and Heckman\n",
    "- Repeat 500 times and show the bias distribution\n",
    "- Verify that Heckman is unbiased while OLS is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Exercise 1: Alternative Exclusion Restrictions ----\n",
    "# Try re-estimating with a different set of exclusion restrictions\n",
    "\n",
    "# Your code here:\n",
    "# Z_alt_raw = df[['education', 'experience', 'age',\n",
    "#                  'children_lt6', 'husband_income']].values\n",
    "# Z_alt = sm.add_constant(Z_alt_raw)\n",
    "# model_alt = PanelHeckman(endog=y_all, exog=X_outcome,\n",
    "#                          selection=selection, exog_selection=Z_alt,\n",
    "#                          method='two_step')\n",
    "# result_alt = model_alt.fit()\n",
    "# print(result_alt.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Exercise 2: Log-Wage Specification ----\n",
    "# Estimate with log(wage) as the dependent variable\n",
    "\n",
    "# Your code here:\n",
    "# df['log_wage'] = np.log(df['wage'])\n",
    "# y_log = df['log_wage'].fillna(0).values\n",
    "# model_log = PanelHeckman(endog=y_log, exog=X_outcome,\n",
    "#                          selection=selection, exog_selection=Z_selection,\n",
    "#                          method='two_step')\n",
    "# result_log = model_log.fit()\n",
    "# print(result_log.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Exercise 3: Unconditional vs Conditional Predictions ----\n",
    "# Compare the two types of predictions\n",
    "\n",
    "# Your code here:\n",
    "# y_uncond = heckman_result.predict(type='unconditional')\n",
    "# y_cond = heckman_result.predict(type='conditional')\n",
    "# plt.scatter(y_uncond, y_cond, alpha=0.3)\n",
    "# plt.xlabel('Unconditional E[y*]')\n",
    "# plt.ylabel('Conditional E[y|selected]')\n",
    "# plt.title('Unconditional vs Conditional Predictions')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Exercise 4: Monte Carlo Selection Bias ----\n",
    "# Demonstrate selection bias with known DGP\n",
    "\n",
    "# Your code here:\n",
    "# Hint: Use bivariate normal errors with rho=0.5\n",
    "# from scipy.stats import multivariate_normal\n",
    "# n_sims = 500\n",
    "# n = 1000\n",
    "# beta_true = np.array([1.0, 0.5])\n",
    "# gamma_true = np.array([0.0, 0.3])\n",
    "# rho_true = 0.5\n",
    "# ...\n",
    "# Compare distributions of beta_hat_ols vs beta_hat_heckman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results for reference\n",
    "results_summary = {\n",
    "    'outcome_params': heckman_result.outcome_params.tolist(),\n",
    "    'probit_params': heckman_result.probit_params.tolist(),\n",
    "    'sigma': heckman_result.sigma,\n",
    "    'rho': heckman_result.rho,\n",
    "    'n_total': heckman_result.n_total,\n",
    "    'n_selected': int(heckman_result.n_selected),\n",
    "    'selection_test': heckman_result.selection_test(),\n",
    "    'imr_diagnostics': heckman_result.imr_diagnostics(),\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(TABLES_DIR / 'heckman_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "comp_table.to_csv(TABLES_DIR / 'ols_vs_heckman_comparison.csv', index=False)\n",
    "\n",
    "print('Results saved to:')\n",
    "print(f'  {TABLES_DIR / \"heckman_results.json\"}')\n",
    "print(f'  {TABLES_DIR / \"ols_vs_heckman_comparison.csv\"}')\n",
    "print('\\nNotebook complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
