{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heckman MLE Estimation\n",
    "\n",
    "**Tutorial Series**: Censored and Selection Models with PanelBox\n",
    "\n",
    "**Notebook**: 05 - Heckman Maximum Likelihood Estimation\n",
    "\n",
    "**Author**: PanelBox Contributors\n",
    "\n",
    "**Date**: 2026-02-17\n",
    "\n",
    "**Estimated Duration**: 75-90 minutes\n",
    "\n",
    "**Difficulty Level**: Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand the full information maximum likelihood approach to the Heckman model\n",
    "2. Derive and interpret the joint log-likelihood of selection and outcome equations\n",
    "3. Estimate the Heckman model using both two-step and MLE methods in PanelBox\n",
    "4. Compare parameter estimates, standard errors, and efficiency between the two methods\n",
    "5. Understand when MLE is preferred over two-step estimation and vice versa\n",
    "6. Generate and compare unconditional and conditional predictions\n",
    "7. Diagnose convergence issues in MLE estimation\n",
    "\n",
    "**Prerequisites**: Notebook 04 (Heckman two-step), familiarity with maximum likelihood concepts, basic understanding of sample selection bias.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Two-Step vs MLE: Motivation](#section1)\n",
    "2. [The Full Information MLE](#section2)\n",
    "3. [Loading Data](#section3)\n",
    "4. [Two-Step Estimation (Review)](#section4)\n",
    "5. [MLE Estimation](#section5)\n",
    "6. [Comparing Two-Step and MLE](#section6)\n",
    "7. [When to Use Which Method](#section7)\n",
    "8. [Prediction Comparison](#section8)\n",
    "9. [Summary and Key Takeaways](#section9)\n",
    "10. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import all required libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from panelbox.models.selection import PanelHeckman\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Two-Step vs MLE: Motivation\n",
    "\n",
    "### 1.1 Recap of the Two-Step Estimator\n",
    "\n",
    "In Notebook 04, we introduced the Heckman two-step estimator for correcting sample selection bias. The procedure is:\n",
    "\n",
    "**Step 1** -- Estimate a probit model for the selection equation:\n",
    "\n",
    "$$P(s_i = 1 | Z_i) = \\Phi(Z_i' \\gamma)$$\n",
    "\n",
    "**Step 2** -- Compute the inverse Mills ratio (IMR) and include it in the outcome equation:\n",
    "\n",
    "$$y_i = X_i' \\beta + \\rho \\sigma \\lambda(Z_i' \\hat{\\gamma}) + \\eta_i$$\n",
    "\n",
    "where $\\lambda(\\cdot) = \\phi(\\cdot) / \\Phi(\\cdot)$ is the inverse Mills ratio.\n",
    "\n",
    "### 1.2 Limitations of Two-Step\n",
    "\n",
    "While the two-step estimator is consistent and computationally simple, it has several drawbacks:\n",
    "\n",
    "1. **Inefficiency**: The two-step estimator does not use all available information simultaneously. It treats the probit and OLS stages as separate problems, ignoring the cross-equation correlation structure.\n",
    "\n",
    "2. **Standard error complications**: The OLS standard errors from Step 2 are incorrect because they ignore the sampling variability from Step 1 (the estimated IMR). Proper standard errors require the Murphy-Topel or bootstrap correction.\n",
    "\n",
    "3. **Sensitivity to specification**: The two-step estimator can be sensitive to the functional form of the selection equation, particularly when identification relies heavily on the nonlinearity of the IMR rather than exclusion restrictions.\n",
    "\n",
    "4. **No direct likelihood**: Without a likelihood function, standard model comparison tools (AIC, BIC, likelihood ratio tests) are unavailable.\n",
    "\n",
    "### 1.3 Why MLE?\n",
    "\n",
    "Full information maximum likelihood (FIML) estimation addresses these limitations:\n",
    "\n",
    "- **Efficiency**: MLE is asymptotically efficient under correct model specification\n",
    "- **Correct standard errors**: The information matrix provides proper standard errors automatically\n",
    "- **Model comparison**: Log-likelihood enables AIC, BIC, and LR tests\n",
    "- **Joint estimation**: All parameters ($\\beta$, $\\gamma$, $\\sigma$, $\\rho$) are estimated simultaneously\n",
    "\n",
    "The trade-off is that MLE relies more heavily on the bivariate normality assumption and can encounter convergence difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. The Full Information MLE\n",
    "\n",
    "### 2.1 The Joint Distribution\n",
    "\n",
    "The Heckman model assumes that the outcome and selection errors are jointly normal:\n",
    "\n",
    "$$\\begin{pmatrix} \\varepsilon_i \\\\ u_i \\end{pmatrix} \\sim N\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma^2 & \\rho\\sigma \\\\ \\rho\\sigma & 1 \\end{pmatrix} \\right)$$\n",
    "\n",
    "where:\n",
    "- $\\varepsilon_i$ is the error in the outcome equation: $y_i = X_i'\\beta + \\varepsilon_i$\n",
    "- $u_i$ is the error in the selection equation: $s_i^* = Z_i'\\gamma + u_i$, with $s_i = \\mathbb{1}[s_i^* > 0]$\n",
    "- $\\sigma^2 = \\text{Var}(\\varepsilon_i)$ is the outcome error variance\n",
    "- $\\rho = \\text{Corr}(\\varepsilon_i, u_i)$ is the correlation between the errors\n",
    "\n",
    "The variance of $u_i$ is normalized to 1 for identification (as in any probit model).\n",
    "\n",
    "### 2.2 The Log-Likelihood Function\n",
    "\n",
    "The joint density of observing $(y_i, s_i)$ given $(X_i, Z_i)$ can be decomposed as:\n",
    "\n",
    "$$f(y_i, s_i | X_i, Z_i) = f(y_i | s_i = 1, X_i, Z_i)^{s_i} \\cdot P(s_i = 0 | Z_i)^{1 - s_i}$$\n",
    "\n",
    "This yields two types of log-likelihood contributions:\n",
    "\n",
    "**For selected observations** ($s_i = 1$):\n",
    "\n",
    "$$\\ell_i^{\\text{sel}} = \\log\\phi\\left(\\frac{y_i - X_i'\\beta}{\\sigma}\\right) - \\log\\sigma + \\log\\Phi(z_i^*)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$z_i^* = \\frac{Z_i'\\gamma + \\rho(y_i - X_i'\\beta)/\\sigma}{\\sqrt{1 - \\rho^2}}$$\n",
    "\n",
    "This expression captures both the density of observing the outcome value $y_i$ (the first two terms) and the conditional probability of being selected given the outcome realization (the third term).\n",
    "\n",
    "**For non-selected observations** ($s_i = 0$):\n",
    "\n",
    "$$\\ell_i^{\\text{non}} = \\log\\Phi(-Z_i'\\gamma)$$\n",
    "\n",
    "This is simply the log-probability of not being selected.\n",
    "\n",
    "### 2.3 The Full Log-Likelihood\n",
    "\n",
    "The full log-likelihood is the sum over all observations:\n",
    "\n",
    "$$\\mathcal{L}(\\beta, \\gamma, \\sigma, \\rho) = \\sum_{i: s_i=1} \\ell_i^{\\text{sel}} + \\sum_{i: s_i=0} \\ell_i^{\\text{non}}$$\n",
    "\n",
    "### 2.4 Parameter Transformations\n",
    "\n",
    "To ensure the optimizer respects parameter constraints, PanelBox uses:\n",
    "\n",
    "- $\\sigma > 0$: Parameterize as $\\sigma = \\exp(\\alpha_\\sigma)$ where $\\alpha_\\sigma \\in \\mathbb{R}$\n",
    "- $\\rho \\in (-1, 1)$: Parameterize as $\\rho = \\tanh(\\alpha_\\rho)$ where $\\alpha_\\rho \\in \\mathbb{R}$\n",
    "\n",
    "These transformations convert a constrained optimization problem into an unconstrained one.\n",
    "\n",
    "### 2.5 Initialization Strategy\n",
    "\n",
    "MLE requires good starting values for reliable convergence. PanelBox uses a **warm start** strategy:\n",
    "\n",
    "1. Run the two-step estimator to obtain initial $\\hat{\\beta}$, $\\hat{\\gamma}$, $\\hat{\\sigma}$, $\\hat{\\rho}$\n",
    "2. Transform to unconstrained parameters: $\\alpha_\\sigma = \\log(\\hat{\\sigma})$, $\\alpha_\\rho = \\text{arctanh}(\\hat{\\rho})$\n",
    "3. Use these as starting values for BFGS optimization\n",
    "\n",
    "This approach significantly improves convergence reliability compared to starting from zeros or random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the log-likelihood components for intuition\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Panel 1: Normal PDF contribution (selected observations)\n",
    "z_vals = np.linspace(-4, 4, 200)\n",
    "pdf_vals = stats.norm.pdf(z_vals)\n",
    "log_pdf_vals = np.log(pdf_vals)\n",
    "\n",
    "axes[0].plot(z_vals, log_pdf_vals, linewidth=2.5, color='#2980b9')\n",
    "axes[0].set_xlabel('Standardized Residual $(y_i - X_i\\'\\\\beta)/\\\\sigma$', fontsize=11)\n",
    "axes[0].set_ylabel('$\\\\log\\\\phi(\\\\cdot)$', fontsize=11)\n",
    "axes[0].set_title('Outcome Density Contribution', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([-10, 0])\n",
    "\n",
    "# Panel 2: log(Phi(z*)) contribution (selected observations)\n",
    "z_star = np.linspace(-4, 4, 200)\n",
    "log_cdf_vals = np.log(stats.norm.cdf(z_star))\n",
    "\n",
    "axes[1].plot(z_star, log_cdf_vals, linewidth=2.5, color='#27ae60')\n",
    "axes[1].set_xlabel('Adjusted Selection Index $z_i^*$', fontsize=11)\n",
    "axes[1].set_ylabel('$\\\\log\\\\Phi(z_i^*)$', fontsize=11)\n",
    "axes[1].set_title('Selection Correction (Selected)', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([-10, 0])\n",
    "\n",
    "# Panel 3: log(Phi(-Zg)) contribution (non-selected observations)\n",
    "zg = np.linspace(-4, 4, 200)\n",
    "log_1_minus_cdf = np.log(stats.norm.cdf(-zg))\n",
    "\n",
    "axes[2].plot(zg, log_1_minus_cdf, linewidth=2.5, color='#e74c3c')\n",
    "axes[2].set_xlabel('Selection Index $Z_i\\'\\\\gamma$', fontsize=11)\n",
    "axes[2].set_ylabel('$\\\\log\\\\Phi(-Z_i\\'\\\\gamma)$', fontsize=11)\n",
    "axes[2].set_title('Non-Selection Contribution', fontsize=13, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_ylim([-10, 0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'heckman_mle_likelihood_components.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"The full log-likelihood combines these three components:\")\n",
    "print(\"  Selected obs:     log phi(residual) - log(sigma) + log Phi(z*)\")\n",
    "print(\"  Non-selected obs: log Phi(-Z'gamma)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: The three components of the Heckman MLE log-likelihood. Left: the normal density contribution from observed outcomes. Center: the selection correction for selected observations, linking outcome and selection via the correlation parameter. Right: the probit contribution from non-selected observations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Loading Data\n",
    "\n",
    "We use the same Mroz (1987) dataset from Notebook 04. This classic dataset studies married women's labor force participation and wages.\n",
    "\n",
    "- **Outcome**: `wage` (hourly wage, observed only for participants)\n",
    "- **Selection**: `lfp` (labor force participation, 0/1)\n",
    "- **Outcome regressors (X)**: constant, education, experience, experience squared\n",
    "- **Selection regressors (Z)**: constant, education, experience, age, children under 6, children 6-18, husband's income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Mroz 1987 dataset\n",
    "data = pd.read_csv(DATA_DIR / 'mroz_1987.csv')\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nColumns: {list(data.columns)}\")\n",
    "print(f\"\\nParticipation rate: {data['lfp'].mean():.2%}\")\n",
    "print(f\"Participants (lfp=1): {data['lfp'].sum()}\")\n",
    "print(f\"Non-participants (lfp=0): {(1 - data['lfp']).sum():.0f}\")\n",
    "print(f\"\\nWage statistics (participants only):\")\n",
    "print(data.loc[data['lfp'] == 1, 'wage'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"=== Full Sample Summary Statistics ===\")\n",
    "display(data.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare variables for estimation\n",
    "\n",
    "# Selection indicator\n",
    "selection = data['lfp'].values\n",
    "\n",
    "# Outcome variable: wage (use 0 for non-participants)\n",
    "wage = data['wage'].fillna(0).values\n",
    "\n",
    "# Outcome equation regressors: const, education, experience, experience_sq\n",
    "X = sm.add_constant(\n",
    "    data[['education', 'experience', 'experience_sq']].values\n",
    ")\n",
    "x_names = ['const', 'education', 'experience', 'experience_sq']\n",
    "\n",
    "# Selection equation regressors: const, education, experience, age,\n",
    "#   children_lt6, children_6_18, husband_income\n",
    "Z = sm.add_constant(\n",
    "    data[['education', 'experience', 'age',\n",
    "          'children_lt6', 'children_6_18', 'husband_income']].values\n",
    ")\n",
    "z_names = ['const', 'education', 'experience', 'age',\n",
    "           'children_lt6', 'children_6_18', 'husband_income']\n",
    "\n",
    "print(f\"Outcome regressors (X): {x_names}\")\n",
    "print(f\"Selection regressors (Z): {z_names}\")\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"Z shape: {Z.shape}\")\n",
    "print(f\"\\nExclusion restrictions (in Z but not in X):\")\n",
    "print(f\"  age, children_lt6, children_6_18, husband_income\")\n",
    "print(f\"\\nThese variables affect the participation decision but not wages directly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. Two-Step Estimation (Review)\n",
    "\n",
    "Before turning to MLE, let us quickly re-estimate the two-step model as our baseline. This also provides the starting values that PanelBox will use internally for MLE optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-step estimation\n",
    "model_2s = PanelHeckman(\n",
    "    endog=wage,\n",
    "    exog=X,\n",
    "    selection=selection,\n",
    "    exog_selection=Z,\n",
    "    method='two_step'\n",
    ")\n",
    "result_2s = model_2s.fit()\n",
    "\n",
    "print(result_2s.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display two-step estimates in a structured table\n",
    "print(\"=\" * 65)\n",
    "print(\"  TWO-STEP HECKMAN ESTIMATES\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "print(\"\\nOutcome Equation (y = wage):\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Variable':<20} {'Coefficient':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for name, coef in zip(x_names, result_2s.outcome_params):\n",
    "    print(f\"{name:<20} {coef:>12.4f}\")\n",
    "\n",
    "print(\"\\nSelection Equation (s = lfp):\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Variable':<20} {'Coefficient':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for name, coef in zip(z_names, result_2s.probit_params):\n",
    "    print(f\"{name:<20} {coef:>12.4f}\")\n",
    "\n",
    "print(\"\\nSelection Parameters:\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'sigma':<20} {result_2s.sigma:>12.4f}\")\n",
    "print(f\"{'rho':<20} {result_2s.rho:>12.4f}\")\n",
    "print(f\"{'lambda (rho*sigma)':<20} {result_2s.rho * result_2s.sigma:>12.4f}\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-step estimates give us a baseline. Note the estimated $\\rho$ and $\\sigma$ -- these characterize the selection mechanism. A non-zero $\\rho$ indicates that the unobservables affecting wages are correlated with the unobservables affecting participation.\n",
    "\n",
    "Now let us see whether MLE produces materially different estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## 5. MLE Estimation\n",
    "\n",
    "### 5.1 Estimating with `method='mle'`\n",
    "\n",
    "Switching to MLE in PanelBox requires only changing the `method` argument. Internally, PanelBox will:\n",
    "\n",
    "1. Run two-step estimation to obtain initial parameter values (warm start)\n",
    "2. Transform $\\sigma$ and $\\rho$ to unconstrained parameterizations\n",
    "3. Maximize the joint log-likelihood using BFGS optimization\n",
    "4. Transform parameters back and construct the results object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE estimation\n",
    "model_ml = PanelHeckman(\n",
    "    endog=wage,\n",
    "    exog=X,\n",
    "    selection=selection,\n",
    "    exog_selection=Z,\n",
    "    method='mle'\n",
    ")\n",
    "result_ml = model_ml.fit()\n",
    "\n",
    "print(result_ml.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display MLE estimates in a structured table\n",
    "print(\"=\" * 65)\n",
    "print(\"  MAXIMUM LIKELIHOOD HECKMAN ESTIMATES\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "print(\"\\nOutcome Equation (y = wage):\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Variable':<20} {'Coefficient':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for name, coef in zip(x_names, result_ml.outcome_params):\n",
    "    print(f\"{name:<20} {coef:>12.4f}\")\n",
    "\n",
    "print(\"\\nSelection Equation (s = lfp):\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Variable':<20} {'Coefficient':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for name, coef in zip(z_names, result_ml.probit_params):\n",
    "    print(f\"{name:<20} {coef:>12.4f}\")\n",
    "\n",
    "print(\"\\nSelection Parameters:\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'sigma':<20} {result_ml.sigma:>12.4f}\")\n",
    "print(f\"{'rho':<20} {result_ml.rho:>12.4f}\")\n",
    "print(f\"{'lambda (rho*sigma)':<20} {result_ml.rho * result_ml.sigma:>12.4f}\")\n",
    "\n",
    "print(\"\\nModel Diagnostics:\")\n",
    "print(\"-\" * 45)\n",
    "if result_ml.llf is not None:\n",
    "    print(f\"{'Log-likelihood':<20} {result_ml.llf:>12.4f}\")\n",
    "print(f\"{'Converged':<20} {str(result_ml.converged):>12}\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Understanding the MLE Output\n",
    "\n",
    "The MLE results include several additional pieces of information compared to two-step:\n",
    "\n",
    "- **Log-likelihood**: The maximized value of the log-likelihood function. This can be used for AIC, BIC, and likelihood ratio tests.\n",
    "- **Converged**: Whether the BFGS optimizer found a local maximum. Non-convergence may indicate model misspecification, collinearity, or insufficient data.\n",
    "- **Joint estimation**: All parameters are estimated simultaneously, so the correlation between $\\beta$ and $\\gamma$ is accounted for in the estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check convergence and report diagnostics\n",
    "print(\"=== MLE Convergence Diagnostics ===\")\n",
    "print(f\"\\nConverged: {result_ml.converged}\")\n",
    "\n",
    "if result_ml.llf is not None:\n",
    "    n_params = len(result_ml.outcome_params) + len(result_ml.probit_params) + 2  # +2 for sigma, rho\n",
    "    n_obs = result_ml.n_total\n",
    "    \n",
    "    aic = -2 * result_ml.llf + 2 * n_params\n",
    "    bic = -2 * result_ml.llf + np.log(n_obs) * n_params\n",
    "    \n",
    "    print(f\"\\nLog-likelihood: {result_ml.llf:.4f}\")\n",
    "    print(f\"Number of parameters: {n_params}\")\n",
    "    print(f\"Number of observations: {n_obs}\")\n",
    "    print(f\"AIC: {aic:.4f}\")\n",
    "    print(f\"BIC: {bic:.4f}\")\n",
    "\n",
    "print(f\"\\nParameter bounds check:\")\n",
    "print(f\"  sigma = {result_ml.sigma:.4f} (must be > 0: {'OK' if result_ml.sigma > 0 else 'PROBLEM'})\")\n",
    "print(f\"  rho   = {result_ml.rho:.4f} (must be in (-1,1): {'OK' if -1 < result_ml.rho < 1 else 'PROBLEM'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## 6. Comparing Two-Step and MLE\n",
    "\n",
    "### 6.1 Side-by-Side Parameter Comparison\n",
    "\n",
    "Now let us compare the estimates from both methods systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the comparison utility\n",
    "import sys\n",
    "sys.path.insert(0, str(BASE_DIR / 'utils'))\n",
    "from comparison_tools import compare_heckman_methods\n",
    "\n",
    "comparison = compare_heckman_methods(\n",
    "    result_2s, result_ml,\n",
    "    variable_names=x_names\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  TWO-STEP vs MLE: PARAMETER COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison.to_string(float_format=lambda x: f\"{x:.4f}\"))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a more detailed comparison including selection equation\n",
    "print(\"\\n\" + \"=\" * 75)\n",
    "print(\"  COMPREHENSIVE COMPARISON: ALL PARAMETER ESTIMATES\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "# Outcome equation\n",
    "print(\"\\n--- Outcome Equation ---\")\n",
    "print(f\"{'Variable':<18} {'Two-Step':>12} {'MLE':>12} {'Difference':>12} {'% Diff':>10}\")\n",
    "print(\"-\" * 66)\n",
    "for i, name in enumerate(x_names):\n",
    "    ts_val = result_2s.outcome_params[i]\n",
    "    ml_val = result_ml.outcome_params[i]\n",
    "    diff = ts_val - ml_val\n",
    "    pct = 100 * diff / (abs(ts_val) + 1e-10)\n",
    "    print(f\"{name:<18} {ts_val:>12.4f} {ml_val:>12.4f} {diff:>12.4f} {pct:>9.1f}%\")\n",
    "\n",
    "# Selection equation\n",
    "print(\"\\n--- Selection Equation ---\")\n",
    "print(f\"{'Variable':<18} {'Two-Step':>12} {'MLE':>12} {'Difference':>12} {'% Diff':>10}\")\n",
    "print(\"-\" * 66)\n",
    "for i, name in enumerate(z_names):\n",
    "    ts_val = result_2s.probit_params[i]\n",
    "    ml_val = result_ml.probit_params[i]\n",
    "    diff = ts_val - ml_val\n",
    "    pct = 100 * diff / (abs(ts_val) + 1e-10) if abs(ts_val) > 1e-6 else 0\n",
    "    print(f\"{name:<18} {ts_val:>12.4f} {ml_val:>12.4f} {diff:>12.4f} {pct:>9.1f}%\")\n",
    "\n",
    "# Selection parameters\n",
    "print(\"\\n--- Selection Parameters ---\")\n",
    "print(f\"{'Parameter':<18} {'Two-Step':>12} {'MLE':>12} {'Difference':>12}\")\n",
    "print(\"-\" * 56)\n",
    "print(f\"{'sigma':<18} {result_2s.sigma:>12.4f} {result_ml.sigma:>12.4f} {result_2s.sigma - result_ml.sigma:>12.4f}\")\n",
    "print(f\"{'rho':<18} {result_2s.rho:>12.4f} {result_ml.rho:>12.4f} {result_2s.rho - result_ml.rho:>12.4f}\")\n",
    "\n",
    "lambda_2s = result_2s.rho * result_2s.sigma\n",
    "lambda_ml = result_ml.rho * result_ml.sigma\n",
    "print(f\"{'lambda (rho*sigma)':<18} {lambda_2s:>12.4f} {lambda_ml:>12.4f} {lambda_2s - lambda_ml:>12.4f}\")\n",
    "\n",
    "print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of outcome equation coefficients\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Panel 1: Outcome equation coefficients\n",
    "x_pos = np.arange(len(x_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x_pos - width/2, result_2s.outcome_params, width,\n",
    "                     label='Two-Step', color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars2 = axes[0].bar(x_pos + width/2, result_ml.outcome_params, width,\n",
    "                     label='MLE', color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "\n",
    "axes[0].set_xlabel('Variable', fontsize=12)\n",
    "axes[0].set_ylabel('Coefficient', fontsize=12)\n",
    "axes[0].set_title('Outcome Equation: Two-Step vs MLE', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(x_names, rotation=30, ha='right')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.8)\n",
    "\n",
    "# Panel 2: Selection parameters (rho, sigma, lambda)\n",
    "sel_names = ['sigma', 'rho', 'lambda']\n",
    "sel_2s = [result_2s.sigma, result_2s.rho, result_2s.rho * result_2s.sigma]\n",
    "sel_ml = [result_ml.sigma, result_ml.rho, result_ml.rho * result_ml.sigma]\n",
    "\n",
    "x_pos2 = np.arange(len(sel_names))\n",
    "bars3 = axes[1].bar(x_pos2 - width/2, sel_2s, width,\n",
    "                     label='Two-Step', color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars4 = axes[1].bar(x_pos2 + width/2, sel_ml, width,\n",
    "                     label='MLE', color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "\n",
    "axes[1].set_xlabel('Parameter', fontsize=12)\n",
    "axes[1].set_ylabel('Value', fontsize=12)\n",
    "axes[1].set_title('Selection Parameters: Two-Step vs MLE', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xticks(x_pos2)\n",
    "axes[1].set_xticklabels(sel_names)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'heckman_twostep_vs_mle_coefficients.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Side-by-side comparison of parameter estimates from two-step and MLE estimation. Left panel shows outcome equation coefficients. Right panel shows the selection parameters sigma, rho, and lambda. Differences between the two methods reflect efficiency gains from joint estimation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection equation comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x_pos3 = np.arange(len(z_names))\n",
    "width = 0.35\n",
    "\n",
    "bars_sel_2s = ax.bar(x_pos3 - width/2, result_2s.probit_params, width,\n",
    "                      label='Two-Step', color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars_sel_ml = ax.bar(x_pos3 + width/2, result_ml.probit_params, width,\n",
    "                      label='MLE', color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Variable', fontsize=12)\n",
    "ax.set_ylabel('Coefficient', fontsize=12)\n",
    "ax.set_title('Selection Equation (Probit): Two-Step vs MLE', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x_pos3)\n",
    "ax.set_xticklabels(z_names, rotation=30, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.axhline(y=0, color='black', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'heckman_selection_eq_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Comparison of selection equation (probit) coefficients between two-step and MLE. In two-step estimation, these come from a standalone probit; in MLE, they are estimated jointly with the outcome equation, incorporating information about the outcome process.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Interpreting the Differences\n",
    "\n",
    "Several patterns typically emerge when comparing two-step and MLE:\n",
    "\n",
    "1. **Outcome coefficients ($\\beta$)**: Usually similar, with MLE sometimes showing small refinements. Large differences may signal sensitivity to distributional assumptions.\n",
    "\n",
    "2. **Selection coefficients ($\\gamma$)**: In two-step, these come from a standalone probit. In MLE, the selection equation is informed by the outcome data, so estimates may shift slightly.\n",
    "\n",
    "3. **Correlation parameter ($\\rho$)**: Can differ meaningfully between the two methods. MLE estimates $\\rho$ more directly from the joint likelihood.\n",
    "\n",
    "4. **Variance parameter ($\\sigma$)**: MLE often produces a slightly different $\\sigma$ because it accounts for the full error structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify the overall agreement\n",
    "outcome_diff = result_2s.outcome_params - result_ml.outcome_params\n",
    "selection_diff = result_2s.probit_params - result_ml.probit_params\n",
    "\n",
    "print(\"=== Agreement Assessment ===\")\n",
    "print(f\"\\nOutcome equation:\")\n",
    "print(f\"  Mean absolute difference: {np.mean(np.abs(outcome_diff)):.4f}\")\n",
    "print(f\"  Max absolute difference:  {np.max(np.abs(outcome_diff)):.4f}\")\n",
    "print(f\"  RMSD:                     {np.sqrt(np.mean(outcome_diff**2)):.4f}\")\n",
    "\n",
    "print(f\"\\nSelection equation:\")\n",
    "print(f\"  Mean absolute difference: {np.mean(np.abs(selection_diff)):.4f}\")\n",
    "print(f\"  Max absolute difference:  {np.max(np.abs(selection_diff)):.4f}\")\n",
    "print(f\"  RMSD:                     {np.sqrt(np.mean(selection_diff**2)):.4f}\")\n",
    "\n",
    "print(f\"\\nSelection parameters:\")\n",
    "print(f\"  sigma difference: {abs(result_2s.sigma - result_ml.sigma):.4f}\")\n",
    "print(f\"  rho difference:   {abs(result_2s.rho - result_ml.rho):.4f}\")\n",
    "\n",
    "# Overall verdict\n",
    "max_diff = max(np.max(np.abs(outcome_diff)), np.max(np.abs(selection_diff)))\n",
    "if max_diff < 0.5:\n",
    "    print(f\"\\nVerdict: Estimates are broadly consistent (max diff = {max_diff:.4f}).\")\n",
    "    print(\"This suggests the model is well-identified and results are robust.\")\n",
    "else:\n",
    "    print(f\"\\nVerdict: Non-trivial differences detected (max diff = {max_diff:.4f}).\")\n",
    "    print(\"This may reflect sensitivity to distributional assumptions or identification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## 7. When to Use Which Method\n",
    "\n",
    "### 7.1 Summary of Trade-Offs\n",
    "\n",
    "| Criterion | Two-Step | MLE |\n",
    "|-----------|----------|-----|\n",
    "| **Consistency** | Yes (under correct probit) | Yes (under bivariate normality) |\n",
    "| **Efficiency** | Less efficient | Asymptotically efficient |\n",
    "| **Standard errors** | Require correction (Murphy-Topel) | Correct from information matrix |\n",
    "| **Distributional reliance** | Less sensitive | Fully relies on bivariate normality |\n",
    "| **Computational cost** | Very fast (closed-form steps) | Iterative optimization (slower) |\n",
    "| **Convergence** | Always converges | Can fail to converge |\n",
    "| **Model comparison** | No likelihood available | AIC, BIC, LR tests |\n",
    "| **Small samples** | More robust | May be unreliable |\n",
    "| **Large samples** | Fine, but less efficient | Preferred (efficiency gains) |\n",
    "\n",
    "### 7.2 Practical Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual decision guide\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "categories = [\n",
    "    'Efficiency',\n",
    "    'Robustness to\\nMisspecification',\n",
    "    'Computational\\nSpeed',\n",
    "    'Correct\\nStandard Errors',\n",
    "    'Model\\nComparison Tools',\n",
    "    'Small Sample\\nReliability',\n",
    "    'Convergence\\nGuarantee'\n",
    "]\n",
    "\n",
    "# Scores (subjective, for illustration; 1-5 scale)\n",
    "twostep_scores = [2, 4, 5, 2, 1, 4, 5]\n",
    "mle_scores =     [5, 2, 2, 5, 5, 2, 3]\n",
    "\n",
    "x_pos = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.barh(x_pos - width/2, twostep_scores, width,\n",
    "                label='Two-Step', color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax.barh(x_pos + width/2, mle_scores, width,\n",
    "                label='MLE', color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax.set_yticks(x_pos)\n",
    "ax.set_yticklabels(categories, fontsize=11)\n",
    "ax.set_xlabel('Score (1 = Low, 5 = High)', fontsize=12)\n",
    "ax.set_title('Two-Step vs MLE: Method Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12, loc='lower right')\n",
    "ax.set_xlim([0, 6])\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add score labels\n",
    "for bar, score in zip(bars1, twostep_scores):\n",
    "    ax.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,\n",
    "            str(score), va='center', fontsize=10, fontweight='bold', color='#2c3e50')\n",
    "for bar, score in zip(bars2, mle_scores):\n",
    "    ax.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,\n",
    "            str(score), va='center', fontsize=10, fontweight='bold', color='#2c3e50')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'heckman_method_comparison_scores.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Qualitative comparison of two-step and MLE estimation on seven criteria. Two-step excels in robustness, speed, and convergence reliability, while MLE dominates in efficiency, standard error accuracy, and model comparison capabilities.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Decision Framework\n",
    "\n",
    "**Use Two-Step when:**\n",
    "- You are in the exploratory phase and want quick, reliable estimates\n",
    "- The sample size is small (< 200 selected observations)\n",
    "- You are uncertain about the bivariate normality assumption\n",
    "- Convergence of MLE is problematic\n",
    "- You primarily need consistent estimates and are less concerned about efficiency\n",
    "\n",
    "**Use MLE when:**\n",
    "- You have a moderate to large sample (> 500 observations)\n",
    "- You need correct standard errors without bootstrap or Murphy-Topel corrections\n",
    "- You want to compare models using AIC, BIC, or likelihood ratio tests\n",
    "- You are confident in the bivariate normality assumption\n",
    "- Maximum efficiency is important (e.g., for policy analysis)\n",
    "\n",
    "**Best practice:** Run both methods and compare. If estimates are similar, report MLE for its efficiency advantages. If they differ substantially, investigate why -- this often signals model misspecification or sensitivity to distributional assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the computational cost difference\n",
    "import time\n",
    "\n",
    "# Time two-step estimation\n",
    "n_runs = 5\n",
    "times_2s = []\n",
    "for _ in range(n_runs):\n",
    "    start = time.time()\n",
    "    model_tmp = PanelHeckman(endog=wage, exog=X, selection=selection,\n",
    "                             exog_selection=Z, method='two_step')\n",
    "    _ = model_tmp.fit()\n",
    "    times_2s.append(time.time() - start)\n",
    "\n",
    "# Time MLE estimation\n",
    "times_ml = []\n",
    "for _ in range(n_runs):\n",
    "    start = time.time()\n",
    "    model_tmp = PanelHeckman(endog=wage, exog=X, selection=selection,\n",
    "                             exog_selection=Z, method='mle')\n",
    "    _ = model_tmp.fit()\n",
    "    times_ml.append(time.time() - start)\n",
    "\n",
    "print(\"=== Computational Cost Comparison ===\")\n",
    "print(f\"\\nTwo-Step (avg of {n_runs} runs): {np.mean(times_2s)*1000:.1f} ms\")\n",
    "print(f\"MLE      (avg of {n_runs} runs): {np.mean(times_ml)*1000:.1f} ms\")\n",
    "print(f\"\\nMLE is approximately {np.mean(times_ml)/np.mean(times_2s):.1f}x slower than two-step.\")\n",
    "print(f\"\\nNote: MLE internally runs two-step first to obtain starting values,\")\n",
    "print(f\"then runs iterative BFGS optimization on the joint log-likelihood.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Sample Size Considerations\n",
    "\n",
    "The efficiency advantage of MLE over two-step grows with sample size. In small samples, the two methods may produce very similar estimates, making the choice less consequential. In large samples, MLE provides tighter confidence intervals.\n",
    "\n",
    "Let us illustrate this by examining how parameter estimates behave as we vary the effective sample size through subsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo: compare two-step and MLE across different sample fractions\n",
    "fractions = [0.3, 0.5, 0.7, 1.0]\n",
    "n_total = len(selection)\n",
    "\n",
    "results_by_fraction = []\n",
    "np.random.seed(42)\n",
    "\n",
    "for frac in fractions:\n",
    "    n_sub = int(frac * n_total)\n",
    "    idx = np.random.choice(n_total, size=n_sub, replace=False)\n",
    "    idx.sort()\n",
    "    \n",
    "    wage_sub = wage[idx]\n",
    "    X_sub = X[idx]\n",
    "    Z_sub = Z[idx]\n",
    "    sel_sub = selection[idx]\n",
    "    \n",
    "    # Two-step\n",
    "    m_2s = PanelHeckman(endog=wage_sub, exog=X_sub, selection=sel_sub,\n",
    "                         exog_selection=Z_sub, method='two_step')\n",
    "    r_2s = m_2s.fit()\n",
    "    \n",
    "    # MLE\n",
    "    m_ml = PanelHeckman(endog=wage_sub, exog=X_sub, selection=sel_sub,\n",
    "                         exog_selection=Z_sub, method='mle')\n",
    "    r_ml = m_ml.fit()\n",
    "    \n",
    "    results_by_fraction.append({\n",
    "        'fraction': frac,\n",
    "        'n': n_sub,\n",
    "        'n_selected': int(sel_sub.sum()),\n",
    "        'beta_educ_2s': r_2s.outcome_params[1],\n",
    "        'beta_educ_ml': r_ml.outcome_params[1],\n",
    "        'rho_2s': r_2s.rho,\n",
    "        'rho_ml': r_ml.rho,\n",
    "        'sigma_2s': r_2s.sigma,\n",
    "        'sigma_ml': r_ml.sigma,\n",
    "        'converged_ml': r_ml.converged\n",
    "    })\n",
    "\n",
    "frac_df = pd.DataFrame(results_by_fraction)\n",
    "\n",
    "print(\"=== Parameter Estimates by Sample Fraction ===\")\n",
    "print(f\"\\n{'Frac':>6} {'N':>6} {'N_sel':>6} | {'educ_2s':>9} {'educ_ml':>9} | {'rho_2s':>8} {'rho_ml':>8} | {'Conv':>5}\")\n",
    "print(\"-\" * 75)\n",
    "for _, row in frac_df.iterrows():\n",
    "    print(f\"{row['fraction']:>6.1f} {int(row['n']):>6} {int(row['n_selected']):>6} | \"\n",
    "          f\"{row['beta_educ_2s']:>9.4f} {row['beta_educ_ml']:>9.4f} | \"\n",
    "          f\"{row['rho_2s']:>8.4f} {row['rho_ml']:>8.4f} | \"\n",
    "          f\"{str(row['converged_ml']):>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "## 8. Prediction Comparison\n",
    "\n",
    "### 8.1 Types of Predictions\n",
    "\n",
    "The Heckman model supports two types of predictions:\n",
    "\n",
    "1. **Unconditional prediction** $E[y_i^*] = X_i'\\beta$: The expected value of the latent outcome variable, regardless of whether the individual is selected. This answers: \"What would this person's wage be if everyone worked?\"\n",
    "\n",
    "2. **Conditional prediction** $E[y_i | s_i = 1] = X_i'\\beta + \\rho\\sigma\\lambda(Z_i'\\gamma)$: The expected value of the observed outcome, conditional on being selected. This answers: \"What do we expect this person's wage to be, given that they work?\"\n",
    "\n",
    "The difference between these two predictions is the **selection correction** term $\\rho\\sigma\\lambda(Z_i'\\gamma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from both methods\n",
    "pred_2s_uncond = result_2s.predict(type='unconditional')\n",
    "pred_2s_cond = result_2s.predict(type='conditional')\n",
    "\n",
    "pred_ml_uncond = result_ml.predict(type='unconditional')\n",
    "pred_ml_cond = result_ml.predict(type='conditional')\n",
    "\n",
    "# Focus on selected sample for meaningful wage comparisons\n",
    "selected_mask = selection == 1\n",
    "actual_wages = wage[selected_mask]\n",
    "\n",
    "print(\"=== Prediction Summary (Selected Sample Only) ===\")\n",
    "print(f\"\\n{'':>25} {'Two-Step':>12} {'MLE':>12} {'Actual':>12}\")\n",
    "print(\"-\" * 63)\n",
    "print(f\"{'Unconditional mean':>25} {pred_2s_uncond[selected_mask].mean():>12.4f} \"\n",
    "      f\"{pred_ml_uncond[selected_mask].mean():>12.4f} {actual_wages.mean():>12.4f}\")\n",
    "print(f\"{'Conditional mean':>25} {pred_2s_cond[selected_mask].mean():>12.4f} \"\n",
    "      f\"{pred_ml_cond[selected_mask].mean():>12.4f} {actual_wages.mean():>12.4f}\")\n",
    "print(f\"{'Unconditional std':>25} {pred_2s_uncond[selected_mask].std():>12.4f} \"\n",
    "      f\"{pred_ml_uncond[selected_mask].std():>12.4f} {actual_wages.std():>12.4f}\")\n",
    "print(f\"{'Conditional std':>25} {pred_2s_cond[selected_mask].std():>12.4f} \"\n",
    "      f\"{pred_ml_cond[selected_mask].std():>12.4f} {actual_wages.std():>12.4f}\")\n",
    "\n",
    "# Selection correction magnitude\n",
    "correction_2s = pred_2s_cond[selected_mask] - pred_2s_uncond[selected_mask]\n",
    "correction_ml = pred_ml_cond[selected_mask] - pred_ml_uncond[selected_mask]\n",
    "\n",
    "print(f\"\\n{'':>25} {'Two-Step':>12} {'MLE':>12}\")\n",
    "print(\"-\" * 51)\n",
    "print(f\"{'Selection correction mean':>25} {correction_2s.mean():>12.4f} {correction_ml.mean():>12.4f}\")\n",
    "print(f\"{'Selection correction std':>25} {correction_2s.std():>12.4f} {correction_ml.std():>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Unconditional vs Conditional predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Panel 1: Two-Step predictions\n",
    "axes[0].scatter(pred_2s_uncond[selected_mask], pred_2s_cond[selected_mask],\n",
    "                alpha=0.4, s=20, color='#3498db', label='Selected obs')\n",
    "lims = [min(pred_2s_uncond[selected_mask].min(), pred_2s_cond[selected_mask].min()) - 1,\n",
    "        max(pred_2s_uncond[selected_mask].max(), pred_2s_cond[selected_mask].max()) + 1]\n",
    "axes[0].plot(lims, lims, 'r--', linewidth=2, label='45-degree line')\n",
    "axes[0].set_xlabel('Unconditional Prediction $E[y^*]$', fontsize=11)\n",
    "axes[0].set_ylabel('Conditional Prediction $E[y|s=1]$', fontsize=11)\n",
    "axes[0].set_title('Two-Step: Unconditional vs Conditional', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: MLE predictions\n",
    "axes[1].scatter(pred_ml_uncond[selected_mask], pred_ml_cond[selected_mask],\n",
    "                alpha=0.4, s=20, color='#e74c3c', label='Selected obs')\n",
    "lims2 = [min(pred_ml_uncond[selected_mask].min(), pred_ml_cond[selected_mask].min()) - 1,\n",
    "         max(pred_ml_uncond[selected_mask].max(), pred_ml_cond[selected_mask].max()) + 1]\n",
    "axes[1].plot(lims2, lims2, 'r--', linewidth=2, label='45-degree line')\n",
    "axes[1].set_xlabel('Unconditional Prediction $E[y^*]$', fontsize=11)\n",
    "axes[1].set_ylabel('Conditional Prediction $E[y|s=1]$', fontsize=11)\n",
    "axes[1].set_title('MLE: Unconditional vs Conditional', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'heckman_unconditional_vs_conditional.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Unconditional versus conditional predictions for selected observations. The vertical distance from each point to the 45-degree line represents the selection correction for that individual. Points above the line indicate positive selection (conditional wage exceeds unconditional), while points below indicate negative selection.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Cross-method prediction comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Panel 1: Two-Step vs MLE unconditional\n",
    "axes[0].scatter(pred_2s_uncond[selected_mask], pred_ml_uncond[selected_mask],\n",
    "                alpha=0.4, s=20, color='#8e44ad')\n",
    "r_uncond = np.corrcoef(pred_2s_uncond[selected_mask], pred_ml_uncond[selected_mask])[0, 1]\n",
    "combined_range = [\n",
    "    min(pred_2s_uncond[selected_mask].min(), pred_ml_uncond[selected_mask].min()) - 1,\n",
    "    max(pred_2s_uncond[selected_mask].max(), pred_ml_uncond[selected_mask].max()) + 1\n",
    "]\n",
    "axes[0].plot(combined_range, combined_range, 'r--', linewidth=2)\n",
    "axes[0].set_xlabel('Two-Step Prediction', fontsize=11)\n",
    "axes[0].set_ylabel('MLE Prediction', fontsize=11)\n",
    "axes[0].set_title(f'Unconditional Predictions\\n(correlation = {r_uncond:.4f})',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Two-Step vs MLE conditional\n",
    "axes[1].scatter(pred_2s_cond[selected_mask], pred_ml_cond[selected_mask],\n",
    "                alpha=0.4, s=20, color='#16a085')\n",
    "r_cond = np.corrcoef(pred_2s_cond[selected_mask], pred_ml_cond[selected_mask])[0, 1]\n",
    "combined_range2 = [\n",
    "    min(pred_2s_cond[selected_mask].min(), pred_ml_cond[selected_mask].min()) - 1,\n",
    "    max(pred_2s_cond[selected_mask].max(), pred_ml_cond[selected_mask].max()) + 1\n",
    "]\n",
    "axes[1].plot(combined_range2, combined_range2, 'r--', linewidth=2)\n",
    "axes[1].set_xlabel('Two-Step Prediction', fontsize=11)\n",
    "axes[1].set_ylabel('MLE Prediction', fontsize=11)\n",
    "axes[1].set_title(f'Conditional Predictions\\n(correlation = {r_cond:.4f})',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'heckman_cross_method_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Correlation between two-step and MLE:\")\n",
    "print(f\"  Unconditional predictions: {r_uncond:.6f}\")\n",
    "print(f\"  Conditional predictions:   {r_cond:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Cross-method prediction comparison. Left: unconditional predictions from two-step versus MLE. Right: conditional predictions. High correlation indicates that both methods produce similar predictions, reinforcing confidence in the results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Prediction residuals against actual wages\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "resid_configs = [\n",
    "    (pred_2s_uncond[selected_mask], 'Two-Step Unconditional', '#3498db'),\n",
    "    (pred_ml_uncond[selected_mask], 'MLE Unconditional', '#e74c3c'),\n",
    "    (pred_2s_cond[selected_mask], 'Two-Step Conditional', '#27ae60'),\n",
    "    (pred_ml_cond[selected_mask], 'MLE Conditional', '#f39c12'),\n",
    "]\n",
    "\n",
    "for ax, (pred, label, color) in zip(axes.flat, resid_configs):\n",
    "    residuals = actual_wages - pred\n",
    "    rmse = np.sqrt(np.mean(residuals**2))\n",
    "    mae = np.mean(np.abs(residuals))\n",
    "    \n",
    "    ax.scatter(pred, residuals, alpha=0.3, s=15, color=color)\n",
    "    ax.axhline(y=0, color='black', linewidth=1.5, linestyle='--')\n",
    "    ax.set_xlabel('Predicted Wage', fontsize=10)\n",
    "    ax.set_ylabel('Residual', fontsize=10)\n",
    "    ax.set_title(f'{label}\\nRMSE={rmse:.2f}, MAE={mae:.2f}', fontsize=11, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'heckman_prediction_residuals.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Prediction residuals (actual minus predicted wage) for all four prediction types. The conditional predictions typically have lower RMSE because they account for the selection correction. Comparing two-step and MLE residuals reveals whether joint estimation materially improves predictive accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Distribution of the selection correction\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of selection correction magnitudes\n",
    "axes[0].hist(correction_2s, bins=40, alpha=0.6, color='#3498db',\n",
    "             label='Two-Step', edgecolor='black', density=True)\n",
    "axes[0].hist(correction_ml, bins=40, alpha=0.6, color='#e74c3c',\n",
    "             label='MLE', edgecolor='black', density=True)\n",
    "axes[0].axvline(x=0, color='black', linewidth=1.5, linestyle='--')\n",
    "axes[0].set_xlabel('Selection Correction ($\\\\rho\\\\sigma\\\\lambda$)', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('Distribution of Selection Corrections', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter: Two-Step vs MLE corrections\n",
    "axes[1].scatter(correction_2s, correction_ml, alpha=0.4, s=15, color='#8e44ad')\n",
    "combined_corr_range = [\n",
    "    min(correction_2s.min(), correction_ml.min()) - 0.5,\n",
    "    max(correction_2s.max(), correction_ml.max()) + 0.5\n",
    "]\n",
    "axes[1].plot(combined_corr_range, combined_corr_range, 'r--', linewidth=2, label='45-degree line')\n",
    "r_correction = np.corrcoef(correction_2s, correction_ml)[0, 1]\n",
    "axes[1].set_xlabel('Two-Step Correction', fontsize=11)\n",
    "axes[1].set_ylabel('MLE Correction', fontsize=11)\n",
    "axes[1].set_title(f'Selection Correction Comparison\\n(correlation = {r_correction:.4f})',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'heckman_selection_correction_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Comparison of selection correction terms between two-step and MLE. Left: overlapping histograms showing the distribution of corrections. Right: scatter plot comparing individual-level corrections across methods. The sign and magnitude of corrections reflect the estimated rho and sigma parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 5: Predicted wages by education level\n",
    "# Create education-wage profile for both methods\n",
    "\n",
    "educ_range = np.arange(5, 20, 0.5)\n",
    "mean_exp = data['experience'].mean()\n",
    "mean_exp_sq = data['experience_sq'].mean()\n",
    "\n",
    "# Build prediction matrices for outcome equation\n",
    "X_pred = np.column_stack([\n",
    "    np.ones(len(educ_range)),\n",
    "    educ_range,\n",
    "    np.full(len(educ_range), mean_exp),\n",
    "    np.full(len(educ_range), mean_exp_sq)\n",
    "])\n",
    "\n",
    "# Unconditional predictions\n",
    "wage_pred_2s = X_pred @ result_2s.outcome_params\n",
    "wage_pred_ml = X_pred @ result_ml.outcome_params\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(educ_range, wage_pred_2s, linewidth=2.5, color='#3498db',\n",
    "        label='Two-Step', linestyle='-')\n",
    "ax.plot(educ_range, wage_pred_ml, linewidth=2.5, color='#e74c3c',\n",
    "        label='MLE', linestyle='--')\n",
    "\n",
    "# Overlay actual data (selected sample)\n",
    "ax.scatter(data.loc[selected_mask, 'education'],\n",
    "           actual_wages, alpha=0.15, s=10, color='gray', label='Actual wages')\n",
    "\n",
    "ax.set_xlabel('Years of Education', fontsize=12)\n",
    "ax.set_ylabel('Predicted Wage (Unconditional)', fontsize=12)\n",
    "ax.set_title('Wage-Education Profile: Two-Step vs MLE\\n(at mean experience)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'heckman_wage_education_profile.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Return to education (unconditional):\")\n",
    "print(f\"  Two-Step: {result_2s.outcome_params[1]:.4f} per year\")\n",
    "print(f\"  MLE:      {result_ml.outcome_params[1]:.4f} per year\")\n",
    "print(f\"  Difference: {abs(result_2s.outcome_params[1] - result_ml.outcome_params[1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Predicted wage-education profiles at mean experience levels. The two methods produce very similar profiles, with any differences reflecting the efficiency gains of MLE. Actual wages from the selected sample are shown as background scatter points.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Prediction Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive prediction accuracy metrics\n",
    "def prediction_metrics(actual, predicted, label):\n",
    "    \"\"\"Compute prediction accuracy metrics.\"\"\"\n",
    "    residuals = actual - predicted\n",
    "    return {\n",
    "        'Method': label,\n",
    "        'RMSE': np.sqrt(np.mean(residuals**2)),\n",
    "        'MAE': np.mean(np.abs(residuals)),\n",
    "        'Mean Bias': np.mean(residuals),\n",
    "        'Median Bias': np.median(residuals),\n",
    "        'Correlation': np.corrcoef(actual, predicted)[0, 1]\n",
    "    }\n",
    "\n",
    "metrics_list = [\n",
    "    prediction_metrics(actual_wages, pred_2s_uncond[selected_mask], 'Two-Step Uncond'),\n",
    "    prediction_metrics(actual_wages, pred_2s_cond[selected_mask], 'Two-Step Cond'),\n",
    "    prediction_metrics(actual_wages, pred_ml_uncond[selected_mask], 'MLE Uncond'),\n",
    "    prediction_metrics(actual_wages, pred_ml_cond[selected_mask], 'MLE Cond'),\n",
    "]\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list).set_index('Method')\n",
    "\n",
    "print(\"=\" * 75)\n",
    "print(\"  PREDICTION ACCURACY COMPARISON\")\n",
    "print(\"=\" * 75)\n",
    "print(metrics_df.to_string(float_format=lambda x: f\"{x:.4f}\"))\n",
    "print(\"=\" * 75)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - RMSE: Root mean squared error (lower is better)\")\n",
    "print(\"  - MAE: Mean absolute error (lower is better)\")\n",
    "print(\"  - Mean Bias: Average prediction error (closer to 0 is better)\")\n",
    "print(\"  - Conditional predictions include selection correction and should\")\n",
    "print(\"    better match observed wages for the selected sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>\n",
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### 9.1 What We Learned\n",
    "\n",
    "1. **Full information MLE** estimates all parameters of the Heckman model simultaneously by maximizing the joint log-likelihood of the selection and outcome equations.\n",
    "\n",
    "2. **The log-likelihood** has two components: selected observations contribute both an outcome density term and a conditional selection probability, while non-selected observations contribute only the probability of non-selection.\n",
    "\n",
    "3. **MLE is asymptotically efficient** under correct model specification, producing tighter confidence intervals than two-step estimation. However, it relies more heavily on the bivariate normality assumption.\n",
    "\n",
    "4. **PanelBox makes switching easy**: changing `method='two_step'` to `method='mle'` is all that is needed. Internally, MLE uses two-step estimates as starting values for reliable convergence.\n",
    "\n",
    "5. **Parameter transformations** ($\\sigma = e^{\\alpha_\\sigma}$, $\\rho = \\tanh(\\alpha_\\rho)$) ensure that the optimizer respects the constraints $\\sigma > 0$ and $\\rho \\in (-1, 1)$.\n",
    "\n",
    "6. **Comparing the two methods** is good practice. When estimates agree, we gain confidence in the results. When they disagree, we should investigate model specification and distributional assumptions.\n",
    "\n",
    "7. **Predictions** come in two flavors: unconditional ($E[y^*]$) and conditional ($E[y|s=1]$). The difference is the selection correction $\\rho\\sigma\\lambda$.\n",
    "\n",
    "### 9.2 Key Formulas\n",
    "\n",
    "| Component | Formula |\n",
    "|-----------|--------|\n",
    "| Selected contribution | $\\log\\phi\\left(\\frac{y_i - X'\\beta}{\\sigma}\\right) - \\log\\sigma + \\log\\Phi(z^*)$ |\n",
    "| $z^*$ | $\\frac{Z'\\gamma + \\rho(y_i - X'\\beta)/\\sigma}{\\sqrt{1-\\rho^2}}$ |\n",
    "| Non-selected contribution | $\\log\\Phi(-Z'\\gamma)$ |\n",
    "| Unconditional prediction | $E[y^*] = X'\\beta$ |\n",
    "| Conditional prediction | $E[y|s=1] = X'\\beta + \\rho\\sigma\\lambda(Z'\\gamma)$ |\n",
    "\n",
    "### 9.3 Practical Checklist\n",
    "\n",
    "- [ ] Always start with two-step estimation as a baseline\n",
    "- [ ] Check that MLE converges successfully\n",
    "- [ ] Compare estimates across methods -- large discrepancies warrant investigation\n",
    "- [ ] Verify that $\\rho$ is within a plausible range\n",
    "- [ ] Use conditional predictions when comparing to observed outcomes\n",
    "- [ ] Report both methods in sensitivity analyses\n",
    "- [ ] Use MLE-based AIC/BIC for model comparison when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "print(\"=\" * 70)\n",
    "print(\"  FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'N (total)', 'N (selected)', 'Selection rate',\n",
    "        'Return to education (beta_educ)',\n",
    "        'Experience effect (beta_exp)',\n",
    "        'sigma', 'rho', 'lambda (rho*sigma)',\n",
    "        'Log-likelihood', 'Converged'\n",
    "    ],\n",
    "    'Two-Step': [\n",
    "        result_2s.n_total, result_2s.n_selected,\n",
    "        f\"{result_2s.n_selected/result_2s.n_total:.2%}\",\n",
    "        f\"{result_2s.outcome_params[1]:.4f}\",\n",
    "        f\"{result_2s.outcome_params[2]:.4f}\",\n",
    "        f\"{result_2s.sigma:.4f}\",\n",
    "        f\"{result_2s.rho:.4f}\",\n",
    "        f\"{result_2s.rho * result_2s.sigma:.4f}\",\n",
    "        'N/A',\n",
    "        'N/A (always)'\n",
    "    ],\n",
    "    'MLE': [\n",
    "        result_ml.n_total, result_ml.n_selected,\n",
    "        f\"{result_ml.n_selected/result_ml.n_total:.2%}\",\n",
    "        f\"{result_ml.outcome_params[1]:.4f}\",\n",
    "        f\"{result_ml.outcome_params[2]:.4f}\",\n",
    "        f\"{result_ml.sigma:.4f}\",\n",
    "        f\"{result_ml.rho:.4f}\",\n",
    "        f\"{result_ml.rho * result_ml.sigma:.4f}\",\n",
    "        f\"{result_ml.llf:.4f}\" if result_ml.llf is not None else 'N/A',\n",
    "        str(result_ml.converged)\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).set_index('Metric')\n",
    "print(summary_df.to_string())\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "## 10. Exercises\n",
    "\n",
    "Test your understanding with these exercises.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 1: Log-Likelihood Computation (Easy)\n",
    "\n",
    "**Task**: Manually compute the log-likelihood contribution for a single observation using the MLE estimates.\n",
    "\n",
    "Pick the first selected observation in the dataset. Using the MLE estimates of $\\beta$, $\\gamma$, $\\sigma$, and $\\rho$, compute:\n",
    "\n",
    "1. The standardized residual $(y_i - X_i'\\beta)/\\sigma$\n",
    "2. The adjusted selection index $z_i^*$\n",
    "3. The full log-likelihood contribution $\\ell_i^{\\text{sel}}$\n",
    "4. For a non-selected observation, compute $\\ell_i^{\\text{non}}$\n",
    "\n",
    "**Hint**: Use `stats.norm.pdf()` and `stats.norm.cdf()` from scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n",
    "# Step 1: Get MLE parameter estimates\n",
    "beta_hat = result_ml.outcome_params\n",
    "gamma_hat = result_ml.probit_params\n",
    "sigma_hat = result_ml.sigma\n",
    "rho_hat = result_ml.rho\n",
    "\n",
    "# Step 2: Pick the first selected observation\n",
    "first_selected_idx = np.where(selection == 1)[0][0]\n",
    "y_i = wage[first_selected_idx]\n",
    "X_i = X[first_selected_idx]\n",
    "Z_i = Z[first_selected_idx]\n",
    "\n",
    "# TODO: Compute the standardized residual\n",
    "# residual = ...\n",
    "\n",
    "# TODO: Compute z_i^*\n",
    "# z_star = ...\n",
    "\n",
    "# TODO: Compute the selected log-likelihood contribution\n",
    "# ell_selected = ...\n",
    "\n",
    "# Step 3: Pick the first non-selected observation\n",
    "first_nonselected_idx = np.where(selection == 0)[0][0]\n",
    "Z_j = Z[first_nonselected_idx]\n",
    "\n",
    "# TODO: Compute the non-selected log-likelihood contribution\n",
    "# ell_nonselected = ...\n",
    "\n",
    "# Print results\n",
    "# print(f\"Selected observation: ell_i = {ell_selected:.6f}\")\n",
    "# print(f\"Non-selected observation: ell_j = {ell_nonselected:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 2: Sensitivity to Exclusion Restrictions (Medium)\n",
    "\n",
    "**Task**: Investigate how the choice of exclusion restrictions affects MLE estimates.\n",
    "\n",
    "1. Estimate the model using only `husband_income` as the exclusion restriction (remove `children_lt6`, `children_6_18`, and `age` from Z, but keep them conceptually excluded from X).\n",
    "2. Estimate the model using all four exclusion restrictions (the baseline specification).\n",
    "3. Compare $\\hat{\\rho}$, $\\hat{\\sigma}$, and the outcome coefficients across specifications.\n",
    "4. Discuss: How sensitive are the results to the choice of exclusion restrictions?\n",
    "\n",
    "**Hint**: Build a new Z matrix with fewer columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "# Specification 1: Minimal exclusion restriction (husband_income only)\n",
    "# Z_minimal = sm.add_constant(\n",
    "#     data[['education', 'experience', 'husband_income']].values\n",
    "# )\n",
    "\n",
    "# TODO: Estimate with minimal Z\n",
    "\n",
    "# Specification 2: Full exclusion restrictions (baseline)\n",
    "# Already estimated as result_ml\n",
    "\n",
    "# TODO: Compare the results\n",
    "# print(\"Comparison of exclusion restriction specifications:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3: Convergence Diagnostics (Medium)\n",
    "\n",
    "**Task**: Explore the sensitivity of MLE convergence to starting values.\n",
    "\n",
    "1. Estimate the Heckman MLE model using the default warm start (from two-step).\n",
    "2. Try perturbing the starting values by adding noise to the two-step estimates:\n",
    "   - Small perturbation: multiply two-step params by `1 + 0.1 * np.random.randn()`\n",
    "   - Large perturbation: multiply by `1 + 0.5 * np.random.randn()`\n",
    "3. Do all three specifications converge to the same estimates?\n",
    "4. Discuss: What does this tell us about the log-likelihood surface?\n",
    "\n",
    "**Note**: You will need to work with the internal `_log_likelihood` method and `scipy.optimize.minimize` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Step 1: Get the default starting values (from two-step)\n",
    "k_outcome = X.shape[1]\n",
    "k_selection = Z.shape[1]\n",
    "\n",
    "init_params_default = np.concatenate([\n",
    "    result_2s.outcome_params,\n",
    "    result_2s.probit_params,\n",
    "    [np.log(result_2s.sigma)],\n",
    "    [np.arctanh(np.clip(result_2s.rho, -0.99, 0.99))]\n",
    "])\n",
    "\n",
    "# TODO: Optimize with default start\n",
    "# result_default = minimize(model_ml._log_likelihood, init_params_default, method='BFGS')\n",
    "\n",
    "# TODO: Add small perturbation and re-optimize\n",
    "# np.random.seed(123)\n",
    "# init_params_small = init_params_default * (1 + 0.1 * np.random.randn(len(init_params_default)))\n",
    "# result_small = minimize(model_ml._log_likelihood, init_params_small, method='BFGS')\n",
    "\n",
    "# TODO: Add large perturbation and re-optimize\n",
    "# init_params_large = init_params_default * (1 + 0.5 * np.random.randn(len(init_params_default)))\n",
    "# result_large = minimize(model_ml._log_likelihood, init_params_large, method='BFGS')\n",
    "\n",
    "# TODO: Compare final parameter values\n",
    "# print(\"Convergence comparison:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 4: Monte Carlo Comparison (Hard)\n",
    "\n",
    "**Task**: Conduct a small Monte Carlo simulation to compare the finite-sample properties of two-step and MLE.\n",
    "\n",
    "1. Generate synthetic data from a known Heckman model:\n",
    "   - True parameters: $\\beta = [1, 0.5]$, $\\gamma = [0.3, 0.8, -0.5]$, $\\sigma = 2$, $\\rho = -0.5$\n",
    "   - $n = 500$ observations\n",
    "2. Run 100 replications, estimating both two-step and MLE on each.\n",
    "3. Compare:\n",
    "   - Bias: $E[\\hat{\\theta}] - \\theta_0$\n",
    "   - RMSE: $\\sqrt{E[(\\hat{\\theta} - \\theta_0)^2]}$\n",
    "   - Coverage of 95% confidence intervals (for MLE, using log-likelihood SE)\n",
    "4. Which estimator has lower RMSE? Is the MLE efficiency advantage apparent in this sample size?\n",
    "\n",
    "**Hint**: Use `np.random.multivariate_normal` to generate correlated errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your solution here\n",
    "\n",
    "# True parameters\n",
    "# beta_true = np.array([1.0, 0.5])\n",
    "# gamma_true = np.array([0.3, 0.8, -0.5])\n",
    "# sigma_true = 2.0\n",
    "# rho_true = -0.5\n",
    "\n",
    "# n_obs = 500\n",
    "# n_reps = 100\n",
    "\n",
    "# results_mc = []\n",
    "# np.random.seed(42)\n",
    "\n",
    "# for rep in range(n_reps):\n",
    "#     # TODO: Generate X and Z\n",
    "#     # TODO: Generate correlated errors\n",
    "#     # TODO: Generate selection and outcome\n",
    "#     # TODO: Estimate both methods\n",
    "#     # TODO: Store estimates\n",
    "#     pass\n",
    "\n",
    "# TODO: Compute bias and RMSE\n",
    "# TODO: Create comparison table\n",
    "\n",
    "print(\"Monte Carlo exercise: implement the simulation above.\")\n",
    "print(\"This will reveal the finite-sample efficiency comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "### Essential Reading\n",
    "\n",
    "1. **Heckman, J.J. (1979)**. \"Sample Selection Bias as a Specification Error.\" *Econometrica*, 47(1), 153-161.\n",
    "\n",
    "2. **Wooldridge, J.M. (2010)**. *Econometric Analysis of Cross Section and Panel Data* (2nd ed.). MIT Press. Chapter 19.\n",
    "\n",
    "3. **Cameron, A.C. & Trivedi, P.K. (2005)**. *Microeconometrics: Methods and Applications*. Cambridge University Press. Chapter 16.\n",
    "\n",
    "### Additional References\n",
    "\n",
    "4. **Greene, W.H. (2018)**. *Econometric Analysis* (8th ed.). Pearson. Chapter 19 (Sample Selection).\n",
    "\n",
    "5. **Mroz, T.A. (1987)**. \"The Sensitivity of an Empirical Model of Married Women's Hours of Work to Economic and Statistical Assumptions.\" *Econometrica*, 55(4), 765-799.\n",
    "\n",
    "6. **Nawata, K. (1994)**. \"Estimation of Sample Selection Bias Models by the Maximum Likelihood Estimator and Heckman's Two-Step Estimator.\" *Economics Letters*, 45(1), 33-40.\n",
    "\n",
    "7. **Puhani, P.A. (2000)**. \"The Heckman Correction for Sample Selection and Its Critique.\" *Journal of Economic Surveys*, 14(1), 53-68.\n",
    "\n",
    "---\n",
    "\n",
    "**Next notebook**: 06 - Advanced Topics in Selection Models (panel data extensions, correlated random effects)\n",
    "\n",
    "**Thank you for completing this tutorial!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
