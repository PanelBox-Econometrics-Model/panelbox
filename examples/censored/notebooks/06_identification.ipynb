{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification and Exclusion Restrictions in Heckman Selection Models\n",
    "\n",
    "**Tutorial Series**: Censored and Selection Models with PanelBox\n",
    "\n",
    "**Notebook**: 06 - Identification and Exclusion Restrictions\n",
    "\n",
    "**Author**: PanelBox Contributors\n",
    "\n",
    "**Estimated Duration**: 60-75 minutes\n",
    "\n",
    "**Difficulty Level**: Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand why exclusion restrictions are critical for identification in Heckman selection models\n",
    "2. Distinguish between models identified by functional form versus exclusion restrictions\n",
    "3. Implement Heckman models with proper exclusion restrictions using PanelBox\n",
    "4. Diagnose identification problems through coefficient instability and collinearity\n",
    "5. Evaluate the economic validity of candidate exclusion restrictions\n",
    "6. Conduct sensitivity analyses across alternative instrument specifications\n",
    "7. Apply best practices for selecting exclusion restrictions in applied research\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Familiarity with the Heckman two-step estimator (Notebooks 01-05)\n",
    "- Understanding of probit models and the inverse Mills ratio\n",
    "- Basic knowledge of instrumental variable logic\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [The Identification Problem](#section1)\n",
    "2. [What Are Exclusion Restrictions?](#section2)\n",
    "3. [Loading Data](#section3)\n",
    "4. [Example 1: Labor Supply with Exclusion Restrictions](#section4)\n",
    "5. [Example 2: College Wages with Exclusion Restrictions](#section5)\n",
    "6. [What Happens Without Exclusion Restrictions?](#section6)\n",
    "7. [Testing Exclusion Restrictions](#section7)\n",
    "8. [Sensitivity Analysis](#section8)\n",
    "9. [Best Practices](#section9)\n",
    "10. [Summary and Key Takeaways](#section10)\n",
    "11. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import all required libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from panelbox.models.selection import PanelHeckman\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. The Identification Problem\n",
    "\n",
    "### 1.1 Review: The Heckman Selection Model\n",
    "\n",
    "The Heckman model consists of two equations:\n",
    "\n",
    "**Selection equation** (who is observed?):\n",
    "$$s_i^* = Z_i'\\gamma + u_i, \\quad s_i = \\mathbf{1}[s_i^* > 0]$$\n",
    "\n",
    "**Outcome equation** (what is the outcome for those observed?):\n",
    "$$y_i = X_i'\\beta + \\varepsilon_i \\quad \\text{if } s_i = 1$$\n",
    "\n",
    "where $(u_i, \\varepsilon_i) \\sim \\text{Bivariate Normal}$ with correlation $\\rho$.\n",
    "\n",
    "The key correction formula for the expected outcome, conditional on being selected, is:\n",
    "\n",
    "$$E[y_i | s_i = 1, X_i] = X_i'\\beta + \\rho\\sigma_\\varepsilon \\lambda(Z_i'\\gamma)$$\n",
    "\n",
    "where $\\lambda(\\cdot) = \\phi(\\cdot) / \\Phi(\\cdot)$ is the **inverse Mills ratio** (IMR).\n",
    "\n",
    "### 1.2 The Core Problem: Collinearity\n",
    "\n",
    "The Heckman two-step estimator augments the outcome equation with $\\lambda(Z_i'\\gamma)$:\n",
    "\n",
    "$$y_i = X_i'\\beta + \\theta \\lambda(Z_i'\\gamma) + \\eta_i$$\n",
    "\n",
    "where $\\theta = \\rho \\sigma_\\varepsilon$.\n",
    "\n",
    "**What happens if $Z = X$** (no exclusion restrictions)?\n",
    "\n",
    "- $\\lambda(X_i'\\gamma)$ is a nonlinear function of $X_i$\n",
    "- But over the range of typical data, $\\lambda(\\cdot)$ is **approximately linear**\n",
    "- This means $\\lambda(X_i'\\gamma) \\approx a + b \\cdot X_i'\\gamma$ for some constants\n",
    "- The augmented equation becomes approximately: $y_i \\approx X_i'\\beta + \\theta(a + b \\cdot X_i'\\gamma) + \\eta_i$\n",
    "- **Result**: Near-perfect multicollinearity between $X$ and $\\lambda$\n",
    "\n",
    "### 1.3 Why This Matters\n",
    "\n",
    "Without exclusion restrictions, identification relies **entirely** on the nonlinearity of $\\lambda(\\cdot)$, which comes from the bivariate normality assumption. This is problematic because:\n",
    "\n",
    "1. **Functional form dependence**: Small deviations from normality can drastically change estimates\n",
    "2. **Unstable estimates**: Parameters become sensitive to sample composition\n",
    "3. **Large standard errors**: Multicollinearity inflates variance\n",
    "4. **Lack of robustness**: Results are not credible for policy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the identification problem:\n",
    "# Show that lambda(z) is approximately linear over typical data ranges\n",
    "\n",
    "z = np.linspace(-3, 3, 500)\n",
    "phi_z = stats.norm.pdf(z)\n",
    "Phi_z = stats.norm.cdf(z)\n",
    "lambda_z = phi_z / np.clip(Phi_z, 1e-10, None)\n",
    "\n",
    "# Fit a linear approximation over a typical range\n",
    "mask_typical = (z > -1.5) & (z < 2.0)  # typical probit index range\n",
    "slope, intercept, r_value, _, _ = stats.linregress(z[mask_typical], lambda_z[mask_typical])\n",
    "lambda_linear = intercept + slope * z\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left panel: IMR function and linear approximation\n",
    "axes[0].plot(z, lambda_z, linewidth=2.5, label=r'$\\lambda(z) = \\phi(z)/\\Phi(z)$', color='#2980b9')\n",
    "axes[0].plot(z, lambda_linear, '--', linewidth=2, label=f'Linear approx. ($R^2$ = {r_value**2:.4f})',\n",
    "             color='#e74c3c')\n",
    "axes[0].axvspan(-1.5, 2.0, alpha=0.1, color='green', label='Typical data range')\n",
    "axes[0].set_xlabel(r\"Probit index $Z'\\gamma$\", fontsize=12)\n",
    "axes[0].set_ylabel(r'$\\lambda(z)$', fontsize=12)\n",
    "axes[0].set_title('Inverse Mills Ratio: Nearly Linear\\nin Typical Data Range', fontsize=13)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].set_xlim([-3, 3])\n",
    "axes[0].set_ylim([0, 4])\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right panel: Residuals from linear approximation\n",
    "residuals = lambda_z - lambda_linear\n",
    "axes[1].plot(z, residuals, linewidth=2, color='#8e44ad')\n",
    "axes[1].axvspan(-1.5, 2.0, alpha=0.1, color='green', label='Typical data range')\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.8, linestyle='-')\n",
    "axes[1].set_xlabel(r\"Probit index $Z'\\gamma$\", fontsize=12)\n",
    "axes[1].set_ylabel(r'$\\lambda(z) - $ linear approx.', fontsize=12)\n",
    "axes[1].set_title('Nonlinear Component of IMR\\n(Source of Identification Without Exclusion)', fontsize=13)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].set_xlim([-3, 3])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'imr_linearity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nLinear approximation R-squared in typical range: {r_value**2:.4f}')\n",
    "print(f'Max deviation in typical range: {np.max(np.abs(residuals[mask_typical])):.4f}')\n",
    "print(f'\\nConclusion: The IMR is very nearly linear over typical probit index values.')\n",
    "print('Without exclusion restrictions, the only \"identification\" comes from this')\n",
    "print('tiny nonlinear residual -- a very fragile basis for estimation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: The inverse Mills ratio $\\lambda(z) = \\phi(z)/\\Phi(z)$ is plotted alongside its linear approximation (left). Over the typical data range of the probit index, the linear fit is nearly perfect ($R^2 \\approx 0.99$). The right panel shows that the nonlinear component -- the sole source of identification without exclusion restrictions -- is negligibly small.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. What Are Exclusion Restrictions?\n",
    "\n",
    "### 2.1 Definition\n",
    "\n",
    "An **exclusion restriction** is a variable that:\n",
    "\n",
    "1. **Appears in the selection equation** ($Z$): it affects whether we observe the outcome\n",
    "2. **Does NOT appear in the outcome equation** ($X$): it does not directly affect the outcome itself\n",
    "\n",
    "Formally, if $Z = [X, W]$ where $W$ are the excluded instruments, then:\n",
    "- $W$ shifts the probability of selection\n",
    "- $W$ has no direct effect on $y$ (conditional on $X$)\n",
    "\n",
    "### 2.2 Why Exclusion Restrictions Solve the Problem\n",
    "\n",
    "With exclusion restrictions:\n",
    "- $\\lambda(Z_i'\\gamma) = \\lambda(X_i'\\gamma_1 + W_i'\\gamma_2)$\n",
    "- Variation in $W$ generates variation in $\\lambda$ that is **independent** of $X$\n",
    "- This breaks the collinearity between $X$ and $\\lambda$\n",
    "- The model is now identified by **genuine exclusion-based variation**, not just functional form\n",
    "\n",
    "### 2.3 The Analogy with Instrumental Variables\n",
    "\n",
    "Exclusion restrictions in sample selection models are analogous to instruments in IV estimation:\n",
    "\n",
    "| IV Estimation | Heckman Selection Model |\n",
    "|---|---|\n",
    "| Instrument $Z$ correlated with endogenous $X$ | Exclusion $W$ affects selection $s$ |\n",
    "| Instrument $Z$ uncorrelated with $\\varepsilon$ | Exclusion $W$ does not affect outcome $y$ |\n",
    "| Relevance condition | Strong predictor of selection |\n",
    "| Exclusion restriction | Excluded from outcome equation |\n",
    "\n",
    "### 2.4 Classic Examples\n",
    "\n",
    "| Application | Selection | Outcome | Exclusion Restriction |\n",
    "|---|---|---|---|\n",
    "| Female wages | Labor force participation | Log wage | Number of children, husband's income |\n",
    "| College wage premium | College attendance | Post-college wage | Distance to college, tuition |\n",
    "| Union wage effect | Union membership | Log wage | State right-to-work laws |\n",
    "| Program evaluation | Program participation | Earnings | Distance to program site |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Loading Data\n",
    "\n",
    "We will work with two classic datasets throughout this notebook:\n",
    "\n",
    "1. **Mroz (1987)**: Married women's labor force participation and wages\n",
    "2. **College Wage**: College attendance decisions and post-college earnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "mroz = pd.read_csv(DATA_DIR / 'mroz_1987.csv')\n",
    "college = pd.read_csv(DATA_DIR / 'college_wage.csv')\n",
    "\n",
    "print('=== Mroz (1987) Dataset ===')\n",
    "print(f'Observations: {len(mroz)}')\n",
    "print(f'Participation rate: {mroz[\"lfp\"].mean():.1%}')\n",
    "print(f'\\nColumns: {list(mroz.columns)}')\n",
    "print(f'\\nSummary statistics:')\n",
    "display(mroz.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== College Wage Dataset ===')\n",
    "print(f'Observations: {len(college)}')\n",
    "print(f'College attendance rate: {college[\"college\"].mean():.1%}')\n",
    "print(f'\\nColumns: {list(college.columns)}')\n",
    "print(f'\\nSummary statistics:')\n",
    "display(college.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the selection patterns in both datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Mroz: wage distribution by participation status\n",
    "wages_observed = mroz.loc[mroz['lfp'] == 1, 'wage'].dropna()\n",
    "axes[0].hist(wages_observed, bins=30, edgecolor='black', alpha=0.7, color='#3498db')\n",
    "axes[0].axvline(wages_observed.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean = {wages_observed.mean():.2f}')\n",
    "axes[0].set_xlabel('Observed Wage', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title(f'Mroz: Wage Distribution (Workers Only)\\n'\n",
    "                   f'N observed = {len(wages_observed)}, '\n",
    "                   f'N censored = {(mroz[\"lfp\"] == 0).sum()}', fontsize=12)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# College: wage distribution by college attendance\n",
    "wages_college = college.loc[college['college'] == 1, 'wage'].dropna()\n",
    "axes[1].hist(wages_college, bins=30, edgecolor='black', alpha=0.7, color='#27ae60')\n",
    "axes[1].axvline(wages_college.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean = {wages_college.mean():.2f}')\n",
    "axes[1].set_xlabel('Observed Wage', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title(f'College: Wage Distribution (Graduates Only)\\n'\n",
    "                   f'N observed = {len(wages_college)}, '\n",
    "                   f'N censored = {(college[\"college\"] == 0).sum()}', fontsize=12)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'selection_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Distribution of observed wages for the two datasets. Left: Mroz data shows wages only for women who participate in the labor force. Right: College data shows wages only for individuals who attended college. In both cases, a substantial portion of the sample is censored (outcome not observed), creating the sample selection problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. Example 1: Labor Supply with Exclusion Restrictions\n",
    "\n",
    "### 4.1 The Economic Setting\n",
    "\n",
    "In Mroz (1987), married women decide whether to participate in the labor market. We observe:\n",
    "\n",
    "- **Outcome**: Log wage (observed only for participants)\n",
    "- **Selection**: Labor force participation (lfp = 1 if working)\n",
    "\n",
    "### 4.2 Choosing Exclusion Restrictions\n",
    "\n",
    "For the labor supply application, the classic exclusion restrictions are:\n",
    "\n",
    "1. **Number of young children (children_lt6)**: \n",
    "   - Affects participation (childcare costs reduce labor supply)\n",
    "   - Should not directly affect hourly wage rate (conditional on experience, education)\n",
    "\n",
    "2. **Number of older children (children_6_18)**:\n",
    "   - Similar logic, weaker effect than young children\n",
    "\n",
    "3. **Husband's income (husband_income)**:\n",
    "   - Higher household income reduces need to work (income effect)\n",
    "   - Should not affect the woman's own wage rate\n",
    "\n",
    "### 4.3 Equation Specification\n",
    "\n",
    "**Outcome equation** (wage determination):\n",
    "$$\\log(wage_i) = \\beta_0 + \\beta_1 \\cdot education_i + \\beta_2 \\cdot experience_i + \\beta_3 \\cdot experience^2_i + \\varepsilon_i$$\n",
    "\n",
    "**Selection equation** (labor force participation):\n",
    "$$s_i^* = \\gamma_0 + \\gamma_1 \\cdot education_i + \\gamma_2 \\cdot experience_i + \\gamma_3 \\cdot experience^2_i + \\gamma_4 \\cdot age_i + \\underbrace{\\gamma_5 \\cdot children\\_lt6_i + \\gamma_6 \\cdot children\\_6\\_18_i + \\gamma_7 \\cdot husband\\_income_i}_{\\text{Exclusion restrictions}} + u_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Mroz example\n",
    "# Outcome: log wage (for workers)\n",
    "mroz['log_wage'] = np.log(mroz['wage'])\n",
    "\n",
    "# Selection indicator\n",
    "selection = mroz['lfp'].values\n",
    "\n",
    "# For outcome equation: replace NaN wages with 0 (PanelHeckman uses selection indicator)\n",
    "y = mroz['log_wage'].fillna(0).values\n",
    "\n",
    "# Outcome equation variables (X): const, education, experience, experience_sq\n",
    "X = sm.add_constant(mroz[['education', 'experience', 'experience_sq']].values)\n",
    "X_names = ['const', 'education', 'experience', 'experience_sq']\n",
    "\n",
    "# Selection equation variables (Z): X + exclusion restrictions\n",
    "Z = sm.add_constant(mroz[['education', 'experience', 'experience_sq',\n",
    "                           'age', 'children_lt6', 'children_6_18',\n",
    "                           'husband_income']].values)\n",
    "Z_names = ['const', 'education', 'experience', 'experience_sq',\n",
    "           'age', 'children_lt6', 'children_6_18', 'husband_income']\n",
    "\n",
    "print('Outcome equation (X) variables:', X_names)\n",
    "print(f'  Shape: {X.shape}')\n",
    "print(f'\\nSelection equation (Z) variables:', Z_names)\n",
    "print(f'  Shape: {Z.shape}')\n",
    "print(f'\\nExclusion restrictions: age, children_lt6, children_6_18, husband_income')\n",
    "print(f'  (Variables in Z but NOT in X)')\n",
    "print(f'\\nSelected observations: {selection.sum()} / {len(selection)} ({selection.mean():.1%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Heckman model WITH exclusion restrictions (properly identified)\n",
    "model_excl = PanelHeckman(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    selection=selection,\n",
    "    exog_selection=Z,\n",
    "    method='two_step'\n",
    ")\n",
    "result_excl = model_excl.fit()\n",
    "\n",
    "print('=' * 70)\n",
    "print('   HECKMAN MODEL WITH EXCLUSION RESTRICTIONS (Mroz 1987)')\n",
    "print('=' * 70)\n",
    "print(result_excl.summary())\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('   OUTCOME EQUATION COEFFICIENTS')\n",
    "print('=' * 70)\n",
    "for name, coef in zip(X_names, result_excl.outcome_params):\n",
    "    print(f'  {name:20s}: {coef:10.4f}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('   SELECTION EQUATION COEFFICIENTS (Probit)')\n",
    "print('=' * 70)\n",
    "for name, coef in zip(Z_names, result_excl.probit_params):\n",
    "    print(f'  {name:20s}: {coef:10.4f}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('   SELECTION PARAMETERS')\n",
    "print('=' * 70)\n",
    "print(f'  sigma:               {result_excl.sigma:10.4f}')\n",
    "print(f'  rho:                 {result_excl.rho:10.4f}')\n",
    "print(f'  lambda (rho*sigma):  {result_excl.rho * result_excl.sigma:10.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the exclusion restrictions are strong predictors of selection\n",
    "# Run probit and check significance of excluded variables\n",
    "\n",
    "print('=== Exclusion Restriction Relevance Check ===')\n",
    "print('\\nAre the exclusion restrictions significant in the selection equation?')\n",
    "print()\n",
    "\n",
    "# Approximate z-statistics for probit coefficients\n",
    "# (using the selection equation estimates)\n",
    "exclusion_vars = ['age', 'children_lt6', 'children_6_18', 'husband_income']\n",
    "exclusion_indices = [Z_names.index(v) for v in exclusion_vars]\n",
    "\n",
    "print(f'{\"Variable\":20s} {\"Coefficient\":>12s} {\"Interpretation\"}')\n",
    "print('-' * 70)\n",
    "for var, idx in zip(exclusion_vars, exclusion_indices):\n",
    "    coef = result_excl.probit_params[idx]\n",
    "    sign = '+' if coef > 0 else '-'\n",
    "    if var == 'children_lt6':\n",
    "        interp = f'({sign}) Young children reduce participation'\n",
    "    elif var == 'children_6_18':\n",
    "        interp = f'({sign}) Older children reduce participation'\n",
    "    elif var == 'husband_income':\n",
    "        interp = f'({sign}) Higher household income reduces need to work'\n",
    "    elif var == 'age':\n",
    "        interp = f'({sign}) Age affects labor supply decision'\n",
    "    else:\n",
    "        interp = ''\n",
    "    print(f'{var:20s} {coef:12.4f}   {interp}')\n",
    "\n",
    "print('\\nAll exclusion restrictions have economically meaningful signs.')\n",
    "print('These variables shift the probability of working without directly')\n",
    "print('affecting the wage rate, providing genuine identifying variation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## 5. Example 2: College Wages with Exclusion Restrictions\n",
    "\n",
    "### 5.1 The Economic Setting\n",
    "\n",
    "Individuals choose whether to attend college. We observe wages only for college graduates. The central question is: what is the return to college education, after correcting for the fact that those who attend college are a self-selected group?\n",
    "\n",
    "### 5.2 Choosing Exclusion Restrictions\n",
    "\n",
    "For the college wage application:\n",
    "\n",
    "1. **Distance to nearest college (distance_college)**:\n",
    "   - Greater distance increases costs of attending (travel, relocation)\n",
    "   - Distance to college should not directly affect a worker's productivity or wage\n",
    "   - Classic instrument from Card (1995)\n",
    "\n",
    "2. **Local tuition (tuition)**:\n",
    "   - Higher tuition increases the cost of attendance\n",
    "   - Tuition paid years ago should not directly affect current wages\n",
    "\n",
    "### 5.3 Equation Specification\n",
    "\n",
    "**Outcome equation** (wage determination):\n",
    "$$\\log(wage_i) = \\beta_0 + \\beta_1 \\cdot ability_i + \\beta_2 \\cdot parent\\_educ_i + \\beta_3 \\cdot family\\_income_i + \\beta_4 \\cdot urban_i + \\beta_5 \\cdot female_i + \\varepsilon_i$$\n",
    "\n",
    "**Selection equation** (college attendance):\n",
    "$$s_i^* = \\gamma_0 + \\gamma_1 \\cdot ability_i + ... + \\underbrace{\\gamma_6 \\cdot distance\\_college_i + \\gamma_7 \\cdot tuition_i}_{\\text{Exclusion restrictions}} + u_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for College Wage example\n",
    "college['log_wage'] = np.log(college['wage'])\n",
    "\n",
    "selection_c = college['college'].values\n",
    "y_c = college['log_wage'].fillna(0).values\n",
    "\n",
    "# Outcome equation variables (X)\n",
    "X_c = sm.add_constant(college[['ability', 'parent_education', 'family_income',\n",
    "                                'urban', 'female']].values)\n",
    "X_c_names = ['const', 'ability', 'parent_education', 'family_income', 'urban', 'female']\n",
    "\n",
    "# Selection equation variables (Z) = X + exclusion restrictions\n",
    "Z_c = sm.add_constant(college[['ability', 'parent_education', 'family_income',\n",
    "                                'urban', 'female',\n",
    "                                'distance_college', 'tuition']].values)\n",
    "Z_c_names = ['const', 'ability', 'parent_education', 'family_income', 'urban', 'female',\n",
    "             'distance_college', 'tuition']\n",
    "\n",
    "print('Outcome equation (X) variables:', X_c_names)\n",
    "print(f'Selection equation (Z) variables:', Z_c_names)\n",
    "print(f'Exclusion restrictions: distance_college, tuition')\n",
    "print(f'\\nCollege attendance: {selection_c.sum()} / {len(selection_c)} ({selection_c.mean():.1%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Heckman model WITH exclusion restrictions\n",
    "model_college_excl = PanelHeckman(\n",
    "    endog=y_c,\n",
    "    exog=X_c,\n",
    "    selection=selection_c,\n",
    "    exog_selection=Z_c,\n",
    "    method='two_step'\n",
    ")\n",
    "result_college_excl = model_college_excl.fit()\n",
    "\n",
    "print('=' * 70)\n",
    "print('   HECKMAN MODEL WITH EXCLUSION RESTRICTIONS (College Wage)')\n",
    "print('=' * 70)\n",
    "print(result_college_excl.summary())\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('   OUTCOME EQUATION COEFFICIENTS')\n",
    "print('=' * 70)\n",
    "for name, coef in zip(X_c_names, result_college_excl.outcome_params):\n",
    "    print(f'  {name:20s}: {coef:10.4f}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('   SELECTION EQUATION COEFFICIENTS (Probit)')\n",
    "print('=' * 70)\n",
    "for name, coef in zip(Z_c_names, result_college_excl.probit_params):\n",
    "    print(f'  {name:20s}: {coef:10.4f}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('   SELECTION PARAMETERS')\n",
    "print('=' * 70)\n",
    "print(f'  sigma:               {result_college_excl.sigma:10.4f}')\n",
    "print(f'  rho:                 {result_college_excl.rho:10.4f}')\n",
    "print(f'  lambda (rho*sigma):  {result_college_excl.rho * result_college_excl.sigma:10.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check relevance of exclusion restrictions in the college equation\n",
    "print('=== Exclusion Restriction Relevance: College Application ===')\n",
    "print()\n",
    "\n",
    "excl_vars_c = ['distance_college', 'tuition']\n",
    "excl_indices_c = [Z_c_names.index(v) for v in excl_vars_c]\n",
    "\n",
    "print(f'{\"Variable\":20s} {\"Coefficient\":>12s} {\"Interpretation\"}')\n",
    "print('-' * 70)\n",
    "for var, idx in zip(excl_vars_c, excl_indices_c):\n",
    "    coef = result_college_excl.probit_params[idx]\n",
    "    sign = '+' if coef > 0 else '-'\n",
    "    if var == 'distance_college':\n",
    "        interp = f'({sign}) Greater distance reduces college attendance'\n",
    "    elif var == 'tuition':\n",
    "        interp = f'({sign}) Higher tuition reduces college attendance'\n",
    "    print(f'{var:20s} {coef:12.4f}   {interp}')\n",
    "\n",
    "print('\\nBoth instruments have the expected negative signs:')\n",
    "print('  - Higher costs (distance, tuition) reduce college attendance')\n",
    "print('  - But these costs should not affect post-college wages directly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## 6. What Happens Without Exclusion Restrictions?\n",
    "\n",
    "Now we demonstrate what goes wrong when the model is identified only through the functional form (normality) assumption. We compare three specifications:\n",
    "\n",
    "- **(a) Properly identified**: with valid exclusion restrictions\n",
    "- **(b) No exclusion restrictions**: $Z = X$ (same variables in both equations)\n",
    "- **(c) Weak exclusion restrictions**: poorly chosen instruments\n",
    "\n",
    "### 6.1 Mroz Data: Three Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specification (a): With proper exclusion restrictions (already estimated)\n",
    "# result_excl is our baseline\n",
    "\n",
    "# Specification (b): No exclusion restrictions (Z = X)\n",
    "# Selection equation uses the SAME variables as outcome equation\n",
    "model_no_excl = PanelHeckman(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    selection=selection,\n",
    "    exog_selection=X,  # Z = X: NO exclusion restrictions!\n",
    "    method='two_step'\n",
    ")\n",
    "result_no_excl = model_no_excl.fit()\n",
    "\n",
    "# Specification (c): Weak exclusion restriction\n",
    "# Use 'age' alone as exclusion (weak because age correlates with experience/wages)\n",
    "Z_weak = sm.add_constant(mroz[['education', 'experience', 'experience_sq', 'age']].values)\n",
    "Z_weak_names = ['const', 'education', 'experience', 'experience_sq', 'age']\n",
    "\n",
    "model_weak = PanelHeckman(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    selection=selection,\n",
    "    exog_selection=Z_weak,\n",
    "    method='two_step'\n",
    ")\n",
    "result_weak = model_weak.fit()\n",
    "\n",
    "print('All three specifications estimated successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outcome equation coefficients across the three specifications\n",
    "comparison_mroz = pd.DataFrame({\n",
    "    'Variable': X_names,\n",
    "    '(a) With Exclusion': result_excl.outcome_params,\n",
    "    '(b) No Exclusion (Z=X)': result_no_excl.outcome_params,\n",
    "    '(c) Weak Exclusion': result_weak.outcome_params,\n",
    "}).set_index('Variable')\n",
    "\n",
    "# Add selection parameters\n",
    "selection_params = pd.DataFrame({\n",
    "    'Variable': ['sigma', 'rho', 'lambda (rho*sigma)'],\n",
    "    '(a) With Exclusion': [result_excl.sigma, result_excl.rho,\n",
    "                           result_excl.rho * result_excl.sigma],\n",
    "    '(b) No Exclusion (Z=X)': [result_no_excl.sigma, result_no_excl.rho,\n",
    "                                result_no_excl.rho * result_no_excl.sigma],\n",
    "    '(c) Weak Exclusion': [result_weak.sigma, result_weak.rho,\n",
    "                            result_weak.rho * result_weak.sigma],\n",
    "}).set_index('Variable')\n",
    "\n",
    "full_comparison = pd.concat([comparison_mroz, selection_params])\n",
    "\n",
    "print('=' * 75)\n",
    "print('  COMPARISON: MROZ DATA -- EFFECT OF EXCLUSION RESTRICTIONS')\n",
    "print('=' * 75)\n",
    "print()\n",
    "print(full_comparison.round(4).to_string())\n",
    "\n",
    "print('\\n' + '=' * 75)\n",
    "print('  INTERPRETATION')\n",
    "print('=' * 75)\n",
    "print()\n",
    "print('(a) With proper exclusion restrictions:')\n",
    "print('    - Coefficients are economically meaningful and stable')\n",
    "print(f'    - rho = {result_excl.rho:.4f}: indicates selection bias')\n",
    "print()\n",
    "print('(b) Without exclusion restrictions (Z = X):')\n",
    "print('    - Model relies ONLY on functional form for identification')\n",
    "print(f'    - rho = {result_no_excl.rho:.4f}: may be unreliable')\n",
    "print('    - Coefficients can differ substantially from (a)')\n",
    "print()\n",
    "print('(c) Weak exclusion restriction (age only):')\n",
    "print('    - Age is correlated with experience and may affect wages')\n",
    "print(f'    - rho = {result_weak.rho:.4f}')\n",
    "print('    - Validity of exclusion is questionable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficient instability across specifications\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Outcome equation coefficients (excluding constant for scale)\n",
    "vars_to_plot = ['education', 'experience', 'experience_sq']\n",
    "idx_to_plot = [X_names.index(v) for v in vars_to_plot]\n",
    "\n",
    "x_pos = np.arange(len(vars_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "bars_a = axes[0].bar(x_pos - width, [result_excl.outcome_params[i] for i in idx_to_plot],\n",
    "                      width, label='(a) With Exclusion', color='#27ae60', alpha=0.8)\n",
    "bars_b = axes[0].bar(x_pos, [result_no_excl.outcome_params[i] for i in idx_to_plot],\n",
    "                      width, label='(b) No Exclusion', color='#e74c3c', alpha=0.8)\n",
    "bars_c = axes[0].bar(x_pos + width, [result_weak.outcome_params[i] for i in idx_to_plot],\n",
    "                      width, label='(c) Weak Exclusion', color='#f39c12', alpha=0.8)\n",
    "\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(vars_to_plot, fontsize=11)\n",
    "axes[0].set_ylabel('Coefficient Value', fontsize=12)\n",
    "axes[0].set_title('Outcome Equation: Coefficient Comparison\\n(Mroz Data)', fontsize=13)\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.8)\n",
    "\n",
    "# Right: Selection parameters\n",
    "sel_labels = ['sigma', 'rho', 'lambda']\n",
    "sel_a = [result_excl.sigma, result_excl.rho, result_excl.rho * result_excl.sigma]\n",
    "sel_b = [result_no_excl.sigma, result_no_excl.rho, result_no_excl.rho * result_no_excl.sigma]\n",
    "sel_c = [result_weak.sigma, result_weak.rho, result_weak.rho * result_weak.sigma]\n",
    "\n",
    "x_pos2 = np.arange(len(sel_labels))\n",
    "\n",
    "axes[1].bar(x_pos2 - width, sel_a, width, label='(a) With Exclusion', color='#27ae60', alpha=0.8)\n",
    "axes[1].bar(x_pos2, sel_b, width, label='(b) No Exclusion', color='#e74c3c', alpha=0.8)\n",
    "axes[1].bar(x_pos2 + width, sel_c, width, label='(c) Weak Exclusion', color='#f39c12', alpha=0.8)\n",
    "\n",
    "axes[1].set_xticks(x_pos2)\n",
    "axes[1].set_xticklabels([r'$\\sigma$', r'$\\rho$', r'$\\lambda = \\rho\\sigma$'], fontsize=12)\n",
    "axes[1].set_ylabel('Parameter Value', fontsize=12)\n",
    "axes[1].set_title('Selection Parameters: Sensitivity\\nto Exclusion Restrictions', fontsize=13)\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'exclusion_comparison_mroz.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Comparison of outcome equation coefficients (left) and selection parameters (right) across three specifications of the Mroz model. The properly identified model (green) provides the benchmark. When exclusion restrictions are removed (red), or when weak instruments are used (orange), both the outcome coefficients and the estimated selection correction can shift substantially, demonstrating the sensitivity of the Heckman estimator to identification strategy.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 College Data: Three Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specification (a): With proper exclusion restrictions (already estimated)\n",
    "# result_college_excl is our baseline\n",
    "\n",
    "# Specification (b): No exclusion restrictions (Z = X)\n",
    "model_college_no_excl = PanelHeckman(\n",
    "    endog=y_c,\n",
    "    exog=X_c,\n",
    "    selection=selection_c,\n",
    "    exog_selection=X_c,  # Z = X\n",
    "    method='two_step'\n",
    ")\n",
    "result_college_no_excl = model_college_no_excl.fit()\n",
    "\n",
    "# Specification (c): Weak exclusion (distance_college only, without tuition)\n",
    "Z_c_weak = sm.add_constant(college[['ability', 'parent_education', 'family_income',\n",
    "                                     'urban', 'female', 'distance_college']].values)\n",
    "Z_c_weak_names = ['const', 'ability', 'parent_education', 'family_income',\n",
    "                   'urban', 'female', 'distance_college']\n",
    "\n",
    "model_college_weak = PanelHeckman(\n",
    "    endog=y_c,\n",
    "    exog=X_c,\n",
    "    selection=selection_c,\n",
    "    exog_selection=Z_c_weak,\n",
    "    method='two_step'\n",
    ")\n",
    "result_college_weak = model_college_weak.fit()\n",
    "\n",
    "# Build comparison table\n",
    "comparison_college = pd.DataFrame({\n",
    "    'Variable': X_c_names,\n",
    "    '(a) With Exclusion': result_college_excl.outcome_params,\n",
    "    '(b) No Exclusion (Z=X)': result_college_no_excl.outcome_params,\n",
    "    '(c) Single Exclusion': result_college_weak.outcome_params,\n",
    "}).set_index('Variable')\n",
    "\n",
    "sel_params_c = pd.DataFrame({\n",
    "    'Variable': ['sigma', 'rho', 'lambda (rho*sigma)'],\n",
    "    '(a) With Exclusion': [result_college_excl.sigma, result_college_excl.rho,\n",
    "                            result_college_excl.rho * result_college_excl.sigma],\n",
    "    '(b) No Exclusion (Z=X)': [result_college_no_excl.sigma, result_college_no_excl.rho,\n",
    "                                result_college_no_excl.rho * result_college_no_excl.sigma],\n",
    "    '(c) Single Exclusion': [result_college_weak.sigma, result_college_weak.rho,\n",
    "                              result_college_weak.rho * result_college_weak.sigma],\n",
    "}).set_index('Variable')\n",
    "\n",
    "full_comparison_c = pd.concat([comparison_college, sel_params_c])\n",
    "\n",
    "print('=' * 75)\n",
    "print('  COMPARISON: COLLEGE DATA -- EFFECT OF EXCLUSION RESTRICTIONS')\n",
    "print('=' * 75)\n",
    "print()\n",
    "print(full_comparison_c.round(4).to_string())\n",
    "\n",
    "print('\\n' + '-' * 75)\n",
    "print('Note: Specification (c) uses only distance_college as exclusion restriction.')\n",
    "print('This may still provide reasonable identification if distance is a strong predictor.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize college comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot outcome equation coefficients (exclude constant for readability)\n",
    "vars_plot_c = ['ability', 'parent_education', 'family_income', 'urban', 'female']\n",
    "idx_plot_c = [X_c_names.index(v) for v in vars_plot_c]\n",
    "\n",
    "x_pos = np.arange(len(vars_plot_c))\n",
    "width = 0.25\n",
    "\n",
    "axes[0].bar(x_pos - width, [result_college_excl.outcome_params[i] for i in idx_plot_c],\n",
    "            width, label='(a) With Exclusion', color='#27ae60', alpha=0.8)\n",
    "axes[0].bar(x_pos, [result_college_no_excl.outcome_params[i] for i in idx_plot_c],\n",
    "            width, label='(b) No Exclusion', color='#e74c3c', alpha=0.8)\n",
    "axes[0].bar(x_pos + width, [result_college_weak.outcome_params[i] for i in idx_plot_c],\n",
    "            width, label='(c) Single Exclusion', color='#f39c12', alpha=0.8)\n",
    "\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(vars_plot_c, fontsize=9, rotation=15)\n",
    "axes[0].set_ylabel('Coefficient Value', fontsize=12)\n",
    "axes[0].set_title('Outcome Equation: Coefficient Comparison\\n(College Data)', fontsize=13)\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.8)\n",
    "\n",
    "# Selection parameters\n",
    "sel_labels = ['sigma', 'rho', 'lambda']\n",
    "sel_a = [result_college_excl.sigma, result_college_excl.rho,\n",
    "         result_college_excl.rho * result_college_excl.sigma]\n",
    "sel_b = [result_college_no_excl.sigma, result_college_no_excl.rho,\n",
    "         result_college_no_excl.rho * result_college_no_excl.sigma]\n",
    "sel_c = [result_college_weak.sigma, result_college_weak.rho,\n",
    "         result_college_weak.rho * result_college_weak.sigma]\n",
    "\n",
    "x_pos2 = np.arange(len(sel_labels))\n",
    "axes[1].bar(x_pos2 - width, sel_a, width, label='(a) With Exclusion', color='#27ae60', alpha=0.8)\n",
    "axes[1].bar(x_pos2, sel_b, width, label='(b) No Exclusion', color='#e74c3c', alpha=0.8)\n",
    "axes[1].bar(x_pos2 + width, sel_c, width, label='(c) Single Exclusion', color='#f39c12', alpha=0.8)\n",
    "\n",
    "axes[1].set_xticks(x_pos2)\n",
    "axes[1].set_xticklabels([r'$\\sigma$', r'$\\rho$', r'$\\lambda = \\rho\\sigma$'], fontsize=12)\n",
    "axes[1].set_ylabel('Parameter Value', fontsize=12)\n",
    "axes[1].set_title('Selection Parameters: Sensitivity\\nto Exclusion Restrictions', fontsize=13)\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'exclusion_comparison_college.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Coefficient comparison across three specifications of the college wage model. With proper exclusion restrictions (green), estimates are stable and economically meaningful. Removing exclusion restrictions (red) leads to identification purely through functional form, potentially distorting the estimated returns to ability, parental education, and other factors. The single-exclusion specification (orange) provides an intermediate case.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Diagnosing the Problem: Collinearity Analysis\n",
    "\n",
    "When exclusion restrictions are absent, the inverse Mills ratio $\\lambda(X'\\hat{\\gamma})$ is a near-linear function of $X$, creating severe multicollinearity in the augmented outcome equation. We can diagnose this directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate collinearity: Correlate IMR with X variables\n",
    "\n",
    "# Model WITH exclusion restrictions\n",
    "Zg_excl = Z @ result_excl.probit_params\n",
    "Phi_excl = stats.norm.cdf(Zg_excl)\n",
    "imr_excl = stats.norm.pdf(Zg_excl) / np.clip(Phi_excl, 1e-10, None)\n",
    "\n",
    "# Model WITHOUT exclusion restrictions\n",
    "Xg_no_excl = X @ result_no_excl.probit_params\n",
    "Phi_no_excl = stats.norm.cdf(Xg_no_excl)\n",
    "imr_no_excl = stats.norm.pdf(Xg_no_excl) / np.clip(Phi_no_excl, 1e-10, None)\n",
    "\n",
    "# Compute correlations between IMR and outcome equation variables\n",
    "# (for selected observations only)\n",
    "sel_mask = selection == 1\n",
    "\n",
    "print('=== Correlation Between IMR and Outcome Variables (Selected Sample) ===')\n",
    "print()\n",
    "print(f'{\"Variable\":20s} {\"With Exclusion\":>15s} {\"Without Exclusion\":>18s}')\n",
    "print('-' * 58)\n",
    "\n",
    "for i, name in enumerate(X_names):\n",
    "    if name == 'const':\n",
    "        continue\n",
    "    corr_excl = np.corrcoef(X[sel_mask, i], imr_excl[sel_mask])[0, 1]\n",
    "    corr_no_excl = np.corrcoef(X[sel_mask, i], imr_no_excl[sel_mask])[0, 1]\n",
    "    flag = ' *** HIGH' if abs(corr_no_excl) > 0.7 else ''\n",
    "    print(f'{name:20s} {corr_excl:15.4f} {corr_no_excl:18.4f}{flag}')\n",
    "\n",
    "# Also check correlation of IMR with linear predictor Xb\n",
    "Xb = X[sel_mask] @ result_excl.outcome_params\n",
    "corr_xb_excl = np.corrcoef(Xb, imr_excl[sel_mask])[0, 1]\n",
    "corr_xb_no = np.corrcoef(Xb, imr_no_excl[sel_mask])[0, 1]\n",
    "\n",
    "print('-' * 58)\n",
    "print(f'{\"X*beta (linear pred)\":20s} {corr_xb_excl:15.4f} {corr_xb_no:18.4f}')\n",
    "\n",
    "print('\\n*** HIGH marks correlations above 0.7 in absolute value')\n",
    "print('\\nConclusion: Without exclusion restrictions, the IMR is highly')\n",
    "print('correlated with X variables, creating the multicollinearity')\n",
    "print('that makes estimation unreliable.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots: IMR vs linear predictor (with vs without exclusion)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# With exclusion restrictions\n",
    "axes[0].scatter(Xb, imr_excl[sel_mask], alpha=0.4, s=15, color='#27ae60')\n",
    "axes[0].set_xlabel(r\"$X'\\hat{\\beta}$ (outcome linear predictor)\", fontsize=12)\n",
    "axes[0].set_ylabel(r'$\\lambda(Z\\'\\hat{\\gamma})$ (IMR)', fontsize=12)\n",
    "axes[0].set_title(f'WITH Exclusion Restrictions\\n'\n",
    "                   f'Corr(X\\'b, IMR) = {corr_xb_excl:.3f}', fontsize=13)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Without exclusion restrictions\n",
    "axes[1].scatter(Xb, imr_no_excl[sel_mask], alpha=0.4, s=15, color='#e74c3c')\n",
    "axes[1].set_xlabel(r\"$X'\\hat{\\beta}$ (outcome linear predictor)\", fontsize=12)\n",
    "axes[1].set_ylabel(r\"$\\lambda(X'\\hat{\\gamma})$ (IMR)\", fontsize=12)\n",
    "axes[1].set_title(f'WITHOUT Exclusion Restrictions\\n'\n",
    "                   f'Corr(X\\'b, IMR) = {corr_xb_no:.3f}', fontsize=13)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'imr_collinearity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('With exclusion restrictions (left):')\n",
    "print('  - The IMR has variation independent of X\\'b')\n",
    "print('  - The scatter shows dispersion, NOT a tight line')\n",
    "print()\n",
    "print('Without exclusion restrictions (right):')\n",
    "print('  - The IMR is nearly perfectly correlated with X\\'b')\n",
    "print('  - Points form a tight curve (near-linear relationship)')\n",
    "print('  - This collinearity makes it impossible to separately estimate')\n",
    "print('    the effect of X and the selection correction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Scatter plots of the inverse Mills ratio against the outcome linear predictor $X'\\hat{\\beta}$ for selected observations. Left: With exclusion restrictions, the IMR has substantial variation independent of $X'\\beta$, providing genuine identifying power. Right: Without exclusion restrictions, the IMR collapses onto a near-perfect function of $X'\\beta$, making it impossible to disentangle the selection correction from the outcome equation covariates.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## 7. Testing Exclusion Restrictions\n",
    "\n",
    "A credible exclusion restriction must satisfy two conditions:\n",
    "\n",
    "1. **Relevance**: The instrument must meaningfully predict selection\n",
    "2. **Validity (Excludability)**: The instrument must not directly affect the outcome\n",
    "\n",
    "### 7.1 Testing Relevance\n",
    "\n",
    "We can test this statistically: are the exclusion restrictions jointly significant in the probit selection equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test relevance: Compare restricted vs unrestricted probit models\n",
    "# Restricted: probit without exclusion variables\n",
    "# Unrestricted: probit with exclusion variables\n",
    "\n",
    "def probit_log_likelihood(gamma, Z, selection):\n",
    "    \"\"\"Compute probit log-likelihood.\"\"\"\n",
    "    linear_pred = Z @ gamma\n",
    "    prob = stats.norm.cdf(linear_pred)\n",
    "    prob = np.clip(prob, 1e-10, 1 - 1e-10)\n",
    "    return np.sum(selection * np.log(prob) + (1 - selection) * np.log(1 - prob))\n",
    "\n",
    "# --- Mroz dataset ---\n",
    "print('=== Relevance Test: Mroz Data ===')\n",
    "print()\n",
    "\n",
    "# Unrestricted: full selection equation\n",
    "ll_unrestricted = probit_log_likelihood(result_excl.probit_params, Z, selection)\n",
    "\n",
    "# Restricted: selection equation WITHOUT exclusion variables (same as outcome vars)\n",
    "from scipy.optimize import minimize as sp_minimize\n",
    "\n",
    "def neg_probit_llf(gamma, Z, sel):\n",
    "    return -probit_log_likelihood(gamma, Z, sel)\n",
    "\n",
    "# Fit restricted probit (Z = X only)\n",
    "res_restricted = sp_minimize(neg_probit_llf, np.zeros(X.shape[1]),\n",
    "                              args=(X, selection), method='BFGS')\n",
    "ll_restricted = -res_restricted.fun\n",
    "\n",
    "# Likelihood ratio test\n",
    "n_restrictions = Z.shape[1] - X.shape[1]  # Number of excluded variables\n",
    "lr_stat = 2 * (ll_unrestricted - ll_restricted)\n",
    "lr_pvalue = 1 - stats.chi2.cdf(lr_stat, df=n_restrictions)\n",
    "\n",
    "print(f'Log-likelihood (unrestricted, with exclusions): {ll_unrestricted:.2f}')\n",
    "print(f'Log-likelihood (restricted, without exclusions): {ll_restricted:.2f}')\n",
    "print(f'Number of exclusion restrictions tested: {n_restrictions}')\n",
    "print(f'\\nLikelihood Ratio statistic: {lr_stat:.4f}')\n",
    "print(f'Chi-squared degrees of freedom: {n_restrictions}')\n",
    "print(f'p-value: {lr_pvalue:.6f}')\n",
    "print()\n",
    "\n",
    "if lr_pvalue < 0.05:\n",
    "    print('RESULT: Reject H0 at 5% level.')\n",
    "    print('The exclusion restrictions are JOINTLY SIGNIFICANT in the selection equation.')\n",
    "    print('This confirms the RELEVANCE condition is satisfied.')\n",
    "else:\n",
    "    print('RESULT: Fail to reject H0 at 5% level.')\n",
    "    print('WARNING: The exclusion restrictions are NOT jointly significant.')\n",
    "    print('The instruments may be too weak for reliable identification.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- College dataset ---\n",
    "print('=== Relevance Test: College Data ===')\n",
    "print()\n",
    "\n",
    "ll_unrestricted_c = probit_log_likelihood(result_college_excl.probit_params, Z_c, selection_c)\n",
    "\n",
    "res_restricted_c = sp_minimize(neg_probit_llf, np.zeros(X_c.shape[1]),\n",
    "                                args=(X_c, selection_c), method='BFGS')\n",
    "ll_restricted_c = -res_restricted_c.fun\n",
    "\n",
    "n_restrictions_c = Z_c.shape[1] - X_c.shape[1]\n",
    "lr_stat_c = 2 * (ll_unrestricted_c - ll_restricted_c)\n",
    "lr_pvalue_c = 1 - stats.chi2.cdf(lr_stat_c, df=n_restrictions_c)\n",
    "\n",
    "print(f'Log-likelihood (unrestricted, with exclusions): {ll_unrestricted_c:.2f}')\n",
    "print(f'Log-likelihood (restricted, without exclusions): {ll_restricted_c:.2f}')\n",
    "print(f'Number of exclusion restrictions tested: {n_restrictions_c}')\n",
    "print(f'\\nLikelihood Ratio statistic: {lr_stat_c:.4f}')\n",
    "print(f'Chi-squared degrees of freedom: {n_restrictions_c}')\n",
    "print(f'p-value: {lr_pvalue_c:.6f}')\n",
    "print()\n",
    "\n",
    "if lr_pvalue_c < 0.05:\n",
    "    print('RESULT: Reject H0 at 5% level.')\n",
    "    print('The exclusion restrictions are JOINTLY SIGNIFICANT in the selection equation.')\n",
    "    print('Relevance condition is satisfied.')\n",
    "else:\n",
    "    print('RESULT: Fail to reject H0 at 5% level.')\n",
    "    print('WARNING: Weak instruments detected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Testing Validity (The Hard Part)\n",
    "\n",
    "The **validity** of exclusion restrictions -- that the instrument does not directly affect the outcome -- is fundamentally **untestable** with the data alone. This is the same problem as with instrumental variables: the exclusion restriction is an identifying assumption.\n",
    "\n",
    "However, we can provide supporting evidence:\n",
    "\n",
    "#### A. Economic Reasoning (Most Important)\n",
    "\n",
    "The strongest case for validity comes from **economic theory**:\n",
    "\n",
    "| Instrument | Argument for Validity |\n",
    "|---|---|\n",
    "| Children (Mroz) | Number of children affects time allocation, not hourly wage rate |\n",
    "| Husband's income | Other household income affects reservation wage, not market wage |\n",
    "| Distance to college | Geographic proximity affects costs, not worker productivity |\n",
    "| Tuition | Historical tuition costs do not affect current employer's willingness to pay |\n",
    "\n",
    "#### B. Informal Over-Identification Test\n",
    "\n",
    "If we have **more exclusion restrictions than needed**, we can test whether results are stable when using different subsets of instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over-identification style check: Mroz data\n",
    "# Estimate with each exclusion restriction individually\n",
    "\n",
    "print('=== Over-Identification Check: Mroz Data ===')\n",
    "print('Estimate model with EACH exclusion restriction individually.')\n",
    "print('If results are consistent, it supports validity of all instruments.')\n",
    "print()\n",
    "\n",
    "individual_results = {}\n",
    "exclusion_sets = {\n",
    "    'children_lt6 only': ['education', 'experience', 'experience_sq', 'children_lt6'],\n",
    "    'children_6_18 only': ['education', 'experience', 'experience_sq', 'children_6_18'],\n",
    "    'husband_income only': ['education', 'experience', 'experience_sq', 'husband_income'],\n",
    "    'All children vars': ['education', 'experience', 'experience_sq', 'children_lt6', 'children_6_18'],\n",
    "    'Full (all excl.)': ['education', 'experience', 'experience_sq', 'age',\n",
    "                          'children_lt6', 'children_6_18', 'husband_income'],\n",
    "}\n",
    "\n",
    "for name, z_cols in exclusion_sets.items():\n",
    "    Z_test = sm.add_constant(mroz[z_cols].values)\n",
    "    try:\n",
    "        model_test = PanelHeckman(\n",
    "            endog=y, exog=X, selection=selection,\n",
    "            exog_selection=Z_test, method='two_step'\n",
    "        )\n",
    "        res_test = model_test.fit()\n",
    "        individual_results[name] = {\n",
    "            'education': res_test.outcome_params[1],\n",
    "            'experience': res_test.outcome_params[2],\n",
    "            'rho': res_test.rho,\n",
    "            'sigma': res_test.sigma,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        individual_results[name] = {'error': str(e)}\n",
    "\n",
    "overid_df = pd.DataFrame(individual_results).T\n",
    "print(overid_df.round(4).to_string())\n",
    "\n",
    "print('\\nInterpretation:')\n",
    "print('If the education and experience coefficients are similar across')\n",
    "print('specifications, it supports the validity of the exclusion restrictions.')\n",
    "if 'education' in overid_df.columns:\n",
    "    edu_range = overid_df['education'].max() - overid_df['education'].min()\n",
    "    print(f'\\nRange of education coefficient: {edu_range:.4f}')\n",
    "    if edu_range < 0.05:\n",
    "        print('  -> Coefficients are stable. Good evidence for instrument validity.')\n",
    "    elif edu_range < 0.15:\n",
    "        print('  -> Moderate variation. Results are somewhat sensitive to instrument choice.')\n",
    "    else:\n",
    "        print('  -> Large variation. Instruments may not all be valid.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the over-identification check\n",
    "if 'education' in overid_df.columns and 'rho' in overid_df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Education coefficient across specifications\n",
    "    specs = overid_df.index.tolist()\n",
    "    x_pos = np.arange(len(specs))\n",
    "\n",
    "    colors = ['#3498db', '#e74c3c', '#f39c12', '#9b59b6', '#27ae60']\n",
    "\n",
    "    axes[0].bar(x_pos, overid_df['education'].values, color=colors[:len(specs)], alpha=0.8,\n",
    "                edgecolor='black', linewidth=0.5)\n",
    "    axes[0].axhline(y=overid_df['education'].mean(), color='black', linestyle='--',\n",
    "                     linewidth=1.5, label=f'Mean = {overid_df[\"education\"].mean():.4f}')\n",
    "    axes[0].set_xticks(x_pos)\n",
    "    axes[0].set_xticklabels(specs, fontsize=8, rotation=20, ha='right')\n",
    "    axes[0].set_ylabel('Education Coefficient', fontsize=12)\n",
    "    axes[0].set_title('Stability of Education Coefficient\\nAcross Instrument Sets', fontsize=13)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Rho across specifications\n",
    "    axes[1].bar(x_pos, overid_df['rho'].values, color=colors[:len(specs)], alpha=0.8,\n",
    "                edgecolor='black', linewidth=0.5)\n",
    "    axes[1].axhline(y=overid_df['rho'].mean(), color='black', linestyle='--',\n",
    "                     linewidth=1.5, label=f'Mean = {overid_df[\"rho\"].mean():.4f}')\n",
    "    axes[1].set_xticks(x_pos)\n",
    "    axes[1].set_xticklabels(specs, fontsize=8, rotation=20, ha='right')\n",
    "    axes[1].set_ylabel(r'$\\rho$ (selection correlation)', fontsize=12)\n",
    "    axes[1].set_title(r'Stability of $\\rho$ Across Instrument Sets', fontsize=13)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'overidentification_check.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Stability of key estimates (education coefficient on the left, selection correlation rho on the right) across different subsets of exclusion restrictions. Consistent estimates across instrument sets provide informal evidence supporting the validity of the exclusion restrictions. If estimates varied wildly, it would raise concerns about instrument validity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Potential Problems with Common Exclusion Restrictions\n",
    "\n",
    "Not all commonly used exclusion restrictions are above reproach:\n",
    "\n",
    "| Instrument | Potential Concern |\n",
    "|---|---|\n",
    "| **Number of children** | Children might affect human capital accumulation (time out of labor force reduces skills), which in turn affects wages |\n",
    "| **Husband's income** | Assortative mating: women married to high-income men may have different unobserved skills |\n",
    "| **Distance to college** | Distance correlates with rurality, which may directly affect wages through labor market thickness |\n",
    "| **Tuition** | Tuition varies by state/time, correlating with other state-level factors affecting wages |\n",
    "\n",
    "**Key lesson**: There is no perfect exclusion restriction. The researcher must make a judgment call and defend it with economic reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "## 8. Sensitivity Analysis\n",
    "\n",
    "A thorough applied analysis should examine how results change with different instrument specifications. This builds confidence (or reveals fragility) in the estimates.\n",
    "\n",
    "### 8.1 Systematic Sensitivity Analysis: Mroz Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive sensitivity analysis for Mroz data\n",
    "# Try many different combinations of exclusion restrictions\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, str(BASE_DIR / 'utils'))\n",
    "from comparison_tools import sensitivity_analysis\n",
    "\n",
    "sensitivity_specs = {\n",
    "    '1. No exclusion (Z=X)': X,\n",
    "    '2. children_lt6': sm.add_constant(mroz[['education', 'experience', 'experience_sq',\n",
    "                                              'children_lt6']].values),\n",
    "    '3. children_6_18': sm.add_constant(mroz[['education', 'experience', 'experience_sq',\n",
    "                                               'children_6_18']].values),\n",
    "    '4. husband_income': sm.add_constant(mroz[['education', 'experience', 'experience_sq',\n",
    "                                                'husband_income']].values),\n",
    "    '5. Both children': sm.add_constant(mroz[['education', 'experience', 'experience_sq',\n",
    "                                               'children_lt6', 'children_6_18']].values),\n",
    "    '6. Children + income': sm.add_constant(mroz[['education', 'experience', 'experience_sq',\n",
    "                                                    'children_lt6', 'children_6_18',\n",
    "                                                    'husband_income']].values),\n",
    "    '7. Full (all + age)': Z,\n",
    "}\n",
    "\n",
    "sensitivity_results = []\n",
    "for name, Z_spec in sensitivity_specs.items():\n",
    "    try:\n",
    "        m = PanelHeckman(endog=y, exog=X, selection=selection,\n",
    "                         exog_selection=Z_spec, method='two_step')\n",
    "        r = m.fit()\n",
    "        sensitivity_results.append({\n",
    "            'Specification': name,\n",
    "            'beta_education': r.outcome_params[1],\n",
    "            'beta_experience': r.outcome_params[2],\n",
    "            'beta_exper_sq': r.outcome_params[3],\n",
    "            'sigma': r.sigma,\n",
    "            'rho': r.rho,\n",
    "            'lambda': r.rho * r.sigma,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        sensitivity_results.append({\n",
    "            'Specification': name,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "sens_df = pd.DataFrame(sensitivity_results).set_index('Specification')\n",
    "\n",
    "print('=' * 85)\n",
    "print('  SENSITIVITY ANALYSIS: MROZ DATA')\n",
    "print('  How do estimates change across different exclusion restrictions?')\n",
    "print('=' * 85)\n",
    "print()\n",
    "print(sens_df.round(4).to_string())\n",
    "\n",
    "print('\\n' + '-' * 85)\n",
    "if 'beta_education' in sens_df.columns:\n",
    "    edu_std = sens_df['beta_education'].std()\n",
    "    rho_std = sens_df['rho'].std()\n",
    "    print(f'Std. dev. of education coefficient: {edu_std:.4f}')\n",
    "    print(f'Std. dev. of rho across specs:      {rho_std:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensitivity analysis\n",
    "if 'beta_education' in sens_df.columns:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    specs = sens_df.index.tolist()\n",
    "    x_pos = np.arange(len(specs))\n",
    "\n",
    "    # Top: Outcome coefficients across specifications\n",
    "    width = 0.25\n",
    "    axes[0].bar(x_pos - width, sens_df['beta_education'].values, width,\n",
    "                label='Education', color='#3498db', alpha=0.8)\n",
    "    axes[0].bar(x_pos, sens_df['beta_experience'].values, width,\n",
    "                label='Experience', color='#27ae60', alpha=0.8)\n",
    "    axes[0].bar(x_pos + width, sens_df['beta_exper_sq'].values * 100, width,\n",
    "                label=r'Experience$^2$ ($\\times 100$)', color='#e74c3c', alpha=0.8)\n",
    "    axes[0].set_xticks(x_pos)\n",
    "    axes[0].set_xticklabels(specs, fontsize=8, rotation=25, ha='right')\n",
    "    axes[0].set_ylabel('Coefficient Value', fontsize=12)\n",
    "    axes[0].set_title('Outcome Equation Coefficients Across Specifications', fontsize=13)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0].axhline(y=0, color='black', linewidth=0.8)\n",
    "\n",
    "    # Bottom: rho and lambda across specifications\n",
    "    ax2 = axes[1]\n",
    "    color_rho = '#8e44ad'\n",
    "    color_lambda = '#d35400'\n",
    "\n",
    "    bars1 = ax2.bar(x_pos - 0.15, sens_df['rho'].values, 0.3,\n",
    "                     label=r'$\\rho$', color=color_rho, alpha=0.8)\n",
    "    bars2 = ax2.bar(x_pos + 0.15, sens_df['lambda'].values, 0.3,\n",
    "                     label=r'$\\lambda = \\rho\\sigma$', color=color_lambda, alpha=0.8)\n",
    "\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(specs, fontsize=8, rotation=25, ha='right')\n",
    "    ax2.set_ylabel('Parameter Value', fontsize=12)\n",
    "    ax2.set_title('Selection Parameters Across Specifications', fontsize=13)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.axhline(y=0, color='black', linewidth=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'sensitivity_analysis_mroz.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Comprehensive sensitivity analysis showing how outcome equation coefficients (top) and selection parameters (bottom) vary across seven instrument specifications for the Mroz data. Stable estimates across properly identified specifications (3-7) provide confidence in the results. Specification 1 (no exclusion) stands out as potentially unreliable, illustrating the importance of having at least one valid exclusion restriction.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Sensitivity Analysis: College Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity analysis for College data\n",
    "sensitivity_specs_c = {\n",
    "    '1. No exclusion (Z=X)': X_c,\n",
    "    '2. distance only': sm.add_constant(college[['ability', 'parent_education', 'family_income',\n",
    "                                                  'urban', 'female', 'distance_college']].values),\n",
    "    '3. tuition only': sm.add_constant(college[['ability', 'parent_education', 'family_income',\n",
    "                                                 'urban', 'female', 'tuition']].values),\n",
    "    '4. Both (dist + tuit)': Z_c,\n",
    "}\n",
    "\n",
    "sensitivity_results_c = []\n",
    "for name, Z_spec in sensitivity_specs_c.items():\n",
    "    try:\n",
    "        m = PanelHeckman(endog=y_c, exog=X_c, selection=selection_c,\n",
    "                         exog_selection=Z_spec, method='two_step')\n",
    "        r = m.fit()\n",
    "        sensitivity_results_c.append({\n",
    "            'Specification': name,\n",
    "            'beta_ability': r.outcome_params[1],\n",
    "            'beta_parent_ed': r.outcome_params[2],\n",
    "            'beta_family_inc': r.outcome_params[3],\n",
    "            'beta_urban': r.outcome_params[4],\n",
    "            'beta_female': r.outcome_params[5],\n",
    "            'sigma': r.sigma,\n",
    "            'rho': r.rho,\n",
    "            'lambda': r.rho * r.sigma,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        sensitivity_results_c.append({'Specification': name, 'error': str(e)})\n",
    "\n",
    "sens_df_c = pd.DataFrame(sensitivity_results_c).set_index('Specification')\n",
    "\n",
    "print('=' * 85)\n",
    "print('  SENSITIVITY ANALYSIS: COLLEGE DATA')\n",
    "print('=' * 85)\n",
    "print()\n",
    "print(sens_df_c.round(4).to_string())\n",
    "\n",
    "print('\\n' + '-' * 85)\n",
    "if 'beta_ability' in sens_df_c.columns:\n",
    "    ability_range = sens_df_c['beta_ability'].max() - sens_df_c['beta_ability'].min()\n",
    "    print(f'Range of ability coefficient: {ability_range:.4f}')\n",
    "    print(f'Range of rho: {sens_df_c[\"rho\"].max() - sens_df_c[\"rho\"].min():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLS (ignoring selection) vs Heckman (with correction)\n",
    "# This shows the bias from ignoring selection\n",
    "\n",
    "print('=' * 70)\n",
    "print('  OLS vs HECKMAN: THE COST OF IGNORING SELECTION BIAS')\n",
    "print('=' * 70)\n",
    "\n",
    "# --- Mroz ---\n",
    "sel_mask_m = selection == 1\n",
    "beta_ols_m = np.linalg.lstsq(X[sel_mask_m], y[sel_mask_m], rcond=None)[0]\n",
    "\n",
    "print('\\n--- Mroz Data (Log Wage Equation) ---')\n",
    "print(f'{\"Variable\":20s} {\"OLS (biased)\":>12s} {\"Heckman\":>12s} {\"Difference\":>12s} {\"Bias %\":>10s}')\n",
    "print('-' * 70)\n",
    "for i, name in enumerate(X_names):\n",
    "    diff = beta_ols_m[i] - result_excl.outcome_params[i]\n",
    "    pct = 100 * diff / (abs(result_excl.outcome_params[i]) + 1e-10)\n",
    "    print(f'{name:20s} {beta_ols_m[i]:12.4f} {result_excl.outcome_params[i]:12.4f} '\n",
    "          f'{diff:12.4f} {pct:9.1f}%')\n",
    "\n",
    "# --- College ---\n",
    "sel_mask_c = selection_c == 1\n",
    "beta_ols_c = np.linalg.lstsq(X_c[sel_mask_c], y_c[sel_mask_c], rcond=None)[0]\n",
    "\n",
    "print('\\n--- College Data (Log Wage Equation) ---')\n",
    "print(f'{\"Variable\":20s} {\"OLS (biased)\":>12s} {\"Heckman\":>12s} {\"Difference\":>12s} {\"Bias %\":>10s}')\n",
    "print('-' * 70)\n",
    "for i, name in enumerate(X_c_names):\n",
    "    diff = beta_ols_c[i] - result_college_excl.outcome_params[i]\n",
    "    pct = 100 * diff / (abs(result_college_excl.outcome_params[i]) + 1e-10)\n",
    "    print(f'{name:20s} {beta_ols_c[i]:12.4f} {result_college_excl.outcome_params[i]:12.4f} '\n",
    "          f'{diff:12.4f} {pct:9.1f}%')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('OLS on the selected sample ignores selection bias.')\n",
    "print('When rho is non-zero, OLS coefficients can be substantially biased.')\n",
    "print('The Heckman model with proper exclusion restrictions corrects this.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>\n",
    "## 9. Best Practices\n",
    "\n",
    "### 9.1 Guidelines for Choosing Exclusion Restrictions\n",
    "\n",
    "Based on the analysis above and the econometric literature, here are guidelines for selecting exclusion restrictions:\n",
    "\n",
    "#### Rule 1: Start with Economic Theory\n",
    "\n",
    "The exclusion restriction must be motivated by a clear economic argument:\n",
    "- **Why does the variable affect selection?** There must be a plausible economic mechanism\n",
    "- **Why does it NOT affect the outcome?** This must be defensible\n",
    "- Think of the variable as shifting the **cost** of participation without affecting the **return**\n",
    "\n",
    "#### Rule 2: Use Multiple Exclusion Restrictions\n",
    "\n",
    "- Having more than one exclusion restriction allows:\n",
    "  - Over-identification tests (checking consistency across instruments)\n",
    "  - Stronger first-stage prediction of selection\n",
    "  - More robust identification\n",
    "- But additional instruments must also be valid!\n",
    "\n",
    "#### Rule 3: Test Relevance Statistically\n",
    "\n",
    "- Run the probit selection equation and test joint significance of excluded variables\n",
    "- If the exclusion restrictions are weak predictors, the model is poorly identified\n",
    "- Rule of thumb: F-statistic > 10 in linear probability selection equation\n",
    "\n",
    "#### Rule 4: Report Sensitivity Analyses\n",
    "\n",
    "- Always show results with and without exclusion restrictions\n",
    "- Show results with different subsets of instruments\n",
    "- If results are highly sensitive to the choice of instruments, be transparent about this\n",
    "\n",
    "#### Rule 5: Be Honest About Limitations\n",
    "\n",
    "- No exclusion restriction is perfect\n",
    "- Acknowledge potential threats to validity\n",
    "- Present the case for your chosen instruments clearly\n",
    "\n",
    "### 9.2 Common Pitfalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of best practices\n",
    "practices = pd.DataFrame({\n",
    "    'Practice': [\n",
    "        'Use exclusion restrictions',\n",
    "        'Justify with economic theory',\n",
    "        'Test instrument relevance',\n",
    "        'Use multiple instruments',\n",
    "        'Run sensitivity analysis',\n",
    "        'Compare OLS vs Heckman',\n",
    "        'Report specifications without exclusion',\n",
    "        'Check IMR collinearity',\n",
    "    ],\n",
    "    'Why': [\n",
    "        'Identification from functional form alone is fragile',\n",
    "        'Statistical tests cannot validate the exclusion restriction',\n",
    "        'Weak instruments lead to unreliable estimates',\n",
    "        'Allows over-identification checks and stronger first stage',\n",
    "        'Reveals how robust conclusions are to instrument choice',\n",
    "        'Quantifies the magnitude and direction of selection bias',\n",
    "        'Transparency about identification strategy',\n",
    "        'Diagnoses multicollinearity in the augmented equation',\n",
    "    ],\n",
    "    'How in PanelBox': [\n",
    "        'Set exog_selection to include variables not in exog',\n",
    "        'Think about DGP and institutional details',\n",
    "        'LR test on probit selection equation',\n",
    "        'Include 2+ variables in Z but not X',\n",
    "        'Re-estimate with different subsets of exclusions',\n",
    "        'Use result.compare_ols_heckman()',\n",
    "        'Estimate with exog_selection=exog as a specification',\n",
    "        'Correlate IMR with X variables',\n",
    "    ],\n",
    "})\n",
    "\n",
    "print('=' * 90)\n",
    "print('  BEST PRACTICES FOR EXCLUSION RESTRICTIONS IN HECKMAN MODELS')\n",
    "print('=' * 90)\n",
    "print()\n",
    "for _, row in practices.iterrows():\n",
    "    print(f'  {row[\"Practice\"]}')\n",
    "    print(f'    WHY: {row[\"Why\"]}')\n",
    "    print(f'    HOW: {row[\"How in PanelBox\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Decision Flowchart\n",
    "\n",
    "When deciding on your identification strategy:\n",
    "\n",
    "```\n",
    "1. Do you have a candidate exclusion restriction?\n",
    "   |\n",
    "   +-- YES --> Is there a clear economic argument for validity?\n",
    "   |           |\n",
    "   |           +-- YES --> Does it significantly predict selection? (LR test)\n",
    "   |           |           |\n",
    "   |           |           +-- YES --> Use it! Run sensitivity analysis.\n",
    "   |           |           +-- NO  --> Instrument is too weak. Find a better one.\n",
    "   |           |\n",
    "   |           +-- NO  --> Do NOT use it. Seek alternatives.\n",
    "   |\n",
    "   +-- NO  --> Options:\n",
    "               a) Search for valid instruments (institutional details, policy variation)\n",
    "               b) Estimate without exclusion restrictions (acknowledge limitations)\n",
    "               c) Use alternative methods (bounds, control function approaches)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section10'></a>\n",
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **The identification problem**: Without exclusion restrictions, the Heckman model is identified only through the normality assumption. The inverse Mills ratio $\\lambda(X'\\gamma)$ is nearly linear in $X'\\gamma$ over typical data ranges, creating severe multicollinearity.\n",
    "\n",
    "2. **Exclusion restrictions**: Variables that affect selection but not the outcome break the collinearity and provide genuine identifying variation. They are the econometric analog of instrumental variables.\n",
    "\n",
    "3. **Practical demonstration**: We showed that removing exclusion restrictions from both the Mroz and College datasets leads to substantially different (and potentially unreliable) estimates of outcome equation parameters and selection correlation.\n",
    "\n",
    "4. **Testing**: Relevance can be tested statistically (LR test on the probit). Validity, however, relies on economic reasoning -- the same untestable assumption as in IV estimation.\n",
    "\n",
    "5. **Sensitivity analysis**: A credible applied analysis reports results across multiple instrument specifications. Stability of key estimates across specifications provides confidence in the identification strategy.\n",
    "\n",
    "### Key Equations\n",
    "\n",
    "| Concept | Formula |\n",
    "|---|---|\n",
    "| Conditional expectation | $E[y|s=1, X] = X'\\beta + \\rho\\sigma\\lambda(Z'\\gamma)$ |\n",
    "| Without exclusion ($Z=X$) | $\\lambda(X'\\gamma) \\approx a + bX'\\gamma$ (collinearity!) |\n",
    "| Relevance test | LR = $2(\\ell_{\\text{unrestricted}} - \\ell_{\\text{restricted}}) \\sim \\chi^2_q$ |\n",
    "| Selection correction | $\\hat{\\theta} = \\hat{\\rho}\\hat{\\sigma}$ (coefficient on IMR) |\n",
    "\n",
    "### Practical Takeaway\n",
    "\n",
    "> **Always use exclusion restrictions when possible.** The strongest applied work combines credible economic reasoning for the exclusion restriction with statistical evidence of relevance and sensitivity analyses showing robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "## 11. Exercises\n",
    "\n",
    "Test your understanding with these exercises!\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 1: Evaluate Candidate Instruments (Conceptual)\n",
    "\n",
    "For each proposed exclusion restriction below, evaluate whether it satisfies:\n",
    "- **Relevance**: Does it plausibly affect selection?\n",
    "- **Validity**: Can we argue it does NOT directly affect the outcome?\n",
    "\n",
    "| Application | Selection | Outcome | Proposed Instrument |\n",
    "|---|---|---|---|\n",
    "| (a) Female labor supply | Work vs not work | Hourly wage | Husband's age |\n",
    "| (b) College wage premium | Attend college | Post-college wage | SAT score |\n",
    "| (c) Union wage gap | Union member | Log wage | State right-to-work law |\n",
    "| (d) Training program | Participate in training | Quarterly earnings | Distance to training site |\n",
    "\n",
    "For each, write 2-3 sentences explaining your assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Write your assessments here\n",
    "\n",
    "# (a) Husband's age as instrument for female labor supply:\n",
    "# YOUR ANSWER:\n",
    "# Relevance: ...\n",
    "# Validity: ...\n",
    "# Assessment: ...\n",
    "\n",
    "# (b) SAT score as instrument for college wage premium:\n",
    "# YOUR ANSWER:\n",
    "# Relevance: ...\n",
    "# Validity: ...\n",
    "# Assessment: ...\n",
    "\n",
    "# (c) State right-to-work law for union wage gap:\n",
    "# YOUR ANSWER:\n",
    "# Relevance: ...\n",
    "# Validity: ...\n",
    "# Assessment: ...\n",
    "\n",
    "# (d) Distance to training site:\n",
    "# YOUR ANSWER:\n",
    "# Relevance: ...\n",
    "# Validity: ...\n",
    "# Assessment: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 2: Implement and Compare Specifications (Hands-On)\n",
    "\n",
    "Using the Mroz dataset, implement the following three models and compare:\n",
    "\n",
    "1. **Model A**: Exclusion restrictions = `children_lt6` + `husband_income`\n",
    "2. **Model B**: Exclusion restrictions = `children_6_18` + `husband_income`\n",
    "3. **Model C**: Exclusion restrictions = `age` only\n",
    "\n",
    "**Tasks**:\n",
    "- Estimate all three models\n",
    "- Create a comparison table of outcome coefficients and selection parameters\n",
    "- Which specification produces the most stable results? Why?\n",
    "- Run the LR relevance test for each set of exclusion restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "# Step 1: Define the three selection equation variable matrices\n",
    "# Model A: X + children_lt6 + husband_income\n",
    "# Z_A = sm.add_constant(mroz[['education', 'experience', 'experience_sq',\n",
    "#                              'children_lt6', 'husband_income']].values)\n",
    "\n",
    "# Model B: X + children_6_18 + husband_income\n",
    "# Z_B = ...\n",
    "\n",
    "# Model C: X + age\n",
    "# Z_C = ...\n",
    "\n",
    "# Step 2: Estimate each model\n",
    "# TODO: Use PanelHeckman with method='two_step'\n",
    "\n",
    "# Step 3: Compare results\n",
    "# TODO: Create comparison DataFrame\n",
    "\n",
    "# Step 4: LR relevance test for each specification\n",
    "# TODO: Compute LR statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3: Collinearity Diagnostic (Intermediate)\n",
    "\n",
    "For the College Wage dataset:\n",
    "\n",
    "1. Estimate the Heckman model with and without exclusion restrictions\n",
    "2. For each specification, compute the correlation matrix between the IMR and each X variable (for selected observations only)\n",
    "3. Create a heatmap visualization of both correlation matrices side by side\n",
    "4. Compute the condition number of the augmented design matrix $[X, \\lambda]$ for both cases\n",
    "5. Discuss: how does adding exclusion restrictions reduce the condition number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n",
    "# Step 1: Estimate both models (already done above)\n",
    "# With exclusion: result_college_excl\n",
    "# Without exclusion: result_college_no_excl\n",
    "\n",
    "# Step 2: Compute IMR for both specifications\n",
    "# TODO: Compute lambda(Z'gamma) for both models\n",
    "\n",
    "# Step 3: Correlation matrices\n",
    "# TODO: Correlate IMR with each X variable in the selected sample\n",
    "\n",
    "# Step 4: Condition numbers\n",
    "# Hint: np.linalg.cond(X_augmented)\n",
    "# TODO: Compute condition numbers\n",
    "\n",
    "# Step 5: Create heatmap visualization\n",
    "# TODO: Use seaborn heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 4: Monte Carlo Simulation (Advanced)\n",
    "\n",
    "Design a Monte Carlo experiment to demonstrate the importance of exclusion restrictions:\n",
    "\n",
    "1. Generate data from a known DGP with:\n",
    "   - True $\\beta = [1.0, 0.5]$ (outcome equation)\n",
    "   - True $\\rho = 0.5$ (selection correlation)\n",
    "   - A valid exclusion restriction $W$ that affects selection but not the outcome\n",
    "\n",
    "2. For 200 replications, estimate the Heckman model:\n",
    "   - (a) With the exclusion restriction\n",
    "   - (b) Without the exclusion restriction\n",
    "\n",
    "3. Compare the sampling distributions of $\\hat{\\beta}_1$, $\\hat{\\rho}$, and $\\hat{\\sigma}$ across both specifications\n",
    "\n",
    "4. Create histograms showing the distributions and mark the true parameter values\n",
    "\n",
    "**Expected result**: Specification (a) should show less bias and smaller variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your solution here\n",
    "\n",
    "# Step 1: Define DGP\n",
    "# np.random.seed(42)\n",
    "# n = 500\n",
    "# true_beta = np.array([1.0, 0.5])\n",
    "# true_rho = 0.5\n",
    "# true_sigma = 1.0\n",
    "\n",
    "# Step 2: Monte Carlo loop\n",
    "# n_reps = 200\n",
    "# results_with_excl = []\n",
    "# results_without_excl = []\n",
    "\n",
    "# for rep in range(n_reps):\n",
    "#     # Generate data\n",
    "#     # TODO: Create X, W, selection, y\n",
    "#     \n",
    "#     # Estimate with exclusion\n",
    "#     # TODO\n",
    "#     \n",
    "#     # Estimate without exclusion\n",
    "#     # TODO\n",
    "\n",
    "# Step 3: Compare distributions\n",
    "# TODO: Create histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "### Essential Reading\n",
    "\n",
    "1. **Heckman, J. J. (1979)**. \"Sample Selection Bias as a Specification Error.\" *Econometrica*, 47(1), 153-161.\n",
    "\n",
    "2. **Wooldridge, J. M. (2010)**. *Econometric Analysis of Cross Section and Panel Data* (2nd ed.). MIT Press. Chapter 19.\n",
    "\n",
    "3. **Cameron, A. C., & Trivedi, P. K. (2005)**. *Microeconometrics: Methods and Applications*. Cambridge University Press. Chapter 16.\n",
    "\n",
    "### On Identification\n",
    "\n",
    "4. **Mroz, T. A. (1987)**. \"The Sensitivity of an Empirical Model of Married Women's Hours of Work to Economic and Statistical Assumptions.\" *Econometrica*, 55(4), 765-799.\n",
    "\n",
    "5. **Puhani, P. A. (2000)**. \"The Heckman Correction for Sample Selection and Its Critique.\" *Journal of Economic Surveys*, 14(1), 53-68.\n",
    "\n",
    "6. **Card, D. (1995)**. \"Using Geographic Variation in College Proximity to Estimate the Return to Schooling.\" In *Aspects of Labour Market Behaviour: Essays in Honour of John Vanderkamp*.\n",
    "\n",
    "### On Weak Identification\n",
    "\n",
    "7. **Leung, S. F., & Yu, S. (1996)**. \"On the Choice Between Sample Selection and Two-Part Models.\" *Journal of Econometrics*, 72(1-2), 197-229.\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing this tutorial!**\n",
    "\n",
    "Questions or feedback? Visit: https://github.com/panelbox/panelbox/issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
