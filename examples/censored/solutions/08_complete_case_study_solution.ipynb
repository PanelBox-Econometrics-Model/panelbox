{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Case Study: Solutions\n",
    "## Notebook 08 -- Exercise Solutions\n",
    "\n",
    "This notebook provides complete, annotated solutions for all four exercises\n",
    "from the **Complete Case Study** notebook (08). Each solution builds on\n",
    "the models fitted in the main notebook.\n",
    "\n",
    "| Exercise | Topic | Duration |\n",
    "|----------|-------|----------|\n",
    "| 1 | Extended Model Comparison (Log OLS, AIC/BIC) | 20 min |\n",
    "| 2 | Marginal Effects at Representative Values | 15 min |\n",
    "| 3 | Heckman MLE Estimation | 20 min |\n",
    "| 4 | Prediction and Model Validation | 15 min |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "We reproduce the setup and model estimation from the main notebook so that\n",
    "the exercise solutions are self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Imports and configuration\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from panelbox.models.censored import PooledTobit, RandomEffectsTobit\n",
    "from panelbox.models.selection import PanelHeckman\n",
    "from panelbox.marginal_effects.censored_me import compute_tobit_ame, compute_tobit_mem\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.insert(0, str(BASE_DIR / 'utils'))\n",
    "from comparison_tools import compare_tobit_ols, compare_heckman_methods\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Loading and Baseline Models\n",
    "\n",
    "We load both datasets and fit the baseline models needed by the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load health expenditure panel\n",
    "# ============================================================\n",
    "\n",
    "df = pd.read_csv(DATA_DIR / 'health_expenditure_panel.csv')\n",
    "\n",
    "depvar = 'expenditure'\n",
    "covariates = ['income', 'age', 'chronic', 'insurance', 'female', 'bmi']\n",
    "\n",
    "y = df[depvar].values\n",
    "X_raw = df[covariates].values\n",
    "X = sm.add_constant(X_raw)\n",
    "var_names = ['const'] + covariates\n",
    "groups = df['id'].values\n",
    "\n",
    "n_censored = (y == 0).sum()\n",
    "pct_censored = n_censored / len(y) * 100\n",
    "\n",
    "print(f'Dataset: {df.shape[0]} obs, {df[\"id\"].nunique()} individuals')\n",
    "print(f'Censored at zero: {n_censored} ({pct_censored:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fit baseline OLS\n",
    "# ============================================================\n",
    "\n",
    "ols_model = sm.OLS(y, X)\n",
    "ols_result = ols_model.fit(cov_type='cluster', cov_kwds={'groups': groups})\n",
    "\n",
    "print('OLS fitted.')\n",
    "print(f'  R-squared: {ols_result.rsquared:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fit Pooled Tobit\n",
    "# ============================================================\n",
    "\n",
    "tobit_pooled = PooledTobit(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    groups=groups,\n",
    "    censoring_point=0.0,\n",
    ")\n",
    "tobit_pooled.fit()\n",
    "tobit_pooled.exog_names = var_names\n",
    "\n",
    "n_beta = len(var_names)\n",
    "print('Pooled Tobit fitted.')\n",
    "print(f'  Log-likelihood: {tobit_pooled.llf:.2f}')\n",
    "print(f'  sigma: {tobit_pooled.sigma:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fit Random Effects Tobit\n",
    "# ============================================================\n",
    "\n",
    "tobit_re = RandomEffectsTobit(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    groups=groups,\n",
    "    censoring_point=0.0,\n",
    "    quadrature_points=12,\n",
    ")\n",
    "tobit_re.fit(method='BFGS', maxiter=2000, options={'disp': False})\n",
    "\n",
    "print('RE Tobit fitted.')\n",
    "print(f'  Log-likelihood: {tobit_re.llf:.2f}')\n",
    "print(f'  sigma_eps: {tobit_re.sigma_eps:.4f}')\n",
    "print(f'  sigma_alpha: {tobit_re.sigma_alpha:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load Mroz data and fit Heckman two-step\n",
    "# ============================================================\n",
    "\n",
    "mroz = pd.read_csv(DATA_DIR / 'mroz_1987.csv')\n",
    "\n",
    "selection = mroz['lfp'].values.astype(int)\n",
    "wage = mroz['wage'].fillna(0).values\n",
    "\n",
    "outcome_vars = ['education', 'experience', 'experience_sq']\n",
    "X_outcome = sm.add_constant(mroz[outcome_vars].values)\n",
    "outcome_names = ['const'] + outcome_vars\n",
    "\n",
    "selection_vars = ['education', 'experience', 'experience_sq', 'age',\n",
    "                  'children_lt6', 'children_6_18', 'husband_income']\n",
    "Z_selection = sm.add_constant(mroz[selection_vars].values)\n",
    "selection_names = ['const'] + selection_vars\n",
    "\n",
    "heckman_model = PanelHeckman(\n",
    "    endog=wage,\n",
    "    exog=X_outcome,\n",
    "    selection=selection,\n",
    "    exog_selection=Z_selection,\n",
    "    method='two_step',\n",
    ")\n",
    "heckman_result = heckman_model.fit()\n",
    "\n",
    "print('Heckman two-step fitted.')\n",
    "print(f'  rho: {heckman_result.rho:.4f}')\n",
    "print(f'  sigma: {heckman_result.sigma:.4f}')\n",
    "print(f'  Selected: {heckman_result.n_selected} / {heckman_result.n_total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Extended Model Comparison (20 min)\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "a) Fit a **log-transformed OLS** model using `log(1 + expenditure)` as the dependent\n",
    "   variable and compare its predicted values with the Tobit model.\n",
    "\n",
    "b) Compute **AIC** and **BIC** for the Pooled Tobit and RE Tobit models.\n",
    "   Recall: $\\text{AIC} = -2 \\ln L + 2k$ and $\\text{BIC} = -2 \\ln L + k \\ln n$.\n",
    "\n",
    "c) Which model does each criterion prefer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1a: Log-Transformed OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1a: Fit log-transformed OLS\n",
    "# ============================================================\n",
    "\n",
    "# Transform the dependent variable: log(1 + expenditure)\n",
    "# This is a common \"quick fix\" for censored data -- it reduces\n",
    "# skewness and compresses the zero pile-up, but does NOT properly\n",
    "# account for the censoring mechanism.\n",
    "\n",
    "y_log = np.log1p(y)  # log(1 + y)\n",
    "\n",
    "ols_log_model = sm.OLS(y_log, X)\n",
    "ols_log_result = ols_log_model.fit(cov_type='cluster', cov_kwds={'groups': groups})\n",
    "\n",
    "print('Log-Transformed OLS Results')\n",
    "print('=' * 65)\n",
    "\n",
    "log_ols_table = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Coefficient': ols_log_result.params,\n",
    "    'Std. Error': ols_log_result.bse,\n",
    "    't-stat': ols_log_result.tvalues,\n",
    "    'p-value': ols_log_result.pvalues,\n",
    "}).set_index('Variable')\n",
    "\n",
    "display(log_ols_table.round(4))\n",
    "\n",
    "print(f'\\nR-squared:    {ols_log_result.rsquared:.4f}')\n",
    "print(f'Observations: {int(ols_log_result.nobs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Compare predictions: Log OLS vs Tobit vs OLS (levels)\n",
    "# ============================================================\n",
    "\n",
    "# To compare predictions on the same scale, we need to transform\n",
    "# the log-OLS predictions back to levels: exp(y_hat_log) - 1\n",
    "# Note: this is E[log(1+y)|X], not E[y|X]. The retransformation\n",
    "# introduces bias (Jensen's inequality), but we use it here for\n",
    "# a rough comparison.\n",
    "\n",
    "y_pred_log_ols = np.expm1(ols_log_result.fittedvalues)  # exp(fitted) - 1\n",
    "y_pred_ols = ols_result.fittedvalues\n",
    "y_pred_tobit = tobit_pooled.predict(pred_type='censored')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Panel A: OLS (levels) predicted vs actual\n",
    "axes[0].scatter(y, y_pred_ols, alpha=0.3, s=8, color='steelblue')\n",
    "max_val = max(y.max(), y_pred_ols.max())\n",
    "axes[0].plot([0, max_val], [0, max_val], 'r--', linewidth=1.5, label='45-degree line')\n",
    "axes[0].set_xlabel('Actual Expenditure')\n",
    "axes[0].set_ylabel('Predicted Expenditure')\n",
    "axes[0].set_title('A. OLS (Levels)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Panel B: Log OLS predicted vs actual\n",
    "axes[1].scatter(y, y_pred_log_ols, alpha=0.3, s=8, color='darkorange')\n",
    "max_val_log = max(y.max(), y_pred_log_ols.max())\n",
    "axes[1].plot([0, max_val_log], [0, max_val_log], 'r--', linewidth=1.5, label='45-degree line')\n",
    "axes[1].set_xlabel('Actual Expenditure')\n",
    "axes[1].set_ylabel('Predicted Expenditure')\n",
    "axes[1].set_title('B. Log OLS (retransformed)')\n",
    "axes[1].legend()\n",
    "\n",
    "# Panel C: Tobit predicted vs actual\n",
    "axes[2].scatter(y, y_pred_tobit, alpha=0.3, s=8, color='seagreen')\n",
    "max_val_tobit = max(y.max(), y_pred_tobit.max())\n",
    "axes[2].plot([0, max_val_tobit], [0, max_val_tobit], 'r--', linewidth=1.5, label='45-degree line')\n",
    "axes[2].set_xlabel('Actual Expenditure')\n",
    "axes[2].set_ylabel('Predicted Expenditure')\n",
    "axes[2].set_title('C. Pooled Tobit (censored predictions)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle('Exercise 1a: Predicted vs Actual -- OLS, Log OLS, Tobit', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex1a_prediction_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# RMSE comparison (all on original scale)\n",
    "rmse_ols = np.sqrt(np.mean((y - y_pred_ols)**2))\n",
    "rmse_log_ols = np.sqrt(np.mean((y - y_pred_log_ols)**2))\n",
    "rmse_tobit = np.sqrt(np.mean((y - y_pred_tobit)**2))\n",
    "\n",
    "print(f'RMSE (original scale):')\n",
    "print(f'  OLS (levels):        {rmse_ols:.4f}')\n",
    "print(f'  Log OLS (retransf.): {rmse_log_ols:.4f}')\n",
    "print(f'  Pooled Tobit:        {rmse_tobit:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion (1a):**\n",
    "\n",
    "The log-transformed OLS is a commonly used ad-hoc approach to handle skewed and\n",
    "censored data. However, it has several drawbacks:\n",
    "\n",
    "1. The coefficients are in the log scale, making direct comparison with Tobit\n",
    "   coefficients difficult.\n",
    "2. Retransformation to levels introduces bias due to Jensen's inequality:\n",
    "   $E[\\exp(\\hat{y})] \\neq \\exp(E[\\hat{y}])$. A smearing estimator (Duan, 1983)\n",
    "   would partially correct this.\n",
    "3. Unlike Tobit, log-OLS does not model the censoring mechanism explicitly and\n",
    "   cannot decompose effects into the intensive and extensive margins.\n",
    "\n",
    "The Tobit model is preferred because it directly models the data generating\n",
    "process (left-censoring at zero) and yields interpretable marginal effects\n",
    "through the McDonald-Moffitt decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1b: AIC and BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1b: Compute AIC and BIC for Pooled and RE Tobit\n",
    "# ============================================================\n",
    "\n",
    "# Pooled Tobit parameters: K betas + 1 sigma = K + 1\n",
    "k_pooled = len(var_names) + 1  # betas + sigma\n",
    "n = tobit_pooled.n_obs\n",
    "\n",
    "aic_pooled = -2 * tobit_pooled.llf + 2 * k_pooled\n",
    "bic_pooled = -2 * tobit_pooled.llf + k_pooled * np.log(n)\n",
    "\n",
    "# RE Tobit parameters: K betas + sigma_eps + sigma_alpha = K + 2\n",
    "k_re = len(var_names) + 2  # betas + sigma_eps + sigma_alpha\n",
    "n_re = tobit_re.n_obs\n",
    "\n",
    "aic_re = -2 * tobit_re.llf + 2 * k_re\n",
    "bic_re = -2 * tobit_re.llf + k_re * np.log(n_re)\n",
    "\n",
    "# Display results\n",
    "ic_table = pd.DataFrame({\n",
    "    'Model': ['Pooled Tobit', 'RE Tobit'],\n",
    "    'k (params)': [k_pooled, k_re],\n",
    "    'Log-Lik': [tobit_pooled.llf, tobit_re.llf],\n",
    "    'AIC': [aic_pooled, aic_re],\n",
    "    'BIC': [bic_pooled, bic_re],\n",
    "}).set_index('Model')\n",
    "\n",
    "print('Information Criteria Comparison')\n",
    "print('=' * 65)\n",
    "display(ic_table.round(2))\n",
    "\n",
    "print(f'\\nAIC difference (Pooled - RE): {aic_pooled - aic_re:.2f}')\n",
    "print(f'BIC difference (Pooled - RE): {bic_pooled - bic_re:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1c: Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1c: Which model does each criterion prefer?\n",
    "# ============================================================\n",
    "\n",
    "# Lower AIC/BIC is preferred\n",
    "aic_preferred = 'RE Tobit' if aic_re < aic_pooled else 'Pooled Tobit'\n",
    "bic_preferred = 'RE Tobit' if bic_re < bic_pooled else 'Pooled Tobit'\n",
    "\n",
    "print('Model Selection Summary')\n",
    "print('=' * 65)\n",
    "print(f'  AIC prefers: {aic_preferred}')\n",
    "print(f'  BIC prefers: {bic_preferred}')\n",
    "print()\n",
    "\n",
    "# Additional context: likelihood ratio test\n",
    "# The Pooled Tobit is nested within the RE Tobit (sigma_alpha = 0)\n",
    "# LR = 2 * (ll_RE - ll_Pooled)\n",
    "# Under H0, LR ~ mixture of chi2(0) and chi2(1) (boundary problem)\n",
    "lr_stat = 2 * (tobit_re.llf - tobit_pooled.llf)\n",
    "# Conservative: use chi2(1) critical value\n",
    "lr_pvalue = 0.5 * stats.chi2.sf(lr_stat, df=1)  # one-sided boundary test\n",
    "\n",
    "print(f'Likelihood Ratio Test (H0: sigma_alpha = 0):')\n",
    "print(f'  LR statistic:  {lr_stat:.4f}')\n",
    "print(f'  p-value:       {lr_pvalue:.6f} (mixture chi2, conservative)')\n",
    "if lr_pvalue < 0.05:\n",
    "    print(f'  => Reject H0 at 5%. Individual heterogeneity is significant.')\n",
    "    print(f'     The RE Tobit is preferred over the Pooled Tobit.')\n",
    "else:\n",
    "    print(f'  => Cannot reject H0. Pooled Tobit may be adequate.')\n",
    "\n",
    "print()\n",
    "print('Discussion:')\n",
    "print('-' * 65)\n",
    "print('AIC penalizes model complexity less heavily than BIC (2k vs k*ln(n)).')\n",
    "print('For large n, BIC penalizes the extra parameter in the RE model more.')\n",
    "print('Both criteria select the model with better fit-complexity trade-off.')\n",
    "print('The LR test directly tests whether the random effect variance is zero.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Marginal Effects at Representative Values (15 min)\n",
    "\n",
    "**Task:** Compute the marginal effect of `insurance` for two profiles:\n",
    "\n",
    "- **Profile A**: age=35, income=30, chronic=0, female=1, bmi=25\n",
    "  (young healthy woman)\n",
    "- **Profile B**: age=65, income=50, chronic=3, female=0, bmi=30\n",
    "  (older man with chronic conditions)\n",
    "\n",
    "Compute both the **unconditional** and **probability** marginal effects at\n",
    "each profile using the Pooled Tobit coefficients.\n",
    "\n",
    "**Formulas (left-censored at $c = 0$):**\n",
    "\n",
    "- Unconditional ME: $\\frac{\\partial E[y|X]}{\\partial x_k} = \\beta_k \\cdot \\Phi\\left(\\frac{X'\\beta}{\\sigma}\\right)$\n",
    "\n",
    "- Probability ME: $\\frac{\\partial P(y > 0 | X)}{\\partial x_k} = \\frac{\\beta_k}{\\sigma} \\cdot \\phi\\left(\\frac{X'\\beta}{\\sigma}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 2: Define the two profiles\n",
    "# ============================================================\n",
    "\n",
    "# Variable order in X: [const, income, age, chronic, insurance, female, bmi]\n",
    "# We set insurance=0 initially; the marginal effect is the discrete change\n",
    "# for a binary variable, or for a continuous approximation, we evaluate at\n",
    "# a given insurance level. Here we use the calculus-based marginal effect\n",
    "# evaluated at the profile covariates.\n",
    "\n",
    "# Profile A: young healthy woman\n",
    "x_a = np.array([1.0, 30.0, 35.0, 0.0, 0.0, 1.0, 25.0])\n",
    "\n",
    "# Profile B: older man with chronic conditions\n",
    "x_b = np.array([1.0, 50.0, 65.0, 3.0, 0.0, 0.0, 30.0])\n",
    "\n",
    "beta = tobit_pooled.beta\n",
    "sigma = tobit_pooled.sigma\n",
    "\n",
    "print('Pooled Tobit coefficients:')\n",
    "for i, name in enumerate(var_names):\n",
    "    print(f'  {name:12s}: {beta[i]:.4f}')\n",
    "print(f'  {\"sigma\":12s}: {sigma:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 2: Compute marginal effects at each profile\n",
    "# ============================================================\n",
    "\n",
    "# Index of 'insurance' in the variable list\n",
    "ins_idx = var_names.index('insurance')\n",
    "beta_ins = beta[ins_idx]\n",
    "\n",
    "def compute_me_at_profile(x_profile, beta, sigma, var_idx, var_names):\n",
    "    \"\"\"\n",
    "    Compute unconditional and probability marginal effects\n",
    "    at a given covariate profile.\n",
    "\n",
    "    For left-censoring at c = 0:\n",
    "      z = X'beta / sigma\n",
    "      ME_unconditional = beta_k * Phi(z)\n",
    "      ME_probability   = (beta_k / sigma) * phi(z)\n",
    "    \"\"\"\n",
    "    # Linear prediction at the profile\n",
    "    xb = x_profile @ beta\n",
    "\n",
    "    # z = (X'beta - c) / sigma, with c = 0\n",
    "    z = xb / sigma\n",
    "\n",
    "    # CDF and PDF at z\n",
    "    Phi_z = stats.norm.cdf(z)\n",
    "    phi_z = stats.norm.pdf(z)\n",
    "\n",
    "    beta_k = beta[var_idx]\n",
    "\n",
    "    me_unconditional = beta_k * Phi_z\n",
    "    me_probability = (beta_k / sigma) * phi_z\n",
    "\n",
    "    return {\n",
    "        'xb': xb,\n",
    "        'z': z,\n",
    "        'Phi_z': Phi_z,\n",
    "        'phi_z': phi_z,\n",
    "        'me_unconditional': me_unconditional,\n",
    "        'me_probability': me_probability,\n",
    "    }\n",
    "\n",
    "\n",
    "# Profile A\n",
    "me_a = compute_me_at_profile(x_a, beta, sigma, ins_idx, var_names)\n",
    "\n",
    "# Profile B\n",
    "me_b = compute_me_at_profile(x_b, beta, sigma, ins_idx, var_names)\n",
    "\n",
    "# Display results\n",
    "print('Marginal Effect of Insurance at Representative Profiles')\n",
    "print('=' * 70)\n",
    "print(f'{\"\":30s} {\"Profile A\":>15s} {\"Profile B\":>15s}')\n",
    "print(f'{\"\":30s} {\"(Young woman)\":>15s} {\"(Older man)\":>15s}')\n",
    "print('-' * 70)\n",
    "print(f'{\"X\\'beta (linear prediction)\":30s} {me_a[\"xb\"]:>15.4f} {me_b[\"xb\"]:>15.4f}')\n",
    "print(f'{\"z = X\\'beta / sigma\":30s} {me_a[\"z\"]:>15.4f} {me_b[\"z\"]:>15.4f}')\n",
    "print(f'{\"Phi(z) [P(y > 0 | X)]\":30s} {me_a[\"Phi_z\"]:>15.4f} {me_b[\"Phi_z\"]:>15.4f}')\n",
    "print(f'{\"phi(z)\":30s} {me_a[\"phi_z\"]:>15.4f} {me_b[\"phi_z\"]:>15.4f}')\n",
    "print('-' * 70)\n",
    "print(f'{\"ME unconditional\":30s} {me_a[\"me_unconditional\"]:>15.4f} {me_b[\"me_unconditional\"]:>15.4f}')\n",
    "print(f'{\"ME probability\":30s} {me_a[\"me_probability\"]:>15.4f} {me_b[\"me_probability\"]:>15.4f}')\n",
    "print('-' * 70)\n",
    "print(f'{\"beta (insurance, latent)\":30s} {beta_ins:>15.4f} {beta_ins:>15.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize the differences between profiles\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of marginal effects\n",
    "labels = ['Unconditional ME', 'Probability ME']\n",
    "vals_a = [me_a['me_unconditional'], me_a['me_probability']]\n",
    "vals_b = [me_b['me_unconditional'], me_b['me_probability']]\n",
    "\n",
    "idx = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(idx - width/2, vals_a, width, label='Profile A (Young woman)', color='steelblue', alpha=0.8)\n",
    "axes[0].bar(idx + width/2, vals_b, width, label='Profile B (Older man)', color='darkorange', alpha=0.8)\n",
    "axes[0].set_xticks(idx)\n",
    "axes[0].set_xticklabels(labels, fontsize=11)\n",
    "axes[0].set_ylabel('Marginal Effect of Insurance')\n",
    "axes[0].set_title('Insurance Marginal Effects by Profile')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.8)\n",
    "\n",
    "# Show the latent prediction and probability of non-censoring\n",
    "profile_labels = ['Profile A\\n(Young woman)', 'Profile B\\n(Older man)']\n",
    "phi_vals = [me_a['Phi_z'], me_b['Phi_z']]\n",
    "\n",
    "axes[1].bar(profile_labels, phi_vals, color=['steelblue', 'darkorange'], alpha=0.8, edgecolor='black')\n",
    "axes[1].set_ylabel('P(expenditure > 0 | X)')\n",
    "axes[1].set_title('Probability of Positive Expenditure')\n",
    "axes[1].set_ylim(0, 1)\n",
    "for i, v in enumerate(phi_vals):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex2_me_profiles.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion (Exercise 2):**\n",
    "\n",
    "The key insight is that marginal effects in the Tobit model **vary across the\n",
    "covariate space**. Two individuals with different characteristics experience\n",
    "different marginal effects of insurance, even though the underlying latent\n",
    "coefficient $\\beta_{\\text{insurance}}$ is constant.\n",
    "\n",
    "- **Profile A** (young healthy woman): Lower baseline probability of positive\n",
    "  expenditure ($\\Phi(z)$ is smaller because fewer chronic conditions and younger\n",
    "  age yield a lower latent prediction). The unconditional ME is therefore scaled\n",
    "  down more, but the probability ME may be relatively larger because $\\phi(z)$\n",
    "  is large near the censoring threshold.\n",
    "\n",
    "- **Profile B** (older man, 3 chronic conditions): Higher baseline probability\n",
    "  of positive expenditure ($\\Phi(z)$ is closer to 1). The unconditional ME is\n",
    "  closer to the raw $\\beta$, but the probability ME is smaller because this\n",
    "  person is already very likely to have positive expenditure.\n",
    "\n",
    "This illustrates the nonlinearity of the Tobit model: insurance has a larger\n",
    "effect on the **probability** of spending for people near the censoring\n",
    "threshold, and a larger effect on the **level** for people who are likely\n",
    "to spend already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Heckman MLE Estimation (20 min)\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "a) Re-estimate the Heckman model on the Mroz data using **MLE** instead of two-step.\n",
    "\n",
    "b) Compare the two-step and MLE estimates. How different are the outcome\n",
    "   coefficients? How different are $\\rho$ and $\\sigma$?\n",
    "\n",
    "c) Use the `compare_heckman_methods` utility to generate a formatted comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3a: Heckman MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 3a: Estimate Heckman with MLE\n",
    "# ============================================================\n",
    "\n",
    "heckman_mle_model = PanelHeckman(\n",
    "    endog=wage,\n",
    "    exog=X_outcome,\n",
    "    selection=selection,\n",
    "    exog_selection=Z_selection,\n",
    "    method='mle',\n",
    ")\n",
    "\n",
    "heckman_mle_result = heckman_mle_model.fit()\n",
    "\n",
    "print('Heckman MLE Results')\n",
    "print('=' * 60)\n",
    "print(heckman_mle_result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3b: Compare Two-Step vs MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 3b: Side-by-side comparison\n",
    "# ============================================================\n",
    "\n",
    "print('Outcome Equation Coefficients')\n",
    "print('=' * 65)\n",
    "print(f'{\"Variable\":20s} {\"Two-Step\":>12s} {\"MLE\":>12s} {\"Difference\":>12s} {\"% Diff\":>10s}')\n",
    "print('-' * 65)\n",
    "\n",
    "for i, name in enumerate(outcome_names):\n",
    "    ts_coef = heckman_result.outcome_params[i]\n",
    "    ml_coef = heckman_mle_result.outcome_params[i]\n",
    "    diff = ts_coef - ml_coef\n",
    "    pct = 100 * diff / (abs(ml_coef) + 1e-10)\n",
    "    print(f'{name:20s} {ts_coef:>12.4f} {ml_coef:>12.4f} {diff:>12.4f} {pct:>9.2f}%')\n",
    "\n",
    "print('-' * 65)\n",
    "\n",
    "# Selection parameters\n",
    "print(f'\\nSelection Parameters:')\n",
    "print(f'{\"rho\":20s} {heckman_result.rho:>12.4f} {heckman_mle_result.rho:>12.4f} '\n",
    "      f'{heckman_result.rho - heckman_mle_result.rho:>12.4f}')\n",
    "print(f'{\"sigma\":20s} {heckman_result.sigma:>12.4f} {heckman_mle_result.sigma:>12.4f} '\n",
    "      f'{heckman_result.sigma - heckman_mle_result.sigma:>12.4f}')\n",
    "\n",
    "lambda_ts = heckman_result.rho * heckman_result.sigma\n",
    "lambda_ml = heckman_mle_result.rho * heckman_mle_result.sigma\n",
    "print(f'{\"lambda (rho*sigma)\":20s} {lambda_ts:>12.4f} {lambda_ml:>12.4f} '\n",
    "      f'{lambda_ts - lambda_ml:>12.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3c: Formatted Comparison Using Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 3c: Use compare_heckman_methods utility\n",
    "# ============================================================\n",
    "\n",
    "comparison_table = compare_heckman_methods(\n",
    "    heckman_result,\n",
    "    heckman_mle_result,\n",
    "    variable_names=outcome_names,\n",
    ")\n",
    "\n",
    "print('Heckman Two-Step vs MLE Comparison (via compare_heckman_methods)')\n",
    "print('=' * 65)\n",
    "display(comparison_table.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization: coefficient comparison\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel A: Outcome coefficients\n",
    "idx = np.arange(len(outcome_names))\n",
    "width = 0.35\n",
    "\n",
    "ts_coefs = heckman_result.outcome_params\n",
    "ml_coefs = heckman_mle_result.outcome_params\n",
    "\n",
    "axes[0].barh(idx + width/2, ts_coefs, width, label='Two-Step', color='steelblue', alpha=0.8)\n",
    "axes[0].barh(idx - width/2, ml_coefs, width, label='MLE', color='darkorange', alpha=0.8)\n",
    "axes[0].set_yticks(idx)\n",
    "axes[0].set_yticklabels(outcome_names, fontsize=11)\n",
    "axes[0].axvline(x=0, color='black', linewidth=0.8)\n",
    "axes[0].set_xlabel('Coefficient')\n",
    "axes[0].set_title('A. Outcome Equation Coefficients')\n",
    "axes[0].legend(fontsize=10)\n",
    "\n",
    "# Panel B: Selection parameters (rho, sigma, lambda)\n",
    "param_names_sel = ['rho', 'sigma', 'lambda']\n",
    "ts_sel = [heckman_result.rho, heckman_result.sigma, lambda_ts]\n",
    "ml_sel = [heckman_mle_result.rho, heckman_mle_result.sigma, lambda_ml]\n",
    "\n",
    "idx2 = np.arange(len(param_names_sel))\n",
    "axes[1].bar(idx2 - width/2, ts_sel, width, label='Two-Step', color='steelblue', alpha=0.8)\n",
    "axes[1].bar(idx2 + width/2, ml_sel, width, label='MLE', color='darkorange', alpha=0.8)\n",
    "axes[1].set_xticks(idx2)\n",
    "axes[1].set_xticklabels(param_names_sel, fontsize=11)\n",
    "axes[1].set_ylabel('Parameter Value')\n",
    "axes[1].set_title('B. Selection Parameters')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.8)\n",
    "\n",
    "plt.suptitle('Exercise 3: Heckman Two-Step vs MLE Comparison', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex3_heckman_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion (Exercise 3):**\n",
    "\n",
    "Comparing two-step and MLE estimation for the Heckman model:\n",
    "\n",
    "1. **Efficiency**: MLE is asymptotically more efficient than two-step because it\n",
    "   uses all the information in the joint distribution of the selection and outcome\n",
    "   errors. However, two-step is more robust to misspecification of the joint\n",
    "   distribution.\n",
    "\n",
    "2. **Outcome coefficients**: The differences are typically small when the model\n",
    "   is well-specified. Large discrepancies would suggest misspecification or\n",
    "   identification problems.\n",
    "\n",
    "3. **Selection parameters ($\\rho$, $\\sigma$)**: MLE estimates both jointly, while\n",
    "   two-step derives them sequentially. The MLE estimate of $\\rho$ is generally\n",
    "   more reliable because it does not depend on the ad-hoc Hessian approximation\n",
    "   used in the two-step procedure.\n",
    "\n",
    "4. **Practical guidance**: Use two-step for initial exploration and robustness\n",
    "   checking. Use MLE for final estimates when the model is well-specified and\n",
    "   the normality assumption is reasonable. If the two methods give very different\n",
    "   results, investigate model specification carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Prediction and Model Validation (15 min)\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "a) Generate **in-sample predictions** from the Pooled Tobit (censored predictions)\n",
    "   and compare them to OLS fitted values. Plot predicted vs. actual for both.\n",
    "\n",
    "b) Compute the **RMSE** for both models (on the observed scale).\n",
    "\n",
    "c) Which model produces predictions more consistent with the observed distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4a: In-Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 4a: Generate predictions\n",
    "# ============================================================\n",
    "\n",
    "# Tobit censored predictions: E[y | X] accounting for censoring\n",
    "y_pred_tobit = tobit_pooled.predict(pred_type='censored')\n",
    "\n",
    "# OLS fitted values (can be negative, which is unrealistic)\n",
    "y_pred_ols = ols_result.fittedvalues\n",
    "\n",
    "print('Prediction Summary Statistics')\n",
    "print('=' * 60)\n",
    "print(f'{\"Statistic\":20s} {\"Actual\":>12s} {\"OLS\":>12s} {\"Tobit\":>12s}')\n",
    "print('-' * 60)\n",
    "print(f'{\"Mean\":20s} {y.mean():>12.4f} {y_pred_ols.mean():>12.4f} {y_pred_tobit.mean():>12.4f}')\n",
    "print(f'{\"Std Dev\":20s} {y.std():>12.4f} {y_pred_ols.std():>12.4f} {y_pred_tobit.std():>12.4f}')\n",
    "print(f'{\"Min\":20s} {y.min():>12.4f} {y_pred_ols.min():>12.4f} {y_pred_tobit.min():>12.4f}')\n",
    "print(f'{\"Max\":20s} {y.max():>12.4f} {y_pred_ols.max():>12.4f} {y_pred_tobit.max():>12.4f}')\n",
    "print(f'{\"% Negative\":20s} {(y < 0).mean()*100:>11.1f}% {(y_pred_ols < 0).mean()*100:>11.1f}% {(y_pred_tobit < 0).mean()*100:>11.1f}%')\n",
    "print(f'{\"% Zero\":20s} {(y == 0).mean()*100:>11.1f}% {(np.abs(y_pred_ols) < 1e-10).mean()*100:>11.1f}% {(np.abs(y_pred_tobit) < 1e-10).mean()*100:>11.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 4a: Plot predicted vs actual\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Panel A: OLS predicted vs actual\n",
    "axes[0].scatter(y, y_pred_ols, alpha=0.3, s=8, color='steelblue', label='Predictions')\n",
    "max_val = max(y.max(), y_pred_ols.max()) * 1.05\n",
    "min_val = min(y.min(), y_pred_ols.min()) * 1.05\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=1.5, label='45-degree line')\n",
    "axes[0].axhline(y=0, color='gray', linestyle=':', linewidth=0.8)\n",
    "axes[0].axvline(x=0, color='gray', linestyle=':', linewidth=0.8)\n",
    "axes[0].set_xlabel('Actual Expenditure', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Expenditure', fontsize=12)\n",
    "axes[0].set_title('A. OLS Predictions', fontsize=13)\n",
    "axes[0].legend(fontsize=10)\n",
    "\n",
    "# Highlight negative predictions\n",
    "neg_mask = y_pred_ols < 0\n",
    "if neg_mask.sum() > 0:\n",
    "    axes[0].scatter(y[neg_mask], y_pred_ols[neg_mask], alpha=0.6, s=15,\n",
    "                    color='red', label=f'Negative pred. (n={neg_mask.sum()})', zorder=5)\n",
    "    axes[0].legend(fontsize=10)\n",
    "\n",
    "# Panel B: Tobit predicted vs actual\n",
    "axes[1].scatter(y, y_pred_tobit, alpha=0.3, s=8, color='seagreen', label='Predictions')\n",
    "max_val_t = max(y.max(), y_pred_tobit.max()) * 1.05\n",
    "axes[1].plot([0, max_val_t], [0, max_val_t], 'r--', linewidth=1.5, label='45-degree line')\n",
    "axes[1].axhline(y=0, color='gray', linestyle=':', linewidth=0.8)\n",
    "axes[1].axvline(x=0, color='gray', linestyle=':', linewidth=0.8)\n",
    "axes[1].set_xlabel('Actual Expenditure', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Expenditure', fontsize=12)\n",
    "axes[1].set_title('B. Tobit Predictions (censored)', fontsize=13)\n",
    "axes[1].legend(fontsize=10)\n",
    "\n",
    "plt.suptitle('Exercise 4a: Predicted vs Actual Expenditure', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex4a_pred_vs_actual.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4b: RMSE Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 4b: Compute RMSE for both models\n",
    "# ============================================================\n",
    "\n",
    "rmse_ols = np.sqrt(np.mean((y - y_pred_ols)**2))\n",
    "rmse_tobit = np.sqrt(np.mean((y - y_pred_tobit)**2))\n",
    "\n",
    "# Also compute MAE for robustness\n",
    "mae_ols = np.mean(np.abs(y - y_pred_ols))\n",
    "mae_tobit = np.mean(np.abs(y - y_pred_tobit))\n",
    "\n",
    "# Correlation between predicted and actual\n",
    "corr_ols = np.corrcoef(y, y_pred_ols)[0, 1]\n",
    "corr_tobit = np.corrcoef(y, y_pred_tobit)[0, 1]\n",
    "\n",
    "# RMSE conditional on y > 0 (positive expenditure only)\n",
    "pos_mask = y > 0\n",
    "rmse_ols_pos = np.sqrt(np.mean((y[pos_mask] - y_pred_ols[pos_mask])**2))\n",
    "rmse_tobit_pos = np.sqrt(np.mean((y[pos_mask] - y_pred_tobit[pos_mask])**2))\n",
    "\n",
    "# RMSE for censored observations (y = 0)\n",
    "cens_mask = y == 0\n",
    "rmse_ols_cens = np.sqrt(np.mean((y[cens_mask] - y_pred_ols[cens_mask])**2))\n",
    "rmse_tobit_cens = np.sqrt(np.mean((y[cens_mask] - y_pred_tobit[cens_mask])**2))\n",
    "\n",
    "print('Model Fit Comparison')\n",
    "print('=' * 55)\n",
    "print(f'{\"Metric\":25s} {\"OLS\":>12s} {\"Tobit\":>12s}')\n",
    "print('-' * 55)\n",
    "print(f'{\"RMSE (all obs)\":25s} {rmse_ols:>12.4f} {rmse_tobit:>12.4f}')\n",
    "print(f'{\"MAE (all obs)\":25s} {mae_ols:>12.4f} {mae_tobit:>12.4f}')\n",
    "print(f'{\"Correlation (pred, y)\":25s} {corr_ols:>12.4f} {corr_tobit:>12.4f}')\n",
    "print(f'{\"RMSE (y > 0 only)\":25s} {rmse_ols_pos:>12.4f} {rmse_tobit_pos:>12.4f}')\n",
    "print(f'{\"RMSE (y = 0 only)\":25s} {rmse_ols_cens:>12.4f} {rmse_tobit_cens:>12.4f}')\n",
    "print('-' * 55)\n",
    "\n",
    "# Which is better?\n",
    "if rmse_tobit < rmse_ols:\n",
    "    pct_improvement = 100 * (rmse_ols - rmse_tobit) / rmse_ols\n",
    "    print(f'\\nTobit RMSE is {pct_improvement:.1f}% lower than OLS.')\n",
    "else:\n",
    "    pct_improvement = 100 * (rmse_tobit - rmse_ols) / rmse_tobit\n",
    "    print(f'\\nOLS RMSE is {pct_improvement:.1f}% lower than Tobit.')\n",
    "    print('Note: This can happen because Tobit optimizes the censored likelihood,')\n",
    "    print('not the mean squared error. The Tobit model is still preferred for')\n",
    "    print('consistent estimation of the structural parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4c: Distributional Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 4c: Compare predicted distributions\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Panel A: Distribution of actual values\n",
    "axes[0, 0].hist(y, bins=50, edgecolor='black', alpha=0.7, color='gray', density=True)\n",
    "axes[0, 0].axvline(x=0, color='red', linestyle='--', linewidth=1.5)\n",
    "axes[0, 0].set_xlabel('Expenditure')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title('A. Observed Distribution')\n",
    "\n",
    "# Panel B: Distribution of OLS predictions\n",
    "axes[0, 1].hist(y_pred_ols, bins=50, edgecolor='black', alpha=0.7, color='steelblue', density=True)\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--', linewidth=1.5)\n",
    "axes[0, 1].set_xlabel('Predicted Expenditure')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('B. OLS Predictions')\n",
    "\n",
    "# Panel C: Distribution of Tobit predictions\n",
    "axes[1, 0].hist(y_pred_tobit, bins=50, edgecolor='black', alpha=0.7, color='seagreen', density=True)\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=1.5)\n",
    "axes[1, 0].set_xlabel('Predicted Expenditure')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_title('C. Tobit Predictions (censored)')\n",
    "\n",
    "# Panel D: Residual distributions\n",
    "resid_ols = y - y_pred_ols\n",
    "resid_tobit = y - y_pred_tobit\n",
    "\n",
    "axes[1, 1].hist(resid_ols, bins=50, alpha=0.5, color='steelblue', density=True, label='OLS residuals')\n",
    "axes[1, 1].hist(resid_tobit, bins=50, alpha=0.5, color='seagreen', density=True, label='Tobit residuals')\n",
    "axes[1, 1].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[1, 1].set_xlabel('Residual (Actual - Predicted)')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].set_title('D. Residual Distributions')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "\n",
    "plt.suptitle('Exercise 4c: Distributional Comparison', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex4c_distributional_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Quantitative assessment of distributional match\n",
    "# ============================================================\n",
    "\n",
    "# Key distributional features to compare\n",
    "print('Distributional Consistency Assessment')\n",
    "print('=' * 65)\n",
    "print(f'{\"Feature\":30s} {\"Actual\":>10s} {\"OLS\":>10s} {\"Tobit\":>10s}')\n",
    "print('-' * 65)\n",
    "\n",
    "# Fraction of predictions at or below zero\n",
    "print(f'{\"% predictions <= 0\":30s} '\n",
    "      f'{(y <= 0).mean()*100:>9.1f}% '\n",
    "      f'{(y_pred_ols <= 0).mean()*100:>9.1f}% '\n",
    "      f'{(y_pred_tobit <= 0).mean()*100:>9.1f}%')\n",
    "\n",
    "# Quartiles\n",
    "for q_val in [25, 50, 75]:\n",
    "    q_actual = np.percentile(y, q_val)\n",
    "    q_ols = np.percentile(y_pred_ols, q_val)\n",
    "    q_tobit = np.percentile(y_pred_tobit, q_val)\n",
    "    print(f'{f\"Q{q_val}\":30s} {q_actual:>10.2f} {q_ols:>10.2f} {q_tobit:>10.2f}')\n",
    "\n",
    "# Skewness\n",
    "from scipy.stats import skew, kurtosis\n",
    "print(f'{\"Skewness\":30s} {skew(y):>10.3f} {skew(y_pred_ols):>10.3f} {skew(y_pred_tobit):>10.3f}')\n",
    "print(f'{\"Kurtosis\":30s} {kurtosis(y):>10.3f} {kurtosis(y_pred_ols):>10.3f} {kurtosis(y_pred_tobit):>10.3f}')\n",
    "\n",
    "print('-' * 65)\n",
    "\n",
    "print('\\nInterpretation:')\n",
    "print('-' * 65)\n",
    "print('The Tobit model produces predictions that are more consistent with')\n",
    "print('the observed distribution because:')\n",
    "print('  1. All Tobit predictions are non-negative (respects the censoring')\n",
    "print('     boundary), whereas OLS can produce negative predictions.')\n",
    "print('  2. The Tobit predicted distribution better approximates the mass')\n",
    "print('     at/near zero observed in the actual data.')\n",
    "print('  3. The Tobit model captures the right-skewed shape of the')\n",
    "print('     expenditure distribution more accurately.')\n",
    "print('\\nEven if OLS has a slightly lower RMSE in some cases, the Tobit model')\n",
    "print('provides structurally consistent predictions that respect the data')\n",
    "print('generating process. This is critical for policy simulations and')\n",
    "print('counterfactual analysis, where out-of-sample prediction quality matters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary of Exercise Solutions\n",
    "\n",
    "| Exercise | Key Finding |\n",
    "|----------|-------------|\n",
    "| 1a | Log-OLS is an ad-hoc approach that does not model censoring. Retransformation introduces Jensen's inequality bias. |\n",
    "| 1b | AIC and BIC can be computed from the log-likelihood and parameter counts. Both criteria compare fit-complexity trade-offs. |\n",
    "| 1c | The RE Tobit is generally preferred when the ICC is non-trivial, confirmed by the LR test for $\\sigma_\\alpha = 0$. |\n",
    "| 2 | Marginal effects vary across the covariate space. Insurance has a larger probability effect for individuals near the censoring threshold, and a larger unconditional effect for those with higher baseline expenditure. |\n",
    "| 3 | Heckman MLE and two-step generally agree when the model is well-specified. MLE is more efficient; two-step is more robust. Large discrepancies signal misspecification. |\n",
    "| 4 | Tobit predictions respect the censoring boundary (non-negative) and better match the observed distribution. OLS can produce negative predictions, which are economically meaningless. |\n",
    "\n",
    "---\n",
    "\n",
    "*This solution notebook is part of the PanelBox Censored Models Tutorial Series.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
