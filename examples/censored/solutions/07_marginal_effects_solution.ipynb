{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Tutorial 07 -- Marginal Effects in Censored Models: Solutions\n",
    "\n",
    "**Author**: PanelBox Development Team  \n",
    "**Date**: 2026-02-17  \n",
    "**Level**: Intermediate  \n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains complete solutions for all three exercises from Tutorial 07.\n",
    "\n",
    "**Exercises:**\n",
    "1. **Manual Computation** (Easy) -- Compute three types of MEM from given parameters\n",
    "2. **Sensitivity to Censoring Rate** (Moderate) -- Simulate how AME/beta varies with censoring\n",
    "3. **Full Analysis Pipeline** (Challenging) -- Complete Tobit analysis on `college_wage.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from panelbox.models.censored import PooledTobit\n",
    "from panelbox.marginal_effects.censored_me import compute_tobit_ame, compute_tobit_mem\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Manual Computation (Easy)\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Given the following Tobit estimates:\n",
    "- $\\beta_{\\text{education}} = 2.5$\n",
    "- $\\sigma = 8.0$\n",
    "- $z_{\\text{mean}} = (\\bar{X}'\\beta - c) / \\sigma = 1.2$\n",
    "\n",
    "**Tasks:**\n",
    "1. Compute the three types of MEM (unconditional, conditional, probability) for education\n",
    "2. Verify that Unconditional ME < Conditional ME < $\\beta_{\\text{education}}$\n",
    "3. Compute the scaling factor ME / $\\beta_k$ for each type\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "For left-censoring at $c$ with $z = (X'\\beta - c)/\\sigma$:\n",
    "\n",
    "| Type | Formula |\n",
    "|------|---------|\n",
    "| Unconditional | $\\beta_k \\cdot \\Phi(z)$ |\n",
    "| Conditional | $\\beta_k \\cdot [1 - \\lambda(z)(z + \\lambda(z))]$ |\n",
    "| Probability | $(\\beta_k / \\sigma) \\cdot \\phi(z)$ |\n",
    "\n",
    "where $\\lambda(z) = \\phi(z)/\\Phi(z)$ is the inverse Mills ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1-step1-header",
   "metadata": {},
   "source": [
    "### Step 1: Compute the Three Types of MEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given parameters\n",
    "beta_ed = 2.5\n",
    "sigma_ex = 8.0\n",
    "z_mean = 1.2\n",
    "\n",
    "# Intermediate quantities evaluated at z_mean\n",
    "Phi_z = norm.cdf(z_mean)      # Standard normal CDF\n",
    "phi_z = norm.pdf(z_mean)      # Standard normal PDF\n",
    "lambda_z = phi_z / Phi_z      # Inverse Mills ratio\n",
    "\n",
    "print('Intermediate Quantities at z = 1.2')\n",
    "print('=' * 45)\n",
    "print(f'Phi(z) = Phi(1.2) = {Phi_z:.6f}')\n",
    "print(f'phi(z) = phi(1.2) = {phi_z:.6f}')\n",
    "print(f'lambda(z) = phi(z)/Phi(z) = {lambda_z:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-compute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a. Unconditional MEM: beta_k * Phi(z)\n",
    "mem_uncond_ex1 = beta_ed * Phi_z\n",
    "\n",
    "# 1b. Conditional MEM: beta_k * [1 - lambda(z) * (z + lambda(z))]\n",
    "correction_factor = 1 - lambda_z * (z_mean + lambda_z)\n",
    "mem_cond_ex1 = beta_ed * correction_factor\n",
    "\n",
    "# 1c. Probability MEM: (beta_k / sigma) * phi(z)\n",
    "mem_prob_ex1 = (beta_ed / sigma_ex) * phi_z\n",
    "\n",
    "print('Three Types of MEM for Education')\n",
    "print('=' * 55)\n",
    "print(f'Unconditional MEM:  beta * Phi(z)                = {mem_uncond_ex1:.6f}')\n",
    "print(f'  = {beta_ed} * {Phi_z:.6f} = {mem_uncond_ex1:.6f}')\n",
    "print()\n",
    "print(f'Conditional MEM:    beta * [1 - lambda*(z+lambda)] = {mem_cond_ex1:.6f}')\n",
    "print(f'  Correction factor: 1 - {lambda_z:.6f} * ({z_mean} + {lambda_z:.6f})')\n",
    "print(f'                   = 1 - {lambda_z:.6f} * {z_mean + lambda_z:.6f}')\n",
    "print(f'                   = 1 - {lambda_z * (z_mean + lambda_z):.6f}')\n",
    "print(f'                   = {correction_factor:.6f}')\n",
    "print(f'  MEM = {beta_ed} * {correction_factor:.6f} = {mem_cond_ex1:.6f}')\n",
    "print()\n",
    "print(f'Probability MEM:    (beta/sigma) * phi(z)        = {mem_prob_ex1:.6f}')\n",
    "print(f'  = ({beta_ed}/{sigma_ex}) * {phi_z:.6f} = {mem_prob_ex1:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1-step2-header",
   "metadata": {},
   "source": [
    "### Step 2: Verify the Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Verify ordering: Prob ME < Uncond ME < Cond ME < beta\n",
    "print('Verification of Ordering')\n",
    "print('=' * 55)\n",
    "print(f'Probability ME:    {mem_prob_ex1:.6f}')\n",
    "print(f'Unconditional ME:  {mem_uncond_ex1:.6f}')\n",
    "print(f'Conditional ME:    {mem_cond_ex1:.6f}')\n",
    "print(f'Tobit coefficient: {beta_ed:.6f}')\n",
    "print()\n",
    "\n",
    "# Check each inequality\n",
    "check1 = mem_prob_ex1 < mem_uncond_ex1\n",
    "check2 = mem_uncond_ex1 < mem_cond_ex1\n",
    "check3 = mem_cond_ex1 < beta_ed\n",
    "\n",
    "print(f'Prob ME < Uncond ME?     {mem_prob_ex1:.4f} < {mem_uncond_ex1:.4f}  -->  {check1}')\n",
    "print(f'Uncond ME < Cond ME?     {mem_uncond_ex1:.4f} < {mem_cond_ex1:.4f}  -->  {check2}')\n",
    "print(f'Cond ME < beta?          {mem_cond_ex1:.4f} < {beta_ed:.4f}  -->  {check3}')\n",
    "print()\n",
    "\n",
    "if check1 and check2 and check3:\n",
    "    print('All inequalities hold: Prob ME < Uncond ME < Cond ME < beta_k')\n",
    "else:\n",
    "    print('WARNING: Some inequality does not hold. Check computations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1-step3-header",
   "metadata": {},
   "source": [
    "### Step 3: Scaling Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-scaling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute scaling factors: ME / beta_k\n",
    "scale_uncond = mem_uncond_ex1 / beta_ed\n",
    "scale_cond = mem_cond_ex1 / beta_ed\n",
    "scale_prob = mem_prob_ex1 / beta_ed\n",
    "\n",
    "print('Scaling Factors (ME / beta_k)')\n",
    "print('=' * 55)\n",
    "print(f'Unconditional / beta:  {scale_uncond:.6f}  (= Phi(z) = {Phi_z:.6f})')\n",
    "print(f'Conditional / beta:    {scale_cond:.6f}  (= 1 - lambda*(z+lambda))')\n",
    "print(f'Probability / beta:    {scale_prob:.6f}  (= phi(z)/sigma = {phi_z/sigma_ex:.6f})')\n",
    "print()\n",
    "print('Interpretation:')\n",
    "print(f'  - The unconditional ME is {scale_uncond*100:.1f}% of the Tobit coefficient.')\n",
    "print(f'  - The conditional ME is {scale_cond*100:.1f}% of the Tobit coefficient.')\n",
    "print(f'  - The probability ME is {scale_prob*100:.1f}% of the Tobit coefficient.')\n",
    "print()\n",
    "print(f'  Reporting beta_ed = {beta_ed} as the marginal effect would overstate')\n",
    "print(f'  the true unconditional effect by a factor of {1/scale_uncond:.2f}x.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-summary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table for Exercise 1\n",
    "ex1_table = pd.DataFrame({\n",
    "    'ME Type': ['Unconditional', 'Conditional', 'Probability', 'Tobit Coefficient'],\n",
    "    'Formula': [\n",
    "        'beta_k * Phi(z)',\n",
    "        'beta_k * [1 - lambda(z+lambda)]',\n",
    "        '(beta_k/sigma) * phi(z)',\n",
    "        'beta_k (no correction)'\n",
    "    ],\n",
    "    'Value': [mem_uncond_ex1, mem_cond_ex1, mem_prob_ex1, beta_ed],\n",
    "    'Scaling Factor (ME/beta)': [scale_uncond, scale_cond, scale_prob, 1.0],\n",
    "})\n",
    "ex1_table = ex1_table.set_index('ME Type')\n",
    "\n",
    "print('Exercise 1: Summary Table')\n",
    "print('=' * 70)\n",
    "display(ex1_table.round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Sensitivity to Censoring Rate (Moderate)\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Investigate how the ratio AME/$\\beta_k$ changes with the censoring rate.\n",
    "\n",
    "**Steps:**\n",
    "1. Simulate data from a Tobit DGP with $\\beta = 3.0$, $\\sigma = 5.0$\n",
    "2. Vary the intercept to change the censoring rate from ~10% to ~80%\n",
    "3. For each censoring rate, compute the unconditional AME\n",
    "4. Plot the ratio AME/$\\beta$ against the censoring rate\n",
    "\n",
    "**Expected finding:** As censoring increases, the AME becomes a smaller fraction of $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2-step1-header",
   "metadata": {},
   "source": [
    "### Step 1: Simulate Data with Varying Censoring Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-simulate",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_ex = 2000\n",
    "beta_ex = 3.0\n",
    "sigma_ex2 = 5.0\n",
    "\n",
    "# Generate covariate and errors once (fixed across experiments)\n",
    "x_ex = np.random.normal(2, 1, n_ex)\n",
    "eps_ex = np.random.normal(0, sigma_ex2, n_ex)\n",
    "\n",
    "# Vary intercept to change censoring rate\n",
    "# More negative intercept -> more censoring\n",
    "intercepts = np.linspace(-10, 8, 30)\n",
    "\n",
    "censoring_rates = []\n",
    "ame_ratios = []\n",
    "ame_values = []\n",
    "\n",
    "for alpha_0 in intercepts:\n",
    "    # Latent variable: y* = alpha_0 + beta * x + eps\n",
    "    y_star_ex = alpha_0 + beta_ex * x_ex + eps_ex\n",
    "    y_ex = np.maximum(0, y_star_ex)  # Left-censored at 0\n",
    "    \n",
    "    # Empirical censoring rate\n",
    "    cens_rate = np.mean(y_ex == 0)\n",
    "    censoring_rates.append(cens_rate)\n",
    "    \n",
    "    # Analytical unconditional AME:\n",
    "    # ME_i = beta * Phi(z_i) where z_i = (alpha_0 + beta*x_i) / sigma\n",
    "    # AME = mean(ME_i)\n",
    "    z_i = (alpha_0 + beta_ex * x_ex) / sigma_ex2\n",
    "    me_i = beta_ex * norm.cdf(z_i)\n",
    "    ame = np.mean(me_i)\n",
    "    ratio = ame / beta_ex\n",
    "    \n",
    "    ame_values.append(ame)\n",
    "    ame_ratios.append(ratio)\n",
    "\n",
    "# Convert to arrays\n",
    "censoring_rates = np.array(censoring_rates)\n",
    "ame_ratios = np.array(ame_ratios)\n",
    "ame_values = np.array(ame_values)\n",
    "\n",
    "# Print a summary table for selected points\n",
    "print('Censoring Rate vs. AME/beta Ratio')\n",
    "print('=' * 55)\n",
    "print(f'{\"Intercept\":>10s} {\"Cens. Rate\":>12s} {\"AME\":>10s} {\"AME/beta\":>10s}')\n",
    "print('-' * 55)\n",
    "for i in range(0, len(intercepts), 5):\n",
    "    print(f'{intercepts[i]:>10.2f} {censoring_rates[i]:>11.1%} {ame_values[i]:>10.4f} {ame_ratios[i]:>10.4f}')\n",
    "print('-' * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2-step2-header",
   "metadata": {},
   "source": [
    "### Step 2: Plot AME/beta vs. Censoring Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5.5))\n",
    "\n",
    "# Left panel: AME/beta ratio vs censoring rate\n",
    "ax = axes[0]\n",
    "ax.plot(censoring_rates * 100, ame_ratios, 'o-', color='#1565C0',\n",
    "        markersize=5, linewidth=2, label='AME / $\\\\beta$')\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=1.5, alpha=0.7,\n",
    "           label='Ratio = 1 (no correction needed)')\n",
    "ax.set_xlabel('Censoring Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('AME / $\\\\beta$', fontsize=12, fontweight='bold')\n",
    "ax.set_title('How Censoring Attenuates the Marginal Effect',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.set_xlim(-2, 85)\n",
    "ax.set_ylim(-0.05, 1.1)\n",
    "ax.legend(fontsize=10, loc='upper right')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate('As censoring increases,\\nAME becomes a smaller\\nfraction of $\\\\beta$',\n",
    "            xy=(60, 0.35), fontsize=10, fontstyle='italic',\n",
    "            bbox=dict(boxstyle='round,pad=0.4', facecolor='lightyellow', alpha=0.9))\n",
    "\n",
    "# Right panel: AME value vs censoring rate\n",
    "ax = axes[1]\n",
    "ax.plot(censoring_rates * 100, ame_values, 's-', color='#2E7D32',\n",
    "        markersize=5, linewidth=2, label='Unconditional AME')\n",
    "ax.axhline(y=beta_ex, color='red', linestyle='--', linewidth=1.5, alpha=0.7,\n",
    "           label=f'$\\\\beta$ = {beta_ex}')\n",
    "ax.set_xlabel('Censoring Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('AME (Unconditional)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Unconditional AME vs. Censoring Rate',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.set_xlim(-2, 85)\n",
    "ax.legend(fontsize=10, loc='upper right')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex2_censoring_sensitivity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nKey findings:')\n",
    "print('  - When censoring is low (~10%), AME is close to beta (ratio ~ 0.9).')\n",
    "print('  - When censoring is high (~80%), AME is much smaller than beta (ratio ~ 0.2).')\n",
    "print('  - The relationship is monotonically decreasing: more censoring = larger gap.')\n",
    "print('  - This confirms that reporting raw coefficients is increasingly misleading')\n",
    "print('    as the censoring rate increases.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-theoretical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical check: the unconditional AME/beta ratio\n",
    "# should equal the average Phi(z) = average P(y > 0 | X).\n",
    "# This is approximately (1 - censoring_rate) when the model is correct.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax.scatter(1 - censoring_rates, ame_ratios, s=60, color='#1565C0',\n",
    "           edgecolors='black', linewidths=0.8, zorder=5,\n",
    "           label='Empirical points')\n",
    "\n",
    "# 45-degree line\n",
    "ax.plot([0, 1], [0, 1], 'r--', linewidth=2, alpha=0.7,\n",
    "        label='45-degree line (perfect match)')\n",
    "\n",
    "ax.set_xlabel('1 - Censoring Rate = Average P(y > 0 | X)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('AME / $\\\\beta$ = Average $\\\\Phi(z)$', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Unconditional Scaling Factor vs. Non-Censoring Rate',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlim(-0.05, 1.05)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex2_scaling_vs_noncensoring.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('The points fall close to the 45-degree line, confirming that')\n",
    "print('the unconditional AME scaling factor equals the average probability')\n",
    "print('of non-censoring: AME/beta = E[Phi(z)] ~ 1 - censoring_rate.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Full Analysis Pipeline (Challenging)\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Conduct a complete marginal effects analysis on the `college_wage.csv` dataset.\n",
    "\n",
    "**Steps:**\n",
    "1. Load `college_wage.csv` and explore the data\n",
    "2. Identify the censored variable\n",
    "3. Estimate a Tobit model\n",
    "4. Compute AME (all three types) and MEM (unconditional)\n",
    "5. Perform the McDonald-Moffitt decomposition\n",
    "6. Create a publication-quality figure with at least two panels\n",
    "7. Write a brief interpretation of the key findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-step1-header",
   "metadata": {},
   "source": [
    "### Step 1: Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the college_wage dataset\n",
    "college_df = pd.read_csv(DATA_DIR / 'college_wage.csv')\n",
    "\n",
    "print(f'Dataset shape: {college_df.shape}')\n",
    "print(f'\\nVariables: {list(college_df.columns)}')\n",
    "print(f'\\nFirst 10 rows:')\n",
    "display(college_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-explore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print('Descriptive Statistics')\n",
    "print('=' * 70)\n",
    "display(college_df.describe().round(3))\n",
    "\n",
    "print(f'\\nMissing values:\\n{college_df.isnull().sum()}')\n",
    "print(f'\\nCollege attendance rate: {college_df[\"college\"].mean():.1%}')\n",
    "print(f'Observations with wage observed: {college_df[\"wage\"].notna().sum()}')\n",
    "print(f'Observations with wage missing:  {college_df[\"wage\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-identify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the censored variable\n",
    "# Wages are observed only for college attendees (college=1).\n",
    "# For non-attendees (college=0), wage is missing (NaN).\n",
    "#\n",
    "# For a Tobit model, we treat wage as left-censored at 0:\n",
    "# - Non-attendees have their latent wage censored to 0\n",
    "# - Attendees have observed positive wages\n",
    "#\n",
    "# This is a simplification: in reality, the college_wage dataset\n",
    "# is better suited for a Heckman selection model. However, for the\n",
    "# purpose of this exercise, we use the Tobit framework.\n",
    "\n",
    "# Create the dependent variable: wage = 0 for non-attendees, observed wage for attendees\n",
    "college_df['wage_censored'] = college_df['wage'].fillna(0.0)\n",
    "\n",
    "n_censored = (college_df['wage_censored'] == 0).sum()\n",
    "n_total = len(college_df)\n",
    "pct_censored = 100 * n_censored / n_total\n",
    "\n",
    "print('Censored Variable: wage_censored')\n",
    "print('=' * 50)\n",
    "print(f'Total observations:    {n_total}')\n",
    "print(f'Censored (wage = 0):   {n_censored} ({pct_censored:.1f}%)')\n",
    "print(f'Uncensored (wage > 0): {n_total - n_censored} ({100 - pct_censored:.1f}%)')\n",
    "print(f'\\nMean wage (all):       {college_df[\"wage_censored\"].mean():.2f}')\n",
    "print(f'Mean wage (uncensored): {college_df.loc[college_df[\"wage_censored\"] > 0, \"wage_censored\"].mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-visualize-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the censored distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Distribution of censored wages\n",
    "ax = axes[0]\n",
    "ax.hist(college_df['wage_censored'], bins=35, edgecolor='black',\n",
    "        alpha=0.7, color='steelblue')\n",
    "ax.set_xlabel('Wage (Censored at 0)', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Distribution of Censored Wages', fontsize=13, fontweight='bold')\n",
    "ax.axvline(college_df['wage_censored'].mean(), color='red', linestyle='--',\n",
    "           linewidth=2, label=f'Mean: {college_df[\"wage_censored\"].mean():.1f}')\n",
    "ax.annotate(f'Censored at 0\\n(n={n_censored})',\n",
    "            xy=(0, n_censored * 0.7), fontsize=11, fontweight='bold',\n",
    "            color='darkred',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', alpha=0.8))\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Right: Observed wages only (non-censored)\n",
    "ax = axes[1]\n",
    "wages_obs = college_df.loc[college_df['wage_censored'] > 0, 'wage_censored']\n",
    "ax.hist(wages_obs, bins=30, edgecolor='black', alpha=0.7, color='darkorange')\n",
    "ax.set_xlabel('Wage (Uncensored Only)', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Distribution of Observed Wages (College Attendees)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.axvline(wages_obs.mean(), color='red', linestyle='--', linewidth=2,\n",
    "           label=f'Mean: {wages_obs.mean():.1f}')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex3_wage_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-step3-header",
   "metadata": {},
   "source": [
    "### Step 3: Estimate the Tobit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-tobit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the design matrix\n",
    "# We use: ability, parent_education, family_income, urban, female as regressors\n",
    "# We exclude distance_college and tuition (they are exclusion restrictions\n",
    "# that affect college attendance but not wages directly).\n",
    "\n",
    "y_cw = college_df['wage_censored'].values\n",
    "\n",
    "feature_cols_cw = ['ability', 'parent_education', 'family_income', 'urban', 'female']\n",
    "X_df_cw = college_df[feature_cols_cw].copy()\n",
    "X_df_cw.insert(0, 'const', 1.0)\n",
    "X_cw = X_df_cw.values\n",
    "\n",
    "var_names_cw = list(X_df_cw.columns)\n",
    "\n",
    "print(f'Dependent variable: wage_censored')\n",
    "print(f'Regressors: {var_names_cw}')\n",
    "print(f'Design matrix shape: {X_cw.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Tobit model\n",
    "tobit_cw = PooledTobit(endog=y_cw, exog=X_cw, censoring_point=0.0)\n",
    "\n",
    "# Store variable names for marginal effects computation\n",
    "tobit_cw.exog_names = var_names_cw\n",
    "\n",
    "tobit_cw.fit()\n",
    "\n",
    "# Extract parameters\n",
    "beta_cw = tobit_cw.beta\n",
    "sigma_cw = tobit_cw.sigma\n",
    "\n",
    "print('Tobit Model: Estimated Parameters')\n",
    "print('=' * 55)\n",
    "for name, coef in zip(var_names_cw, beta_cw):\n",
    "    print(f'  {name:<25s} {coef:>10.4f}')\n",
    "print(f'  {\"sigma\":<25s} {sigma_cw:>10.4f}')\n",
    "print(f'\\nLog-likelihood: {tobit_cw.llf:.2f}')\n",
    "print(f'Converged: {tobit_cw.converged}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-ols-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also fit OLS for comparison\n",
    "ols_cw = sm.OLS(y_cw, X_cw).fit()\n",
    "\n",
    "print('Comparison: Tobit vs OLS Coefficients')\n",
    "print('=' * 65)\n",
    "print(f'{\"Variable\":<25s} {\"Tobit\":>10s} {\"OLS\":>10s} {\"Ratio\":>10s}')\n",
    "print('-' * 65)\n",
    "for i, name in enumerate(var_names_cw):\n",
    "    t_coef = beta_cw[i]\n",
    "    o_coef = ols_cw.params[i]\n",
    "    ratio = t_coef / o_coef if abs(o_coef) > 1e-10 else np.nan\n",
    "    print(f'{name:<25s} {t_coef:>10.4f} {o_coef:>10.4f} {ratio:>10.4f}')\n",
    "print('-' * 65)\n",
    "print('\\nNote: Tobit coefficients are typically larger in magnitude than OLS')\n",
    "print('because OLS is attenuated (biased toward zero) with censored data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-step4-header",
   "metadata": {},
   "source": [
    "### Step 4: Compute Marginal Effects (AME and MEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-ame",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AME for all three types using model method\n",
    "ame_uncond_cw = tobit_cw.marginal_effects(at='overall', which='unconditional')\n",
    "ame_cond_cw = tobit_cw.marginal_effects(at='overall', which='conditional')\n",
    "ame_prob_cw = tobit_cw.marginal_effects(at='overall', which='probability')\n",
    "\n",
    "print('Average Marginal Effects (AME) -- Unconditional')\n",
    "print('Effect on E[wage|X] for the entire population')\n",
    "print('=' * 55)\n",
    "for var in var_names_cw:\n",
    "    if var == 'const':\n",
    "        continue\n",
    "    val = ame_uncond_cw.marginal_effects.get(var, np.nan)\n",
    "    print(f'  {var:<25s} {val:>10.4f}')\n",
    "\n",
    "print()\n",
    "print('Average Marginal Effects (AME) -- Conditional')\n",
    "print('Effect on E[wage|wage>0, X] among wage-earners')\n",
    "print('=' * 55)\n",
    "for var in var_names_cw:\n",
    "    if var == 'const':\n",
    "        continue\n",
    "    val = ame_cond_cw.marginal_effects.get(var, np.nan)\n",
    "    print(f'  {var:<25s} {val:>10.4f}')\n",
    "\n",
    "print()\n",
    "print('Average Marginal Effects (AME) -- Probability')\n",
    "print('Effect on P(wage > 0 | X)')\n",
    "print('=' * 55)\n",
    "for var in var_names_cw:\n",
    "    if var == 'const':\n",
    "        continue\n",
    "    val = ame_prob_cw.marginal_effects.get(var, np.nan)\n",
    "    print(f'  {var:<25s} {val:>10.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-mem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MEM (unconditional) for comparison with AME\n",
    "mem_uncond_cw = tobit_cw.marginal_effects(at='mean', which='unconditional')\n",
    "\n",
    "print('Marginal Effects at Means (MEM) -- Unconditional')\n",
    "print('Effect evaluated at the mean of all covariates')\n",
    "print('=' * 55)\n",
    "for var in var_names_cw:\n",
    "    if var == 'const':\n",
    "        continue\n",
    "    val = mem_uncond_cw.marginal_effects.get(var, np.nan)\n",
    "    print(f'  {var:<25s} {val:>10.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-comparison-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a comprehensive comparison table\n",
    "comparison_rows = []\n",
    "for var in var_names_cw:\n",
    "    if var == 'const':\n",
    "        continue\n",
    "    idx = var_names_cw.index(var)\n",
    "    row = {\n",
    "        'Variable': var,\n",
    "        'Tobit Coef.': beta_cw[idx],\n",
    "        'AME Uncond.': ame_uncond_cw.marginal_effects.get(var, np.nan),\n",
    "        'AME Cond.': ame_cond_cw.marginal_effects.get(var, np.nan),\n",
    "        'AME Prob.': ame_prob_cw.marginal_effects.get(var, np.nan),\n",
    "        'MEM Uncond.': mem_uncond_cw.marginal_effects.get(var, np.nan),\n",
    "        'OLS Coef.': ols_cw.params[idx],\n",
    "    }\n",
    "    comparison_rows.append(row)\n",
    "\n",
    "comparison_table = pd.DataFrame(comparison_rows).set_index('Variable')\n",
    "\n",
    "print('Comprehensive Comparison Table')\n",
    "print('=' * 80)\n",
    "display(comparison_table.round(4))\n",
    "\n",
    "print('\\nKey observations:')\n",
    "print('  1. Tobit coefficients > Conditional ME > Unconditional ME > Probability ME')\n",
    "print('  2. OLS coefficients are attenuated due to censoring')\n",
    "print('  3. AME and MEM are generally close but not identical (nonlinearity effect)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-step5-header",
   "metadata": {},
   "source": [
    "### Step 5: McDonald-Moffitt Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-decomp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the McDonald-Moffitt decomposition\n",
    "# Total ME = Intensive margin + Extensive margin\n",
    "# Intensive = P(y>0) * Conditional ME\n",
    "# Extensive = E[y|y>0] * Probability ME\n",
    "\n",
    "linear_pred_cw = X_cw @ beta_cw\n",
    "z_cw = (linear_pred_cw - 0.0) / sigma_cw\n",
    "\n",
    "Phi_z_cw = norm.cdf(z_cw)\n",
    "phi_z_cw = norm.pdf(z_cw)\n",
    "lambda_z_cw = phi_z_cw / np.maximum(Phi_z_cw, 1e-10)\n",
    "\n",
    "# Conditional expectation: E[y | y > 0, X] = X'beta + sigma * lambda(z)\n",
    "E_y_cond_cw = linear_pred_cw + sigma_cw * lambda_z_cw\n",
    "\n",
    "print('McDonald-Moffitt Decomposition')\n",
    "print('=' * 80)\n",
    "print(f'{\"Variable\":<22s} {\"Total ME\":>10s} {\"Intensive\":>10s} {\"Extensive\":>10s} '\n",
    "      f'{\"Int %\":>8s} {\"Ext %\":>8s}')\n",
    "print('-' * 80)\n",
    "\n",
    "decomposition_data = []\n",
    "\n",
    "for k, name in enumerate(var_names_cw):\n",
    "    if name == 'const':\n",
    "        continue\n",
    "    \n",
    "    beta_k_cw = beta_cw[k]\n",
    "    \n",
    "    # Three types of ME (observation-level)\n",
    "    me_uncond_i = beta_k_cw * Phi_z_cw\n",
    "    me_cond_i = beta_k_cw * (1 - lambda_z_cw * (z_cw + lambda_z_cw))\n",
    "    me_prob_i = (beta_k_cw / sigma_cw) * phi_z_cw\n",
    "    \n",
    "    # Average over observations\n",
    "    total_me = np.mean(me_uncond_i)\n",
    "    \n",
    "    # Decomposition: Total = Intensive + Extensive\n",
    "    intensive = np.mean(Phi_z_cw * me_cond_i)\n",
    "    extensive = np.mean(E_y_cond_cw * me_prob_i)\n",
    "    \n",
    "    # Percentages\n",
    "    pct_int = 100 * intensive / total_me if abs(total_me) > 1e-10 else np.nan\n",
    "    pct_ext = 100 * extensive / total_me if abs(total_me) > 1e-10 else np.nan\n",
    "    \n",
    "    print(f'{name:<22s} {total_me:>10.4f} {intensive:>10.4f} {extensive:>10.4f} '\n",
    "          f'{pct_int:>7.1f}% {pct_ext:>7.1f}%')\n",
    "    \n",
    "    decomposition_data.append({\n",
    "        'Variable': name,\n",
    "        'Total ME': total_me,\n",
    "        'Intensive Margin': intensive,\n",
    "        'Extensive Margin': extensive,\n",
    "        'Intensive %': pct_int,\n",
    "        'Extensive %': pct_ext\n",
    "    })\n",
    "\n",
    "print('-' * 80)\n",
    "print()\n",
    "print('Notes:')\n",
    "print('  - Total ME = Intensive + Extensive')\n",
    "print('  - Intensive = P(y>0) * Conditional ME (how much more do earners earn?)')\n",
    "print('  - Extensive = E[y|y>0] * Probability ME (how many new earners appear?)')\n",
    "\n",
    "decomp_df = pd.DataFrame(decomposition_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-step6-header",
   "metadata": {},
   "source": [
    "### Step 6: Publication-Quality Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a publication-quality figure with three panels\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "plot_vars = [v for v in var_names_cw if v != 'const']\n",
    "x_pos = np.arange(len(plot_vars))\n",
    "\n",
    "# --- Panel A: Three Types of AME ---\n",
    "ax = axes[0]\n",
    "width = 0.22\n",
    "\n",
    "uncond_vals = [ame_uncond_cw.marginal_effects.get(v, 0) for v in plot_vars]\n",
    "cond_vals = [ame_cond_cw.marginal_effects.get(v, 0) for v in plot_vars]\n",
    "prob_vals = [ame_prob_cw.marginal_effects.get(v, 0) for v in plot_vars]\n",
    "\n",
    "ax.bar(x_pos - width, uncond_vals, width,\n",
    "       label='Unconditional', color='#1565C0',\n",
    "       edgecolor='black', linewidth=0.8)\n",
    "ax.bar(x_pos, cond_vals, width,\n",
    "       label='Conditional', color='#2E7D32',\n",
    "       edgecolor='black', linewidth=0.8)\n",
    "ax.bar(x_pos + width, prob_vals, width,\n",
    "       label='Probability', color='#E65100',\n",
    "       edgecolor='black', linewidth=0.8)\n",
    "\n",
    "ax.set_xlabel('Variable', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Average Marginal Effect', fontsize=11, fontweight='bold')\n",
    "ax.set_title('(A) Three Types of AME', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(plot_vars, rotation=35, ha='right', fontsize=9)\n",
    "ax.legend(fontsize=8, loc='best')\n",
    "ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.8)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# --- Panel B: McDonald-Moffitt Decomposition ---\n",
    "ax = axes[1]\n",
    "width_b = 0.35\n",
    "\n",
    "bars1 = ax.bar(x_pos - width_b / 2, decomp_df['Intensive Margin'], width_b,\n",
    "               label='Intensive Margin', color='#2196F3',\n",
    "               edgecolor='black', linewidth=0.8)\n",
    "bars2 = ax.bar(x_pos + width_b / 2, decomp_df['Extensive Margin'], width_b,\n",
    "               label='Extensive Margin', color='#FF9800',\n",
    "               edgecolor='black', linewidth=0.8)\n",
    "\n",
    "ax.scatter(x_pos, decomp_df['Total ME'], color='red', s=80, zorder=5,\n",
    "           marker='D', label='Total ME', edgecolors='black', linewidths=0.8)\n",
    "\n",
    "ax.set_xlabel('Variable', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Marginal Effect', fontsize=11, fontweight='bold')\n",
    "ax.set_title('(B) McDonald-Moffitt Decomposition', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(plot_vars, rotation=35, ha='right', fontsize=9)\n",
    "ax.legend(fontsize=8, loc='best')\n",
    "ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.8)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# --- Panel C: Tobit Coefficient vs AME vs OLS ---\n",
    "ax = axes[2]\n",
    "width_c = 0.25\n",
    "\n",
    "tobit_coefs = [beta_cw[var_names_cw.index(v)] for v in plot_vars]\n",
    "ame_vals = [ame_uncond_cw.marginal_effects.get(v, 0) for v in plot_vars]\n",
    "ols_coefs = [ols_cw.params[var_names_cw.index(v)] for v in plot_vars]\n",
    "\n",
    "ax.bar(x_pos - width_c, tobit_coefs, width_c,\n",
    "       label='Tobit Coef.', color='#B71C1C', alpha=0.85,\n",
    "       edgecolor='black', linewidth=0.8)\n",
    "ax.bar(x_pos, ame_vals, width_c,\n",
    "       label='Uncond. AME', color='#1565C0', alpha=0.85,\n",
    "       edgecolor='black', linewidth=0.8)\n",
    "ax.bar(x_pos + width_c, ols_coefs, width_c,\n",
    "       label='OLS Coef.', color='#388E3C', alpha=0.85,\n",
    "       edgecolor='black', linewidth=0.8)\n",
    "\n",
    "ax.set_xlabel('Variable', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Effect Estimate', fontsize=11, fontweight='bold')\n",
    "ax.set_title('(C) Tobit Coef. vs AME vs OLS', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(plot_vars, rotation=35, ha='right', fontsize=9)\n",
    "ax.legend(fontsize=8, loc='best')\n",
    "ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.8)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex3_publication_figure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Figure saved to:', FIGURES_DIR / 'ex3_publication_figure.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-step7-header",
   "metadata": {},
   "source": [
    "### Step 7: Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Interpretation of Key Findings')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print('1. ABILITY is the strongest predictor of wages. The unconditional AME')\n",
    "print('   indicates that a one-standard-deviation increase in ability raises')\n",
    "print('   expected wages through both intensive (higher wages among earners)')\n",
    "print('   and extensive (more individuals enter the workforce) channels.')\n",
    "print()\n",
    "print('2. The McDonald-Moffitt decomposition reveals that the INTENSIVE margin')\n",
    "print('   dominates for most variables: most of the effect comes from changing')\n",
    "print('   wage levels among those already earning, rather than inducing new')\n",
    "print('   participants into the labor market.')\n",
    "print()\n",
    "print('3. Raw Tobit coefficients overstate the marginal effects substantially.')\n",
    "print('   For example, with a censoring rate of ~{:.0f}%, the unconditional'.format(pct_censored))\n",
    "print('   AME is only about {:.0f}% of the Tobit coefficient on average.'.format(\n",
    "    100 * np.mean([abs(ame_uncond_cw.marginal_effects.get(v, 0) / beta_cw[var_names_cw.index(v)])\n",
    "                   for v in plot_vars if abs(beta_cw[var_names_cw.index(v)]) > 0.01])))\n",
    "print('   Researchers should always report marginal effects alongside or')\n",
    "print('   instead of raw coefficients in Tobit models.')\n",
    "print()\n",
    "print('4. OLS coefficients are attenuated compared to Tobit, confirming that')\n",
    "print('   ignoring censoring leads to biased estimates. However, the')\n",
    "print('   unconditional AME from the Tobit model provides the correct')\n",
    "print('   policy-relevant quantity for the entire population.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-final-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the decomposition table\n",
    "decomp_df.to_csv(TABLES_DIR / 'ex3_mcdonald_moffitt_decomposition.csv', index=False)\n",
    "comparison_table.to_csv(TABLES_DIR / 'ex3_marginal_effects_comparison.csv')\n",
    "\n",
    "print('Tables saved:')\n",
    "print(f'  - {TABLES_DIR / \"ex3_mcdonald_moffitt_decomposition.csv\"}')\n",
    "print(f'  - {TABLES_DIR / \"ex3_marginal_effects_comparison.csv\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Exercise 1 (Manual Computation)\n",
    "- We computed MEM for all three types using the standard formulas\n",
    "- Confirmed the ordering: Probability ME < Unconditional ME < Conditional ME < beta\n",
    "- The scaling factors show that raw coefficients overstate the effect; for z=1.2, the unconditional ME is about 88% of beta\n",
    "\n",
    "### Exercise 2 (Censoring Sensitivity)\n",
    "- Demonstrated that the AME/beta ratio decreases monotonically with the censoring rate\n",
    "- At 10% censoring, AME is about 90% of beta; at 80%, it is only about 20%\n",
    "- The scaling factor closely tracks the non-censoring rate: AME/beta ~ E[Phi(z)] ~ 1 - censoring rate\n",
    "\n",
    "### Exercise 3 (Full Pipeline)\n",
    "- Loaded and explored `college_wage.csv`, identifying wage as censored at 0 for non-college attendees\n",
    "- Estimated a Tobit model and compared coefficients with OLS\n",
    "- Computed all three types of AME and MEM using PanelBox\n",
    "- Performed the McDonald-Moffitt decomposition showing the intensive margin dominates\n",
    "- Created a three-panel publication-quality figure\n",
    "- Provided a brief interpretation highlighting the importance of reporting marginal effects\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "In censored (Tobit) models, **always report marginal effects** rather than raw coefficients. The choice of marginal effect type (unconditional, conditional, probability) should match the research question:\n",
    "- **Unconditional AME** for policy analysis affecting the entire population\n",
    "- **Conditional AME** for understanding effects among participants\n",
    "- **Probability AME** for studying the extensive margin (entry/exit decisions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexable": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
