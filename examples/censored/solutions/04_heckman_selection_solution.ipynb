{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heckman Two-Step Selection Correction -- SOLUTIONS\n",
    "\n",
    "**This is the worked solution notebook.**  \n",
    "It provides complete, working solutions for all 4 exercises from `04_heckman_selection.ipynb`.\n",
    "\n",
    "> Instructors: do not distribute this file to students before they complete the tutorial notebook.\n",
    "\n",
    "## Exercises covered\n",
    "\n",
    "| # | Title | Level |\n",
    "|---|-------|-------|\n",
    "| 1 | Evaluate Candidate Instruments (Conceptual) | Conceptual |\n",
    "| 2 | Implement and Compare Specifications (Hands-On) | Hands-On |\n",
    "| 3 | Collinearity Diagnostic (Intermediate) | Intermediate |\n",
    "| 4 | Monte Carlo Simulation (Advanced) | Advanced |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Setup\n",
    "# ============================================================\n",
    "import sys, pathlib\n",
    "\n",
    "ROOT = pathlib.Path('..').resolve()\n",
    "PANELBOX_ROOT = pathlib.Path('/home/guhaase/projetos/panelbox')\n",
    "for p in [str(ROOT), str(PANELBOX_ROOT)]:\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# PanelBox imports\n",
    "from panelbox.models.selection import PanelHeckman, compute_imr, imr_diagnostics, test_selection_effect\n",
    "\n",
    "# Visualization configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define paths (relative to notebook location in examples/censored/solutions/)\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Solution notebook loaded -- setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "We load both datasets used across the exercises: **Mroz (1987)** for exercises 1-2 and 4, and **College Wage** for exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mroz (1987) dataset\n",
    "df_mroz = pd.read_csv(DATA_DIR / 'mroz_1987.csv')\n",
    "print('Mroz dataset:', df_mroz.shape)\n",
    "print('Columns:', list(df_mroz.columns))\n",
    "print(f'LFP rate: {df_mroz[\"lfp\"].mean():.1%}')\n",
    "print()\n",
    "\n",
    "# Load College Wage dataset\n",
    "df_college = pd.read_csv(DATA_DIR / 'college_wage.csv')\n",
    "print('College Wage dataset:', df_college.shape)\n",
    "print('Columns:', list(df_college.columns))\n",
    "print(f'College attendance rate: {df_college[\"college\"].mean():.1%}')\n",
    "print()\n",
    "\n",
    "# Quick summaries\n",
    "display(df_mroz.describe().round(3))\n",
    "display(df_college.describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 1: Evaluate Candidate Instruments (Conceptual)\n",
    "\n",
    "For each proposed exclusion restriction, evaluate **relevance** (does the instrument\n",
    "predict selection?) and **validity** (is the instrument excludable from the outcome\n",
    "equation?).\n",
    "\n",
    "A valid exclusion restriction must satisfy two conditions:\n",
    "\n",
    "1. **Relevance**: The variable significantly predicts the selection decision  \n",
    "   $\\text{Cov}(Z_i, s_i) \\neq 0$\n",
    "\n",
    "2. **Validity (Excludability)**: The variable does NOT directly affect the outcome,\n",
    "   conditional on the other regressors  \n",
    "   $\\text{Cov}(Z_i, \\varepsilon_i | X_i) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Female labor supply -- Husband's age as instrument\n",
    "\n",
    "**Setting**: We model married women's labor force participation (selection) and\n",
    "hourly wages (outcome). The proposed instrument is the **husband's age**.\n",
    "\n",
    "**Relevance assessment**:\n",
    "- Husband's age is correlated with the household lifecycle stage. Older husbands\n",
    "  may have higher earnings, reducing the wife's financial need to work. This\n",
    "  provides a channel through which husband's age affects the participation\n",
    "  decision.\n",
    "- However, the relationship may be weak once we control for husband's income\n",
    "  directly. If husband's income is already in the selection equation, husband's\n",
    "  age adds little independent variation.\n",
    "- **Verdict**: Moderate relevance. The strength depends on what other variables\n",
    "  (especially husband's income) are already included.\n",
    "\n",
    "**Validity assessment**:\n",
    "- Husband's age could be correlated with the wife's age (assortative mating).\n",
    "  If the wife's age affects her wage (through experience depreciation or\n",
    "  vintage effects), then husband's age indirectly affects wages.\n",
    "- There could also be network effects: older husbands may provide better job\n",
    "  referrals or the couple may live in neighborhoods with different labor markets.\n",
    "- **Verdict**: Questionable validity. The exclusion restriction is plausible only\n",
    "  if the wife's own age (and experience) are already controlled for in the\n",
    "  outcome equation. Even then, assortative mating creates subtle pathways.\n",
    "\n",
    "**Overall**: Weak to moderate instrument. Use with caution and conduct sensitivity\n",
    "analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) College wage premium -- SAT score as instrument\n",
    "\n",
    "**Setting**: We model college attendance (selection) and post-college wages\n",
    "(outcome). The proposed instrument is the **SAT score**.\n",
    "\n",
    "**Relevance assessment**:\n",
    "- SAT scores strongly predict college attendance. Students with higher SAT scores\n",
    "  are more likely to be admitted and to choose to attend college.\n",
    "- This is a very strong predictor of selection.\n",
    "- **Verdict**: High relevance. SAT scores are among the strongest predictors of\n",
    "  college enrollment.\n",
    "\n",
    "**Validity assessment**:\n",
    "- SAT scores measure cognitive ability. Cognitive ability directly affects\n",
    "  wages through productivity, regardless of college attendance.\n",
    "- If ability is in the outcome equation, this is partially addressed, but SAT\n",
    "  scores likely capture dimensions of ability beyond what a single \"ability\"\n",
    "  measure controls for.\n",
    "- Employers may use SAT scores (or correlated signals) directly in hiring and\n",
    "  wage setting, violating excludability.\n",
    "- **Verdict**: Invalid. SAT scores are a direct measure of ability, which\n",
    "  affects wages. This violates the exclusion restriction.\n",
    "\n",
    "**Overall**: Despite high relevance, the SAT score **fails** as an exclusion\n",
    "restriction because it directly affects wages. Better alternatives: distance to\n",
    "college, local tuition levels, or cohort-level college capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Union wage gap -- State right-to-work law\n",
    "\n",
    "**Setting**: We model union membership (selection) and wages (outcome). The\n",
    "proposed instrument is whether the worker lives in a **state with right-to-work\n",
    "laws**.\n",
    "\n",
    "**Relevance assessment**:\n",
    "- Right-to-work laws prohibit mandatory union membership as a condition of\n",
    "  employment, directly reducing union membership rates.\n",
    "- States with right-to-work laws have significantly lower unionization rates\n",
    "  (empirically well-documented).\n",
    "- **Verdict**: High relevance. This is a strong and well-established predictor\n",
    "  of union membership.\n",
    "\n",
    "**Validity assessment**:\n",
    "- Right-to-work states may differ systematically in their labor market\n",
    "  conditions. These states tend to be in the South and have lower cost of\n",
    "  living, different industry compositions, and different overall wage levels.\n",
    "- If we do not control for state-level factors (cost of living, industry mix,\n",
    "  regional labor demand), right-to-work status is likely correlated with\n",
    "  wages through channels other than union membership.\n",
    "- **Verdict**: Potentially valid, but requires careful conditioning. Include\n",
    "  state-level controls (region, industry, cost of living) to make the exclusion\n",
    "  restriction more plausible.\n",
    "\n",
    "**Overall**: Good instrument if accompanied by appropriate state-level controls.\n",
    "Without controls, likely invalid due to regional wage differentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training program -- Distance to training site\n",
    "\n",
    "**Setting**: We model participation in a job training program (selection) and\n",
    "post-training earnings (outcome). The proposed instrument is **distance to the\n",
    "training site**.\n",
    "\n",
    "**Relevance assessment**:\n",
    "- Distance is a practical barrier to participation: individuals who live farther\n",
    "  from the training site face higher transportation costs and time costs, making\n",
    "  them less likely to enroll.\n",
    "- This is similar to the classic Card (1995) \"distance to college\" instrument.\n",
    "- **Verdict**: High relevance. Geographic proximity is a strong predictor of\n",
    "  program participation.\n",
    "\n",
    "**Validity assessment**:\n",
    "- If training sites are located in urban areas, distance may proxy for\n",
    "  urban/rural residence, which correlates with wages independently of training.\n",
    "- However, conditional on observable characteristics (education, experience,\n",
    "  industry, urban/rural indicator), the remaining variation in distance is\n",
    "  plausibly exogenous to earnings.\n",
    "- The key assumption: conditional on observables, an individual's distance to\n",
    "  the training site does not directly affect their earnings potential.\n",
    "- **Verdict**: Plausibly valid, especially with appropriate controls for\n",
    "  location characteristics.\n",
    "\n",
    "**Overall**: Strong instrument. This is one of the most commonly used and\n",
    "well-justified exclusion restrictions in the selection model literature.\n",
    "Analogous to Card's distance-to-college instrument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Table\n",
    "\n",
    "| Scenario | Instrument | Relevance | Validity | Overall |\n",
    "|----------|-----------|-----------|----------|---------|\n",
    "| (a) Female labor supply | Husband's age | Moderate | Questionable | Weak |\n",
    "| (b) College wage premium | SAT score | High | Invalid | Fails |\n",
    "| (c) Union wage gap | Right-to-work law | High | Conditional | Good (with controls) |\n",
    "| (d) Training program | Distance to site | High | Plausible | Strong |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical illustration: demonstrate relevance for instruments\n",
    "# we can actually test with our data\n",
    "\n",
    "# (a) For the Mroz data, we can test relevance of candidate exclusion restrictions\n",
    "print('Empirical Relevance Tests for Mroz Data')\n",
    "print('=' * 60)\n",
    "\n",
    "# Probit: regress LFP on all candidate exclusion restrictions\n",
    "exclusion_candidates = ['children_lt6', 'children_6_18', 'husband_income', 'age']\n",
    "other_vars = ['education', 'experience']\n",
    "\n",
    "Z_full = sm.add_constant(df_mroz[other_vars + exclusion_candidates].values)\n",
    "selection = df_mroz['lfp'].values\n",
    "\n",
    "probit = sm.Probit(selection, Z_full)\n",
    "probit_result = probit.fit(disp=0)\n",
    "\n",
    "var_names = ['const'] + other_vars + exclusion_candidates\n",
    "relevance_table = pd.DataFrame({\n",
    "    'Variable': var_names,\n",
    "    'Coefficient': probit_result.params,\n",
    "    'Std. Error': probit_result.bse,\n",
    "    'z-stat': probit_result.tvalues,\n",
    "    'p-value': probit_result.pvalues,\n",
    "})\n",
    "\n",
    "display(relevance_table.round(4))\n",
    "\n",
    "print()\n",
    "print('Relevance assessment (significant predictors of selection):')\n",
    "for var in exclusion_candidates:\n",
    "    idx = var_names.index(var)\n",
    "    sig = 'Yes' if probit_result.pvalues[idx] < 0.05 else 'No'\n",
    "    print(f'  {var:20s}: z = {probit_result.tvalues[idx]:7.3f}, '\n",
    "          f'p = {probit_result.pvalues[idx]:.4f}, Significant: {sig}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) For college wage data, test distance_college and tuition as instruments\n",
    "print('Empirical Relevance Tests for College Wage Data')\n",
    "print('=' * 60)\n",
    "\n",
    "exclusion_cands_cw = ['distance_college', 'tuition']\n",
    "other_vars_cw = ['ability', 'parent_education', 'family_income', 'urban', 'female']\n",
    "\n",
    "Z_cw = sm.add_constant(df_college[other_vars_cw + exclusion_cands_cw].values)\n",
    "sel_cw = df_college['college'].values\n",
    "\n",
    "probit_cw = sm.Probit(sel_cw, Z_cw)\n",
    "probit_cw_result = probit_cw.fit(disp=0)\n",
    "\n",
    "var_names_cw = ['const'] + other_vars_cw + exclusion_cands_cw\n",
    "rel_table_cw = pd.DataFrame({\n",
    "    'Variable': var_names_cw,\n",
    "    'Coefficient': probit_cw_result.params,\n",
    "    'Std. Error': probit_cw_result.bse,\n",
    "    'z-stat': probit_cw_result.tvalues,\n",
    "    'p-value': probit_cw_result.pvalues,\n",
    "})\n",
    "\n",
    "display(rel_table_cw.round(4))\n",
    "\n",
    "print()\n",
    "for var in exclusion_cands_cw:\n",
    "    idx = var_names_cw.index(var)\n",
    "    sig = 'Yes' if probit_cw_result.pvalues[idx] < 0.05 else 'No'\n",
    "    print(f'  {var:20s}: z = {probit_cw_result.tvalues[idx]:7.3f}, '\n",
    "          f'p = {probit_cw_result.pvalues[idx]:.4f}, Significant: {sig}')\n",
    "\n",
    "print()\n",
    "print('Both distance_college and tuition are strong, plausibly valid instruments.')\n",
    "print('They affect college attendance but (conditional on ability and other controls)')\n",
    "print('should not directly affect wages.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 2: Implement and Compare Specifications (Hands-On)\n",
    "\n",
    "Using the Mroz dataset, compare three Heckman model specifications that differ\n",
    "in their choice of exclusion restrictions:\n",
    "\n",
    "| Model | Exclusion restrictions |\n",
    "|-------|----------------------|\n",
    "| A | `children_lt6` + `husband_income` |\n",
    "| B | `children_6_18` + `husband_income` |\n",
    "| C | `age` only |\n",
    "\n",
    "All models share the same **outcome equation**:\n",
    "$$\\text{wage}_i = \\beta_0 + \\beta_1 \\text{education}_i + \\beta_2 \\text{experience}_i + \\beta_3 \\text{experience\\_sq}_i + \\varepsilon_i$$\n",
    "\n",
    "The **selection equation** includes the outcome regressors plus the exclusion\n",
    "restrictions specific to each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare common data\n",
    "y_all = df_mroz['wage'].fillna(0).values\n",
    "selection = df_mroz['lfp'].values.astype(float)\n",
    "\n",
    "# Outcome equation regressors (same for all models)\n",
    "X_outcome = sm.add_constant(\n",
    "    df_mroz[['education', 'experience', 'experience_sq']].values\n",
    ")\n",
    "\n",
    "# Common variables in selection equation (always included)\n",
    "common_sel_vars = ['education', 'experience', 'age']\n",
    "\n",
    "# Define the three specifications\n",
    "specs = {\n",
    "    'Model A': common_sel_vars + ['children_lt6', 'husband_income'],\n",
    "    'Model B': common_sel_vars + ['children_6_18', 'husband_income'],\n",
    "    'Model C': common_sel_vars,  # age is already in common; it serves as the\n",
    "                                 # exclusion restriction since age is NOT in the\n",
    "                                 # outcome equation\n",
    "}\n",
    "\n",
    "print('Outcome equation regressors: const, education, experience, experience_sq')\n",
    "print()\n",
    "for name, sel_vars in specs.items():\n",
    "    excl = [v for v in sel_vars if v not in ['education', 'experience']]\n",
    "    print(f'{name}: selection vars = {sel_vars}')\n",
    "    print(f'        exclusion restrictions = {excl}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate all three models\n",
    "results = {}\n",
    "\n",
    "for name, sel_vars in specs.items():\n",
    "    print(f'Estimating {name}...')\n",
    "    \n",
    "    Z = sm.add_constant(df_mroz[sel_vars].values)\n",
    "    \n",
    "    model = PanelHeckman(\n",
    "        endog=y_all,\n",
    "        exog=X_outcome,\n",
    "        selection=selection,\n",
    "        exog_selection=Z,\n",
    "        method='two_step'\n",
    "    )\n",
    "    result = model.fit()\n",
    "    results[name] = result\n",
    "    \n",
    "    print(f'  rho = {result.rho:.4f}, sigma = {result.sigma:.4f}')\n",
    "    print(f'  lambda = rho*sigma = {result.rho * result.sigma:.4f}')\n",
    "    print()\n",
    "\n",
    "print('All models estimated successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full summary for each model\n",
    "for name, result in results.items():\n",
    "    print(f'\\n{\"=\" * 60}')\n",
    "    print(f'  {name}')\n",
    "    print(f'{\"=\" * 60}')\n",
    "    print(result.summary())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outcome coefficients across specifications\n",
    "outcome_var_names = ['const', 'education', 'experience', 'experience_sq']\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    name: pd.Series(result.outcome_params, index=outcome_var_names)\n",
    "    for name, result in results.items()\n",
    "})\n",
    "\n",
    "# Add selection parameters\n",
    "sel_params = pd.DataFrame({\n",
    "    name: pd.Series({\n",
    "        'rho': result.rho,\n",
    "        'sigma': result.sigma,\n",
    "        'lambda (rho*sigma)': result.rho * result.sigma,\n",
    "    })\n",
    "    for name, result in results.items()\n",
    "})\n",
    "\n",
    "print('Outcome Equation Coefficients Across Specifications')\n",
    "print('=' * 60)\n",
    "display(comparison_df.round(4))\n",
    "\n",
    "print('\\nSelection Parameters')\n",
    "print('=' * 60)\n",
    "display(sel_params.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficient comparison across models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Outcome coefficients (excluding constant for clarity)\n",
    "coef_vars = ['education', 'experience', 'experience_sq']\n",
    "x_pos = np.arange(len(coef_vars))\n",
    "width = 0.25\n",
    "colors = ['steelblue', '#D55E00', '#009E73']\n",
    "\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    coefs = [result.outcome_params[j+1] for j in range(len(coef_vars))]\n",
    "    axes[0].bar(x_pos + i * width, coefs, width, label=name,\n",
    "                color=colors[i], alpha=0.8, edgecolor='black')\n",
    "\n",
    "axes[0].set_xticks(x_pos + width)\n",
    "axes[0].set_xticklabels(coef_vars, rotation=15)\n",
    "axes[0].set_ylabel('Coefficient')\n",
    "axes[0].set_title('Outcome Coefficients by Specification')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "# Right: Selection parameters (rho, sigma, lambda)\n",
    "sel_labels = ['rho', 'sigma', 'lambda']\n",
    "x_sel = np.arange(len(sel_labels))\n",
    "\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    vals = [result.rho, result.sigma, result.rho * result.sigma]\n",
    "    axes[1].bar(x_sel + i * width, vals, width, label=name,\n",
    "                color=colors[i], alpha=0.8, edgecolor='black')\n",
    "\n",
    "axes[1].set_xticks(x_sel + width)\n",
    "axes[1].set_xticklabels(sel_labels)\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('Selection Parameters by Specification')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.suptitle('Sensitivity to Choice of Exclusion Restrictions',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex2_specification_comparison.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare IMR diagnostics across specifications\n",
    "print('IMR Diagnostics Comparison')\n",
    "print('=' * 60)\n",
    "\n",
    "diag_rows = []\n",
    "for name, result in results.items():\n",
    "    diag = result.imr_diagnostics()\n",
    "    diag_rows.append({\n",
    "        'Model': name,\n",
    "        'IMR Mean': diag['imr_mean'],\n",
    "        'IMR Std': diag['imr_std'],\n",
    "        'IMR Min': diag['imr_min'],\n",
    "        'IMR Max': diag['imr_max'],\n",
    "        'High IMR (>2)': diag['high_imr_count'],\n",
    "    })\n",
    "\n",
    "diag_df = pd.DataFrame(diag_rows).set_index('Model')\n",
    "display(diag_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection tests for each specification\n",
    "print('Selection Bias Tests')\n",
    "print('=' * 60)\n",
    "\n",
    "for name, result in results.items():\n",
    "    test = result.selection_test()\n",
    "    print(f'{name}:')\n",
    "    print(f'  rho = {test[\"rho\"]:.4f}, z = {test[\"z_statistic\"]:.4f}, '\n",
    "          f'p = {test[\"p_value\"]:.4f}, Significant: {test[\"significant\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 -- Discussion\n",
    "\n",
    "**Key findings**:\n",
    "\n",
    "1. **Model A** (children_lt6 + husband_income) uses the two strongest exclusion\n",
    "   restrictions. Young children have a large, significant effect on participation,\n",
    "   and the model is well-identified.\n",
    "\n",
    "2. **Model B** (children_6_18 + husband_income) replaces young children with\n",
    "   school-age children. Since school-age children have a weaker effect on\n",
    "   participation than young children, this specification is somewhat more\n",
    "   weakly identified.\n",
    "\n",
    "3. **Model C** (age only) relies solely on age as the exclusion restriction. Age\n",
    "   appears in the selection equation but not in the outcome equation (where\n",
    "   experience and experience-squared capture age-related productivity effects).\n",
    "   This is the most weakly identified model because age is partially collinear\n",
    "   with experience.\n",
    "\n",
    "**Sensitivity**: The education and experience coefficients are reasonably stable\n",
    "across specifications A and B but may shift more under specification C. This\n",
    "illustrates why strong exclusion restrictions matter: they help pin down the\n",
    "selection correction, making the outcome coefficients less sensitive to\n",
    "specification choices.\n",
    "\n",
    "**Recommendation**: Model A is preferred because `children_lt6` is the strongest\n",
    "and most theoretically motivated exclusion restriction (young children constrain\n",
    "labor supply but do not affect the wage rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 3: Collinearity Diagnostic (Intermediate)\n",
    "\n",
    "For the **College Wage** dataset, we investigate the collinearity problem that\n",
    "arises when the Inverse Mills Ratio is highly correlated with the outcome\n",
    "regressors. Without proper exclusion restrictions, the IMR is approximately\n",
    "a linear function of the outcome regressors, leading to multicollinearity and\n",
    "imprecise estimates.\n",
    "\n",
    "**Steps**:\n",
    "1. Estimate the model **with** exclusion restrictions (distance_college, tuition)\n",
    "2. Estimate the model **without** exclusion restrictions\n",
    "3. Compute IMR-X correlation matrices and visualize with heatmaps\n",
    "4. Compute condition numbers to quantify collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare College Wage data\n",
    "y_college = df_college['wage'].fillna(0).values\n",
    "sel_college = df_college['college'].values.astype(float)\n",
    "\n",
    "# Outcome equation regressors\n",
    "outcome_vars_cw = ['ability', 'parent_education', 'family_income', 'urban', 'female']\n",
    "X_cw = sm.add_constant(df_college[outcome_vars_cw].values)\n",
    "\n",
    "# Selection equation WITH exclusion restrictions\n",
    "sel_vars_with = outcome_vars_cw + ['distance_college', 'tuition']\n",
    "Z_cw_with = sm.add_constant(df_college[sel_vars_with].values)\n",
    "\n",
    "# Selection equation WITHOUT exclusion restrictions\n",
    "# (same variables as outcome equation -- identification from functional form only)\n",
    "Z_cw_without = sm.add_constant(df_college[outcome_vars_cw].values)\n",
    "\n",
    "print('College Wage Model Setup')\n",
    "print('=' * 60)\n",
    "print(f'Total observations:     {len(df_college)}')\n",
    "print(f'College attendees:      {int(sel_college.sum())} ({sel_college.mean():.1%})')\n",
    "print(f'Outcome regressors:     {outcome_vars_cw}')\n",
    "print(f'Selection WITH excl:    {sel_vars_with}')\n",
    "print(f'Selection WITHOUT excl: {outcome_vars_cw}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Model WITH exclusion restrictions\n",
    "print('Model WITH Exclusion Restrictions')\n",
    "print('=' * 60)\n",
    "\n",
    "model_with = PanelHeckman(\n",
    "    endog=y_college,\n",
    "    exog=X_cw,\n",
    "    selection=sel_college,\n",
    "    exog_selection=Z_cw_with,\n",
    "    method='two_step'\n",
    ")\n",
    "result_with = model_with.fit()\n",
    "print(result_with.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Model WITHOUT exclusion restrictions\n",
    "print('Model WITHOUT Exclusion Restrictions')\n",
    "print('=' * 60)\n",
    "\n",
    "model_without = PanelHeckman(\n",
    "    endog=y_college,\n",
    "    exog=X_cw,\n",
    "    selection=sel_college,\n",
    "    exog_selection=Z_cw_without,\n",
    "    method='two_step'\n",
    ")\n",
    "result_without = model_without.fit()\n",
    "print(result_without.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outcome coefficients side by side\n",
    "cw_outcome_names = ['const'] + outcome_vars_cw\n",
    "\n",
    "cw_comparison = pd.DataFrame({\n",
    "    'Variable': cw_outcome_names,\n",
    "    'With Exclusion': result_with.outcome_params,\n",
    "    'Without Exclusion': result_without.outcome_params,\n",
    "})\n",
    "cw_comparison['Difference'] = (\n",
    "    cw_comparison['With Exclusion'] - cw_comparison['Without Exclusion']\n",
    ")\n",
    "\n",
    "print('Coefficient Comparison: With vs Without Exclusion Restrictions')\n",
    "print('=' * 70)\n",
    "display(cw_comparison.round(4))\n",
    "\n",
    "print(f'\\nWith exclusion:    rho = {result_with.rho:.4f}, sigma = {result_with.sigma:.4f}')\n",
    "print(f'Without exclusion: rho = {result_without.rho:.4f}, sigma = {result_without.sigma:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IMR for both specifications and build augmented design matrices\n",
    "selected_mask = sel_college == 1\n",
    "\n",
    "# --- WITH exclusion restrictions ---\n",
    "# IMR from the model with exclusion restrictions\n",
    "imr_with = result_with.lambda_imr[selected_mask]\n",
    "\n",
    "# --- WITHOUT exclusion restrictions ---\n",
    "# IMR from the model without exclusion restrictions\n",
    "imr_without = result_without.lambda_imr[selected_mask]\n",
    "\n",
    "# Build DataFrames with outcome regressors and IMR for selected sample\n",
    "df_selected = df_college[df_college['college'] == 1].copy()\n",
    "\n",
    "# Augmented data WITH exclusion\n",
    "aug_with = df_selected[outcome_vars_cw].copy()\n",
    "aug_with['IMR'] = imr_with\n",
    "\n",
    "# Augmented data WITHOUT exclusion\n",
    "aug_without = df_selected[outcome_vars_cw].copy()\n",
    "aug_without['IMR'] = imr_without\n",
    "\n",
    "print('IMR Statistics (selected sample)')\n",
    "print('=' * 50)\n",
    "print(f'WITH exclusion:    mean={imr_with.mean():.4f}, '\n",
    "      f'std={imr_with.std():.4f}, range=[{imr_with.min():.4f}, {imr_with.max():.4f}]')\n",
    "print(f'WITHOUT exclusion: mean={imr_without.mean():.4f}, '\n",
    "      f'std={imr_without.std():.4f}, range=[{imr_without.min():.4f}, {imr_without.max():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrices\n",
    "corr_with = aug_with.corr()\n",
    "corr_without = aug_without.corr()\n",
    "\n",
    "print('Correlation Matrix WITH Exclusion Restrictions')\n",
    "print('=' * 60)\n",
    "display(corr_with.round(3))\n",
    "\n",
    "print('\\nCorrelation Matrix WITHOUT Exclusion Restrictions')\n",
    "print('=' * 60)\n",
    "display(corr_without.round(3))\n",
    "\n",
    "# Highlight the IMR correlations\n",
    "print('\\nIMR correlations with outcome regressors:')\n",
    "print('-' * 50)\n",
    "imr_corr_comparison = pd.DataFrame({\n",
    "    'With Exclusion': corr_with['IMR'].drop('IMR'),\n",
    "    'Without Exclusion': corr_without['IMR'].drop('IMR'),\n",
    "})\n",
    "imr_corr_comparison['Abs Diff'] = (\n",
    "    imr_corr_comparison['Without Exclusion'].abs() -\n",
    "    imr_corr_comparison['With Exclusion'].abs()\n",
    ")\n",
    "display(imr_corr_comparison.round(4))\n",
    "\n",
    "print('\\nHigher absolute correlations WITHOUT exclusion indicate more collinearity.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Heatmap: WITH exclusion\n",
    "mask_with = np.triu(np.ones_like(corr_with, dtype=bool), k=1)\n",
    "sns.heatmap(corr_with, mask=mask_with, annot=True, fmt='.2f',\n",
    "            cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "            square=True, linewidths=0.5, ax=axes[0],\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "axes[0].set_title('WITH Exclusion Restrictions\\n(distance_college, tuition)',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "\n",
    "# Heatmap: WITHOUT exclusion\n",
    "mask_without = np.triu(np.ones_like(corr_without, dtype=bool), k=1)\n",
    "sns.heatmap(corr_without, mask=mask_without, annot=True, fmt='.2f',\n",
    "            cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "            square=True, linewidths=0.5, ax=axes[1],\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "axes[1].set_title('WITHOUT Exclusion Restrictions\\n(functional form identification only)',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('IMR--Regressor Correlation: Effect of Exclusion Restrictions',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex3_collinearity_heatmap.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Key observation: Without exclusion restrictions, the IMR row/column')\n",
    "print('shows higher correlations with the outcome regressors, indicating')\n",
    "print('that the IMR is nearly collinear with X.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute condition numbers\n",
    "# The condition number measures how close to singular a matrix is.\n",
    "# Higher condition number = more collinearity = less numerically stable.\n",
    "\n",
    "# Augmented design matrix WITH exclusion (selected sample only)\n",
    "X_sel = X_cw[selected_mask]\n",
    "X_aug_with = np.column_stack([X_sel, imr_with])\n",
    "X_aug_without = np.column_stack([X_sel, imr_without])\n",
    "\n",
    "# Condition numbers\n",
    "cond_X = np.linalg.cond(X_sel)\n",
    "cond_with = np.linalg.cond(X_aug_with)\n",
    "cond_without = np.linalg.cond(X_aug_without)\n",
    "\n",
    "print('Condition Number Analysis')\n",
    "print('=' * 60)\n",
    "print(f'X only (no IMR):                     {cond_X:.2f}')\n",
    "print(f'X + IMR (WITH exclusion):             {cond_with:.2f}')\n",
    "print(f'X + IMR (WITHOUT exclusion):           {cond_without:.2f}')\n",
    "print()\n",
    "print(f'Ratio (without / with): {cond_without / cond_with:.2f}x')\n",
    "print()\n",
    "print('Interpretation:')\n",
    "print('- Condition number < 30: acceptable collinearity')\n",
    "print('- Condition number 30-300: moderate collinearity')\n",
    "print('- Condition number > 300: severe collinearity')\n",
    "print()\n",
    "if cond_without > 2 * cond_with:\n",
    "    print('The model WITHOUT exclusion restrictions has substantially higher')\n",
    "    print('collinearity. Exclusion restrictions reduce the IMR-X correlation,')\n",
    "    print('improving numerical stability and estimation precision.')\n",
    "else:\n",
    "    print('Both specifications show similar collinearity levels.')\n",
    "    print('However, the model with exclusion restrictions is still preferred')\n",
    "    print('for identification and interpretability reasons.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional visualization: scatter plot of IMR vs dominant regressors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Most correlated regressor for each specification\n",
    "# Use 'ability' since it is likely the strongest predictor\n",
    "ability_sel = df_selected['ability'].values\n",
    "\n",
    "# WITH exclusion\n",
    "axes[0].scatter(ability_sel, imr_with, alpha=0.4, s=15, color='steelblue')\n",
    "z = np.polyfit(ability_sel, imr_with, 1)\n",
    "p = np.poly1d(z)\n",
    "ability_sorted = np.sort(ability_sel)\n",
    "axes[0].plot(ability_sorted, p(ability_sorted), 'r--', linewidth=2,\n",
    "             label=f'r = {np.corrcoef(ability_sel, imr_with)[0,1]:.3f}')\n",
    "axes[0].set_xlabel('Ability')\n",
    "axes[0].set_ylabel('IMR')\n",
    "axes[0].set_title('WITH Exclusion Restrictions')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# WITHOUT exclusion\n",
    "axes[1].scatter(ability_sel, imr_without, alpha=0.4, s=15, color='#D55E00')\n",
    "z2 = np.polyfit(ability_sel, imr_without, 1)\n",
    "p2 = np.poly1d(z2)\n",
    "axes[1].plot(ability_sorted, p2(ability_sorted), 'r--', linewidth=2,\n",
    "             label=f'r = {np.corrcoef(ability_sel, imr_without)[0,1]:.3f}')\n",
    "axes[1].set_xlabel('Ability')\n",
    "axes[1].set_ylabel('IMR')\n",
    "axes[1].set_title('WITHOUT Exclusion Restrictions')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('IMR vs Ability: Collinearity with and without Exclusion Restrictions',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex3_imr_vs_ability.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Without exclusion restrictions, the IMR is almost perfectly determined')\n",
    "print('by the outcome regressors (ability, parent_education, etc.), since the')\n",
    "print('selection equation uses the same variables. This makes the IMR nearly')\n",
    "print('collinear with X, inflating standard errors and destabilizing estimates.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 -- Discussion\n",
    "\n",
    "**Why exclusion restrictions reduce collinearity**:\n",
    "\n",
    "Without exclusion restrictions, the selection equation and outcome equation use\n",
    "the same regressors. The IMR is then $\\lambda_i = \\phi(X_i'\\hat{\\gamma}) / \\Phi(X_i'\\hat{\\gamma})$,\n",
    "which is a smooth nonlinear function of $X_i'\\hat{\\gamma}$. Over the range of\n",
    "typical data, this function is approximately linear, so $\\lambda_i \\approx a + b \\cdot X_i'\\hat{\\gamma}$.\n",
    "Since $X_i'\\hat{\\gamma}$ is a linear combination of the same regressors in the\n",
    "outcome equation, $\\lambda_i$ is nearly collinear with $X_i$.\n",
    "\n",
    "With exclusion restrictions, the selection equation includes additional variables\n",
    "(e.g., distance_college, tuition) that are NOT in the outcome equation. This\n",
    "means $Z_i'\\hat{\\gamma}$ varies independently of $X_i$, breaking the approximate\n",
    "linear dependence between $\\lambda_i$ and $X_i$.\n",
    "\n",
    "**Practical consequences of high collinearity**:\n",
    "- Standard errors of the outcome coefficients are inflated\n",
    "- Coefficient estimates become unstable (sensitive to small data changes)\n",
    "- The selection correction parameter is poorly estimated\n",
    "- The model is \"identified\" only through the nonlinearity of the normal CDF,\n",
    "  which is a very fragile form of identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 4: Monte Carlo Simulation (Advanced)\n",
    "\n",
    "We conduct a Monte Carlo experiment to demonstrate that:\n",
    "1. OLS on the selected sample is **biased** when selection is present\n",
    "2. The Heckman two-step estimator is **consistent** with proper exclusion restrictions\n",
    "3. Without exclusion restrictions, the Heckman estimator is imprecise\n",
    "\n",
    "**Data Generating Process**:\n",
    "- Selection: $s_i^* = \\gamma_0 + \\gamma_1 x_i + \\gamma_2 z_i + u_i$, $s_i = \\mathbf{1}[s_i^* > 0]$\n",
    "- Outcome: $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ (observed only if $s_i = 1$)\n",
    "- $(u_i, \\varepsilon_i) \\sim N(0, \\Sigma)$ with $\\rho = 0.5$\n",
    "- $z_i$ is the exclusion restriction (affects selection but not outcome)\n",
    "\n",
    "**Design**: 200 replications, N=500 per replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo setup\n",
    "n_sims = 200          # Number of replications\n",
    "n_obs = 500           # Observations per replication\n",
    "\n",
    "# True parameters\n",
    "beta_true = np.array([1.0, 0.5])    # Outcome: y = 1.0 + 0.5*x + eps\n",
    "gamma_true = np.array([-0.5, 0.3, 0.6])  # Selection: s* = -0.5 + 0.3*x + 0.6*z + u\n",
    "rho_true = 0.5                       # Error correlation\n",
    "sigma_eps_true = 1.0                 # Outcome error std dev\n",
    "\n",
    "# Covariance matrix for bivariate normal errors\n",
    "# u ~ N(0,1) and eps ~ N(0, sigma_eps^2) with correlation rho\n",
    "Sigma = np.array([\n",
    "    [1.0,                         rho_true * sigma_eps_true],\n",
    "    [rho_true * sigma_eps_true,   sigma_eps_true**2]\n",
    "])\n",
    "\n",
    "print('Monte Carlo Design')\n",
    "print('=' * 60)\n",
    "print(f'Replications:  {n_sims}')\n",
    "print(f'Sample size:   {n_obs}')\n",
    "print(f'True beta:     {beta_true}')\n",
    "print(f'True gamma:    {gamma_true}')\n",
    "print(f'True rho:      {rho_true}')\n",
    "print(f'True sigma:    {sigma_eps_true}')\n",
    "print()\n",
    "print(f'Error covariance matrix:')\n",
    "print(f'  [[{Sigma[0,0]:.2f}, {Sigma[0,1]:.2f}]')\n",
    "print(f'   [{Sigma[1,0]:.2f}, {Sigma[1,1]:.2f}]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Monte Carlo simulation\n",
    "np.random.seed(42)\n",
    "\n",
    "# Storage for results\n",
    "beta1_ols = np.zeros(n_sims)           # OLS estimate of beta_1\n",
    "beta1_heckman_with = np.zeros(n_sims)  # Heckman WITH exclusion restriction\n",
    "beta1_heckman_no = np.zeros(n_sims)    # Heckman WITHOUT exclusion restriction\n",
    "rho_heckman_with = np.zeros(n_sims)    # Estimated rho (with excl)\n",
    "rho_heckman_no = np.zeros(n_sims)      # Estimated rho (without excl)\n",
    "sel_rate = np.zeros(n_sims)            # Selection rate per replication\n",
    "\n",
    "for sim in range(n_sims):\n",
    "    if (sim + 1) % 50 == 0:\n",
    "        print(f'  Replication {sim + 1}/{n_sims}...')\n",
    "    \n",
    "    # Generate regressors\n",
    "    x = np.random.normal(0, 1, n_obs)   # Outcome regressor\n",
    "    z = np.random.normal(0, 1, n_obs)   # Exclusion restriction (instrument)\n",
    "    \n",
    "    # Generate correlated errors\n",
    "    errors = np.random.multivariate_normal([0, 0], Sigma, n_obs)\n",
    "    u = errors[:, 0]    # Selection error\n",
    "    eps = errors[:, 1]  # Outcome error\n",
    "    \n",
    "    # Selection equation\n",
    "    s_star = gamma_true[0] + gamma_true[1] * x + gamma_true[2] * z + u\n",
    "    s = (s_star > 0).astype(float)\n",
    "    sel_rate[sim] = s.mean()\n",
    "    \n",
    "    # Outcome equation (latent for all, observed only if s=1)\n",
    "    y_latent = beta_true[0] + beta_true[1] * x + eps\n",
    "    y_observed = np.where(s == 1, y_latent, 0)\n",
    "    \n",
    "    # Skip if too few selected or too few censored\n",
    "    if s.sum() < 30 or (1 - s).sum() < 10:\n",
    "        beta1_ols[sim] = np.nan\n",
    "        beta1_heckman_with[sim] = np.nan\n",
    "        beta1_heckman_no[sim] = np.nan\n",
    "        rho_heckman_with[sim] = np.nan\n",
    "        rho_heckman_no[sim] = np.nan\n",
    "        continue\n",
    "    \n",
    "    # --- OLS on selected sample (biased) ---\n",
    "    sel_mask = s == 1\n",
    "    X_sel_ols = sm.add_constant(x[sel_mask])\n",
    "    ols_result = np.linalg.lstsq(X_sel_ols, y_latent[sel_mask], rcond=None)[0]\n",
    "    beta1_ols[sim] = ols_result[1]\n",
    "    \n",
    "    # --- Heckman WITH exclusion restriction ---\n",
    "    X_out = sm.add_constant(x.reshape(-1, 1))\n",
    "    Z_sel_with = sm.add_constant(np.column_stack([x, z]))\n",
    "    \n",
    "    try:\n",
    "        model_w = PanelHeckman(\n",
    "            endog=y_observed,\n",
    "            exog=X_out,\n",
    "            selection=s,\n",
    "            exog_selection=Z_sel_with,\n",
    "            method='two_step'\n",
    "        )\n",
    "        res_w = model_w.fit()\n",
    "        beta1_heckman_with[sim] = res_w.outcome_params[1]\n",
    "        rho_heckman_with[sim] = res_w.rho\n",
    "    except Exception:\n",
    "        beta1_heckman_with[sim] = np.nan\n",
    "        rho_heckman_with[sim] = np.nan\n",
    "    \n",
    "    # --- Heckman WITHOUT exclusion restriction ---\n",
    "    Z_sel_no = sm.add_constant(x.reshape(-1, 1))  # Same as X (no instrument)\n",
    "    \n",
    "    try:\n",
    "        model_no = PanelHeckman(\n",
    "            endog=y_observed,\n",
    "            exog=X_out,\n",
    "            selection=s,\n",
    "            exog_selection=Z_sel_no,\n",
    "            method='two_step'\n",
    "        )\n",
    "        res_no = model_no.fit()\n",
    "        beta1_heckman_no[sim] = res_no.outcome_params[1]\n",
    "        rho_heckman_no[sim] = res_no.rho\n",
    "    except Exception:\n",
    "        beta1_heckman_no[sim] = np.nan\n",
    "        rho_heckman_no[sim] = np.nan\n",
    "\n",
    "print(f'\\nSimulation complete.')\n",
    "print(f'Average selection rate: {np.nanmean(sel_rate):.1%}')\n",
    "print(f'Valid replications: OLS={np.isfinite(beta1_ols).sum()}, '\n",
    "      f'Heckman(with)={np.isfinite(beta1_heckman_with).sum()}, '\n",
    "      f'Heckman(no)={np.isfinite(beta1_heckman_no).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of the Monte Carlo experiment\n",
    "def mc_summary(estimates, true_value, name):\n",
    "    \"\"\"Compute Monte Carlo summary statistics.\"\"\"\n",
    "    valid = estimates[np.isfinite(estimates)]\n",
    "    bias = np.mean(valid) - true_value\n",
    "    rmse = np.sqrt(np.mean((valid - true_value)**2))\n",
    "    return {\n",
    "        'Estimator': name,\n",
    "        'True Value': true_value,\n",
    "        'Mean': np.mean(valid),\n",
    "        'Median': np.median(valid),\n",
    "        'Std Dev': np.std(valid),\n",
    "        'Bias': bias,\n",
    "        'RMSE': rmse,\n",
    "        'Valid Reps': len(valid),\n",
    "    }\n",
    "\n",
    "mc_results = pd.DataFrame([\n",
    "    mc_summary(beta1_ols, beta_true[1], 'OLS (selected sample)'),\n",
    "    mc_summary(beta1_heckman_with, beta_true[1], 'Heckman (with excl.)'),\n",
    "    mc_summary(beta1_heckman_no, beta_true[1], 'Heckman (no excl.)'),\n",
    "])\n",
    "\n",
    "print('Monte Carlo Results for beta_1 (true value = 0.5)')\n",
    "print('=' * 80)\n",
    "display(mc_results.round(4))\n",
    "\n",
    "print()\n",
    "print('Key findings:')\n",
    "print(f'  OLS bias:              {mc_results.iloc[0][\"Bias\"]:.4f} '\n",
    "      f'({mc_results.iloc[0][\"Bias\"]/beta_true[1]*100:.1f}% of true value)')\n",
    "print(f'  Heckman (with) bias:   {mc_results.iloc[1][\"Bias\"]:.4f} '\n",
    "      f'({mc_results.iloc[1][\"Bias\"]/beta_true[1]*100:.1f}% of true value)')\n",
    "print(f'  Heckman (no) bias:     {mc_results.iloc[2][\"Bias\"]:.4f} '\n",
    "      f'({mc_results.iloc[2][\"Bias\"]/beta_true[1]*100:.1f}% of true value)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rho estimation summary\n",
    "rho_results = pd.DataFrame([\n",
    "    mc_summary(rho_heckman_with, rho_true, 'Heckman (with excl.)'),\n",
    "    mc_summary(rho_heckman_no, rho_true, 'Heckman (no excl.)'),\n",
    "])\n",
    "\n",
    "print('Monte Carlo Results for rho (true value = 0.5)')\n",
    "print('=' * 80)\n",
    "display(rho_results.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sampling distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top-left: Histogram of beta_1 estimates\n",
    "bins = np.linspace(-0.5, 1.5, 50)\n",
    "\n",
    "valid_ols = beta1_ols[np.isfinite(beta1_ols)]\n",
    "valid_hw = beta1_heckman_with[np.isfinite(beta1_heckman_with)]\n",
    "valid_hn = beta1_heckman_no[np.isfinite(beta1_heckman_no)]\n",
    "\n",
    "axes[0, 0].hist(valid_ols, bins=bins, alpha=0.5, density=True,\n",
    "                color='#D55E00', label='OLS', edgecolor='black')\n",
    "axes[0, 0].hist(valid_hw, bins=bins, alpha=0.5, density=True,\n",
    "                color='steelblue', label='Heckman (with excl.)', edgecolor='black')\n",
    "axes[0, 0].axvline(beta_true[1], color='black', linewidth=2.5,\n",
    "                   linestyle='--', label=f'True value = {beta_true[1]}')\n",
    "axes[0, 0].axvline(np.mean(valid_ols), color='#D55E00', linewidth=1.5,\n",
    "                   linestyle=':', label=f'OLS mean = {np.mean(valid_ols):.3f}')\n",
    "axes[0, 0].axvline(np.mean(valid_hw), color='steelblue', linewidth=1.5,\n",
    "                   linestyle=':', label=f'Heckman mean = {np.mean(valid_hw):.3f}')\n",
    "axes[0, 0].set_xlabel(r'$\\hat{\\beta}_1$')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title(r'Sampling Distribution of $\\hat{\\beta}_1$')\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Top-right: Heckman with vs without exclusion\n",
    "axes[0, 1].hist(valid_hw, bins=bins, alpha=0.5, density=True,\n",
    "                color='steelblue', label='With exclusion', edgecolor='black')\n",
    "axes[0, 1].hist(valid_hn, bins=bins, alpha=0.5, density=True,\n",
    "                color='#009E73', label='Without exclusion', edgecolor='black')\n",
    "axes[0, 1].axvline(beta_true[1], color='black', linewidth=2.5,\n",
    "                   linestyle='--', label=f'True value = {beta_true[1]}')\n",
    "axes[0, 1].set_xlabel(r'$\\hat{\\beta}_1$')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('With vs Without Exclusion Restriction')\n",
    "axes[0, 1].legend(fontsize=9)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Bottom-left: Rho estimates\n",
    "valid_rho_w = rho_heckman_with[np.isfinite(rho_heckman_with)]\n",
    "valid_rho_n = rho_heckman_no[np.isfinite(rho_heckman_no)]\n",
    "\n",
    "rho_bins = np.linspace(-1, 1, 50)\n",
    "axes[1, 0].hist(valid_rho_w, bins=rho_bins, alpha=0.5, density=True,\n",
    "                color='steelblue', label='With exclusion', edgecolor='black')\n",
    "axes[1, 0].hist(valid_rho_n, bins=rho_bins, alpha=0.5, density=True,\n",
    "                color='#009E73', label='Without exclusion', edgecolor='black')\n",
    "axes[1, 0].axvline(rho_true, color='black', linewidth=2.5,\n",
    "                   linestyle='--', label=f'True rho = {rho_true}')\n",
    "axes[1, 0].set_xlabel(r'$\\hat{\\rho}$')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_title(r'Sampling Distribution of $\\hat{\\rho}$')\n",
    "axes[1, 0].legend(fontsize=9)\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Bottom-right: Bias boxplot\n",
    "bias_data = [\n",
    "    valid_ols - beta_true[1],\n",
    "    valid_hw - beta_true[1],\n",
    "    valid_hn - beta_true[1],\n",
    "]\n",
    "bp = axes[1, 1].boxplot(\n",
    "    bias_data,\n",
    "    labels=['OLS', 'Heckman\\n(with excl.)', 'Heckman\\n(no excl.)'],\n",
    "    patch_artist=True,\n",
    "    widths=0.5,\n",
    ")\n",
    "colors_box = ['#D55E00', 'steelblue', '#009E73']\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "axes[1, 1].axhline(y=0, color='black', linewidth=1.5, linestyle='--')\n",
    "axes[1, 1].set_ylabel(r'Bias ($\\hat{\\beta}_1 - \\beta_1$)')\n",
    "axes[1, 1].set_title('Estimation Bias Distribution')\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(f'Monte Carlo Simulation: {n_sims} Replications, N={n_obs}, '\n",
    "             f'rho={rho_true}',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex4_monte_carlo_results.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formal comparison: bias, variance, and MSE\n",
    "print('Formal Comparison: Bias, Variance, and MSE')\n",
    "print('=' * 70)\n",
    "print(f'{\"\":30s} {\"Bias\":>10s} {\"Variance\":>10s} {\"MSE\":>10s} {\"Coverage\":>10s}')\n",
    "print('-' * 70)\n",
    "\n",
    "for name, est in [('OLS (selected)', valid_ols),\n",
    "                   ('Heckman (with excl.)', valid_hw),\n",
    "                   ('Heckman (no excl.)', valid_hn)]:\n",
    "    bias = np.mean(est) - beta_true[1]\n",
    "    var = np.var(est)\n",
    "    mse = np.mean((est - beta_true[1])**2)\n",
    "    # Approximate 95% coverage\n",
    "    se = np.std(est) / np.sqrt(len(est))\n",
    "    ci_lower = est - 1.96 * np.std(est)\n",
    "    ci_upper = est + 1.96 * np.std(est)\n",
    "    coverage = np.mean((ci_lower <= beta_true[1]) & (beta_true[1] <= ci_upper))\n",
    "    \n",
    "    print(f'{name:30s} {bias:10.4f} {var:10.4f} {mse:10.4f} {coverage:10.1%}')\n",
    "\n",
    "print()\n",
    "print('MSE = Bias^2 + Variance')\n",
    "print('Coverage = fraction of replications where true value is within 95% CI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: how does bias change with rho?\n",
    "# Quick demonstration with a few values of rho\n",
    "\n",
    "rho_values = [0.0, 0.25, 0.5, 0.75]\n",
    "n_sims_quick = 100\n",
    "n_obs_quick = 500\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "bias_by_rho = {'rho': [], 'OLS_bias': [], 'Heckman_bias': []}\n",
    "\n",
    "for rho_val in rho_values:\n",
    "    print(f'Running rho = {rho_val}...')\n",
    "    Sigma_val = np.array([\n",
    "        [1.0, rho_val * sigma_eps_true],\n",
    "        [rho_val * sigma_eps_true, sigma_eps_true**2]\n",
    "    ])\n",
    "    \n",
    "    ols_ests = []\n",
    "    heck_ests = []\n",
    "    \n",
    "    for _ in range(n_sims_quick):\n",
    "        x_q = np.random.normal(0, 1, n_obs_quick)\n",
    "        z_q = np.random.normal(0, 1, n_obs_quick)\n",
    "        errs = np.random.multivariate_normal([0, 0], Sigma_val, n_obs_quick)\n",
    "        \n",
    "        s_star_q = gamma_true[0] + gamma_true[1] * x_q + gamma_true[2] * z_q + errs[:, 0]\n",
    "        s_q = (s_star_q > 0).astype(float)\n",
    "        y_q = beta_true[0] + beta_true[1] * x_q + errs[:, 1]\n",
    "        y_obs_q = np.where(s_q == 1, y_q, 0)\n",
    "        \n",
    "        if s_q.sum() < 30 or (1 - s_q).sum() < 10:\n",
    "            continue\n",
    "        \n",
    "        # OLS\n",
    "        X_q = sm.add_constant(x_q[s_q == 1])\n",
    "        b_ols = np.linalg.lstsq(X_q, y_q[s_q == 1], rcond=None)[0]\n",
    "        ols_ests.append(b_ols[1])\n",
    "        \n",
    "        # Heckman\n",
    "        try:\n",
    "            model_q = PanelHeckman(\n",
    "                endog=y_obs_q,\n",
    "                exog=sm.add_constant(x_q.reshape(-1, 1)),\n",
    "                selection=s_q,\n",
    "                exog_selection=sm.add_constant(np.column_stack([x_q, z_q])),\n",
    "                method='two_step'\n",
    "            )\n",
    "            res_q = model_q.fit()\n",
    "            heck_ests.append(res_q.outcome_params[1])\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    bias_by_rho['rho'].append(rho_val)\n",
    "    bias_by_rho['OLS_bias'].append(np.mean(ols_ests) - beta_true[1])\n",
    "    bias_by_rho['Heckman_bias'].append(np.mean(heck_ests) - beta_true[1])\n",
    "\n",
    "bias_df = pd.DataFrame(bias_by_rho)\n",
    "\n",
    "print('\\nBias as a Function of rho')\n",
    "print('=' * 50)\n",
    "display(bias_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bias vs rho\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax.plot(bias_df['rho'], bias_df['OLS_bias'], 'o-',\n",
    "        color='#D55E00', linewidth=2, markersize=8, label='OLS (biased)')\n",
    "ax.plot(bias_df['rho'], bias_df['Heckman_bias'], 's-',\n",
    "        color='steelblue', linewidth=2, markersize=8, label='Heckman (corrected)')\n",
    "ax.axhline(y=0, color='black', linewidth=1, linestyle='--', alpha=0.7)\n",
    "ax.set_xlabel(r'True $\\rho$ (selection-outcome error correlation)', fontsize=12)\n",
    "ax.set_ylabel(r'Bias in $\\hat{\\beta}_1$', fontsize=12)\n",
    "ax.set_title(r'Selection Bias Increases with $\\rho$', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex4_bias_vs_rho.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('As expected:')\n",
    "print('- When rho = 0, there is no selection bias and OLS is unbiased')\n",
    "print('- As rho increases, OLS bias grows monotonically')\n",
    "print('- The Heckman estimator remains approximately unbiased for all rho values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 -- Discussion\n",
    "\n",
    "**Key findings from the Monte Carlo experiment**:\n",
    "\n",
    "1. **OLS is biased**: When $\\rho \\neq 0$, OLS on the selected sample produces\n",
    "   estimates that are systematically different from the true value. The bias\n",
    "   grows with $|\\rho|$.\n",
    "\n",
    "2. **Heckman with exclusion restriction is consistent**: The Heckman two-step\n",
    "   estimator with a proper exclusion restriction ($z_i$) produces estimates\n",
    "   centered around the true value with reasonable variance.\n",
    "\n",
    "3. **Heckman without exclusion restriction is imprecise**: When the selection\n",
    "   equation uses the same variables as the outcome equation (no exclusion\n",
    "   restriction), the estimator may be approximately unbiased but has much\n",
    "   larger variance due to collinearity between the IMR and the regressors.\n",
    "\n",
    "4. **Bias-variance tradeoff**: The Heckman estimator trades some variance\n",
    "   (wider sampling distribution) for bias reduction. This is the typical\n",
    "   econometric tradeoff when correcting for endogeneity/selection.\n",
    "\n",
    "5. **Rho determines the severity**: When $\\rho = 0$, there is no selection bias\n",
    "   and all three estimators perform similarly. The Heckman correction only\n",
    "   matters when $\\rho$ is substantially different from zero.\n",
    "\n",
    "**Practical implications**:\n",
    "- Always test for selection bias before deciding whether to use the Heckman\n",
    "  correction\n",
    "- Invest effort in finding credible exclusion restrictions; they dramatically\n",
    "  improve estimation precision\n",
    "- The Heckman estimator without exclusion restrictions relies on functional\n",
    "  form alone and should be used only as a robustness check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "This solution notebook covered four exercises on the Heckman two-step selection\n",
    "model:\n",
    "\n",
    "| Exercise | Key Takeaway |\n",
    "|----------|-------------|\n",
    "| 1. Evaluate instruments | Relevance and validity must both hold; SAT scores fail validity |\n",
    "| 2. Compare specifications | Strong exclusion restrictions (children_lt6) yield stable estimates |\n",
    "| 3. Collinearity diagnostic | Without exclusion restrictions, IMR is collinear with X |\n",
    "| 4. Monte Carlo | OLS is biased with selection; Heckman with instruments is consistent |\n",
    "\n",
    "**Central theme**: The Heckman model corrects for selection bias, but its\n",
    "performance depends critically on having credible exclusion restrictions.\n",
    "Without them, the model is weakly identified and estimates are imprecise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
