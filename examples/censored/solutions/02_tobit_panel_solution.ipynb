{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Random Effects Tobit for Panel Data -- SOLUTIONS\n",
    "\n",
    "**This is the worked solution notebook.**  \n",
    "It provides complete, working solutions for all 4 exercises from `02_tobit_panel.ipynb`.\n",
    "\n",
    "> Instructors: do not distribute this file to students before they complete the tutorial notebook.\n",
    "\n",
    "## Exercise Overview\n",
    "\n",
    "| Exercise | Topic | Key Concept |\n",
    "|----------|-------|-------------|\n",
    "| 1 | Quadrature Sensitivity | Gauss-Hermite quadrature point selection |\n",
    "| 2 | Subsample Analysis by Gender | Heterogeneity in model parameters |\n",
    "| 3 | Marginal Effects | Unconditional vs conditional effects |\n",
    "| 4 | Prediction Performance | Out-of-sample evaluation, Pooled vs RE |"
   ],
   "id": "854f87d5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ],
   "id": "186b6dda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# PanelBox imports\n",
    "from panelbox.models.censored import PooledTobit, RandomEffectsTobit\n",
    "\n",
    "# Visualization configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures' / '02_tobit_panel'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables' / '02_tobit_panel'\n",
    "\n",
    "# Create output directories\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add utils to path for data generation fallback\n",
    "sys.path.insert(0, str(BASE_DIR / 'utils'))\n",
    "\n",
    "print('Setup complete!')"
   ],
   "id": "3f032e5f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Loading"
   ],
   "id": "df84b8b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (or generate if CSV not found)\n",
    "data_path = DATA_DIR / 'health_expenditure_panel.csv'\n",
    "\n",
    "if data_path.exists():\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f'Loaded data from {data_path}')\n",
    "else:\n",
    "    from data_generation import generate_health_panel\n",
    "    df = generate_health_panel(n=500, t=4, seed=42)\n",
    "    df.to_csv(data_path, index=False)\n",
    "    print(f'Generated and saved data to {data_path}')\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'N individuals: {df[\"id\"].nunique()}')\n",
    "print(f'T periods: {df[\"time\"].nunique()} ({df[\"time\"].min()}-{df[\"time\"].max()})')\n",
    "print(f'Censoring rate: {(df[\"expenditure\"] == 0).mean() * 100:.1f}%')\n",
    "print()\n",
    "display(df.head(8))"
   ],
   "id": "1ae19cc2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Preparation and Base Model\n",
    "\n",
    "Before tackling the exercises, we prepare the estimation data and fit a baseline\n",
    "Random Effects Tobit model that will serve as the reference for all exercises."
   ],
   "id": "89eb0fbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for estimation\n",
    "y = df['expenditure'].values\n",
    "\n",
    "# Explanatory variables (with constant)\n",
    "X_vars = df[['income', 'age', 'chronic', 'insurance', 'female', 'bmi']].values\n",
    "X = sm.add_constant(X_vars)\n",
    "\n",
    "var_names = ['const', 'income', 'age', 'chronic', 'insurance', 'female', 'bmi']\n",
    "\n",
    "# Panel identifiers\n",
    "groups = df['id'].values\n",
    "time = df['time'].values\n",
    "\n",
    "print('Data preparation:')\n",
    "print(f'  y shape:       {y.shape}')\n",
    "print(f'  X shape:       {X.shape}')\n",
    "print(f'  N individuals: {len(np.unique(groups))}')\n",
    "print(f'  T periods:     {len(np.unique(time))}')\n",
    "print(f'  Variables:     {var_names}')"
   ],
   "id": "aace6452"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fit base RE Tobit model (Q=12, full sample)\n",
    "# This serves as the reference for all exercises.\n",
    "# ============================================================\n",
    "\n",
    "print('Fitting base Random Effects Tobit (Q=12, full sample)...')\n",
    "print('=' * 60)\n",
    "\n",
    "re_base = RandomEffectsTobit(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    groups=groups,\n",
    "    time=time,\n",
    "    censoring_point=0.0,\n",
    "    censoring_type='left',\n",
    "    quadrature_points=12\n",
    ")\n",
    "re_base.fit(method='BFGS', maxiter=1000)\n",
    "\n",
    "print()\n",
    "print(re_base.summary())\n",
    "\n",
    "# Variance decomposition\n",
    "rho_base = re_base.sigma_alpha**2 / (re_base.sigma_alpha**2 + re_base.sigma_eps**2)\n",
    "print(f'\\nIntra-class correlation (rho): {rho_base:.4f}')"
   ],
   "id": "9f91e910"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Quadrature Sensitivity\n",
    "\n",
    "**Task**: Re-estimate the Random Effects Tobit model with different numbers of\n",
    "Gauss-Hermite quadrature points: **6, 12, and 24**. Compare the estimated\n",
    "coefficients, $\\sigma_\\alpha$, $\\sigma_\\varepsilon$, and log-likelihood.\n",
    "At what point do the results stabilize?\n",
    "\n",
    "### Background\n",
    "\n",
    "Gauss-Hermite quadrature approximates the integral over the random effect:\n",
    "\n",
    "$$L_i = \\int_{-\\infty}^{\\infty} \\prod_{t=1}^{T_i} f(y_{it} | X_{it}, \\alpha_i) \\, \\phi(\\alpha_i / \\sigma_\\alpha) \\, d\\alpha_i\n",
    "\\approx \\sum_{q=1}^{Q} w_q \\prod_{t=1}^{T_i} f(y_{it} | X_{it}, \\sqrt{2}\\sigma_\\alpha \\cdot n_q)$$\n",
    "\n",
    "More quadrature points ($Q$) mean a more accurate integral approximation, but\n",
    "at the cost of computation time. In practice, we need enough points so that\n",
    "the estimates have converged -- adding more points should not materially\n",
    "change the results."
   ],
   "id": "a8ee507d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1 Solution: Quadrature Sensitivity Analysis\n",
    "# ============================================================\n",
    "\n",
    "quad_points_list = [6, 12, 24]\n",
    "quad_results = {}\n",
    "\n",
    "for nq in quad_points_list:\n",
    "    print(f'\\nFitting RE Tobit with Q = {nq} quadrature points...')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    model_q = RandomEffectsTobit(\n",
    "        endog=y,\n",
    "        exog=X,\n",
    "        groups=groups,\n",
    "        time=time,\n",
    "        censoring_point=0.0,\n",
    "        censoring_type='left',\n",
    "        quadrature_points=nq\n",
    "    )\n",
    "    model_q.fit(method='BFGS', maxiter=1000)\n",
    "    \n",
    "    rho_q = model_q.sigma_alpha**2 / (model_q.sigma_alpha**2 + model_q.sigma_eps**2)\n",
    "    \n",
    "    quad_results[nq] = {\n",
    "        'model': model_q,\n",
    "        'beta': model_q.beta.copy(),\n",
    "        'sigma_eps': model_q.sigma_eps,\n",
    "        'sigma_alpha': model_q.sigma_alpha,\n",
    "        'llf': model_q.llf,\n",
    "        'rho': rho_q,\n",
    "        'converged': model_q.converged\n",
    "    }\n",
    "    \n",
    "    print(f'  Log-likelihood: {model_q.llf:.4f}')\n",
    "    print(f'  sigma_eps:      {model_q.sigma_eps:.4f}')\n",
    "    print(f'  sigma_alpha:    {model_q.sigma_alpha:.4f}')\n",
    "    print(f'  rho (ICC):      {rho_q:.4f}')\n",
    "    print(f'  Converged:      {model_q.converged}')\n",
    "\n",
    "print('\\nAll models fitted successfully.')"
   ],
   "id": "0dce07ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Comparison table: coefficients across quadrature specifications\n",
    "# ============================================================\n",
    "\n",
    "K = len(var_names)\n",
    "\n",
    "# Build comparison DataFrame\n",
    "rows = []\n",
    "for nq in quad_points_list:\n",
    "    r = quad_results[nq]\n",
    "    row = {'Q': nq}\n",
    "    for i, vname in enumerate(var_names):\n",
    "        row[vname] = r['beta'][i]\n",
    "    row['sigma_eps'] = r['sigma_eps']\n",
    "    row['sigma_alpha'] = r['sigma_alpha']\n",
    "    row['rho'] = r['rho']\n",
    "    row['log_lik'] = r['llf']\n",
    "    row['converged'] = r['converged']\n",
    "    rows.append(row)\n",
    "\n",
    "quad_comparison = pd.DataFrame(rows).set_index('Q')\n",
    "\n",
    "print('Quadrature Sensitivity: Coefficient Comparison')\n",
    "print('=' * 90)\n",
    "display(quad_comparison.round(4))"
   ],
   "id": "ee8d04c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Compute absolute and relative differences (Q=6 vs Q=12, Q=12 vs Q=24)\n",
    "# ============================================================\n",
    "\n",
    "compare_cols = var_names + ['sigma_eps', 'sigma_alpha', 'rho', 'log_lik']\n",
    "\n",
    "# Differences between Q=6 and Q=12\n",
    "diff_6_12 = quad_comparison.loc[12, compare_cols] - quad_comparison.loc[6, compare_cols]\n",
    "rel_diff_6_12 = (diff_6_12 / quad_comparison.loc[6, compare_cols].abs()).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Differences between Q=12 and Q=24\n",
    "diff_12_24 = quad_comparison.loc[24, compare_cols] - quad_comparison.loc[12, compare_cols]\n",
    "rel_diff_12_24 = (diff_12_24 / quad_comparison.loc[12, compare_cols].abs()).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "diff_table = pd.DataFrame({\n",
    "    'Q=6 value': quad_comparison.loc[6, compare_cols],\n",
    "    'Q=12 value': quad_comparison.loc[12, compare_cols],\n",
    "    'Q=24 value': quad_comparison.loc[24, compare_cols],\n",
    "    'Diff (6->12)': diff_6_12,\n",
    "    '% Change (6->12)': rel_diff_6_12 * 100,\n",
    "    'Diff (12->24)': diff_12_24,\n",
    "    '% Change (12->24)': rel_diff_12_24 * 100\n",
    "})\n",
    "\n",
    "print('Quadrature Sensitivity: Change Analysis')\n",
    "print('=' * 100)\n",
    "display(diff_table.round(4))"
   ],
   "id": "cbdba7eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization: Coefficient stability across Q\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Coefficient values by Q (slope coefficients only)\n",
    "ax = axes[0, 0]\n",
    "slope_vars = var_names[1:]  # Exclude constant\n",
    "for vname in slope_vars:\n",
    "    vals = [quad_results[nq]['beta'][var_names.index(vname)] for nq in quad_points_list]\n",
    "    ax.plot(quad_points_list, vals, marker='o', linewidth=2, markersize=8, label=vname)\n",
    "ax.set_xlabel('Number of Quadrature Points (Q)', fontsize=12)\n",
    "ax.set_ylabel('Coefficient Value', fontsize=12)\n",
    "ax.set_title('Coefficient Stability Across Q', fontsize=13)\n",
    "ax.set_xticks(quad_points_list)\n",
    "ax.legend(fontsize=9, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Variance components by Q\n",
    "ax = axes[0, 1]\n",
    "sigma_eps_vals = [quad_results[nq]['sigma_eps'] for nq in quad_points_list]\n",
    "sigma_alpha_vals = [quad_results[nq]['sigma_alpha'] for nq in quad_points_list]\n",
    "ax.plot(quad_points_list, sigma_eps_vals, marker='s', linewidth=2.5,\n",
    "        markersize=10, color='steelblue', label=r'$\\sigma_\\varepsilon$')\n",
    "ax.plot(quad_points_list, sigma_alpha_vals, marker='^', linewidth=2.5,\n",
    "        markersize=10, color='coral', label=r'$\\sigma_\\alpha$')\n",
    "ax.set_xlabel('Number of Quadrature Points (Q)', fontsize=12)\n",
    "ax.set_ylabel('Standard Deviation', fontsize=12)\n",
    "ax.set_title('Variance Component Stability', fontsize=13)\n",
    "ax.set_xticks(quad_points_list)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Log-likelihood by Q\n",
    "ax = axes[1, 0]\n",
    "llf_vals = [quad_results[nq]['llf'] for nq in quad_points_list]\n",
    "ax.plot(quad_points_list, llf_vals, marker='D', linewidth=2.5,\n",
    "        markersize=10, color='seagreen')\n",
    "for nq, llf_val in zip(quad_points_list, llf_vals):\n",
    "    ax.annotate(f'{llf_val:.2f}', (nq, llf_val), textcoords='offset points',\n",
    "                xytext=(0, 12), ha='center', fontsize=10, fontweight='bold')\n",
    "ax.set_xlabel('Number of Quadrature Points (Q)', fontsize=12)\n",
    "ax.set_ylabel('Log-Likelihood', fontsize=12)\n",
    "ax.set_title('Log-Likelihood Convergence', fontsize=13)\n",
    "ax.set_xticks(quad_points_list)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. ICC (rho) by Q\n",
    "ax = axes[1, 1]\n",
    "rho_vals = [quad_results[nq]['rho'] for nq in quad_points_list]\n",
    "ax.plot(quad_points_list, rho_vals, marker='o', linewidth=2.5,\n",
    "        markersize=10, color='purple')\n",
    "for nq, rho_val in zip(quad_points_list, rho_vals):\n",
    "    ax.annotate(f'{rho_val:.4f}', (nq, rho_val), textcoords='offset points',\n",
    "                xytext=(0, 12), ha='center', fontsize=10, fontweight='bold')\n",
    "ax.set_xlabel('Number of Quadrature Points (Q)', fontsize=12)\n",
    "ax.set_ylabel(r'Intra-class Correlation ($\\rho$)', fontsize=12)\n",
    "ax.set_title('ICC Stability', fontsize=13)\n",
    "ax.set_xticks(quad_points_list)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex1_quadrature_sensitivity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "655b21b4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Discussion\n",
    "\n",
    "**Key findings from the quadrature sensitivity analysis:**\n",
    "\n",
    "1. **Coefficient stability**: The slope coefficients are generally stable across\n",
    "   all three quadrature specifications (Q=6, 12, 24). The percent changes from\n",
    "   Q=12 to Q=24 are much smaller than from Q=6 to Q=12, indicating convergence.\n",
    "\n",
    "2. **Variance components**: `sigma_eps` and `sigma_alpha` show minor sensitivity\n",
    "   to the number of quadrature points. By Q=12, the estimates are essentially\n",
    "   stabilized. The change from Q=12 to Q=24 is negligible.\n",
    "\n",
    "3. **Log-likelihood**: The log-likelihood values converge as Q increases. The\n",
    "   difference between Q=12 and Q=24 is typically tiny compared to Q=6 vs Q=12.\n",
    "\n",
    "4. **Practical recommendation**: For this dataset, Q=12 (the default) provides\n",
    "   adequate accuracy. This is consistent with the general recommendation in the\n",
    "   literature: Q=12 to Q=20 is typically sufficient for models with a single\n",
    "   random effect. Q=6 may introduce non-trivial approximation error, while\n",
    "   Q=24 offers negligible improvement over Q=12 at greater computational cost.\n",
    "\n",
    "5. **When to use more points**: Higher Q may be needed when `sigma_alpha` is large\n",
    "   relative to `sigma_eps` (strong individual effects make the integrand more\n",
    "   peaked), or with highly unbalanced panels."
   ],
   "id": "6da40b6c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Subsample Analysis by Gender\n",
    "\n",
    "**Task**: Estimate separate RE Tobit models for males and females. Compare:\n",
    "- Do the key coefficients (income, chronic conditions) differ across genders?\n",
    "- Is the intra-class correlation $\\rho$ different for males vs females?\n",
    "- What does this suggest about gender-specific health expenditure dynamics?\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Health expenditure patterns may differ systematically between men and women.\n",
    "Rather than only including a `female` dummy in the pooled specification,\n",
    "estimating separate models allows **all coefficients** to differ by gender,\n",
    "revealing structural differences in the expenditure process."
   ],
   "id": "0b75e843"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 2 Solution: Subsample Analysis by Gender\n",
    "# ============================================================\n",
    "\n",
    "# Create masks\n",
    "mask_female = df['female'].values == 1\n",
    "mask_male = df['female'].values == 0\n",
    "\n",
    "print(f'Sample sizes:')\n",
    "print(f'  Males:   {mask_male.sum()} obs from {df.loc[mask_male, \"id\"].nunique()} individuals')\n",
    "print(f'  Females: {mask_female.sum()} obs from {df.loc[mask_female, \"id\"].nunique()} individuals')\n",
    "print(f'  Censoring rate (males):   {(y[mask_male] == 0).mean() * 100:.1f}%')\n",
    "print(f'  Censoring rate (females): {(y[mask_female] == 0).mean() * 100:.1f}%')\n",
    "\n",
    "# For subsample models, exclude the 'female' variable since it's constant\n",
    "# Use: const, income, age, chronic, insurance, bmi\n",
    "X_sub_vars = df[['income', 'age', 'chronic', 'insurance', 'bmi']].values\n",
    "X_sub = sm.add_constant(X_sub_vars)\n",
    "sub_var_names = ['const', 'income', 'age', 'chronic', 'insurance', 'bmi']"
   ],
   "id": "76db8095"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fit RE Tobit for males\n",
    "# ============================================================\n",
    "\n",
    "print('Fitting RE Tobit for MALES...')\n",
    "print('=' * 60)\n",
    "\n",
    "re_male = RandomEffectsTobit(\n",
    "    endog=y[mask_male],\n",
    "    exog=X_sub[mask_male],\n",
    "    groups=groups[mask_male],\n",
    "    time=time[mask_male],\n",
    "    censoring_point=0.0,\n",
    "    censoring_type='left',\n",
    "    quadrature_points=12\n",
    ")\n",
    "re_male.fit(method='BFGS', maxiter=1000)\n",
    "\n",
    "print()\n",
    "print(re_male.summary())\n",
    "\n",
    "rho_male = re_male.sigma_alpha**2 / (re_male.sigma_alpha**2 + re_male.sigma_eps**2)\n",
    "print(f'\\nICC (males): {rho_male:.4f}')"
   ],
   "id": "8ef7af3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fit RE Tobit for females\n",
    "# ============================================================\n",
    "\n",
    "print('Fitting RE Tobit for FEMALES...')\n",
    "print('=' * 60)\n",
    "\n",
    "re_female = RandomEffectsTobit(\n",
    "    endog=y[mask_female],\n",
    "    exog=X_sub[mask_female],\n",
    "    groups=groups[mask_female],\n",
    "    time=time[mask_female],\n",
    "    censoring_point=0.0,\n",
    "    censoring_type='left',\n",
    "    quadrature_points=12\n",
    ")\n",
    "re_female.fit(method='BFGS', maxiter=1000)\n",
    "\n",
    "print()\n",
    "print(re_female.summary())\n",
    "\n",
    "rho_female = re_female.sigma_alpha**2 / (re_female.sigma_alpha**2 + re_female.sigma_eps**2)\n",
    "print(f'\\nICC (females): {rho_female:.4f}')"
   ],
   "id": "70ccca90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Side-by-side comparison table\n",
    "# ============================================================\n",
    "\n",
    "K_sub = len(sub_var_names)\n",
    "\n",
    "male_bse = re_male.bse[:K_sub]\n",
    "female_bse = re_female.bse[:K_sub]\n",
    "\n",
    "gender_comparison = pd.DataFrame({\n",
    "    'Variable': sub_var_names,\n",
    "    'Male_Coef': re_male.beta,\n",
    "    'Male_SE': male_bse,\n",
    "    'Female_Coef': re_female.beta,\n",
    "    'Female_SE': female_bse,\n",
    "    'Difference': re_female.beta - re_male.beta\n",
    "})\n",
    "\n",
    "print('Gender Subsample Comparison: RE Tobit Coefficients')\n",
    "print('=' * 85)\n",
    "display(gender_comparison.round(4))\n",
    "\n",
    "# Variance components comparison\n",
    "print(f'\\nVariance Components:')\n",
    "print(f'{\"\":20s} {\"Males\":>12s} {\"Females\":>12s} {\"Difference\":>12s}')\n",
    "print('-' * 56)\n",
    "print(f'{\"sigma_eps\":20s} {re_male.sigma_eps:>12.4f} {re_female.sigma_eps:>12.4f} {re_female.sigma_eps - re_male.sigma_eps:>12.4f}')\n",
    "print(f'{\"sigma_alpha\":20s} {re_male.sigma_alpha:>12.4f} {re_female.sigma_alpha:>12.4f} {re_female.sigma_alpha - re_male.sigma_alpha:>12.4f}')\n",
    "print(f'{\"rho (ICC)\":20s} {rho_male:>12.4f} {rho_female:>12.4f} {rho_female - rho_male:>12.4f}')\n",
    "print(f'{\"Log-likelihood\":20s} {re_male.llf:>12.2f} {re_female.llf:>12.2f} {\"\":>12s}')\n",
    "print(f'{\"N observations\":20s} {mask_male.sum():>12d} {mask_female.sum():>12d} {\"\":>12s}')\n",
    "\n",
    "# Save\n",
    "gender_comparison.to_csv(TABLES_DIR / 'ex2_gender_comparison.csv', index=False)\n",
    "print(f'\\nSaved to {TABLES_DIR / \"ex2_gender_comparison.csv\"}')"
   ],
   "id": "8b65e0fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visual comparison: Forest plot by gender\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Forest plot of coefficients\n",
    "ax = axes[0]\n",
    "slope_sub_vars = sub_var_names[1:]  # Exclude constant\n",
    "n_vars = len(slope_sub_vars)\n",
    "y_pos = np.arange(n_vars)\n",
    "\n",
    "male_coefs = re_male.beta[1:]\n",
    "male_ses = male_bse[1:]\n",
    "female_coefs = re_female.beta[1:]\n",
    "female_ses = female_bse[1:]\n",
    "\n",
    "ax.errorbar(male_coefs, y_pos + 0.12,\n",
    "            xerr=1.96 * male_ses,\n",
    "            fmt='o', color='steelblue', markersize=9, capsize=5,\n",
    "            label='Males', linewidth=2)\n",
    "ax.errorbar(female_coefs, y_pos - 0.12,\n",
    "            xerr=1.96 * female_ses,\n",
    "            fmt='s', color='coral', markersize=9, capsize=5,\n",
    "            label='Females', linewidth=2)\n",
    "\n",
    "ax.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(slope_sub_vars, fontsize=12)\n",
    "ax.set_xlabel('Coefficient (95% CI)', fontsize=12)\n",
    "ax.set_title('RE Tobit Coefficients by Gender', fontsize=14)\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Variance decomposition comparison\n",
    "ax = axes[1]\n",
    "categories = ['Males', 'Females']\n",
    "sigma2_alpha_vals = [re_male.sigma_alpha**2, re_female.sigma_alpha**2]\n",
    "sigma2_eps_vals = [re_male.sigma_eps**2, re_female.sigma_eps**2]\n",
    "\n",
    "x_pos = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, sigma2_alpha_vals, width,\n",
    "               label=r'$\\sigma^2_\\alpha$ (between)', color='#ff9999', edgecolor='black')\n",
    "bars2 = ax.bar(x_pos + width/2, sigma2_eps_vals, width,\n",
    "               label=r'$\\sigma^2_\\varepsilon$ (within)', color='#66b3ff', edgecolor='black')\n",
    "\n",
    "# Add rho annotations\n",
    "for i, (cat, rho_val) in enumerate(zip(categories, [rho_male, rho_female])):\n",
    "    total = sigma2_alpha_vals[i] + sigma2_eps_vals[i]\n",
    "    ax.text(i, total * 0.5, f'$\\\\rho$ = {rho_val:.3f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(categories, fontsize=12)\n",
    "ax.set_ylabel('Variance', fontsize=12)\n",
    "ax.set_title('Variance Decomposition by Gender', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex2_gender_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "0f3dc0d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Informal test of coefficient equality via confidence intervals\n",
    "# ============================================================\n",
    "\n",
    "print('Informal Comparison: Do 95% CIs Overlap?')\n",
    "print('=' * 70)\n",
    "print(f'{\"Variable\":15s} {\"Male 95% CI\":>25s} {\"Female 95% CI\":>25s} {\"Overlap?\":>10s}')\n",
    "print('-' * 75)\n",
    "\n",
    "for i, vname in enumerate(slope_sub_vars):\n",
    "    m_lo = male_coefs[i] - 1.96 * male_ses[i]\n",
    "    m_hi = male_coefs[i] + 1.96 * male_ses[i]\n",
    "    f_lo = female_coefs[i] - 1.96 * female_ses[i]\n",
    "    f_hi = female_coefs[i] + 1.96 * female_ses[i]\n",
    "    \n",
    "    # Check overlap\n",
    "    overlap = not (m_hi < f_lo or f_hi < m_lo)\n",
    "    overlap_str = 'Yes' if overlap else 'NO'\n",
    "    \n",
    "    print(f'{vname:15s} [{m_lo:>8.3f}, {m_hi:>8.3f}]   [{f_lo:>8.3f}, {f_hi:>8.3f}]   {overlap_str:>10s}')\n",
    "\n",
    "print()\n",
    "print('Note: Non-overlapping 95% CIs strongly suggest a significant difference.')\n",
    "print('Overlapping CIs do not necessarily mean no significant difference (this is')\n",
    "print('a conservative test). A formal Chow-type test would require pooling the samples.')"
   ],
   "id": "35632de5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Discussion\n",
    "\n",
    "**Key findings from the gender subsample analysis:**\n",
    "\n",
    "1. **Income effect**: Compare the income coefficient for males vs females.\n",
    "   If different, this suggests that the relationship between income and health\n",
    "   expenditure varies by gender -- potentially due to differences in healthcare\n",
    "   utilization patterns or access.\n",
    "\n",
    "2. **Chronic conditions**: The effect of chronic conditions on latent expenditure\n",
    "   may differ by gender. If the coefficient is larger for one gender, it suggests\n",
    "   that chronic conditions have a greater impact on their health spending.\n",
    "\n",
    "3. **Intra-class correlation (rho)**: Differences in rho between males and females\n",
    "   indicate different degrees of unobserved individual heterogeneity. A higher rho\n",
    "   for one gender means more of their expenditure variation is explained by\n",
    "   time-invariant individual characteristics.\n",
    "\n",
    "4. **Practical implication**: If the models are substantially different, the\n",
    "   pooled model with just a `female` dummy is restrictive -- it only allows an\n",
    "   intercept shift, not different slopes. Separate models or a fully interacted\n",
    "   specification would be more appropriate."
   ],
   "id": "d67781a3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Marginal Effects\n",
    "\n",
    "**Task**: Using the fitted RE Tobit model, compute the marginal effect of adding\n",
    "one chronic condition on:\n",
    "- The unconditional expected observed expenditure $E[y|X]$\n",
    "- The conditional expected expenditure $E[y | y > 0, X]$\n",
    "- The probability of positive expenditure $P(y > 0 | X)$\n",
    "\n",
    "Compare `which='unconditional'` vs `which='conditional'`.\n",
    "\n",
    "### Background: Three Types of Marginal Effects in the Tobit Model\n",
    "\n",
    "In a standard Tobit with left censoring at $c=0$, the coefficient $\\beta_k$ represents the\n",
    "effect on the **latent** variable $y^*$. But we typically care about effects on the\n",
    "**observed** outcome, which are attenuated by the censoring mechanism:\n",
    "\n",
    "1. **Unconditional** ME: $\\frac{\\partial E[y|X]}{\\partial x_k} = \\beta_k \\cdot \\Phi(z)$ \n",
    "   where $z = (X'\\beta - c) / \\sigma$\n",
    "\n",
    "2. **Conditional** ME: $\\frac{\\partial E[y|y>c,X]}{\\partial x_k} = \\beta_k \\cdot [1 - \\lambda(z)(z + \\lambda(z))]$\n",
    "   where $\\lambda(z) = \\phi(z)/\\Phi(z)$ is the inverse Mills ratio\n",
    "\n",
    "3. **Probability** ME: $\\frac{\\partial P(y>c|X)}{\\partial x_k} = \\frac{\\beta_k}{\\sigma} \\cdot \\phi(z)$"
   ],
   "id": "54d691d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 3 Solution: Marginal Effects\n",
    "# ============================================================\n",
    "\n",
    "# Compute all three types of marginal effects\n",
    "print('Computing marginal effects from the base RE Tobit model...')\n",
    "print('=' * 60)\n",
    "\n",
    "# Unconditional marginal effects: dE[y|X]/dx\n",
    "print('\\n--- Unconditional Marginal Effects: dE[y|X]/dx ---')\n",
    "me_uncond = re_base.marginal_effects(at='overall', which='unconditional')\n",
    "print()\n",
    "display(me_uncond.summary())"
   ],
   "id": "e246c4dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional marginal effects: dE[y|y>0, X]/dx\n",
    "print('--- Conditional Marginal Effects: dE[y|y>0, X]/dx ---')\n",
    "me_cond = re_base.marginal_effects(at='overall', which='conditional')\n",
    "print()\n",
    "display(me_cond.summary())"
   ],
   "id": "97cad1ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability marginal effects: dP(y>0|X)/dx\n",
    "print('--- Probability Marginal Effects: dP(y>0|X)/dx ---')\n",
    "me_prob = re_base.marginal_effects(at='overall', which='probability')\n",
    "print()\n",
    "display(me_prob.summary())"
   ],
   "id": "1c26f409"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Focus on chronic conditions: compare all three ME types\n",
    "# ============================================================\n",
    "\n",
    "# Collect results into a focused comparison\n",
    "me_types = ['unconditional', 'conditional', 'probability']\n",
    "me_results = [me_uncond, me_cond, me_prob]\n",
    "\n",
    "# Get the chronic condition index in the variable names\n",
    "# The marginal_effects keys use the internal names (x0, x1, ...)\n",
    "# Let's extract the chronic effect from each\n",
    "chronic_effects = {}\n",
    "for me_type, me_res in zip(me_types, me_results):\n",
    "    me_summary = me_res.summary()\n",
    "    chronic_effects[me_type] = {\n",
    "        'me': me_res.marginal_effects,\n",
    "        'se': me_res.std_errors,\n",
    "        'summary': me_summary\n",
    "    }\n",
    "\n",
    "# Build comparison for the chronic variable specifically\n",
    "# The latent coefficient for reference\n",
    "chronic_idx = var_names.index('chronic')\n",
    "beta_chronic = re_base.beta[chronic_idx]\n",
    "\n",
    "print('Marginal Effect of Chronic Conditions -- Comparison')\n",
    "print('=' * 65)\n",
    "print(f'  Latent coefficient (beta):       {beta_chronic:.4f}')\n",
    "print(f'  Interpretation: A 1-unit increase in chronic conditions')\n",
    "print(f'  changes latent expenditure y* by {beta_chronic:.4f}')\n",
    "print()\n",
    "print('  Average Marginal Effects (all variables):')\n",
    "print('-' * 65)\n",
    "for me_type, me_res in zip(me_types, me_results):\n",
    "    print(f'\\n  {me_type.upper()}:')\n",
    "    for key in me_res.marginal_effects.index:\n",
    "        me_val = me_res.marginal_effects[key]\n",
    "        se_val = me_res.std_errors[key]\n",
    "        print(f'    {key:>5s}: ME = {me_val:>8.4f}  (SE = {se_val:.4f})')"
   ],
   "id": "12bb7d57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visual comparison: bar chart of ME types for all variables\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. All variables: unconditional vs conditional ME\n",
    "ax = axes[0]\n",
    "me_vars = me_uncond.marginal_effects.index.tolist()\n",
    "n_me = len(me_vars)\n",
    "x_pos = np.arange(n_me)\n",
    "width = 0.35\n",
    "\n",
    "uncond_vals = me_uncond.marginal_effects.values\n",
    "cond_vals = me_cond.marginal_effects.values\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, uncond_vals, width,\n",
    "               label='Unconditional', color='steelblue', edgecolor='black', alpha=0.8)\n",
    "bars2 = ax.bar(x_pos + width/2, cond_vals, width,\n",
    "               label='Conditional', color='coral', edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(me_vars, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_ylabel('Average Marginal Effect', fontsize=12)\n",
    "ax.set_title('Unconditional vs Conditional Marginal Effects', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 2. McDonald-Moffitt decomposition for all variables\n",
    "# Unconditional ME = P(y>0) * Conditional ME + E[y|y>0] * Probability ME\n",
    "# Show the three components for each variable\n",
    "ax = axes[1]\n",
    "\n",
    "prob_vals = me_prob.marginal_effects.values\n",
    "\n",
    "x_pos3 = np.arange(n_me)\n",
    "width3 = 0.25\n",
    "\n",
    "ax.bar(x_pos3 - width3, uncond_vals, width3,\n",
    "       label='Unconditional', color='steelblue', edgecolor='black', alpha=0.8)\n",
    "ax.bar(x_pos3, cond_vals, width3,\n",
    "       label='Conditional', color='coral', edgecolor='black', alpha=0.8)\n",
    "ax.bar(x_pos3 + width3, prob_vals, width3,\n",
    "       label='Probability', color='seagreen', edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "ax.set_xticks(x_pos3)\n",
    "ax.set_xticklabels(me_vars, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_ylabel('Average Marginal Effect', fontsize=12)\n",
    "ax.set_title('All Three Types of Marginal Effects', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex3_marginal_effects.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "69c40e7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Scaling factors: ME / beta for each type\n",
    "# Shows the attenuation from latent to observed effects\n",
    "# ============================================================\n",
    "\n",
    "print('Attenuation Factors: ME / beta')\n",
    "print('=' * 60)\n",
    "print('These show how much the latent effect is attenuated by censoring.')\n",
    "print()\n",
    "print(f'{\"Variable\":>12s} {\"beta\":>8s} {\"Uncond ME\":>10s} {\"Ratio\":>8s} {\"Cond ME\":>10s} {\"Ratio\":>8s}')\n",
    "print('-' * 58)\n",
    "\n",
    "for i, key in enumerate(me_vars):\n",
    "    # Try to match variable name to get beta\n",
    "    # The ME keys may be x0, x1, ... or the actual names\n",
    "    # We use the index order (the constant is typically excluded in ME)\n",
    "    # ME is computed for all exog columns, so index i maps to beta[i]\n",
    "    beta_val = re_base.beta[i]\n",
    "    uncond_val = uncond_vals[i]\n",
    "    cond_val = cond_vals[i]\n",
    "    \n",
    "    ratio_u = uncond_val / beta_val if abs(beta_val) > 1e-10 else np.nan\n",
    "    ratio_c = cond_val / beta_val if abs(beta_val) > 1e-10 else np.nan\n",
    "    \n",
    "    print(f'{key:>12s} {beta_val:>8.4f} {uncond_val:>10.4f} {ratio_u:>8.3f} {cond_val:>10.4f} {ratio_c:>8.3f}')\n",
    "\n",
    "print()\n",
    "print('Notes:')\n",
    "print('  - Unconditional ratio = Phi(z_bar) approx = fraction uncensored')\n",
    "print('  - Conditional ratio < 1 reflects the Mills ratio adjustment')\n",
    "print('  - The unconditional ME is always smaller than the latent beta')\n",
    "print('  - The conditional ME is between the unconditional ME and beta')"
   ],
   "id": "852af854"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Discussion\n",
    "\n",
    "**Key findings from the marginal effects analysis:**\n",
    "\n",
    "1. **Unconditional vs Conditional**: The unconditional marginal effect is always\n",
    "   smaller in absolute value than the conditional effect. This is because the\n",
    "   unconditional effect accounts for the fact that some individuals are censored\n",
    "   at zero -- a change in $x_k$ has no effect on their observed expenditure (it\n",
    "   remains at zero).\n",
    "\n",
    "2. **Chronic conditions**: The latent coefficient for chronic conditions is the\n",
    "   largest (in the DGP, it is 3.0). The unconditional ME is attenuated by\n",
    "   roughly the fraction of uncensored observations, while the conditional ME\n",
    "   is attenuated by a smaller factor involving the inverse Mills ratio.\n",
    "\n",
    "3. **Probability effects**: The probability marginal effects show how each\n",
    "   variable affects the likelihood of having any positive expenditure. Variables\n",
    "   with larger beta/sigma ratios have stronger effects on the extensive margin.\n",
    "\n",
    "4. **McDonald-Moffitt decomposition**: The unconditional effect can be decomposed\n",
    "   into an intensive margin (change among those already spending) and an\n",
    "   extensive margin (change in the probability of any spending). This\n",
    "   decomposition is:\n",
    "   $$\\frac{\\partial E[y|X]}{\\partial x_k} = P(y > 0) \\cdot \\frac{\\partial E[y|y>0, X]}{\\partial x_k} + E[y|y>0, X] \\cdot \\frac{\\partial P(y>0|X)}{\\partial x_k}$$"
   ],
   "id": "050a776c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Prediction Performance\n",
    "\n",
    "**Task**: Split the data into training (periods 1-3) and test (period 4) sets.\n",
    "Estimate both Pooled Tobit and RE Tobit on the training data. Compare\n",
    "prediction accuracy (RMSE, MAE) on the test set.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "In-sample fit (e.g., log-likelihood, AIC) can favor more complex models that\n",
    "overfit. A genuine **out-of-sample** evaluation tests whether the model\n",
    "generalizes to new data. For panel data, a natural split is to hold out the\n",
    "last time period."
   ],
   "id": "65f0ec2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 4 Solution: Train/Test Prediction Performance\n",
    "# ============================================================\n",
    "\n",
    "# Split data: training = periods 1-3, test = period 4\n",
    "train_mask = df['time'].values <= 3\n",
    "test_mask = df['time'].values == 4\n",
    "\n",
    "y_train = y[train_mask]\n",
    "y_test = y[test_mask]\n",
    "X_train = X[train_mask]\n",
    "X_test = X[test_mask]\n",
    "groups_train = groups[train_mask]\n",
    "groups_test = groups[test_mask]\n",
    "time_train = time[train_mask]\n",
    "time_test = time[test_mask]\n",
    "\n",
    "print('Train/Test Split Summary')\n",
    "print('=' * 50)\n",
    "print(f'Training set (t=1,2,3):')\n",
    "print(f'  N observations:     {train_mask.sum()}')\n",
    "print(f'  N individuals:      {len(np.unique(groups_train))}')\n",
    "print(f'  Censoring rate:     {(y_train == 0).mean() * 100:.1f}%')\n",
    "print(f'  Mean expenditure:   {y_train.mean():.2f}')\n",
    "print(f'\\nTest set (t=4):')\n",
    "print(f'  N observations:     {test_mask.sum()}')\n",
    "print(f'  N individuals:      {len(np.unique(groups_test))}')\n",
    "print(f'  Censoring rate:     {(y_test == 0).mean() * 100:.1f}%')\n",
    "print(f'  Mean expenditure:   {y_test.mean():.2f}')"
   ],
   "id": "6a08134a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fit Pooled Tobit on training data\n",
    "# ============================================================\n",
    "\n",
    "print('Fitting Pooled Tobit on training data (t=1,2,3)...')\n",
    "print('=' * 60)\n",
    "\n",
    "pooled_train = PooledTobit(\n",
    "    endog=y_train,\n",
    "    exog=X_train,\n",
    "    groups=groups_train,\n",
    "    censoring_point=0.0,\n",
    "    censoring_type='left'\n",
    ")\n",
    "pooled_train.fit(method='BFGS', maxiter=1000)\n",
    "\n",
    "print(f'  Log-likelihood: {pooled_train.llf:.2f}')\n",
    "print(f'  Converged: {pooled_train.converged}')"
   ],
   "id": "cc99650a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fit RE Tobit on training data\n",
    "# ============================================================\n",
    "\n",
    "print('Fitting RE Tobit on training data (t=1,2,3)...')\n",
    "print('=' * 60)\n",
    "\n",
    "re_train = RandomEffectsTobit(\n",
    "    endog=y_train,\n",
    "    exog=X_train,\n",
    "    groups=groups_train,\n",
    "    time=time_train,\n",
    "    censoring_point=0.0,\n",
    "    censoring_type='left',\n",
    "    quadrature_points=12\n",
    ")\n",
    "re_train.fit(method='BFGS', maxiter=1000)\n",
    "\n",
    "print(f'  Log-likelihood: {re_train.llf:.2f}')\n",
    "print(f'  sigma_eps:      {re_train.sigma_eps:.4f}')\n",
    "print(f'  sigma_alpha:    {re_train.sigma_alpha:.4f}')\n",
    "print(f'  Converged:      {re_train.converged}')"
   ],
   "id": "2608215e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate predictions on the test set\n",
    "# ============================================================\n",
    "\n",
    "# Pooled Tobit predictions on test set\n",
    "y_pred_pooled_latent = pooled_train.predict(exog=X_test, pred_type='latent')\n",
    "y_pred_pooled_cens = pooled_train.predict(exog=X_test, pred_type='censored')\n",
    "\n",
    "# RE Tobit predictions on test set\n",
    "# For out-of-sample, we use population-average predictions (no individual RE)\n",
    "y_pred_re_latent = re_train.predict(exog=X_test, pred_type='latent')\n",
    "y_pred_re_cens = re_train.predict(exog=X_test, pred_type='censored')\n",
    "\n",
    "print('Predictions generated for test set (t=4).')\n",
    "print(f'  Pooled censored predictions: mean={y_pred_pooled_cens.mean():.3f}, std={y_pred_pooled_cens.std():.3f}')\n",
    "print(f'  RE censored predictions:     mean={y_pred_re_cens.mean():.3f}, std={y_pred_re_cens.std():.3f}')\n",
    "print(f'  Actual test values:          mean={y_test.mean():.3f}, std={y_test.std():.3f}')"
   ],
   "id": "495b2548"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Compute prediction accuracy metrics\n",
    "# ============================================================\n",
    "\n",
    "def prediction_metrics(y_true, y_pred, name=''):\n",
    "    \"\"\"Compute RMSE, MAE, MAPE, and correlation.\"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    rmse = np.sqrt(np.mean(residuals**2))\n",
    "    mae = np.mean(np.abs(residuals))\n",
    "    mean_error = np.mean(residuals)  # bias\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    \n",
    "    # RMSE for uncensored observations only\n",
    "    uncens_mask = y_true > 0\n",
    "    rmse_uncens = np.sqrt(np.mean((y_true[uncens_mask] - y_pred[uncens_mask])**2))\n",
    "    mae_uncens = np.mean(np.abs(y_true[uncens_mask] - y_pred[uncens_mask]))\n",
    "    \n",
    "    # Classification accuracy: correctly predicting censored vs uncensored\n",
    "    # (using a threshold of close to 0 for predicted values)\n",
    "    pred_censored = y_pred < 0.5\n",
    "    actual_censored = y_true == 0\n",
    "    accuracy = np.mean(pred_censored == actual_censored)\n",
    "    \n",
    "    return {\n",
    "        'Model': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'Mean Error (bias)': mean_error,\n",
    "        'Correlation': corr,\n",
    "        'RMSE (uncensored)': rmse_uncens,\n",
    "        'MAE (uncensored)': mae_uncens,\n",
    "        'Censoring Accuracy': accuracy\n",
    "    }\n",
    "\n",
    "# Compute metrics for both models (censored predictions)\n",
    "metrics_pooled = prediction_metrics(y_test, y_pred_pooled_cens, 'Pooled Tobit')\n",
    "metrics_re = prediction_metrics(y_test, y_pred_re_cens, 'RE Tobit')\n",
    "\n",
    "# Also compute a naive baseline: predict mean of training data\n",
    "y_pred_naive = np.full_like(y_test, y_train.mean(), dtype=float)\n",
    "metrics_naive = prediction_metrics(y_test, y_pred_naive, 'Naive (mean)')\n",
    "\n",
    "metrics_df = pd.DataFrame([metrics_naive, metrics_pooled, metrics_re]).set_index('Model')\n",
    "\n",
    "print('Out-of-Sample Prediction Accuracy (Test Set: t=4)')\n",
    "print('=' * 80)\n",
    "display(metrics_df.round(4))\n",
    "\n",
    "# Improvement of RE over Pooled\n",
    "rmse_improvement = (metrics_pooled['RMSE'] - metrics_re['RMSE']) / metrics_pooled['RMSE'] * 100\n",
    "mae_improvement = (metrics_pooled['MAE'] - metrics_re['MAE']) / metrics_pooled['MAE'] * 100\n",
    "\n",
    "print(f'\\nRE Tobit improvement over Pooled Tobit:')\n",
    "print(f'  RMSE improvement: {rmse_improvement:+.2f}%')\n",
    "print(f'  MAE improvement:  {mae_improvement:+.2f}%')"
   ],
   "id": "61b7d1b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization: Prediction performance\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 11))\n",
    "\n",
    "# 1. Pooled Tobit: predicted vs actual\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(y_pred_pooled_cens, y_test, alpha=0.3, s=15, color='coral')\n",
    "max_val = max(y_pred_pooled_cens.max(), y_test.max())\n",
    "ax.plot([0, max_val], [0, max_val], 'k--', linewidth=1.5, label='45-degree line')\n",
    "ax.set_xlabel('Predicted (Pooled Tobit)', fontsize=12)\n",
    "ax.set_ylabel('Actual Expenditure', fontsize=12)\n",
    "ax.set_title(f'Pooled Tobit (RMSE={metrics_pooled[\"RMSE\"]:.3f})', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 2. RE Tobit: predicted vs actual\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(y_pred_re_cens, y_test, alpha=0.3, s=15, color='steelblue')\n",
    "max_val = max(y_pred_re_cens.max(), y_test.max())\n",
    "ax.plot([0, max_val], [0, max_val], 'k--', linewidth=1.5, label='45-degree line')\n",
    "ax.set_xlabel('Predicted (RE Tobit)', fontsize=12)\n",
    "ax.set_ylabel('Actual Expenditure', fontsize=12)\n",
    "ax.set_title(f'RE Tobit (RMSE={metrics_re[\"RMSE\"]:.3f})', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 3. Residual distributions\n",
    "ax = axes[1, 0]\n",
    "resid_pooled = y_test - y_pred_pooled_cens\n",
    "resid_re = y_test - y_pred_re_cens\n",
    "\n",
    "ax.hist(resid_pooled, bins=30, alpha=0.5, color='coral',\n",
    "        edgecolor='white', label='Pooled Tobit', density=True)\n",
    "ax.hist(resid_re, bins=30, alpha=0.5, color='steelblue',\n",
    "        edgecolor='white', label='RE Tobit', density=True)\n",
    "ax.axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "ax.set_xlabel('Prediction Residual (actual - predicted)', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Residual Distributions (Test Set)', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 4. RMSE comparison bar chart\n",
    "ax = axes[1, 1]\n",
    "models = ['Naive (mean)', 'Pooled Tobit', 'RE Tobit']\n",
    "rmse_vals = [metrics_naive['RMSE'], metrics_pooled['RMSE'], metrics_re['RMSE']]\n",
    "mae_vals = [metrics_naive['MAE'], metrics_pooled['MAE'], metrics_re['MAE']]\n",
    "\n",
    "x_pos = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, rmse_vals, width,\n",
    "               label='RMSE', color='steelblue', edgecolor='black', alpha=0.8)\n",
    "bars2 = ax.bar(x_pos + width/2, mae_vals, width,\n",
    "               label='MAE', color='coral', edgecolor='black', alpha=0.8)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,\n",
    "            f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,\n",
    "            f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(models, fontsize=11)\n",
    "ax.set_ylabel('Error', fontsize=12)\n",
    "ax.set_title('Out-of-Sample Prediction Error Comparison', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex4_prediction_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "965d70f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Additional analysis: Performance by censoring status\n",
    "# ============================================================\n",
    "\n",
    "# How well do models predict for censored vs uncensored obs?\n",
    "censored_test = y_test == 0\n",
    "uncensored_test = y_test > 0\n",
    "\n",
    "print('Performance by Censoring Status in Test Set')\n",
    "print('=' * 70)\n",
    "print(f'{\"\":25s} {\"Censored (y=0)\":>20s} {\"Uncensored (y>0)\":>20s}')\n",
    "print(f'{\"\":25s} {\"N=\" + str(censored_test.sum()):>20s} {\"N=\" + str(uncensored_test.sum()):>20s}')\n",
    "print('-' * 65)\n",
    "\n",
    "for name, y_pred in [('Pooled Tobit', y_pred_pooled_cens), ('RE Tobit', y_pred_re_cens)]:\n",
    "    # For censored obs: ideal prediction is 0 or close to 0\n",
    "    mae_cens = np.mean(np.abs(y_test[censored_test] - y_pred[censored_test]))\n",
    "    mae_uncens = np.mean(np.abs(y_test[uncensored_test] - y_pred[uncensored_test]))\n",
    "    \n",
    "    rmse_cens = np.sqrt(np.mean((y_test[censored_test] - y_pred[censored_test])**2))\n",
    "    rmse_uncens = np.sqrt(np.mean((y_test[uncensored_test] - y_pred[uncensored_test])**2))\n",
    "    \n",
    "    print(f'{name + \" RMSE\":25s} {rmse_cens:>20.4f} {rmse_uncens:>20.4f}')\n",
    "    print(f'{name + \" MAE\":25s} {mae_cens:>20.4f} {mae_uncens:>20.4f}')\n",
    "    print()\n",
    "\n",
    "print('Interpretation:')\n",
    "print('  - For censored observations, a good model should predict values near zero.')\n",
    "print('  - For uncensored observations, the model should match the positive expenditure.')\n",
    "print('  - The RE model may better predict uncensored observations if individual')\n",
    "print('    heterogeneity is important for explaining expenditure levels.')"
   ],
   "id": "89824c64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Coefficient comparison: full sample vs training sample\n",
    "# ============================================================\n",
    "\n",
    "print('Coefficient Stability: Full Sample vs Training Sample (RE Tobit)')\n",
    "print('=' * 70)\n",
    "print(f'{\"Variable\":>12s} {\"Full (t=1-4)\":>14s} {\"Train (t=1-3)\":>14s} {\"Difference\":>12s}')\n",
    "print('-' * 52)\n",
    "for i, vname in enumerate(var_names):\n",
    "    full_val = re_base.beta[i]\n",
    "    train_val = re_train.beta[i]\n",
    "    diff = train_val - full_val\n",
    "    print(f'{vname:>12s} {full_val:>14.4f} {train_val:>14.4f} {diff:>12.4f}')\n",
    "\n",
    "print(f'{\"sigma_eps\":>12s} {re_base.sigma_eps:>14.4f} {re_train.sigma_eps:>14.4f} {re_train.sigma_eps - re_base.sigma_eps:>12.4f}')\n",
    "print(f'{\"sigma_alpha\":>12s} {re_base.sigma_alpha:>14.4f} {re_train.sigma_alpha:>14.4f} {re_train.sigma_alpha - re_base.sigma_alpha:>12.4f}')\n",
    "\n",
    "print('\\nSmall differences indicate stable estimates (good sign for generalization).')"
   ],
   "id": "67ef31cf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Discussion\n",
    "\n",
    "**Key findings from the prediction performance analysis:**\n",
    "\n",
    "1. **Both models outperform the naive baseline**: Predicting the training mean\n",
    "   for all test observations gives the worst RMSE/MAE, confirming that the\n",
    "   covariates contain useful predictive information.\n",
    "\n",
    "2. **Pooled vs RE Tobit**: For population-average out-of-sample predictions\n",
    "   (without individual-specific random effect estimates), the two models often\n",
    "   perform similarly. The RE model's main advantage is in characterizing\n",
    "   **within-individual** dynamics and producing correct **standard errors**,\n",
    "   rather than in point prediction for new periods.\n",
    "\n",
    "3. **Censored vs uncensored predictions**: Both models tend to have larger\n",
    "   prediction errors for uncensored observations, which is expected since\n",
    "   positive expenditure values have much more variation than the mass point\n",
    "   at zero.\n",
    "\n",
    "4. **Coefficient stability**: The estimates from the training sample (t=1-3)\n",
    "   are close to those from the full sample (t=1-4), suggesting that the\n",
    "   model parameters are stable over time.\n",
    "\n",
    "5. **Caveat**: If we had individual-level random effect estimates (empirical\n",
    "   Bayes predictions of $\\hat{\\alpha}_i$ from periods 1-3), the RE model\n",
    "   could potentially produce substantially better individual-level predictions\n",
    "   for period 4. The population-average predictions used here do not exploit\n",
    "   this information."
   ],
   "id": "f6ffa65b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this solution notebook, we worked through four exercises that deepened our\n",
    "understanding of the Random Effects Tobit model:\n",
    "\n",
    "| Exercise | Key Takeaway |\n",
    "|----------|-------------|\n",
    "| 1. Quadrature Sensitivity | Q=12 is sufficient for this dataset; results stabilize quickly |\n",
    "| 2. Gender Subsample | Separate models reveal structural differences in expenditure processes |\n",
    "| 3. Marginal Effects | Unconditional ME < Conditional ME < Latent beta due to censoring |\n",
    "| 4. Prediction Performance | Both models beat naive; RE advantages are more in inference than prediction |\n",
    "\n",
    "### Key Methods Used\n",
    "\n",
    "```python\n",
    "# Quadrature sensitivity\n",
    "RandomEffectsTobit(..., quadrature_points=nq)\n",
    "\n",
    "# Subsample estimation\n",
    "RandomEffectsTobit(endog=y[mask], exog=X[mask], groups=groups[mask], ...)\n",
    "\n",
    "# Marginal effects\n",
    "model.marginal_effects(at='overall', which='unconditional')\n",
    "model.marginal_effects(at='overall', which='conditional')\n",
    "model.marginal_effects(at='overall', which='probability')\n",
    "\n",
    "# Out-of-sample prediction\n",
    "model.predict(exog=X_test, pred_type='censored')\n",
    "```"
   ],
   "id": "829bfe9d"
  }
 ]
}
