{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Notebook 01: Introduction to Tobit Models - SOLUTIONS\n",
    "\n",
    "**Tutorial 01 Solutions - Censored Models Series**\n",
    "\n",
    "**Version**: 1.0  \n",
    "**Date**: 2026-02-17  \n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook contains complete, working solutions for all 4 exercises from\n",
    "Notebook 01: Introduction to Tobit Models for Censored Data.\n",
    "\n",
    "**Exercises covered:**\n",
    "\n",
    "1. **Exercise 1: Variable Selection (Easy)** -- Re-estimate Tobit with a restricted set of variables and perform a likelihood ratio test\n",
    "2. **Exercise 2: Prediction Profiles (Medium)** -- Create profiles and compute latent, censored, and probability predictions\n",
    "3. **Exercise 3: Right-Censoring (Medium)** -- Estimate a right-censored Tobit and compare with OLS and left-censored Tobit\n",
    "4. **Exercise 4: Manual McDonald-Moffitt Decomposition (Hard)** -- Manually compute all three marginal effects and verify against PanelBox\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and configure visualization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical functions\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# PanelBox imports\n",
    "from panelbox.models.censored import PooledTobit\n",
    "\n",
    "# Visualization configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Setup complete!')\n",
    "print(f'  Data directory: {DATA_DIR}')\n",
    "print(f'  Output directory: {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Regenerate the labor supply data using the same DGP as the main notebook.\n",
    "The latent variable (desired hours) is a function of wages, education,\n",
    "experience, and household characteristics. Observed hours are censored at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_censored_labor_supply(n=500, seed=42):\n",
    "    \"\"\"\n",
    "    Generate labor supply data with left-censoring at zero.\n",
    "    \n",
    "    The latent variable (desired hours) is a function of wages, education,\n",
    "    experience, and household characteristics. Observed hours are:\n",
    "        hours = max(0, latent_hours)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Generate covariates\n",
    "    education = rng.integers(8, 21, size=n)\n",
    "    age = rng.integers(25, 60, size=n)\n",
    "    experience = np.clip(age - education - 6 + rng.normal(0, 2, n), 0, None)\n",
    "    experience_sq = experience ** 2\n",
    "    children = rng.poisson(0.8, size=n)\n",
    "    married = rng.binomial(1, 0.6, size=n)\n",
    "    non_labor_income = np.abs(rng.normal(20, 15, n))\n",
    "    wage = np.exp(\n",
    "        0.8 + 0.07 * education + 0.03 * experience\n",
    "        - 0.0005 * experience_sq + rng.normal(0, 0.4, n)\n",
    "    )\n",
    "    \n",
    "    # Latent hours (desired hours)\n",
    "    # Lower intercept to ensure ~30-35% censoring\n",
    "    latent_hours = (\n",
    "        -5.0\n",
    "        + 3.0 * np.log(wage)\n",
    "        + 0.8 * education\n",
    "        + 1.2 * experience\n",
    "        - 0.02 * experience_sq\n",
    "        - 3.5 * children\n",
    "        + 1.5 * married\n",
    "        - 0.25 * non_labor_income\n",
    "        + rng.normal(0, 12, n)\n",
    "    )\n",
    "    \n",
    "    # Apply censoring: observed hours = max(0, latent_hours)\n",
    "    hours = np.maximum(latent_hours, 0.0)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'hours': np.round(hours, 1),\n",
    "        'wage': np.round(wage, 2),\n",
    "        'education': education,\n",
    "        'experience': np.round(experience, 1),\n",
    "        'experience_sq': np.round(experience_sq, 1),\n",
    "        'age': age,\n",
    "        'children': children,\n",
    "        'married': married,\n",
    "        'non_labor_income': np.round(non_labor_income, 2),\n",
    "    }), latent_hours\n",
    "\n",
    "\n",
    "# Generate data\n",
    "df, latent_hours = generate_censored_labor_supply(n=500, seed=42)\n",
    "df['latent_hours'] = np.round(latent_hours, 1)\n",
    "\n",
    "print('=' * 60)\n",
    "print('Labor Supply Dataset')\n",
    "print('=' * 60)\n",
    "print(f'\\nObservations: {len(df)}')\n",
    "print(f'Variables: {list(df.columns)}')\n",
    "print(f'\\nCensored observations (hours = 0): {(df[\"hours\"] == 0).sum()}')\n",
    "print(f'Uncensored observations (hours > 0): {(df[\"hours\"] > 0).sum()}')\n",
    "print(f'Censoring rate: {(df[\"hours\"] == 0).mean():.1%}')\n",
    "print(f'\\nFirst 5 rows:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "base-model-header",
   "metadata": {},
   "source": [
    "## Fit the Base Tobit Model (Reference)\n",
    "\n",
    "We fit the full Tobit model as a baseline for comparison in the exercises.\n",
    "This uses all seven covariates: wage, education, experience, experience_sq,\n",
    "children, married, non_labor_income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "base-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Tobit estimation\n",
    "feature_cols = ['wage', 'education', 'experience', 'experience_sq',\n",
    "                'children', 'married', 'non_labor_income']\n",
    "\n",
    "y = df['hours'].values\n",
    "X = sm.add_constant(df[feature_cols].values)\n",
    "var_names = ['const'] + feature_cols\n",
    "\n",
    "# Fit the full Tobit model\n",
    "tobit_full = PooledTobit(\n",
    "    endog=y,\n",
    "    exog=X,\n",
    "    censoring_point=0.0,\n",
    "    censoring_type='left'\n",
    ")\n",
    "tobit_full = tobit_full.fit()\n",
    "\n",
    "# Store variable names on the model for marginal effects\n",
    "tobit_full.exog_names = var_names\n",
    "\n",
    "print('=' * 60)\n",
    "print('Full Tobit Model (Baseline)')\n",
    "print('=' * 60)\n",
    "print(f'\\nConverged: {tobit_full.converged}')\n",
    "print(f'Log-likelihood: {tobit_full.llf:.3f}')\n",
    "print(f'Sigma: {tobit_full.sigma:.4f}')\n",
    "\n",
    "print(f'\\n{\"Variable\":<22} {\"Coefficient\":>12} {\"Std. Error\":>12}')\n",
    "print('-' * 48)\n",
    "for i, name in enumerate(var_names):\n",
    "    print(f'{name:<22} {tobit_full.beta[i]:>12.4f} {tobit_full.bse[i]:>12.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "base-ols",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also fit OLS for comparison (used in Exercise 3)\n",
    "ols_full = sm.OLS(y, X).fit()\n",
    "\n",
    "print('OLS model (full sample) fitted for comparison.')\n",
    "print(f'R-squared: {ols_full.rsquared:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 1: Variable Selection (Easy)\n",
    "\n",
    "**Task:** Re-estimate the Tobit model using only `wage`, `education`, and `children`\n",
    "as explanatory variables (plus a constant). Compare the coefficients and log-likelihood\n",
    "with the full model. Perform a likelihood ratio test to determine whether the\n",
    "restricted model fits significantly worse.\n",
    "\n",
    "**Likelihood Ratio Test:**\n",
    "\n",
    "$$LR = -2(\\ln L_{\\text{restricted}} - \\ln L_{\\text{full}}) \\sim \\chi^2(q)$$\n",
    "\n",
    "where $q$ is the number of restrictions (omitted variables).\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-restricted-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('EXERCISE 1: VARIABLE SELECTION -- RESTRICTED TOBIT MODEL')\n",
    "print('=' * 80)\n",
    "\n",
    "# Step 1: Estimate the restricted model with only wage, education, children\n",
    "restricted_cols = ['wage', 'education', 'children']\n",
    "X_restricted = sm.add_constant(df[restricted_cols].values)\n",
    "var_names_restricted = ['const'] + restricted_cols\n",
    "\n",
    "tobit_restricted = PooledTobit(\n",
    "    endog=y,\n",
    "    exog=X_restricted,\n",
    "    censoring_point=0.0,\n",
    "    censoring_type='left'\n",
    ")\n",
    "tobit_restricted = tobit_restricted.fit()\n",
    "tobit_restricted.exog_names = var_names_restricted\n",
    "\n",
    "print(f'\\nRestricted model converged: {tobit_restricted.converged}')\n",
    "print(f'Restricted log-likelihood: {tobit_restricted.llf:.3f}')\n",
    "print(f'Restricted sigma: {tobit_restricted.sigma:.4f}')\n",
    "\n",
    "# Step 2: Display coefficient comparison\n",
    "print(f'\\n{\"\":-<80}')\n",
    "print(f'{\"Variable\":<22} {\"Full Model\":>14} {\"Restricted\":>14}')\n",
    "print(f'{\"\":-<80}')\n",
    "\n",
    "for name in var_names_restricted:\n",
    "    idx_full = var_names.index(name)\n",
    "    idx_rest = var_names_restricted.index(name)\n",
    "    print(f'{name:<22} {tobit_full.beta[idx_full]:>14.4f} {tobit_restricted.beta[idx_rest]:>14.4f}')\n",
    "\n",
    "# Show the omitted variables\n",
    "omitted_vars = [v for v in feature_cols if v not in restricted_cols]\n",
    "print(f'\\nOmitted variables: {omitted_vars}')\n",
    "for name in omitted_vars:\n",
    "    idx_full = var_names.index(name)\n",
    "    print(f'  {name}: beta = {tobit_full.beta[idx_full]:.4f} (in full model)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-lr-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Likelihood Ratio Test\n",
    "print('\\n' + '=' * 80)\n",
    "print('LIKELIHOOD RATIO TEST')\n",
    "print('=' * 80)\n",
    "\n",
    "# Number of restrictions = number of omitted variables\n",
    "# We omit: experience, experience_sq, married, non_labor_income\n",
    "q = len(omitted_vars)\n",
    "\n",
    "# LR statistic\n",
    "LR = -2 * (tobit_restricted.llf - tobit_full.llf)\n",
    "\n",
    "# p-value from chi-squared distribution\n",
    "p_value = 1 - stats.chi2.cdf(LR, df=q)\n",
    "\n",
    "print(f'\\nLog-likelihood (full):       {tobit_full.llf:.3f}')\n",
    "print(f'Log-likelihood (restricted): {tobit_restricted.llf:.3f}')\n",
    "print(f'\\nNumber of restrictions (q):  {q}')\n",
    "print(f'LR statistic:                {LR:.4f}')\n",
    "print(f'Chi-squared critical (5%):   {stats.chi2.ppf(0.95, df=q):.4f}')\n",
    "print(f'p-value:                     {p_value:.6f}')\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f'\\nConclusion: REJECT H0 at 5% level.')\n",
    "    print(f'The omitted variables ({omitted_vars}) are JOINTLY SIGNIFICANT.')\n",
    "    print(f'The full model is statistically preferred over the restricted model.')\n",
    "else:\n",
    "    print(f'\\nConclusion: FAIL TO REJECT H0 at 5% level.')\n",
    "    print(f'The restricted model is adequate; the omitted variables are not jointly significant.')\n",
    "\n",
    "print('\\n' + '=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The likelihood ratio test compares the restricted model (wage, education, children only)\n",
    "against the full model. A large LR statistic (relative to the chi-squared critical value)\n",
    "indicates that the omitted variables -- experience, experience_sq, married, and\n",
    "non_labor_income -- contribute significantly to explaining the variation in hours worked.\n",
    "\n",
    "**Key points:**\n",
    "- The restricted model omits 4 variables, so the test has 4 degrees of freedom\n",
    "- The LR test is valid because the restricted model is nested within the full model\n",
    "- Even if individual variables are insignificant, they may be jointly significant\n",
    "- A significant LR test does not necessarily mean we should include *all* omitted variables;\n",
    "  it only tells us that at least some of them matter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 2: Prediction Profiles (Medium)\n",
    "\n",
    "**Task:** Create three hypothetical individual profiles:\n",
    "- A person very **likely to work** (high education, no children, low non-labor income)\n",
    "- A person **on the margin** (moderate characteristics)\n",
    "- A person **unlikely to work** (low education, many children, high non-labor income)\n",
    "\n",
    "For each profile, compute:\n",
    "1. The latent prediction $E[y^*|\\mathbf{X}] = \\mathbf{X}'\\hat{\\boldsymbol{\\beta}}$\n",
    "2. The censored prediction $E[y|\\mathbf{X}]$\n",
    "3. The probability of working $P(y > 0|\\mathbf{X})$\n",
    "4. The conditional expected hours $E[y|y>0, \\mathbf{X}]$\n",
    "\n",
    "Then verify:\n",
    "$$E[y|\\mathbf{X}] = P(y>0|\\mathbf{X}) \\cdot E[y|y>0, \\mathbf{X}]$$\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-profiles",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('EXERCISE 2: PREDICTION PROFILES')\n",
    "print('=' * 80)\n",
    "\n",
    "# Define three profiles\n",
    "profiles = {\n",
    "    'Likely worker': {\n",
    "        'wage': 20.0, 'education': 18, 'experience': 10, 'experience_sq': 100,\n",
    "        'children': 0, 'married': 1, 'non_labor_income': 5.0,\n",
    "    },\n",
    "    'Marginal': {\n",
    "        'wage': 10.0, 'education': 12, 'experience': 8, 'experience_sq': 64,\n",
    "        'children': 2, 'married': 1, 'non_labor_income': 25.0,\n",
    "    },\n",
    "    'Unlikely worker': {\n",
    "        'wage': 5.0, 'education': 9, 'experience': 3, 'experience_sq': 9,\n",
    "        'children': 4, 'married': 0, 'non_labor_income': 40.0,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Extract model parameters\n",
    "beta = tobit_full.beta\n",
    "sigma = tobit_full.sigma\n",
    "\n",
    "print(f'\\nModel parameters:')\n",
    "print(f'  sigma = {sigma:.4f}')\n",
    "print(f'  beta = {beta}')\n",
    "\n",
    "print(f'\\n{\"Profile\":<22} {\"E[y*|X]\":>10} {\"E[y|X]\":>10} {\"P(y>0|X)\":>10} {\"E[y|y>0,X]\":>12}')\n",
    "print('-' * 68)\n",
    "\n",
    "results_profiles = {}\n",
    "\n",
    "for name, vals in profiles.items():\n",
    "    # Build the regressor vector: [1, wage, education, ...]\n",
    "    x_vec = np.array([[1.0] + [vals[col] for col in feature_cols]])\n",
    "    \n",
    "    # 1. Latent prediction: E[y*|X] = X'beta\n",
    "    y_latent = tobit_full.predict(exog=x_vec, pred_type='latent')[0]\n",
    "    \n",
    "    # 2. Censored prediction: E[y|X]\n",
    "    y_censored = tobit_full.predict(exog=x_vec, pred_type='censored')[0]\n",
    "    \n",
    "    # 3. Probability of censoring: P(y=0|X)\n",
    "    # Note: PooledTobit.predict(pred_type='probability') returns P(y=0|X)\n",
    "    prob_censored = tobit_full.predict(exog=x_vec, pred_type='probability')[0]\n",
    "    prob_uncensored = 1.0 - prob_censored\n",
    "    \n",
    "    # 4. Conditional expected hours: E[y|y>0, X]\n",
    "    # Formula: E[y|y>0,X] = X'beta + sigma * lambda(z)\n",
    "    # where z = X'beta / sigma (for censoring at 0)\n",
    "    # and lambda(z) = phi(z) / Phi(z) is the inverse Mills ratio\n",
    "    z = y_latent / sigma\n",
    "    IMR = stats.norm.pdf(z) / stats.norm.cdf(z)\n",
    "    y_conditional = y_latent + sigma * IMR\n",
    "    \n",
    "    results_profiles[name] = {\n",
    "        'latent': y_latent,\n",
    "        'censored': y_censored,\n",
    "        'prob_uncensored': prob_uncensored,\n",
    "        'conditional': y_conditional,\n",
    "    }\n",
    "    \n",
    "    print(f'{name:<22} {y_latent:>10.2f} {y_censored:>10.2f} {prob_uncensored:>10.3f} {y_conditional:>12.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the identity: E[y|X] = P(y>0|X) * E[y|y>0,X]\n",
    "# More precisely, for left-censoring at 0:\n",
    "# E[y|X] = P(y>0|X) * E[y|y>0,X] + P(y=0|X) * 0\n",
    "#        = P(y>0|X) * E[y|y>0,X]\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('VERIFICATION: E[y|X] = P(y>0|X) * E[y|y>0,X]')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f'\\n{\"Profile\":<22} {\"E[y|X]\":>12} {\"P(y>0)*E[y|y>0]\":>18} {\"Difference\":>12}')\n",
    "print('-' * 68)\n",
    "\n",
    "for name, res in results_profiles.items():\n",
    "    lhs = res['censored']  # E[y|X]\n",
    "    rhs = res['prob_uncensored'] * res['conditional']  # P(y>0|X) * E[y|y>0,X]\n",
    "    diff = lhs - rhs\n",
    "    print(f'{name:<22} {lhs:>12.4f} {rhs:>18.4f} {diff:>12.6f}')\n",
    "\n",
    "print(f'\\nThe differences should be very close to zero (within numerical precision).')\n",
    "print(f'This confirms the law of iterated expectations applied to censored data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the profiles\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "profile_names = list(results_profiles.keys())\n",
    "colors = ['steelblue', 'darkorange', 'firebrick']\n",
    "\n",
    "# Panel 1: Bar chart of predictions\n",
    "ax = axes[0]\n",
    "x_pos = np.arange(len(profile_names))\n",
    "width = 0.35\n",
    "latent_vals = [results_profiles[n]['latent'] for n in profile_names]\n",
    "censored_vals = [results_profiles[n]['censored'] for n in profile_names]\n",
    "\n",
    "ax.bar(x_pos - width/2, latent_vals, width, label='Latent E[y*|X]',\n",
    "       color='lightcoral', edgecolor='white')\n",
    "ax.bar(x_pos + width/2, censored_vals, width, label='Censored E[y|X]',\n",
    "       color='steelblue', edgecolor='white')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(profile_names, rotation=15, ha='right')\n",
    "ax.set_ylabel('Predicted Hours')\n",
    "ax.set_title('Latent vs. Censored Predictions')\n",
    "ax.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax.legend()\n",
    "\n",
    "# Panel 2: Probability of working\n",
    "ax = axes[1]\n",
    "prob_vals = [results_profiles[n]['prob_uncensored'] for n in profile_names]\n",
    "bars = ax.bar(x_pos, prob_vals, color=colors, edgecolor='white')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(profile_names, rotation=15, ha='right')\n",
    "ax.set_ylabel('P(hours > 0 | X)')\n",
    "ax.set_title('Probability of Working')\n",
    "ax.set_ylim(0, 1.1)\n",
    "for bar, val in zip(bars, prob_vals):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Panel 3: Decomposition\n",
    "ax = axes[2]\n",
    "cond_vals = [results_profiles[n]['conditional'] for n in profile_names]\n",
    "ax.bar(x_pos - width/2, cond_vals, width, label='E[y|y>0,X]',\n",
    "       color='darkorange', edgecolor='white')\n",
    "ax.bar(x_pos + width/2, censored_vals, width, label='E[y|X]',\n",
    "       color='steelblue', edgecolor='white')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(profile_names, rotation=15, ha='right')\n",
    "ax.set_ylabel('Expected Hours')\n",
    "ax.set_title('Conditional vs. Unconditional Predictions')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / '01_ex2_prediction_profiles.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Figure saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The three profiles illustrate how the Tobit model differentiates between\n",
    "individuals based on their characteristics:\n",
    "\n",
    "- **Likely worker**: High education, high wage, no children, low non-labor income.\n",
    "  The latent prediction is strongly positive, the probability of working is high,\n",
    "  and the censored prediction is close to the latent prediction.\n",
    "\n",
    "- **Marginal**: Moderate characteristics. The latent prediction is near zero,\n",
    "  indicating this person is on the boundary of participating. The censored prediction\n",
    "  is pulled upward relative to the latent prediction because E[y|X] accounts for\n",
    "  the truncated normal.\n",
    "\n",
    "- **Unlikely worker**: Low education, many children, high non-labor income.\n",
    "  The latent prediction is negative (desired hours are below zero), indicating\n",
    "  this person would prefer not to work. The probability of working is low.\n",
    "\n",
    "**Key insight:** The identity $E[y|X] = P(y>0|X) \\cdot E[y|y>0,X]$ holds exactly.\n",
    "This shows that the unconditional expectation decomposes into the probability of\n",
    "participation times the expected hours conditional on participating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 3: Right-Censoring (Medium)\n",
    "\n",
    "**Task:** Suppose weekly hours are **right-censored** at 40 hours (full-time workers\n",
    "are all recorded as 40 hours even if they work more). Create an artificial right-censored\n",
    "variable and estimate:\n",
    "1. A Tobit model with `censoring_type='right'` and `censoring_point=40.0`\n",
    "2. Compare with OLS and the left-censored Tobit\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-right-censored",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('EXERCISE 3: RIGHT-CENSORING AT 40 HOURS')\n",
    "print('=' * 80)\n",
    "\n",
    "# Step 1: Create the right-censored variable\n",
    "hours_right_censored = np.minimum(df['hours'].values, 40.0)\n",
    "\n",
    "n_right_censored = (hours_right_censored == 40.0).sum()\n",
    "n_left_censored = (df['hours'].values == 0).sum()\n",
    "\n",
    "print(f'\\nOriginal hours range: [{df[\"hours\"].min():.1f}, {df[\"hours\"].max():.1f}]')\n",
    "print(f'Right-censored hours range: [{hours_right_censored.min():.1f}, {hours_right_censored.max():.1f}]')\n",
    "print(f'\\nObservations censored at 40: {n_right_censored} ({n_right_censored/len(y):.1%})')\n",
    "print(f'Observations at 0 (not working): {(hours_right_censored == 0).sum()}')\n",
    "print(f'Uncensored observations (0 < h < 40): {((hours_right_censored > 0) & (hours_right_censored < 40)).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Estimate the right-censored Tobit model\n",
    "# Note: For right-censoring, we use only the uncensored observations\n",
    "# plus those censored at the upper limit. We exclude the zeros\n",
    "# (which represent left-censoring, not right-censoring).\n",
    "# However, to keep it comparable, we use the right-censored variable as-is\n",
    "# (treating the 40-hour ceiling as the censoring point).\n",
    "\n",
    "tobit_right = PooledTobit(\n",
    "    endog=hours_right_censored,\n",
    "    exog=X,\n",
    "    censoring_point=40.0,\n",
    "    censoring_type='right'\n",
    ")\n",
    "tobit_right = tobit_right.fit()\n",
    "tobit_right.exog_names = var_names\n",
    "\n",
    "print('Right-censored Tobit model fitted.')\n",
    "print(f'Converged: {tobit_right.converged}')\n",
    "print(f'Log-likelihood: {tobit_right.llf:.3f}')\n",
    "print(f'Sigma: {tobit_right.sigma:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-ols-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Estimate OLS on the right-censored data for comparison\n",
    "ols_right = sm.OLS(hours_right_censored, X).fit()\n",
    "\n",
    "print('OLS on right-censored data fitted.')\n",
    "print(f'R-squared: {ols_right.rsquared:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compare all models\n",
    "print('\\n' + '=' * 100)\n",
    "print('COEFFICIENT COMPARISON: LEFT-CENSORED TOBIT vs. RIGHT-CENSORED TOBIT vs. OLS')\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'\\n{\"Variable\":<22} {\"Left Tobit\":>14} {\"Right Tobit\":>14} {\"OLS (full)\":>14} {\"OLS (right cens.)\":>18}')\n",
    "print('-' * 84)\n",
    "\n",
    "for i, name in enumerate(var_names):\n",
    "    print(f'{name:<22} {tobit_full.beta[i]:>14.4f} {tobit_right.beta[i]:>14.4f} '\n",
    "          f'{ols_full.params[i]:>14.4f} {ols_right.params[i]:>18.4f}')\n",
    "\n",
    "print(f'\\n{\"sigma\":<22} {tobit_full.sigma:>14.4f} {tobit_right.sigma:>14.4f} '\n",
    "      f'{np.sqrt(ols_full.mse_resid):>14.4f} {np.sqrt(ols_right.mse_resid):>18.4f}')\n",
    "print(f'{\"Log-likelihood\":<22} {tobit_full.llf:>14.3f} {tobit_right.llf:>14.3f} '\n",
    "      f'{\"N/A\":>14} {\"N/A\":>18}')\n",
    "\n",
    "print('\\n' + '=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize the right-censored data and predictions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Panel 1: Distribution of right-censored hours\n",
    "ax = axes[0]\n",
    "ax.hist(hours_right_censored, bins=40, color='steelblue', edgecolor='white', alpha=0.8)\n",
    "ax.axvline(x=40, color='red', linestyle='--', linewidth=2, label='Censoring point (40h)')\n",
    "ax.set_xlabel('Hours Worked per Week')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Right-Censored Hours\\n(Pile-up at 40)')\n",
    "ax.legend()\n",
    "\n",
    "# Panel 2: Compare original and right-censored\n",
    "ax = axes[1]\n",
    "ax.hist(df['hours'].values, bins=40, color='lightcoral', edgecolor='white',\n",
    "        alpha=0.5, label='Original hours')\n",
    "ax.hist(hours_right_censored, bins=40, color='steelblue', edgecolor='white',\n",
    "        alpha=0.5, label='Right-censored (at 40)')\n",
    "ax.axvline(x=40, color='red', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Hours Worked per Week')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Original vs. Right-Censored')\n",
    "ax.legend()\n",
    "\n",
    "# Panel 3: Coefficient comparison\n",
    "ax = axes[2]\n",
    "plot_vars = feature_cols\n",
    "x_pos = np.arange(len(plot_vars))\n",
    "width = 0.25\n",
    "\n",
    "left_coefs = [tobit_full.beta[i+1] for i in range(len(plot_vars))]\n",
    "right_coefs = [tobit_right.beta[i+1] for i in range(len(plot_vars))]\n",
    "ols_coefs = [ols_full.params[i+1] for i in range(len(plot_vars))]\n",
    "\n",
    "ax.bar(x_pos - width, left_coefs, width, label='Left Tobit', color='steelblue', edgecolor='white')\n",
    "ax.bar(x_pos, right_coefs, width, label='Right Tobit', color='darkorange', edgecolor='white')\n",
    "ax.bar(x_pos + width, ols_coefs, width, label='OLS', color='lightcoral', edgecolor='white')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(plot_vars, rotation=45, ha='right')\n",
    "ax.set_ylabel('Coefficient')\n",
    "ax.set_title('Coefficient Comparison Across Models')\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / '01_ex3_right_censoring.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Figure saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "**Right-censoring** occurs when the dependent variable is capped at an upper limit.\n",
    "Here, we cap hours at 40 (simulating a full-time constraint). Key observations:\n",
    "\n",
    "1. **Right-censored Tobit vs. Left-censored Tobit**: The left-censored model addresses\n",
    "   the pile-up at zero, while the right-censored model addresses the pile-up at 40.\n",
    "   They recover different aspects of the latent distribution.\n",
    "\n",
    "2. **Right-censored Tobit vs. OLS**: OLS on right-censored data attenuates\n",
    "   coefficients toward zero because the capped observations reduce the apparent\n",
    "   relationship between regressors and hours. The right-censored Tobit corrects\n",
    "   for this by modeling the censoring mechanism.\n",
    "\n",
    "3. **Practical relevance**: Right-censoring is common in survey data where\n",
    "   top-coding is applied (e.g., income capped at a maximum value, test scores\n",
    "   capped at 100%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex4-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 4: Manual McDonald-Moffitt Decomposition (Hard)\n",
    "\n",
    "**Task:** Manually compute all three types of marginal effects using the formulas\n",
    "from Section 6 of the main notebook, and verify that they match the PanelBox output.\n",
    "\n",
    "For each observation $i$, compute:\n",
    "1. $z_i = \\mathbf{X}_i'\\hat{\\boldsymbol{\\beta}} / \\hat{\\sigma}$\n",
    "2. $\\Phi(z_i)$ and $\\phi(z_i)$\n",
    "3. The inverse Mills ratio $\\lambda(z_i) = \\phi(z_i)/\\Phi(z_i)$\n",
    "4. For each variable $k$:\n",
    "   - **Unconditional ME:** $\\hat{\\beta}_k \\cdot \\Phi(z_i)$\n",
    "   - **Conditional ME:** $\\hat{\\beta}_k \\cdot [1 - \\lambda(z_i)(z_i + \\lambda(z_i))]$\n",
    "   - **Probability ME:** $(\\hat{\\beta}_k / \\hat{\\sigma}) \\cdot \\phi(z_i)$\n",
    "5. Average across all observations to get AME\n",
    "6. Compare with `tobit_full.marginal_effects(at='overall', which=...)`\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('EXERCISE 4: MANUAL McDONALD-MOFFITT DECOMPOSITION')\n",
    "print('=' * 80)\n",
    "\n",
    "# Extract estimated parameters\n",
    "beta = tobit_full.beta\n",
    "sigma = tobit_full.sigma\n",
    "\n",
    "print(f'\\nEstimated parameters:')\n",
    "print(f'  sigma = {sigma:.6f}')\n",
    "for i, name in enumerate(var_names):\n",
    "    print(f'  beta[{name}] = {beta[i]:.6f}')\n",
    "\n",
    "# Step 1: Compute z_i = X_i'beta / sigma for all observations\n",
    "linear_pred = X @ beta         # X'beta for each observation\n",
    "z = linear_pred / sigma        # Standardized index\n",
    "\n",
    "print(f'\\nLinear prediction X\\'beta:')\n",
    "print(f'  Mean: {linear_pred.mean():.4f}')\n",
    "print(f'  Range: [{linear_pred.min():.4f}, {linear_pred.max():.4f}]')\n",
    "\n",
    "# Step 2: Compute Phi(z) and phi(z)\n",
    "Phi_z = stats.norm.cdf(z)      # Standard normal CDF\n",
    "phi_z = stats.norm.pdf(z)      # Standard normal PDF\n",
    "\n",
    "print(f'\\nPhi(z) -- P(y > 0 | X):')\n",
    "print(f'  Mean: {Phi_z.mean():.4f}')\n",
    "print(f'  Range: [{Phi_z.min():.4f}, {Phi_z.max():.4f}]')\n",
    "\n",
    "# Step 3: Compute inverse Mills ratio lambda(z) = phi(z) / Phi(z)\n",
    "# Use safe computation to avoid division by zero\n",
    "lambda_z = np.where(Phi_z > 1e-10, phi_z / Phi_z, -z)\n",
    "\n",
    "print(f'\\nInverse Mills ratio lambda(z):')\n",
    "print(f'  Mean: {lambda_z.mean():.4f}')\n",
    "print(f'  Range: [{lambda_z.min():.4f}, {lambda_z.max():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-compute-me",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute the three types of marginal effects for each variable\n",
    "\n",
    "manual_ame = {'unconditional': {}, 'conditional': {}, 'probability': {}}\n",
    "\n",
    "for k, name in enumerate(var_names):\n",
    "    if name == 'const':\n",
    "        continue  # Skip intercept\n",
    "    \n",
    "    beta_k = beta[k]\n",
    "    \n",
    "    # Unconditional ME: beta_k * Phi(z_i)\n",
    "    me_uncond_i = beta_k * Phi_z\n",
    "    manual_ame['unconditional'][name] = np.mean(me_uncond_i)\n",
    "    \n",
    "    # Conditional ME: beta_k * [1 - lambda(z_i) * (z_i + lambda(z_i))]\n",
    "    scaling_factor = 1.0 - lambda_z * (z + lambda_z)\n",
    "    me_cond_i = beta_k * scaling_factor\n",
    "    manual_ame['conditional'][name] = np.mean(me_cond_i)\n",
    "    \n",
    "    # Probability ME: (beta_k / sigma) * phi(z_i)\n",
    "    me_prob_i = (beta_k / sigma) * phi_z\n",
    "    manual_ame['probability'][name] = np.mean(me_prob_i)\n",
    "\n",
    "print('Manual AME computation complete.')\n",
    "print(f'\\nComputed marginal effects for {len(manual_ame[\"unconditional\"])} variables.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-panelbox-me",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Get PanelBox marginal effects for comparison\n",
    "\n",
    "pb_me_uncond = tobit_full.marginal_effects(at='overall', which='unconditional')\n",
    "pb_me_cond = tobit_full.marginal_effects(at='overall', which='conditional')\n",
    "pb_me_prob = tobit_full.marginal_effects(at='overall', which='probability')\n",
    "\n",
    "print('PanelBox AME computation complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Compare manual vs. PanelBox results\n",
    "\n",
    "print('\\n' + '=' * 100)\n",
    "print('COMPARISON: MANUAL vs. PANELBOX -- UNCONDITIONAL MARGINAL EFFECTS')\n",
    "print('  dE[y|X]/dx_k = beta_k * Phi(z), averaged over all observations')\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'\\n{\"Variable\":<22} {\"Manual\":>14} {\"PanelBox\":>14} {\"Difference\":>14}')\n",
    "print('-' * 66)\n",
    "\n",
    "for name in feature_cols:\n",
    "    manual_val = manual_ame['unconditional'][name]\n",
    "    pb_val = pb_me_uncond.marginal_effects.get(name, np.nan)\n",
    "    diff = manual_val - pb_val\n",
    "    print(f'{name:<22} {manual_val:>14.6f} {pb_val:>14.6f} {diff:>14.8f}')\n",
    "\n",
    "\n",
    "print('\\n' + '=' * 100)\n",
    "print('COMPARISON: MANUAL vs. PANELBOX -- CONDITIONAL MARGINAL EFFECTS')\n",
    "print('  dE[y|y>0,X]/dx_k = beta_k * [1 - lambda(z)*(z + lambda(z))], averaged')\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'\\n{\"Variable\":<22} {\"Manual\":>14} {\"PanelBox\":>14} {\"Difference\":>14}')\n",
    "print('-' * 66)\n",
    "\n",
    "for name in feature_cols:\n",
    "    manual_val = manual_ame['conditional'][name]\n",
    "    pb_val = pb_me_cond.marginal_effects.get(name, np.nan)\n",
    "    diff = manual_val - pb_val\n",
    "    print(f'{name:<22} {manual_val:>14.6f} {pb_val:>14.6f} {diff:>14.8f}')\n",
    "\n",
    "\n",
    "print('\\n' + '=' * 100)\n",
    "print('COMPARISON: MANUAL vs. PANELBOX -- PROBABILITY MARGINAL EFFECTS')\n",
    "print('  dP(y>0|X)/dx_k = (beta_k / sigma) * phi(z), averaged')\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'\\n{\"Variable\":<22} {\"Manual\":>14} {\"PanelBox\":>14} {\"Difference\":>14}')\n",
    "print('-' * 66)\n",
    "\n",
    "for name in feature_cols:\n",
    "    manual_val = manual_ame['probability'][name]\n",
    "    pb_val = pb_me_prob.marginal_effects.get(name, np.nan)\n",
    "    diff = manual_val - pb_val\n",
    "    print(f'{name:<22} {manual_val:>14.6f} {pb_val:>14.6f} {diff:>14.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-decomposition-verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Verify the McDonald-Moffitt decomposition\n",
    "# The unconditional ME decomposes as:\n",
    "#   dE[y|X]/dx_k = P(y>0|X) * dE[y|y>0,X]/dx_k + E[y|y>0,X] * dP(y>0|X)/dx_k\n",
    "#\n",
    "# To verify this at the observation level, we compute both sides and average.\n",
    "\n",
    "print('\\n' + '=' * 100)\n",
    "print('VERIFICATION: McDONALD-MOFFITT DECOMPOSITION')\n",
    "print('  dE[y|X]/dx_k = P(y>0) * dE[y|y>0,X]/dx_k + E[y|y>0,X] * dP(y>0)/dx_k')\n",
    "print('=' * 100)\n",
    "\n",
    "# Compute E[y|y>0,X] = X'beta + sigma * lambda(z) for each observation\n",
    "E_y_conditional = linear_pred + sigma * lambda_z\n",
    "\n",
    "# The scaling factor for the conditional ME\n",
    "scaling_cond = 1.0 - lambda_z * (z + lambda_z)\n",
    "\n",
    "print(f'\\n{\"Variable\":<22} {\"Uncond. ME\":>14} {\"Decomposed\":>14} {\"Difference\":>14}')\n",
    "print('-' * 66)\n",
    "\n",
    "for k, name in enumerate(var_names):\n",
    "    if name == 'const':\n",
    "        continue\n",
    "    \n",
    "    beta_k = beta[k]\n",
    "    \n",
    "    # Left-hand side: unconditional ME (averaged)\n",
    "    lhs_i = beta_k * Phi_z\n",
    "    lhs = np.mean(lhs_i)\n",
    "    \n",
    "    # Right-hand side: decomposition (averaged)\n",
    "    # Term 1: P(y>0|X) * dE[y|y>0,X]/dx_k\n",
    "    term1_i = Phi_z * (beta_k * scaling_cond)\n",
    "    \n",
    "    # Term 2: E[y|y>0,X] * dP(y>0|X)/dx_k\n",
    "    term2_i = E_y_conditional * ((beta_k / sigma) * phi_z)\n",
    "    \n",
    "    rhs = np.mean(term1_i + term2_i)\n",
    "    diff = lhs - rhs\n",
    "    \n",
    "    print(f'{name:<22} {lhs:>14.6f} {rhs:>14.6f} {diff:>14.8f}')\n",
    "\n",
    "print(f'\\nAll differences should be near zero (within numerical precision).')\n",
    "print(f'This confirms the McDonald-Moffitt decomposition identity.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-summary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Create a comprehensive summary table\n",
    "\n",
    "print('\\n' + '=' * 100)\n",
    "print('COMPREHENSIVE SUMMARY: ALL MARGINAL EFFECTS')\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'\\n{\"Variable\":<22} {\"Tobit beta\":>12} {\"Uncond. ME\":>12} {\"Cond. ME\":>12} '\n",
    "      f'{\"Prob. ME\":>12} {\"Ratio U/beta\":>12}')\n",
    "print('-' * 84)\n",
    "\n",
    "for name in feature_cols:\n",
    "    idx = var_names.index(name)\n",
    "    b = beta[idx]\n",
    "    me_u = manual_ame['unconditional'][name]\n",
    "    me_c = manual_ame['conditional'][name]\n",
    "    me_p = manual_ame['probability'][name]\n",
    "    ratio = me_u / b if abs(b) > 1e-10 else np.nan\n",
    "    \n",
    "    print(f'{name:<22} {b:>12.4f} {me_u:>12.4f} {me_c:>12.4f} {me_p:>12.4f} {ratio:>12.4f}')\n",
    "\n",
    "print(f'\\nThe Ratio U/beta column shows how much the unconditional ME is scaled')\n",
    "print(f'relative to the raw Tobit coefficient. This ratio equals the average')\n",
    "print(f'Phi(z_i), i.e., the average probability of being uncensored.')\n",
    "print(f'Average P(y>0|X) = {Phi_z.mean():.4f}')\n",
    "print(f'\\nKey takeaway: The Tobit beta OVERESTIMATES the unconditional effect')\n",
    "print(f'by a factor of roughly 1/{Phi_z.mean():.2f} = {1/Phi_z.mean():.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Visualize the three types of marginal effects\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Panel 1: Comparison of all three ME types\n",
    "ax = axes[0]\n",
    "x_pos = np.arange(len(feature_cols))\n",
    "width = 0.25\n",
    "\n",
    "me_u_vals = [manual_ame['unconditional'][name] for name in feature_cols]\n",
    "me_c_vals = [manual_ame['conditional'][name] for name in feature_cols]\n",
    "me_p_vals = [manual_ame['probability'][name] for name in feature_cols]\n",
    "\n",
    "ax.bar(x_pos - width, me_u_vals, width, label='Unconditional (dE[y|X]/dx)',\n",
    "       color='steelblue', edgecolor='white')\n",
    "ax.bar(x_pos, me_c_vals, width, label='Conditional (dE[y|y>0,X]/dx)',\n",
    "       color='darkorange', edgecolor='white')\n",
    "ax.bar(x_pos + width, me_p_vals, width, label='Probability (dP(y>0|X)/dx)',\n",
    "       color='mediumseagreen', edgecolor='white')\n",
    "\n",
    "ax.set_xlabel('Variable')\n",
    "ax.set_ylabel('Average Marginal Effect')\n",
    "ax.set_title('Manual McDonald-Moffitt Decomposition\\nThree Types of Marginal Effects')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(feature_cols, rotation=45, ha='right')\n",
    "ax.legend(loc='lower left', fontsize=9)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Panel 2: Distribution of observation-level unconditional ME for 'children'\n",
    "ax = axes[1]\n",
    "beta_children = beta[var_names.index('children')]\n",
    "me_children_i = beta_children * Phi_z\n",
    "\n",
    "ax.hist(me_children_i, bins=40, color='steelblue', edgecolor='white', alpha=0.8)\n",
    "ax.axvline(x=np.mean(me_children_i), color='red', linestyle='--', linewidth=2,\n",
    "           label=f'AME = {np.mean(me_children_i):.4f}')\n",
    "ax.set_xlabel('Unconditional ME of Children')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Observation-Level\\nUnconditional ME for Children')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / '01_ex4_manual_decomposition.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Figure saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex4-interpretation",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The manual computation confirms several important properties of the Tobit model:\n",
    "\n",
    "1. **Manual vs. PanelBox agreement**: The manually computed AMEs match the PanelBox\n",
    "   output to machine precision, confirming both the formulas and the implementation.\n",
    "\n",
    "2. **Unconditional ME < Tobit beta**: The unconditional marginal effect is always\n",
    "   smaller in absolute value than the Tobit coefficient. The scaling factor is\n",
    "   $\\Phi(z_i)$, the probability of being uncensored. This reflects the fact that\n",
    "   a change in $x_k$ only affects observed hours through the \"uncensored\" channel.\n",
    "\n",
    "3. **Conditional ME < Tobit beta**: The conditional marginal effect is also attenuated\n",
    "   relative to the Tobit beta, but by a different (and generally smaller) factor.\n",
    "   The scaling factor $[1 - \\lambda(z)(z + \\lambda(z))]$ is always between 0 and 1.\n",
    "\n",
    "4. **Probability ME**: The probability marginal effect shows how a one-unit change\n",
    "   in $x_k$ shifts the probability of participating in the labor force. These effects\n",
    "   are on a different scale (probability units rather than hours).\n",
    "\n",
    "5. **McDonald-Moffitt decomposition**: The identity\n",
    "   $\\frac{\\partial E[y|X]}{\\partial x_k} = P(y>0|X) \\cdot \\frac{\\partial E[y|y>0,X]}{\\partial x_k} + E[y|y>0,X] \\cdot \\frac{\\partial P(y>0|X)}{\\partial x_k}$\n",
    "   holds exactly. This decomposition shows that a change in $x_k$ affects observed\n",
    "   hours through two channels:\n",
    "   - The **intensive margin**: changing hours among those who already work\n",
    "   - The **extensive margin**: changing the probability of working at all\n",
    "\n",
    "6. **Heterogeneity**: The observation-level marginal effects vary across individuals.\n",
    "   The histogram for the children variable shows that the negative effect of children\n",
    "   on hours is larger (more negative) for individuals with higher baseline probability\n",
    "   of working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "| Exercise | Topic | Key Result |\n",
    "|----------|-------|------------|\n",
    "| 1 | Variable Selection | LR test determines if restricted model is adequate |\n",
    "| 2 | Prediction Profiles | E[y\\|X] = P(y>0\\|X) * E[y\\|y>0,X] verified |\n",
    "| 3 | Right-Censoring | Right-censored Tobit recovers latent effects at the upper boundary |\n",
    "| 4 | Manual Decomposition | Manual AMEs match PanelBox; decomposition identity holds exactly |\n",
    "\n",
    "### Key Formulas Implemented\n",
    "\n",
    "| Quantity | Formula |\n",
    "|----------|--------|\n",
    "| Unconditional ME | $\\beta_k \\cdot \\Phi(z)$ |\n",
    "| Conditional ME | $\\beta_k \\cdot [1 - \\lambda(z)(z + \\lambda(z))]$ |\n",
    "| Probability ME | $(\\beta_k / \\sigma) \\cdot \\phi(z)$ |\n",
    "| Inverse Mills Ratio | $\\lambda(z) = \\phi(z) / \\Phi(z)$ |\n",
    "| LR Test | $-2(\\ln L_R - \\ln L_U) \\sim \\chi^2(q)$ |\n",
    "\n",
    "where $z = \\mathbf{X}'\\boldsymbol{\\beta}/\\sigma$.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Solutions for Notebook 01: Introduction to Tobit Models**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
