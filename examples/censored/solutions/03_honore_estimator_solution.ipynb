{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# Notebook 03: The Honoré (1992) Trimmed LAD Estimator -- SOLUTIONS\n",
    "\n",
    "**This is the worked solution notebook.**  \n",
    "It provides complete, working solutions for all 4 exercises from `03_honore_estimator.ipynb`.\n",
    "\n",
    "> Instructors: do not distribute this file to students before they complete the tutorial notebook.\n",
    "\n",
    "## Exercise Overview\n",
    "\n",
    "| Exercise | Topic | Difficulty | Key Concept |\n",
    "|----------|-------|------------|-------------|\n",
    "| 1 | Trimming Rate Analysis | Easy | How censoring threshold affects informative pairs |\n",
    "| 2 | Honoré vs RE Tobit Robustness | Medium | Coefficient stability across covariate subsets |\n",
    "| 3 | Sensitivity to Panel Length | Medium | Impact of T on pairwise estimation |\n",
    "| 4 | Simulated Comparison — When RE Fails | Hard | Demonstrating RE bias with correlated effects |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# PanelBox imports\n",
    "from panelbox.models.censored import PooledTobit, RandomEffectsTobit, HonoreTrimmedEstimator\n",
    "\n",
    "# Visualization configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define paths (relative to notebook location in examples/censored/solutions/)\n",
    "BASE_DIR = Path('..')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures' / '03_honore'\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables' / '03_honore'\n",
    "\n",
    "# Create output directories\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load consumer durables panel data\n",
    "df = pd.read_csv(DATA_DIR / 'consumer_durables_panel.csv')\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Unique households: {df[\"id\"].nunique()}')\n",
    "print(f'Time periods: {df[\"time\"].nunique()} ({df[\"time\"].min()}-{df[\"time\"].max()})')\n",
    "print(f'Censoring rate: {(df[\"spending\"] == 0).mean() * 100:.1f}%')\n",
    "print()\n",
    "display(df.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "base-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Preparation and Base Models\n",
    "\n",
    "Before tackling the exercises, we prepare the estimation data and fit baseline\n",
    "Honoré and RE Tobit models that will serve as references for all exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare base arrays\n",
    "var_names_honore = ['income', 'wealth', 'household_size', 'credit_score']\n",
    "\n",
    "# Use subsample of 50 households for Honoré (as in main notebook)\n",
    "np.random.seed(42)\n",
    "all_ids = df['id'].unique()\n",
    "subsample_ids = np.random.choice(all_ids, size=50, replace=False)\n",
    "df_sub = df[df['id'].isin(subsample_ids)].copy().reset_index(drop=True)\n",
    "\n",
    "# Honoré arrays (NO constant)\n",
    "y_honore = df_sub['spending'].values\n",
    "X_honore = df_sub[var_names_honore].values\n",
    "groups_honore = df_sub['id'].values\n",
    "time_honore = df_sub['time'].values\n",
    "\n",
    "# RE Tobit arrays (WITH constant)\n",
    "X_re = sm.add_constant(X_honore)\n",
    "\n",
    "print(f'Subsample: {df_sub[\"id\"].nunique()} households, {len(df_sub)} observations')\n",
    "print(f'Censoring rate: {(df_sub[\"spending\"] == 0).mean() * 100:.1f}%')\n",
    "print(f'Covariates (Honoré): {var_names_honore}')\n",
    "print(f'X_honore shape: {X_honore.shape} (no constant)')\n",
    "print(f'X_re shape: {X_re.shape} (with constant)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "base-honore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fit base Honoré model (subsample, all covariates)\n",
    "# ============================================================\n",
    "\n",
    "print('Fitting base Honoré Trimmed LAD Estimator...')\n",
    "print('=' * 60)\n",
    "\n",
    "try:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        honore_base = HonoreTrimmedEstimator(\n",
    "            endog=y_honore,\n",
    "            exog=X_honore,\n",
    "            groups=groups_honore,\n",
    "            time=time_honore,\n",
    "            censoring_point=0.0\n",
    "        )\n",
    "        honore_base_result = honore_base.fit(verbose=True)\n",
    "    \n",
    "    print(f'\\nConverged: {honore_base_result.converged}')\n",
    "    print(f'Iterations: {honore_base_result.n_iter}')\n",
    "    print(f'Trimmed pairs: {honore_base_result.n_trimmed}')\n",
    "    print('\\nCoefficients:')\n",
    "    for i, name in enumerate(var_names_honore):\n",
    "        print(f'  {name:>20s}: {honore_base_result.params[i]:.4f}')\n",
    "    honore_base_ok = True\n",
    "except Exception as e:\n",
    "    print(f'Base Honoré estimation failed: {e}')\n",
    "    honore_base_ok = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "base-re-tobit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Fit base RE Tobit model (same subsample)\n",
    "# ============================================================\n",
    "\n",
    "print('Fitting base Random Effects Tobit...')\n",
    "print('=' * 60)\n",
    "\n",
    "try:\n",
    "    re_base = RandomEffectsTobit(\n",
    "        endog=y_honore,\n",
    "        exog=X_re,\n",
    "        groups=groups_honore,\n",
    "        time=time_honore,\n",
    "        censoring_point=0.0,\n",
    "        censoring_type='left',\n",
    "        quadrature_points=12\n",
    "    )\n",
    "    re_base.fit(method='BFGS', maxiter=1000)\n",
    "    print(re_base.summary())\n",
    "    re_base_ok = True\n",
    "except Exception as e:\n",
    "    print(f'Base RE Tobit estimation failed: {e}')\n",
    "    re_base_ok = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Trimming Rate Analysis (Easy)\n",
    "\n",
    "**Task**: Using the consumer durables panel, calculate the trimming rate for\n",
    "different censoring thresholds. Vary the censoring point from 0 to 500 in\n",
    "steps of 100. Plot the relationship between censoring threshold and the\n",
    "percentage of pairs trimmed. At what point does the estimator become\n",
    "unreliable (>80% trimmed)?\n",
    "\n",
    "### Background\n",
    "\n",
    "The Honoré estimator trims pairs where **both** observations are censored\n",
    "(i.e., both at or below the censoring point). A higher censoring threshold\n",
    "means more observations are classified as censored, which increases the\n",
    "trimming rate. When too many pairs are trimmed, the estimator uses very\n",
    "little information and becomes unreliable.\n",
    "\n",
    "The trimming rate can be computed analytically without running the full\n",
    "optimization: for each entity, count pairwise differences where both\n",
    "observations are at or below the censoring point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-trimming-calc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1 Solution: Trimming Rate Analysis\n",
    "# ============================================================\n",
    "\n",
    "# Use the full dataset (N=200) for this analysis\n",
    "censoring_thresholds = [0, 100, 200, 300, 400, 500]\n",
    "trimming_results = []\n",
    "\n",
    "for threshold in censoring_thresholds:\n",
    "    total_pairs = 0\n",
    "    trimmed_pairs = 0\n",
    "    \n",
    "    # Count censored observations at this threshold\n",
    "    n_censored_obs = (df['spending'] <= threshold).sum()\n",
    "    pct_censored_obs = 100 * n_censored_obs / len(df)\n",
    "    \n",
    "    for hh_id in df['id'].unique():\n",
    "        hh_data = df[df['id'] == hh_id].sort_values('time')\n",
    "        spending = hh_data['spending'].values\n",
    "        T_hh = len(spending)\n",
    "        \n",
    "        for t in range(T_hh):\n",
    "            for s in range(t + 1, T_hh):\n",
    "                total_pairs += 1\n",
    "                # Both censored at threshold?\n",
    "                if spending[t] <= threshold and spending[s] <= threshold:\n",
    "                    trimmed_pairs += 1\n",
    "    \n",
    "    kept_pairs = total_pairs - trimmed_pairs\n",
    "    pct_trimmed = 100 * trimmed_pairs / total_pairs if total_pairs > 0 else 0\n",
    "    \n",
    "    trimming_results.append({\n",
    "        'Censoring Threshold': threshold,\n",
    "        'Obs Censored (%)': round(pct_censored_obs, 1),\n",
    "        'Total Pairs': total_pairs,\n",
    "        'Trimmed Pairs': trimmed_pairs,\n",
    "        'Kept Pairs': kept_pairs,\n",
    "        'Trimming Rate (%)': round(pct_trimmed, 1)\n",
    "    })\n",
    "\n",
    "trim_df = pd.DataFrame(trimming_results)\n",
    "\n",
    "print('Trimming Rate Analysis: Varying Censoring Threshold')\n",
    "print('=' * 80)\n",
    "print(f'Dataset: N={df[\"id\"].nunique()} households, T={df[\"time\"].nunique()} periods')\n",
    "print(f'Pairs per household: C(5,2) = 10')\n",
    "print(f'Total pairs: {trim_df[\"Total Pairs\"].iloc[0]}')\n",
    "print()\n",
    "display(trim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization: Trimming rate vs censoring threshold\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Trimming rate vs threshold with danger zone\n",
    "ax = axes[0]\n",
    "ax.plot(trim_df['Censoring Threshold'], trim_df['Trimming Rate (%)'],\n",
    "        'o-', markersize=10, linewidth=2.5, color='steelblue', zorder=5)\n",
    "\n",
    "# Danger zone above 80%\n",
    "ax.axhspan(80, 100, alpha=0.15, color='red', label='Danger zone (>80% trimmed)')\n",
    "ax.axhline(y=80, color='red', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Annotate data points\n",
    "for _, row in trim_df.iterrows():\n",
    "    ax.annotate(f'{row[\"Trimming Rate (%)\"]:.0f}%',\n",
    "                (row['Censoring Threshold'], row['Trimming Rate (%)']),\n",
    "                textcoords='offset points', xytext=(0, 12),\n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Censoring Threshold', fontsize=12)\n",
    "ax.set_ylabel('Trimming Rate (%)', fontsize=12)\n",
    "ax.set_title('Trimming Rate vs Censoring Threshold', fontsize=13)\n",
    "ax.set_ylim(-5, 105)\n",
    "ax.legend(fontsize=10, loc='upper left')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Right: Kept pairs vs threshold\n",
    "ax = axes[1]\n",
    "colors = ['#D55E00' if r > 80 else '#009E73' for r in trim_df['Trimming Rate (%)']]\n",
    "ax.bar(range(len(trim_df)), trim_df['Kept Pairs'], color=colors,\n",
    "       alpha=0.8, edgecolor='black')\n",
    "ax.set_xticks(range(len(trim_df)))\n",
    "ax.set_xticklabels([f'c={t}' for t in trim_df['Censoring Threshold']])\n",
    "ax.set_xlabel('Censoring Threshold', fontsize=12)\n",
    "ax.set_ylabel('Number of Informative (Kept) Pairs', fontsize=12)\n",
    "ax.set_title('Informative Pairs Remaining After Trimming', fontsize=13)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Annotate bar values\n",
    "for i, val in enumerate(trim_df['Kept Pairs']):\n",
    "    ax.text(i, val + 15, str(val), ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex1_trimming_rate_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Find the threshold where trimming exceeds 80%\n",
    "# ============================================================\n",
    "\n",
    "print('Analysis: When Does the Estimator Become Unreliable?')\n",
    "print('=' * 60)\n",
    "\n",
    "danger_mask = trim_df['Trimming Rate (%)'] > 80\n",
    "if danger_mask.any():\n",
    "    first_danger = trim_df.loc[danger_mask].iloc[0]\n",
    "    print(f'The trimming rate first exceeds 80% at threshold = {first_danger[\"Censoring Threshold\"]}')\n",
    "    print(f'  Trimming rate: {first_danger[\"Trimming Rate (%)\"]:.1f}%')\n",
    "    print(f'  Kept pairs: {first_danger[\"Kept Pairs\"]} (out of {first_danger[\"Total Pairs\"]})')\n",
    "    print()\n",
    "    print('At this point, the Honoré estimator is using very few')\n",
    "    print('informative pairs and estimates become highly variable.')\n",
    "else:\n",
    "    print('Trimming rate stays below 80% for all tested thresholds.')\n",
    "    print('The estimator remains usable across the tested range.')\n",
    "\n",
    "print()\n",
    "print('Rule of thumb:')\n",
    "print('  - Trimming < 50%: Good — plenty of informative pairs')\n",
    "print('  - Trimming 50-70%: Acceptable — estimates may be noisy')\n",
    "print('  - Trimming 70-80%: Caution — consider alternative methods')\n",
    "print('  - Trimming > 80%: Unreliable — too few informative pairs')\n",
    "\n",
    "# Save results table\n",
    "trim_df.to_csv(TABLES_DIR / 'ex1_trimming_rate_analysis.csv', index=False)\n",
    "print(f'\\nSaved to {TABLES_DIR / \"ex1_trimming_rate_analysis.csv\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex1-discussion",
   "metadata": {},
   "source": [
    "### Exercise 1: Discussion\n",
    "\n",
    "**Key findings from the trimming rate analysis:**\n",
    "\n",
    "1. **Monotonic relationship**: As the censoring threshold increases, more observations\n",
    "   are classified as censored, and consequently more pairs have both members censored.\n",
    "   The trimming rate increases monotonically.\n",
    "\n",
    "2. **Original censoring point (c=0)**: At the natural censoring point (spending cannot\n",
    "   be negative), the trimming rate reflects the inherent censoring in the data\n",
    "   (~40% of observations are zero).\n",
    "\n",
    "3. **Danger zone**: At higher thresholds, the trimming rate rapidly approaches 100%.\n",
    "   Once above 80%, the estimator is using fewer than 20% of available pairs, making\n",
    "   it highly sensitive to the specific pairs that survive trimming.\n",
    "\n",
    "4. **Practical implication**: The Honoré estimator works best when the censoring rate\n",
    "   is moderate (20-50%). With very high censoring (>70% of observations), most pairs\n",
    "   are trimmed and the estimator loses efficiency dramatically. In such cases,\n",
    "   the RE Tobit (despite its stronger assumptions) may be more practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Honoré vs RE Tobit Robustness (Medium)\n",
    "\n",
    "**Task**: Estimate both Honoré and RE Tobit using only a subset of covariates:\n",
    "1. `income` only\n",
    "2. `income` + `household_size`\n",
    "3. All four variables (`income`, `wealth`, `household_size`, `credit_score`)\n",
    "\n",
    "For each specification, compare the `income` coefficient between the two methods\n",
    "and assess whether the discrepancy is consistent across specifications.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "If the RE assumption ($\\alpha_i \\perp X_{it}$) holds, both estimators should\n",
    "give similar results regardless of the specification. If the RE assumption is\n",
    "violated, the discrepancy may vary with the included covariates (omitted\n",
    "variable bias interacts with the correlated effects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-specs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 2 Solution: Honoré vs RE Tobit Across Specifications\n",
    "# ============================================================\n",
    "\n",
    "specifications = [\n",
    "    {'name': 'income only', 'vars': ['income']},\n",
    "    {'name': 'income + household_size', 'vars': ['income', 'household_size']},\n",
    "    {'name': 'all variables', 'vars': ['income', 'wealth', 'household_size', 'credit_score']}\n",
    "]\n",
    "\n",
    "robustness_results = []\n",
    "\n",
    "for spec in specifications:\n",
    "    spec_name = spec['name']\n",
    "    spec_vars = spec['vars']\n",
    "    \n",
    "    print(f'\\n{\"=\" * 60}')\n",
    "    print(f'Specification: {spec_name}')\n",
    "    print(f'Variables: {spec_vars}')\n",
    "    print(f'{\"=\" * 60}')\n",
    "    \n",
    "    # Prepare arrays for this specification\n",
    "    X_spec = df_sub[spec_vars].values\n",
    "    X_spec_re = sm.add_constant(X_spec)\n",
    "    \n",
    "    # --- Honoré ---\n",
    "    honore_income = np.nan\n",
    "    honore_converged = False\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            model_h = HonoreTrimmedEstimator(\n",
    "                endog=y_honore, exog=X_spec,\n",
    "                groups=groups_honore, time=time_honore,\n",
    "                censoring_point=0.0\n",
    "            )\n",
    "            result_h = model_h.fit(verbose=False)\n",
    "        honore_income = result_h.params[0]  # income is always first\n",
    "        honore_converged = result_h.converged\n",
    "        print(f'  Honoré income coef: {honore_income:.4f} (converged: {honore_converged})')\n",
    "    except Exception as e:\n",
    "        print(f'  Honoré failed: {e}')\n",
    "    \n",
    "    # --- RE Tobit ---\n",
    "    re_income = np.nan\n",
    "    re_income_se = np.nan\n",
    "    try:\n",
    "        model_re = RandomEffectsTobit(\n",
    "            endog=y_honore, exog=X_spec_re,\n",
    "            groups=groups_honore, time=time_honore,\n",
    "            censoring_point=0.0, censoring_type='left',\n",
    "            quadrature_points=12\n",
    "        )\n",
    "        model_re.fit(method='BFGS', maxiter=1000)\n",
    "        re_income = model_re.beta[1]  # skip constant\n",
    "        re_income_se = model_re.bse[1]\n",
    "        print(f'  RE Tobit income coef: {re_income:.4f} (SE: {re_income_se:.4f})')\n",
    "    except Exception as e:\n",
    "        print(f'  RE Tobit failed: {e}')\n",
    "    \n",
    "    # Compute difference\n",
    "    diff = honore_income - re_income if not (np.isnan(honore_income) or np.isnan(re_income)) else np.nan\n",
    "    pct_diff = 100 * diff / abs(re_income) if (not np.isnan(diff) and abs(re_income) > 1e-10) else np.nan\n",
    "    \n",
    "    robustness_results.append({\n",
    "        'Specification': spec_name,\n",
    "        'N Variables': len(spec_vars),\n",
    "        'Honoré (income)': honore_income,\n",
    "        'RE Tobit (income)': re_income,\n",
    "        'RE Tobit SE': re_income_se,\n",
    "        'Difference': diff,\n",
    "        'Rel. Diff (%)': pct_diff,\n",
    "        'Honoré Converged': honore_converged\n",
    "    })\n",
    "\n",
    "robust_df = pd.DataFrame(robustness_results)\n",
    "\n",
    "print('\\n\\n' + '=' * 80)\n",
    "print('Summary: Income Coefficient Across Specifications')\n",
    "print('=' * 80)\n",
    "display(robust_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex2-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization: Income coefficient comparison across specs\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Grouped bar chart of income coefficient\n",
    "ax = axes[0]\n",
    "x_pos = np.arange(len(robust_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x_pos - width / 2, robust_df['Honoré (income)'], width,\n",
    "               label='Honoré TLAD', color='steelblue', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax.bar(x_pos + width / 2, robust_df['RE Tobit (income)'], width,\n",
    "               label='RE Tobit', color='#D55E00', alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Add RE Tobit error bars\n",
    "ax.errorbar(x_pos + width / 2, robust_df['RE Tobit (income)'],\n",
    "            yerr=1.96 * robust_df['RE Tobit SE'],\n",
    "            fmt='none', color='black', capsize=4, capthick=1.5)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['income\\nonly', 'income +\\nhh_size', 'all\\nvariables'],\n",
    "                    fontsize=10)\n",
    "ax.set_ylabel('Income Coefficient', fontsize=12)\n",
    "ax.set_title('Income Coefficient: Honoré vs RE Tobit\\nAcross Specifications', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Right: Relative difference\n",
    "ax = axes[1]\n",
    "colors = ['#D55E00' if abs(d) > 20 else '#009E73'\n",
    "          for d in robust_df['Rel. Diff (%)'].fillna(0)]\n",
    "ax.bar(x_pos, robust_df['Rel. Diff (%)'], color=colors, alpha=0.8,\n",
    "       edgecolor='black')\n",
    "\n",
    "for i, val in enumerate(robust_df['Rel. Diff (%)']):\n",
    "    if not np.isnan(val):\n",
    "        ax.text(i, val + (2 if val >= 0 else -4), f'{val:.1f}%',\n",
    "                ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['income\\nonly', 'income +\\nhh_size', 'all\\nvariables'],\n",
    "                    fontsize=10)\n",
    "ax.set_ylabel('Relative Difference (%)', fontsize=12)\n",
    "ax.set_title('Relative Difference (Honoré - RE Tobit)\\nfor Income Coefficient', fontsize=13)\n",
    "ax.axhline(y=0, color='black', linewidth=1)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex2_robustness_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save table\n",
    "robust_df.to_csv(TABLES_DIR / 'ex2_robustness_comparison.csv', index=False)\n",
    "print(f'Saved to {TABLES_DIR / \"ex2_robustness_comparison.csv\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex2-discussion",
   "metadata": {},
   "source": [
    "### Exercise 2: Discussion\n",
    "\n",
    "**Key findings from the robustness analysis:**\n",
    "\n",
    "1. **Consistency of discrepancy**: If the relative difference between Honoré and\n",
    "   RE Tobit is roughly similar across all three specifications, this suggests a\n",
    "   systematic difference rather than specification-dependent noise. A systematic\n",
    "   discrepancy is evidence of **correlated individual effects** ($\\alpha_i$ correlated\n",
    "   with income).\n",
    "\n",
    "2. **Changing discrepancy**: If the discrepancy varies substantially with the\n",
    "   covariates included, it indicates that the omitted variable bias in the RE model\n",
    "   is sensitive to the specification. This is consistent with the RE model absorbing\n",
    "   omitted variable effects into the income coefficient when fewer controls are included.\n",
    "\n",
    "3. **RE assumption assessment**: If both estimators give similar income coefficients\n",
    "   (relative difference < 10-15%) across all specifications, the RE assumption may\n",
    "   be approximately valid, and the RE Tobit can be used with more confidence.\n",
    "\n",
    "4. **Practical recommendation**: Report both estimators. If they agree, use the RE\n",
    "   Tobit (it provides standard errors). If they disagree, discuss the Honoré results\n",
    "   as a robustness check and note that the RE assumption may be violated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Sensitivity to Panel Length (Medium)\n",
    "\n",
    "**Task**: Using the full dataset (T=5), estimate Honoré. Then artificially shorten\n",
    "the panel:\n",
    "- T=4 (drop the last period)\n",
    "- T=3 (drop the last two periods)\n",
    "\n",
    "Estimate Honoré for each panel length. How do the estimates and the number of\n",
    "informative pairs change? At what T does estimation become unreliable?\n",
    "\n",
    "### Background\n",
    "\n",
    "The number of pairwise differences per entity is $\\binom{T}{2} = T(T-1)/2$.\n",
    "For T=5: 10 pairs, T=4: 6 pairs, T=3: 3 pairs, T=2: 1 pair.\n",
    "\n",
    "With fewer pairs, the estimator has less information per entity and estimation\n",
    "becomes less precise. Additionally, with shorter panels the probability that\n",
    "at least one period is uncensored decreases, increasing the trimming rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-estimation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 3 Solution: Panel Length Sensitivity\n",
    "# ============================================================\n",
    "\n",
    "# Get the time periods available\n",
    "all_times = sorted(df_sub['time'].unique())\n",
    "print(f'Available time periods: {all_times}')\n",
    "\n",
    "# Define panel length configurations\n",
    "panel_configs = [\n",
    "    {'label': 'T=5 (full)', 'periods': all_times},\n",
    "    {'label': 'T=4 (drop last)', 'periods': all_times[:4]},\n",
    "    {'label': 'T=3 (drop last 2)', 'periods': all_times[:3]}\n",
    "]\n",
    "\n",
    "panel_results = []\n",
    "\n",
    "for config in panel_configs:\n",
    "    label = config['label']\n",
    "    periods = config['periods']\n",
    "    T_eff = len(periods)\n",
    "    pairs_per_entity = T_eff * (T_eff - 1) // 2\n",
    "    \n",
    "    print(f'\\n{\"=\" * 60}')\n",
    "    print(f'{label}')\n",
    "    print(f'Periods: {periods}')\n",
    "    print(f'Pairs per entity: C({T_eff},2) = {pairs_per_entity}')\n",
    "    print(f'{\"=\" * 60}')\n",
    "    \n",
    "    # Filter data to selected periods\n",
    "    df_t = df_sub[df_sub['time'].isin(periods)].copy().reset_index(drop=True)\n",
    "    \n",
    "    y_t = df_t['spending'].values\n",
    "    X_t = df_t[var_names_honore].values\n",
    "    g_t = df_t['id'].values\n",
    "    t_t = df_t['time'].values\n",
    "    \n",
    "    n_obs = len(df_t)\n",
    "    n_entities = df_t['id'].nunique()\n",
    "    censoring_rate = (df_t['spending'] == 0).mean()\n",
    "    \n",
    "    print(f'Observations: {n_obs}')\n",
    "    print(f'Entities: {n_entities}')\n",
    "    print(f'Censoring rate: {100 * censoring_rate:.1f}%')\n",
    "    \n",
    "    # Estimate Honoré\n",
    "    row = {\n",
    "        'Panel Length': label,\n",
    "        'T': T_eff,\n",
    "        'Pairs/Entity': pairs_per_entity,\n",
    "        'Total Pairs': n_entities * pairs_per_entity,\n",
    "        'Censoring Rate (%)': round(100 * censoring_rate, 1),\n",
    "        'N Obs': n_obs\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            model_t = HonoreTrimmedEstimator(\n",
    "                endog=y_t, exog=X_t,\n",
    "                groups=g_t, time=t_t,\n",
    "                censoring_point=0.0\n",
    "            )\n",
    "            result_t = model_t.fit(verbose=False)\n",
    "        \n",
    "        row['Converged'] = result_t.converged\n",
    "        row['Trimmed Pairs'] = result_t.n_trimmed\n",
    "        row['Trim Rate (%)'] = round(\n",
    "            100 * result_t.n_trimmed / (n_entities * pairs_per_entity), 1\n",
    "        ) if n_entities * pairs_per_entity > 0 else np.nan\n",
    "        \n",
    "        for i, name in enumerate(var_names_honore):\n",
    "            row[name] = result_t.params[i]\n",
    "        \n",
    "        print(f'Converged: {result_t.converged}')\n",
    "        print(f'Trimmed pairs: {result_t.n_trimmed}')\n",
    "        print('Coefficients:')\n",
    "        for i, name in enumerate(var_names_honore):\n",
    "            print(f'  {name}: {result_t.params[i]:.4f}')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Estimation failed: {e}')\n",
    "        row['Converged'] = False\n",
    "        row['Trimmed Pairs'] = np.nan\n",
    "        row['Trim Rate (%)'] = np.nan\n",
    "        for name in var_names_honore:\n",
    "            row[name] = np.nan\n",
    "    \n",
    "    panel_results.append(row)\n",
    "\n",
    "panel_df = pd.DataFrame(panel_results)\n",
    "\n",
    "print('\\n\\n' + '=' * 90)\n",
    "print('Summary: Panel Length Sensitivity')\n",
    "print('=' * 90)\n",
    "display(panel_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization: Coefficient stability across panel lengths\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "T_vals = panel_df['T'].values\n",
    "\n",
    "# Left: Coefficient values for each variable\n",
    "ax = axes[0]\n",
    "for var_name in var_names_honore:\n",
    "    if var_name in panel_df.columns:\n",
    "        vals = panel_df[var_name].values\n",
    "        ax.plot(T_vals, vals, 'o-', markersize=9, linewidth=2, label=var_name)\n",
    "\n",
    "ax.set_xlabel('Panel Length (T)', fontsize=12)\n",
    "ax.set_ylabel('Honoré Coefficient', fontsize=12)\n",
    "ax.set_title('Coefficient Sensitivity\\nto Panel Length', fontsize=13)\n",
    "ax.set_xticks(T_vals)\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Middle: Total pairs and trimmed pairs\n",
    "ax = axes[1]\n",
    "total_p = panel_df['Total Pairs'].values\n",
    "trimmed_p = panel_df['Trimmed Pairs'].values\n",
    "kept_p = total_p - trimmed_p\n",
    "\n",
    "ax.bar(T_vals - 0.15, kept_p, 0.3, label='Kept Pairs', color='#009E73',\n",
    "       alpha=0.8, edgecolor='black')\n",
    "ax.bar(T_vals + 0.15, trimmed_p, 0.3, label='Trimmed Pairs', color='#D55E00',\n",
    "       alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Panel Length (T)', fontsize=12)\n",
    "ax.set_ylabel('Number of Pairs', fontsize=12)\n",
    "ax.set_title('Informative vs Trimmed Pairs\\nby Panel Length', fontsize=13)\n",
    "ax.set_xticks(T_vals)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Right: Trimming rate\n",
    "ax = axes[2]\n",
    "trim_rates = panel_df['Trim Rate (%)'].values\n",
    "colors = ['#D55E00' if r > 70 else '#E69F00' if r > 50 else '#009E73' for r in trim_rates]\n",
    "ax.bar(T_vals, trim_rates, 0.5, color=colors, alpha=0.8, edgecolor='black')\n",
    "\n",
    "for t, rate in zip(T_vals, trim_rates):\n",
    "    if not np.isnan(rate):\n",
    "        ax.text(t, rate + 1.5, f'{rate:.0f}%', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.axhline(y=80, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='Danger zone (80%)')\n",
    "ax.set_xlabel('Panel Length (T)', fontsize=12)\n",
    "ax.set_ylabel('Trimming Rate (%)', fontsize=12)\n",
    "ax.set_title('Trimming Rate\\nby Panel Length', fontsize=13)\n",
    "ax.set_xticks(T_vals)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex3_panel_length_sensitivity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save table\n",
    "panel_df.to_csv(TABLES_DIR / 'ex3_panel_length_sensitivity.csv', index=False)\n",
    "print(f'Saved to {TABLES_DIR / \"ex3_panel_length_sensitivity.csv\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex3-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Theoretical analysis: scaling of pairs with T\n",
    "# ============================================================\n",
    "\n",
    "print('Theoretical Scaling: Pairwise Differences and Panel Length')\n",
    "print('=' * 60)\n",
    "print(f'{\"T\":>4s} {\"C(T,2)\":>8s} {\"Ratio to T=5\":>14s} {\"Information\":>20s}')\n",
    "print('-' * 50)\n",
    "for T_val in [2, 3, 4, 5, 8, 10]:\n",
    "    pairs = T_val * (T_val - 1) // 2\n",
    "    ratio = pairs / 10  # relative to T=5\n",
    "    reliability = ('Too few' if T_val <= 2 else\n",
    "                   'Marginal' if T_val == 3 else\n",
    "                   'Adequate' if T_val <= 5 else 'Good')\n",
    "    print(f'{T_val:>4d} {pairs:>8d} {ratio:>14.1f}x {reliability:>20s}')\n",
    "\n",
    "print()\n",
    "print('With T=2, there is only 1 pair per entity — too few for reliable estimation.')\n",
    "print('With T=3, there are 3 pairs, which is marginal.')\n",
    "print('With T>=5, the estimator has ample pairwise information per entity.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3-discussion",
   "metadata": {},
   "source": [
    "### Exercise 3: Discussion\n",
    "\n",
    "**Key findings from the panel length sensitivity analysis:**\n",
    "\n",
    "1. **Pairs grow quadratically**: The number of pairwise differences per entity\n",
    "   scales as $T(T-1)/2$. Reducing from T=5 to T=3 cuts the number of pairs\n",
    "   from 10 to 3 per entity — a 70% reduction in information.\n",
    "\n",
    "2. **Trimming rate increases**: With shorter panels, each entity has fewer\n",
    "   opportunities to have at least one uncensored observation in a pair.\n",
    "   Entities that are censored in most periods contribute very few (or zero)\n",
    "   informative pairs when the panel is short.\n",
    "\n",
    "3. **Coefficient stability**: If coefficients remain roughly similar across\n",
    "   T=3, T=4, T=5, the estimator is performing well. Large changes suggest\n",
    "   instability due to insufficient information.\n",
    "\n",
    "4. **Practical threshold**: T=3 is the minimum for the Honoré estimator\n",
    "   (T=2 gives only 1 pair per entity and is too unstable). T=5 or more\n",
    "   is recommended for reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex4-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Simulated Comparison — When RE Fails (Hard)\n",
    "\n",
    "**Task**: Generate synthetic data where $\\alpha_i$ is deliberately correlated\n",
    "with $X_i$:\n",
    "\n",
    "```python\n",
    "alpha_i = 2.0 * X_mean_i + u_i\n",
    "```\n",
    "\n",
    "1. Estimate both RE Tobit and Honoré on this DGP\n",
    "2. Compare with the true $\\beta$ used in the simulation\n",
    "3. Show that RE Tobit is biased but Honoré recovers the true $\\beta$\n",
    "\n",
    "### Background\n",
    "\n",
    "The RE Tobit assumes $\\alpha_i \\perp X_{it}$. When this assumption fails\n",
    "(correlated effects), the RE estimator is inconsistent — it conflates the\n",
    "direct effect of $X$ on $y$ with the indirect effect through $\\alpha_i$.\n",
    "The Honoré estimator, by differencing out $\\alpha_i$, is immune to this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-dgp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 4 Solution: Simulated DGP with Correlated Effects\n",
    "# ============================================================\n",
    "\n",
    "# DGP parameters\n",
    "N_sim = 100       # entities\n",
    "T_sim = 5         # periods\n",
    "beta_true = np.array([1.5, 0.8])  # true coefficients for x1, x2\n",
    "sigma_eps = 2.0   # error std dev\n",
    "alpha_corr = 2.0  # correlation strength: alpha_i = alpha_corr * x_mean_i + u_i\n",
    "sigma_u = 1.5     # std dev of the uncorrelated part of alpha\n",
    "censoring_point = 0.0\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Generate data\n",
    "sim_data = []\n",
    "\n",
    "for i in range(N_sim):\n",
    "    # Generate covariates\n",
    "    x1_i = np.random.normal(3.0, 1.5, size=T_sim)\n",
    "    x2_i = np.random.normal(1.0, 0.5, size=T_sim)\n",
    "    \n",
    "    # Individual mean of x1 (used to create correlated alpha)\n",
    "    x1_mean_i = x1_i.mean()\n",
    "    \n",
    "    # Correlated individual effect: alpha_i depends on x1_mean\n",
    "    u_i = np.random.normal(0, sigma_u)\n",
    "    alpha_i = alpha_corr * x1_mean_i + u_i\n",
    "    \n",
    "    # Generate outcomes\n",
    "    for t in range(T_sim):\n",
    "        eps_it = np.random.normal(0, sigma_eps)\n",
    "        y_star = beta_true[0] * x1_i[t] + beta_true[1] * x2_i[t] + alpha_i + eps_it\n",
    "        y_obs = max(censoring_point, y_star)  # left-censoring\n",
    "        \n",
    "        sim_data.append({\n",
    "            'id': i,\n",
    "            'time': t + 1,\n",
    "            'y': y_obs,\n",
    "            'x1': x1_i[t],\n",
    "            'x2': x2_i[t],\n",
    "            'alpha_true': alpha_i\n",
    "        })\n",
    "\n",
    "df_sim = pd.DataFrame(sim_data)\n",
    "\n",
    "print('Simulated Data (Correlated Fixed Effects DGP)')\n",
    "print('=' * 60)\n",
    "print(f'True beta = {beta_true}')\n",
    "print(f'Correlation: alpha_i = {alpha_corr} * mean(x1_i) + u_i')\n",
    "print(f'sigma_eps = {sigma_eps}, sigma_u = {sigma_u}')\n",
    "print(f'N = {N_sim}, T = {T_sim}')\n",
    "print(f'Censoring rate: {(df_sim[\"y\"] == censoring_point).mean() * 100:.1f}%')\n",
    "print(f'Mean alpha: {df_sim.groupby(\"id\")[\"alpha_true\"].first().mean():.2f}')\n",
    "print()\n",
    "\n",
    "# Verify correlation between alpha and X\n",
    "entity_means = df_sim.groupby('id').agg(\n",
    "    x1_mean=('x1', 'mean'),\n",
    "    alpha=('alpha_true', 'first')\n",
    ")\n",
    "corr_alpha_x = entity_means['x1_mean'].corr(entity_means['alpha'])\n",
    "print(f'Corr(alpha_i, mean(x1_i)) = {corr_alpha_x:.3f}')\n",
    "print('This high correlation means the RE assumption is VIOLATED.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-estimation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Estimate RE Tobit and Honoré on simulated data\n",
    "# ============================================================\n",
    "\n",
    "y_sim = df_sim['y'].values\n",
    "X_sim = df_sim[['x1', 'x2']].values\n",
    "X_sim_re = sm.add_constant(X_sim)\n",
    "groups_sim = df_sim['id'].values\n",
    "time_sim = df_sim['time'].values\n",
    "\n",
    "# --- RE Tobit ---\n",
    "print('Estimating RE Tobit on simulated data...')\n",
    "print('=' * 60)\n",
    "\n",
    "re_sim_beta = [np.nan, np.nan]\n",
    "re_sim_se = [np.nan, np.nan]\n",
    "try:\n",
    "    re_sim = RandomEffectsTobit(\n",
    "        endog=y_sim, exog=X_sim_re,\n",
    "        groups=groups_sim, time=time_sim,\n",
    "        censoring_point=censoring_point,\n",
    "        censoring_type='left',\n",
    "        quadrature_points=12\n",
    "    )\n",
    "    re_sim.fit(method='BFGS', maxiter=1000)\n",
    "    re_sim_beta = re_sim.beta[1:3]  # skip constant\n",
    "    re_sim_se = re_sim.bse[1:3]\n",
    "    print(f'  x1 coef: {re_sim_beta[0]:.4f} (SE: {re_sim_se[0]:.4f})')\n",
    "    print(f'  x2 coef: {re_sim_beta[1]:.4f} (SE: {re_sim_se[1]:.4f})')\n",
    "    re_sim_ok = True\n",
    "except Exception as e:\n",
    "    print(f'RE Tobit failed: {e}')\n",
    "    re_sim_ok = False\n",
    "\n",
    "# --- Honoré ---\n",
    "# Use subsample for computational feasibility\n",
    "print(f'\\nEstimating Honoré on simulated data (N={N_sim})...')\n",
    "print('=' * 60)\n",
    "\n",
    "honore_sim_beta = [np.nan, np.nan]\n",
    "try:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        honore_sim = HonoreTrimmedEstimator(\n",
    "            endog=y_sim, exog=X_sim,\n",
    "            groups=groups_sim, time=time_sim,\n",
    "            censoring_point=censoring_point\n",
    "        )\n",
    "        honore_sim_result = honore_sim.fit(verbose=False)\n",
    "    \n",
    "    honore_sim_beta = honore_sim_result.params\n",
    "    print(f'  x1 coef: {honore_sim_beta[0]:.4f}')\n",
    "    print(f'  x2 coef: {honore_sim_beta[1]:.4f}')\n",
    "    print(f'  Converged: {honore_sim_result.converged}')\n",
    "    print(f'  Trimmed pairs: {honore_sim_result.n_trimmed}')\n",
    "    honore_sim_ok = True\n",
    "except Exception as e:\n",
    "    print(f'Honoré failed: {e}')\n",
    "    honore_sim_ok = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Compare: True beta vs RE Tobit vs Honoré\n",
    "# ============================================================\n",
    "\n",
    "var_names_sim = ['x1', 'x2']\n",
    "\n",
    "sim_comparison = pd.DataFrame({\n",
    "    'Variable': var_names_sim,\n",
    "    'True beta': beta_true,\n",
    "    'RE Tobit': re_sim_beta,\n",
    "    'Honoré TLAD': honore_sim_beta\n",
    "})\n",
    "\n",
    "sim_comparison['RE Bias'] = sim_comparison['RE Tobit'] - sim_comparison['True beta']\n",
    "sim_comparison['RE Bias (%)'] = 100 * sim_comparison['RE Bias'] / sim_comparison['True beta'].abs()\n",
    "sim_comparison['Honoré Bias'] = sim_comparison['Honoré TLAD'] - sim_comparison['True beta']\n",
    "sim_comparison['Honoré Bias (%)'] = 100 * sim_comparison['Honoré Bias'] / sim_comparison['True beta'].abs()\n",
    "\n",
    "print('Simulated DGP: Comparison of Estimators')\n",
    "print('=' * 80)\n",
    "print(f'DGP: alpha_i = {alpha_corr} * mean(x1_i) + u_i')\n",
    "print(f'RE assumption VIOLATED: Corr(alpha, x1_mean) = {corr_alpha_x:.3f}')\n",
    "print()\n",
    "display(sim_comparison.round(4))\n",
    "\n",
    "print('\\nInterpretation:')\n",
    "print('-' * 60)\n",
    "print('  RE Tobit is BIASED because it ignores the correlation between')\n",
    "print('  alpha_i and x1. The RE estimate of beta_x1 conflates the direct')\n",
    "print('  effect of x1 and the indirect effect through alpha_i.')\n",
    "print()\n",
    "print('  Honoré eliminates alpha_i through pairwise differencing and')\n",
    "print('  should recover estimates closer to the true beta values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-barplot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization: True beta vs RE Tobit vs Honoré\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Bar chart of coefficient estimates\n",
    "ax = axes[0]\n",
    "x_pos = np.arange(len(var_names_sim))\n",
    "width = 0.25\n",
    "\n",
    "bars_true = ax.bar(x_pos - width, beta_true, width,\n",
    "                    label='True $\\\\beta$', color='#009E73', alpha=0.9, edgecolor='black')\n",
    "bars_re = ax.bar(x_pos, re_sim_beta, width,\n",
    "                  label='RE Tobit', color='#D55E00', alpha=0.8, edgecolor='black')\n",
    "bars_hon = ax.bar(x_pos + width, honore_sim_beta, width,\n",
    "                   label='Honoré TLAD', color='steelblue', alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Add RE Tobit error bars if available\n",
    "if re_sim_ok:\n",
    "    ax.errorbar(x_pos, re_sim_beta, yerr=1.96 * np.array(re_sim_se),\n",
    "                fmt='none', color='black', capsize=4, capthick=1.5)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(var_names_sim, fontsize=12)\n",
    "ax.set_ylabel('Coefficient Estimate', fontsize=12)\n",
    "ax.set_title('True $\\\\beta$ vs Estimates\\n(Correlated Effects DGP)', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.axhline(y=0, color='gray', linewidth=0.5)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Right: Bias comparison\n",
    "ax = axes[1]\n",
    "re_bias_pct = sim_comparison['RE Bias (%)'].values\n",
    "hon_bias_pct = sim_comparison['Honoré Bias (%)'].values\n",
    "\n",
    "x_pos2 = np.arange(len(var_names_sim))\n",
    "width2 = 0.35\n",
    "\n",
    "bars_re_b = ax.bar(x_pos2 - width2 / 2, re_bias_pct, width2,\n",
    "                    label='RE Tobit Bias', color='#D55E00', alpha=0.8, edgecolor='black')\n",
    "bars_hon_b = ax.bar(x_pos2 + width2 / 2, hon_bias_pct, width2,\n",
    "                     label='Honoré Bias', color='steelblue', alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Annotate bias values\n",
    "for i in range(len(var_names_sim)):\n",
    "    val_re = re_bias_pct[i]\n",
    "    val_hon = hon_bias_pct[i]\n",
    "    if not np.isnan(val_re):\n",
    "        offset_re = 2 if val_re >= 0 else -4\n",
    "        ax.text(i - width2 / 2, val_re + offset_re, f'{val_re:.1f}%',\n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "    if not np.isnan(val_hon):\n",
    "        offset_hon = 2 if val_hon >= 0 else -4\n",
    "        ax.text(i + width2 / 2, val_hon + offset_hon, f'{val_hon:.1f}%',\n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.axhline(y=0, color='black', linewidth=1)\n",
    "ax.set_xticks(x_pos2)\n",
    "ax.set_xticklabels(var_names_sim, fontsize=12)\n",
    "ax.set_ylabel('Relative Bias (%)', fontsize=12)\n",
    "ax.set_title('Bias Comparison\\n(Positive = Overestimate)', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex4_simulated_bias_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save comparison table\n",
    "sim_comparison.to_csv(TABLES_DIR / 'ex4_simulated_comparison.csv', index=False)\n",
    "print(f'Saved to {TABLES_DIR / \"ex4_simulated_comparison.csv\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Additional: Visualize the correlated effects\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.scatter(entity_means['x1_mean'], entity_means['alpha'],\n",
    "           alpha=0.6, s=40, color='steelblue', edgecolor='gray', linewidth=0.5)\n",
    "\n",
    "# Fit and plot regression line\n",
    "slope, intercept, r_value, p_value, se = stats.linregress(\n",
    "    entity_means['x1_mean'], entity_means['alpha']\n",
    ")\n",
    "x_line = np.linspace(entity_means['x1_mean'].min(), entity_means['x1_mean'].max(), 100)\n",
    "ax.plot(x_line, slope * x_line + intercept, 'r-', linewidth=2,\n",
    "        label=f'Fitted: slope={slope:.2f} (true={alpha_corr:.1f})\\nr={r_value:.3f}')\n",
    "\n",
    "ax.set_xlabel('Mean $x_{1i}$ (individual mean of covariate)', fontsize=12)\n",
    "ax.set_ylabel('$\\\\alpha_i$ (individual fixed effect)', fontsize=12)\n",
    "ax.set_title('Correlated Effects: $\\\\alpha_i$ vs Mean($x_{1i}$)\\n'\n",
    "             f'DGP: $\\\\alpha_i = {alpha_corr} \\\\cdot \\\\bar{{x}}_{{1i}} + u_i$', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'ex4_correlated_effects.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('The strong positive correlation between alpha_i and x1_mean')\n",
    "print('violates the RE assumption and biases the RE Tobit upward for x1.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex4-bias-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Explain the direction of bias\n",
    "# ============================================================\n",
    "\n",
    "print('Bias Direction Analysis')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print('DGP: y*_it = beta1 * x1_it + beta2 * x2_it + alpha_i + eps_it')\n",
    "print(f'     alpha_i = {alpha_corr} * mean(x1_i) + u_i')\n",
    "print()\n",
    "print('The RE Tobit ignores the correlation Corr(alpha_i, x1_it) > 0.')\n",
    "print('When alpha_i is positively correlated with x1:')\n",
    "print('  - High x1 entities also have high alpha_i')\n",
    "print('  - The RE model attributes part of alpha_i\\'s effect to x1')\n",
    "print('  - Result: RE overestimates beta_x1 (positive bias)')\n",
    "print()\n",
    "if re_sim_ok:\n",
    "    print(f'Observed: RE beta_x1 = {re_sim_beta[0]:.4f} vs True = {beta_true[0]:.4f}')\n",
    "    bias_direction = 'UPWARD' if re_sim_beta[0] > beta_true[0] else 'DOWNWARD'\n",
    "    print(f'  --> {bias_direction} bias, as expected.')\n",
    "print()\n",
    "print('The Honoré estimator eliminates alpha_i through differencing,')\n",
    "print('so it should be unaffected by this correlation.')\n",
    "if honore_sim_ok:\n",
    "    print(f'Observed: Honoré beta_x1 = {honore_sim_beta[0]:.4f} vs True = {beta_true[0]:.4f}')\n",
    "    hon_bias = abs(honore_sim_beta[0] - beta_true[0])\n",
    "    re_bias = abs(re_sim_beta[0] - beta_true[0]) if re_sim_ok else np.nan\n",
    "    print(f'  --> Honoré |bias| = {hon_bias:.4f} vs RE |bias| = {re_bias:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex4-discussion",
   "metadata": {},
   "source": [
    "### Exercise 4: Discussion\n",
    "\n",
    "**Key findings from the simulation:**\n",
    "\n",
    "1. **RE Tobit bias**: The RE Tobit systematically overestimates $\\beta_{x_1}$\n",
    "   because it cannot separate the direct effect of $x_1$ from its indirect\n",
    "   effect through $\\alpha_i$ (which is positively correlated with $x_1$).\n",
    "   The bias is substantial and in the expected direction (upward for x1).\n",
    "\n",
    "2. **Honoré consistency**: The Honoré estimator eliminates $\\alpha_i$ through\n",
    "   pairwise differencing and produces estimates much closer to the true $\\beta$.\n",
    "   Any remaining difference is due to finite-sample variability, not systematic\n",
    "   bias.\n",
    "\n",
    "3. **x2 coefficient**: Since $\\alpha_i$ is correlated with $x_1$ but not with\n",
    "   $x_2$, the RE bias for $x_2$ should be smaller. This is a useful diagnostic:\n",
    "   if RE and Honoré agree on $x_2$ but not on $x_1$, it suggests that the RE\n",
    "   violation specifically involves $x_1$.\n",
    "\n",
    "4. **Policy implication**: In empirical work, we rarely know whether $\\alpha_i$\n",
    "   is correlated with covariates. The Honoré estimator provides a valuable\n",
    "   robustness check. If Honoré and RE Tobit estimates differ substantially,\n",
    "   the RE assumption is suspect and the Honoré results should be preferred\n",
    "   (or at least reported alongside the RE results).\n",
    "\n",
    "5. **Cost of robustness**: The Honoré estimator is less efficient (higher\n",
    "   variance) and does not provide standard errors without bootstrap. This\n",
    "   is the price of relaxing the RE assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this solution notebook, we worked through four exercises that deepened our\n",
    "understanding of the Honoré (1992) Trimmed LAD estimator:\n",
    "\n",
    "| Exercise | Key Takeaway |\n",
    "|----------|-------------|\n",
    "| 1. Trimming Rate | Higher censoring thresholds dramatically increase trimming; above ~80% trimmed, estimates are unreliable |\n",
    "| 2. Robustness | Honoré vs RE Tobit discrepancy should be assessed across multiple specifications |\n",
    "| 3. Panel Length | Pairwise information grows quadratically with T; T=3 is marginal, T=5+ recommended |\n",
    "| 4. Simulated Bias | When RE assumption fails, RE Tobit is biased; Honoré recovers true parameters |\n",
    "\n",
    "### Key Methods Used\n",
    "\n",
    "```python\n",
    "# Honoré Trimmed LAD (NO constant in exog)\n",
    "from panelbox.models.censored import HonoreTrimmedEstimator\n",
    "model = HonoreTrimmedEstimator(endog=y, exog=X, groups=g, time=t, censoring_point=0.0)\n",
    "result = model.fit(verbose=False)\n",
    "\n",
    "# RE Tobit (WITH constant in exog)\n",
    "from panelbox.models.censored import RandomEffectsTobit\n",
    "model = RandomEffectsTobit(endog=y, exog=X_const, groups=g, time=t,\n",
    "                           censoring_point=0.0, censoring_type='left', quadrature_points=12)\n",
    "model.fit(method='BFGS', maxiter=1000)\n",
    "```\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "1. **Start with RE Tobit** for speed and standard errors\n",
    "2. **Run Honoré as a robustness check** if you suspect correlated effects\n",
    "3. **Compare coefficients**: if similar, RE assumption is plausible\n",
    "4. **If they disagree**: report both and discuss which assumptions are more credible\n",
    "5. **Check trimming rate**: if >70-80%, Honoré may be unreliable\n",
    "6. **Ensure T >= 3**: T=2 is insufficient for pairwise estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
